@inproceedings{....2017,
  title = {History of Uniform Random Number Generation},
  booktitle = {Proceedings - Winter Simulation Conference},
  author = {L'Ecuyer, Pierre},
  year = {2018},
  volume = {4},
  number = {1938},
  pages = {202--230},
  issn = {08917736},
  doi = {10.1109/WSC.2017.8247790},
  abstract = {Random number generators were invented before there were symbols for writing numbers, and long before mechanical and electronic computers. All major civilizations through the ages found the urge to make random selections, for various reasons. Today, random number generators, particularly on computers, are an important (although often hidden) ingredient in human activity. In this article, we give a historical account on the design, implementation, and testing of uniform random number generators used for simulation.},
  isbn = {978-1-5386-3428-8},
  keywords = {nosource}
}

@article{10.1093/bioinformatics/btv193,
  title = {{{EBSeq-HMM}}: A {{Bayesian}} Approach for Identifying Gene-Expression Changes in Ordered {{RNA-seq}} Experiments},
  author = {Leng, Ning and Li, Yuan and McIntosh, Brian E. and Nguyen, Bao Kim and Duffin, Bret and Tian, Shulan and Thomson, James A. and Dewey, Colin N. and Stewart, Ron and Kendziorski, Christina},
  year = {2015},
  month = apr,
  journal = {Bioinformatics (Oxford, England)},
  volume = {31},
  number = {16},
  eprint = {https://academic.oup.com/bioinformatics/article-pdf/31/16/2614/17084838/btv193.pdf},
  pages = {2614--2622},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btv193},
  abstract = {Motivation: With improvements in next-generation sequencing technologies and reductions in price, ordered RNA-seq experiments are becoming common. Of primary interest in these experiments is identifying genes that are changing over time or space, for example, and then characterizing the specific expression changes. A number of robust statistical methods are available to identify genes showing differential expression among multiple conditions, but most assume conditions are exchangeable and thereby sacrifice power and precision when applied to ordered data.Results: We propose an empirical Bayes mixture modeling approach called EBSeq-HMM. In EBSeq-HMM, an auto-regressive hidden Markov model is implemented to accommodate dependence in gene expression across ordered conditions. As demonstrated in simulation and case studies, the output proves useful in identifying differentially expressed genes and in specifying gene-specific expression paths. EBSeq-HMM may also be used for inference regarding isoform expression.Availability and implementation: An R package containing examples and sample datasets is available at Bioconductor.Contact:kendzior@biostat.wisc.eduSupplementary information:Supplementary data are available at Bioinformatics online.}
}

@article{10.1093/sysbio/syy005,
  title = {Inference of Adaptive Shifts for Multivariate Correlated Traits},
  author = {Robin, St{\'e}phane and Bastide, Paul and Mariadassou, Mahendra and An{\'e}, C{\'e}cile},
  year = {2018},
  journal = {Systematic Biology},
  volume = {67},
  number = {4},
  pages = {662--680},
  issn = {1063-5157},
  doi = {10.1093/sysbio/syy005},
  abstract = {To study the evolution of several quantitative traits, the classical phylogenetic comparative framework consists of a multivariate random process running along the branches of a phylogenetic tree. The Ornstein--Uhlenbeck (OU) process is sometimes preferred to the simple Brownian motion (BM) as it models stabilizing selection toward an optimum. The optimum for each trait is likely to be changing over the long periods of time spanned by large modern phylogenies. Our goal is to automatically detect the position of these shifts on a phylogenetic tree, while accounting for correlations between traits, which might exist because of structural or evolutionary constraints. We show that, in the presence of shifts, phylogenetic Principal Component Analysis fails to decorrelate traits efficiently, so that any method aiming at finding shifts needs to deal with correlation simultaneously. We introduce here a simplification of the full multivariate OU model, named scalar OU, which allows for noncausal correlations and is still computationally tractable. We extend the equivalence between the OU and a BM on a rescaled tree to our multivariate framework. We describe an Expectation--Maximization (EM) algorithm that allows for a maximum likelihood estimation of the shift positions, associated with a new model selection criterion, accounting for the identifiability issues for the shift localization on the tree. The method, freely available as an R-package (PhylogeneticEM) is fast, and can deal with missing values. We demonstrate its efficiency and accuracy compared to another state-of-the-art method ({\textbackslash}{\textbackslash}\${\textbackslash}{\textbackslash}{\textbackslash}\$1ou) on a wide range of simulated scenarios and use this new framework to reanalyze recently gathered data sets on New World Monkeys and Anolis lizards.},
  keywords = {nosource}
}

@article{10.1214/19-AOS1914,
  title = {Controlled Sequential Monte Carlo},
  author = {Heng, Jeremy and Bishop, Adrian N. and Deligiannidis, George and Doucet, Arnaud},
  year = {2020},
  journal = {The Annals of Statistics},
  volume = {48},
  number = {5},
  pages = {2904--2929},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/19-AOS1914},
  keywords = {annealed importance sampling,approximate dynamic programming,Normalizing constants,optimal control,reinforcement learning,state space models},
  file = {/home/gkonkamking/pCloudDrive/papers/Heng et al_2020_Controlled sequential monte carlo.pdf}
}

@article{10.2307/24308851,
  title = {Bayesian Nonparametric Construction of the {{Fleming-Viot}} Process with Fertility Selection},
  author = {Ruggiero, Matteo and Walker, Stephen G},
  year = {2009},
  journal = {Statistica Sinica},
  volume = {19},
  number = {2},
  pages = {707--720},
  publisher = {Institute of Statistical Science, Academia Sinica},
  issn = {10170405},
  abstract = {This paper provides a construction in the Bayesian framework of the Fleming-Viot measure-valued diffusion with diploid fertility selection, and highlights new connections between Bayesian nonparametrics and population genetics. Via a generalisation of the Blackwell-MacQueen P{\'o}lya-urn scheme, a Markov particle process is defined such that the associated process of empirical measures converges to the Fleming-Viot diffusion. The stationary distribution, known from Ethier and Kurtz (1994), is then derived through an application of the Dirichlet process mixture model and shown to be the de Finetti measure of the particle process. The Fleming-Viot process with haploid selection is derived as a special case.},
  keywords = {nosource}
}

@article{1742-5468-2014-10-P10016,
  title = {The Patient-Zero Problem with Noisy Observations},
  author = {Altarelli, Fabrizio and Braunstein, Alfredo and Dall'Asta, Luca and Ingrosso, Alessandro and Zecchina, Riccardo},
  year = {2014},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2014},
  number = {10},
  pages = {P10016},
  abstract = {A belief propagation approach has been recently proposed for the patient-zero problem in SIR epidemics. The patient-zero problem consists of finding the initial source of an epidemic outbreak given observations at a later time. In this work, we study a more difficult but related inference problem, in which observations are noisy and there is confusion between observed states. In addition to studying the patient-zero problem, we also tackle the problem of completing and correcting the observations to possibly find undiscovered infected individuals and false test results. Moreover, we devise a set of equations, based on the variational expression of the Bethe free energy, to find the patient-zero along with maximum-likelihood epidemic parameters. We show, by means of simulated epidemics, that this method is able to infer details on the past history of an epidemic outbreak based solely on the topology of the contact network and a single snapshot of partial and noisy observations.},
  keywords = {nosource}
}

@article{AA,
  title = {Adaptive {{Bayesian}} Multivariate Density Estimation with {{Dirichlet}} Mixtures},
  author = {Shen, Weining and Ghosal, Subhashis},
  year = {2011},
  journal = {arXiv preprint arXiv:1109.6406},
  eprint = {1109.6406},
  archiveprefix = {arXiv},
  keywords = {duplicate-citation-key,nosource}
}

@article{AA,
  title = {Adaptive {{Bayesian}} Multivariate Density Estimation with {{Dirichlet}} Mixtures},
  author = {Shen, Weining and Tokdar, Surya T. and Ghosal, Subhashis},
  year = {2013},
  journal = {Biometrika},
  volume = {100},
  number = {3},
  eprint = {1109.6406v3},
  pages = {623--640},
  issn = {00063444},
  doi = {10.1093/biomet/ast015},
  abstract = {We show that rate-adaptive multivariate density estimation can be performed using Bayesian methods based on Dirichlet mixtures of normal kernels with a prior distribution on the kernel's covariance matrix parameter. We derive sufficient conditions on the prior specification that guarantee convergence to a true density at a rate that is optimal minimax for the smoothness class to which the true density belongs. No prior knowledge of smoothness is assumed. The sufficient conditions are shown to hold for the Dirichlet location mixture of normals prior with a Gaussian base measure and an inverse-Wishart prior on the covariance matrix parameter. Locally H{\textbackslash}"older smoothness classes and their anisotropic extensions are considered. Our study involves several technical novelties, including sharp approximation of finitely differentiable multivariate densities by normal mixtures and a new sieve on the space of such densities.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1109.6406v3},
  keywords = {Anisotropy,Dirichlet mixture,duplicate-citation-key,Multivariate density estimation,Nonparametric Bayesian method,nosource,Rate adaptation}
}

@article{aagaard2013guidance,
  title = {Guidance on Tiered Risk Assessment for Plant Protection Products for Aquatic Organisms in Edge-of-Field Surface Waters},
  author = {{Efsa}},
  year = {2013},
  journal = {EFSA Journal},
  volume = {11},
  number = {7},
  pages = {1--268},
  doi = {10.2903/j.efsa.2013.3290},
  abstract = {EFSA's Panel on Plant Protection Products and their Residues (PPR) was tasked to revise the Guidance Document (GD) on Aquatic Ecotoxicology under Council Directive 91/414/EEC (SANCO/3268/2001 rev.4 (final), 17 October 2002). This Guidance of the PPR Panel is the first of three requested deliverables within this mandate. It has its focus on tiered acute and chronic effect assessment schemes with detailed guidance on tier 1 and higher tier effect assessments for aquatic organisms in edge-of-field surface waters and on proposals regarding how to link effects to exposure estimates. The exposure assessment methodology was not reviewed and it is assumed that the current FOCUS surface water exposure assessment methodology will continue to be used for exposure assessment at EU level. The current GD is intended to be used for authorisation of active substances at EU level as well as for plant protection products at Member State level. The effect assessment schemes in this GD allow for the derivation of regulatory acceptable concentrations (RACs) on the basis of two options: (1) the ecological threshold option (ETO), accepting negligible population effects only, and (2) the ecological recovery option (ERO), accepting some population-level effects if ecological recovery takes place within an acceptable time period. In the tiered effect assessment schemes, in principle, all tiers (1, 2 and 3) are able to address the ETO, while the model ecosystem approach (tier 3), under certain conditions, is able to also address the ERO. The GD provides the scientific background for the risk assessment to aquatic organisms in edge-of-field surface waters and is structured to give detailed guidance on all assessment steps. An executive summary joining all parts of the guidance and decision schemes in a concise way is provided and is intended to help applicants and regulatory authorities in day-to-day use.{\textbackslash}n{\textbackslash}n{\copyright} European Food Safety Authority, 2013{\textbackslash}nSummary{\textbackslash}n{\textbackslash}nThe European Food Safety Authority (EFSA) asked the Panel on Plant Protection Products and their Residues (PPR) to prepare a revision of the Guidance Document on Aquatic Ecotoxicology under Council Directive 91/414/EEC (SANCO/3268/2001 rev.4 (final), 17 October 2002; EC, 2002a). The PPR Panel was therefore tasked to prepare a revised Guidance Document and two Scientific Opinions. This Guidance of the PPR Panel is the first of these three requested deliverables as outlined in the Terms of Reference below. The revision of the former Guidance Document on Aquatic Ecotoxicology became necessary mainly due to (1) the entry in to force of the new Regulation (EC) No 1107/2009 on authorisation of plant protection products, (2) the revision of the related data requirements and (3) scientific developments. Stakeholders were consulted before the start of the revision process in a public consultation, as well as risk managers in a specific consultation, in October to December 2008. The revision of the Guidance Document on Aquatic Ecotoxicology was started in parallel to the revision of the Guidance Document on Terrestrial Ecotoxicology (SANCO/10329/2002, rev.2 final, 17.10.2002; EC, 2002b) to allow a harmonisation process. As a first step, the PPR Panel developed a framework for deriving specific protection goals (EFSA PPR Panel, 2010a). The approach outlined in that opinion was the starting point for the development of this updated Guidance Document on aquatic risk assessment (RA).{\textbackslash}n{\textbackslash}nThe aquatic RA is the combination of the exposure and the effect assessments and there is considerable interaction between these assessments. The focus of this Guidance Document (GD) is on a tiered effect assessment scheme with detailed guidance on tier 1 and higher tier effect assessments that are mainly based on experimental approaches (chapters 7--10). A scientific opinion on the state of the art of mechanistic effect modelling in the aquatic environment (e.g. toxicokinetic/toxicodynamic and population models) will be delivered later under this mandate. The effect assessment guidance is intended to be used for authorisation of active substances (a.s.) at EU level as well as for plant protection products at Member State level. Furthermore, the appropriate linking between exposure and effect assessment is described. The exposure assessment methodology was not reviewed and it is assumed that the current FOCUS surface water exposure assessment methodology will continue to be used for exposure assessment at EU level. Only a brief overview of the exposure assessment is included in this GD in chapter 6, for details reference is made to the related FOCUS surface water guidance (FOCUS, 2001).{\textbackslash}n{\textbackslash}nThe GD first describes the specific protection goals for aquatic organisms that need to be defined in order to develop an appropriate RA scheme. Proposed specific protection goals (SPGs) were discussed with risk managers in September to November 2012 and are described in chapter 5. The SPGs overall aim is to protect aquatic plants and animals at the population level in surface water. However, the SPG selected for aquatic vertebrates aims at protection at the individual level, so that mortality and suffering due to acute toxicity is avoided. As outlined in the PPR Panel opinion on SPGs (EFSA PPR Panel, 2010a), the exposure assessment goals also have to be defined in parallel to set the overall level of protection. Since the exposure assessment methodology was not revised in parallel to the effect assessment scheme, definitions for exposure assessment goals are not clear.{\textbackslash}n{\textbackslash}nIn this GD, the tiered effect assessment procedure and proposals on how to link effects to exposure estimates are focused on aquatic organisms living in the water column of edge-of-field surface waters. For these organisms, the concentration of the freely dissolved chemical is chosen as the ecotoxicologically relevant concentration (ERC). This GD also presents the tier 1 effect assessment procedure for sediment-dwelling organisms when based on water-sediment toxicity tests. More information for sediment-dwelling organisms will be provided in an opinion on the effect assessment for plant protection products on sediment organisms in edge-of-field surface water to be delivered next under this mandate.{\textbackslash}n{\textbackslash}nTo protect populations of aquatic organisms, effect assessment schemes are developed that allow for the derivation of regulatory acceptable concentrations (RACs) on the basis of two options: (1) the ecological threshold option (ETO), accepting negligible population effects only, and (2) the ecological recovery option (ERO), accepting some population-level effects if ecological recovery takes place within an acceptable time period. In the tiered acute and chronic effect assessment schemes, in principle, all tiers (1, 2 and 3) are able to address the ETO, while the model ecosystem approach (tier 3), under certain conditions, is able to also address the ERO. The ETO from tier 3 is particularly relevant as it is more likely to assure an adequate level of protection, not only for the application of a single plant protection product (PPP), but also in view of the application of (non-approved) tank-mixtures and serial PPP applications during the growing season. It thus may better address issues of the `uniform principles' as laid down in Regulation (EC) No 546/2011 that requires that Member States base their authorisation decision on the `proposed conditions for the use of the plant protection product' and furthermore the standard data requirements for PPP do request: `any information on potentially unacceptable effects of the plant protection product on the environment, on plants and plant products shall be included as well as known and expected cumulative and synergistic effects'.{\textbackslash}n{\textbackslash}nThe GD is structured to give detailed guidance and provide relevant scientific background information for each tier in the respective Chapters, which all end with a section on how to derive RACs and how to perform the RA, including decision schemes. Chapter 7 describes the tier 1 effect assessment based on the revised data requirements. Chapter 8 addresses refinement options based on additional species tested, that is, the Geomean approach and the species sensitivity distribution approach. Chapter 9 addresses higher tier options based on refined exposure laboratory and model ecosystem approaches. This includes guidance on selecting the appropriate refined exposure profiles, on refined exposure laboratory tests, and on designing and evaluating model ecosystem (micro-/mesocosm) studies. Chapter 10 contains detailed guidance on the possible use of non-testing methods, effect assessment for metabolites, and assessment for formulations containing more than one a.s.. Chapter 11 addresses other relevant related issues. Chapter 12 provides guidance on addressing the uncertainties in the assessment.{\textbackslash}n{\textbackslash}nChapter 2 provides an executive summary that joins all guidance and decision schemes in a concise way without the detailed scientific background. This is intended to be helpful to applicants and regulatory authorities providing an overview for day-to-day use.{\textbackslash}n{\textbackslash}nThe guidance was developed based on experience with currently known or approved a.s. and plant protection products. When using this guidance, it should be always checked whether the proposed schemes are appropriate for a.s. with a new mode of action.},
  keywords = {nosource}
}

@techreport{abadieWhenShouldYou2017,
  type = {Working {{Paper}}},
  title = {When {{Should You Adjust Standard Errors}} for {{Clustering}}?},
  author = {Abadie, Alberto and Athey, Susan and Imbens, Guido W. and Wooldridge, Jeffrey},
  year = {2017},
  month = nov,
  series = {Working {{Paper Series}}},
  number = {24003},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w24003},
  urldate = {2021-08-06},
  abstract = {In empirical work in economics it is common to report standard errors that account for clustering of units. Typically, the motivation given for the clustering adjustments is that unobserved components in outcomes for units within clusters are correlated. However, because correlation may occur across more than one dimension, this motivation makes it difficult to justify why researchers use clustering in some dimensions, such as geographic, but not others, such as age cohorts or gender. This motivation also makes it difficult to explain why one should not cluster with data from a randomized experiment. In this paper, we argue that clustering is in essence a design problem, either a sampling design or an experimental design issue. It is a sampling design issue if sampling follows a two stage process where in the first stage, a subset of clusters were sampled randomly from a population of clusters, and in the second stage, units were sampled randomly from the sampled clusters. In this case the clustering adjustment is justified by the fact that there are clusters in the population that we do not see in the sample. Clustering is an experimental design issue if the assignment is correlated within the clusters. We take the view that this second perspective best fits the typical setting in economics where clustering adjustments are used. This perspective allows us to shed new light on three questions: (i) when should one adjust the standard errors for clustering, (ii) when is the conventional adjustment for clustering appropriate, and (iii) when does the conventional adjustment of the standard errors matter.},
  file = {/home/gkonkamking/Zotero/storage/J52H4DN5/Abadie et al. - 2017 - When Should You Adjust Standard Errors for Cluster.pdf}
}

@article{abelerReferencePointsEffort2011,
  title = {Reference {{Points}} and {{Effort Provision}}},
  author = {Abeler, Johannes and Falk, Armin and Goette, Lorenz and Huffman, David},
  year = {2011},
  month = apr,
  journal = {American Economic Review},
  volume = {101},
  number = {2},
  pages = {470--492},
  issn = {0002-8282},
  doi = {10.1257/aer.101.2.470},
  urldate = {2020-03-05},
  langid = {english}
}

@article{abramovich_optimality_2004,
  title = {On Optimality of {{Bayesian}} Wavelet Estimators},
  author = {Abramovich, Felix},
  year = {2004},
  journal = {Scandinavian Journal of {\dots}},
  volume = {31},
  number = {1999},
  pages = {217--234},
  abstract = {2006) explored the minimaxity of the mean estimator for the global and risks are the main results on their - rates and},
  keywords = {bayes factor,bayes model,besov spaces,estimation,minimax estimation,non-linear,non-parametric regression,nosource,posterior mean,posterior median,wavelets}
}

@article{Abramovich:2007p6014,
  title = {Pointwise Optimality of {{Bayesian}} Wavelet Estimators},
  author = {Abramovich, F and Angelini, C and De Canditiis, D},
  year = {2007},
  journal = {Ann. Inst. Statist. Math.},
  volume = {59},
  number = {3},
  pages = {425--434},
  abstract = {2006) explored the minimaxity of the mean estimator for the global and risks are the main results on their - rates and},
  keywords = {nosource}
}

@article{abramovich1998wavelet,
  title = {Wavelet Thresholding via a Bayesian Approach},
  author = {Abramovich, Felix and Sapatinas, Theofanis and Silverman, B. W.},
  year = {1998},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {60},
  number = {4},
  pages = {725--749},
  publisher = {Wiley Online Library},
  issn = {1369-7412},
  doi = {10.1111/1467-9868.00151},
  abstract = {We discuss a Bayesian formalism which gives rise to a type of wavelet threshold estimation in nonparametric regression. A prior distribution is imposed on the wavelet coefficients of the unknown response function, designed to capture the sparseness of wavelet expansion that is common to most applications. For the prior specified, the posterior median yields a thresholding procedure. Our prior model for the underlying function can be adjusted to give functions falling in any specific Besov space. We establish a relationship between the hyperparameters of the prior model and the parameters of those Besov spaces within which realizations from the prior will fall. Such a relationship gives insight into the meaning of the Besov space parameters. Moreover, the relationship established makes it possible in principle to incorporate prior knowledge about the function's regularity properties into the prior model for its wavelet coefficients. However, prior knowledge about a function's regularity properties might be difficult to elicit; with this in mind, we propose a standard choice of prior hyperparameters that works well in our examples. Several simulated examples are used to illustrate our method, and comparisons are made with other thresholding methods. We also present an application to a data set that was collected in an anaesthesiological study.},
  keywords = {adaptive estimation,anaesthetics,bayes model,besov spaces,nonparametric,nosource,regression,thresholding,wavelet transform}
}

@article{abramovich2007optimality,
  title = {On Optimality of Bayesian Testimation in the Normal Means Problem},
  author = {Abramovich, Felix and Grinshtein, Vadim and Pensky, Marianna},
  year = {2007},
  journal = {Annals of Statistics},
  volume = {35},
  number = {5},
  pages = {2261--2286},
  publisher = {Institute of Mathematical Statistics},
  issn = {00905364},
  doi = {10.1214/009053607000000226},
  keywords = {Adaptivity,Complexity penalty,Maximum a posteriori rule,Minimax estimation,nosource,Sequence estimation,Sparsity,Thresholding}
}

@article{abramovich2010bayesian,
  title = {On {{Bayesian}} Testimation and Its Application to Wavelet Thresholding},
  author = {Abramovich, Felix and Grinshtein, Vadim and Petsa, Athanasia and Sapatinas, Theofanis},
  year = {2010},
  journal = {Biometrika},
  volume = {97},
  number = {1},
  pages = {181--198},
  publisher = {Biometrika Trust},
  issn = {00063444},
  doi = {10.1093/biomet/asp080},
  abstract = {We consider the problem of estimating the unknown response function{\textbackslash}nin the Gaussian white noise model. We first utilize the recently{\textbackslash}ndeveloped Bayesian maximum a posteriori testimation procedure of{\textbackslash}nAbramovich et al. (2007) for recovering an unknown high-dimensional{\textbackslash}nGaussian mean vector. The existing results for its upper error bounds{\textbackslash}nover various sparse lp-balls are extended to more general cases.{\textbackslash}nWe show that, for a properly chosen prior on the number of nonzero{\textbackslash}nentries of the mean vector, the corresponding adaptive estimator{\textbackslash}nis asymptotically minimax in a wide range of sparse and dense lp-balls.{\textbackslash}nThe proposed procedure is then applied in a wavelet context to derive{\textbackslash}nadaptive global and level-wise wavelet estimators of the unknown{\textbackslash}nresponse function in the Gaussian white noise model. These estimators{\textbackslash}nare then proven to be, respectively, asymptotically near-minimax{\textbackslash}nand minimax in a wide range of Besov balls. These results are also{\textbackslash}nextended to the estimation of derivatives of the response function.{\textbackslash}nSimulated examples are conducted to illustrate the performance of{\textbackslash}nthe proposed level-wise wavelet estimator in finite sample situations,{\textbackslash}nand to compare it with several existing counterparts.},
  keywords = {Adaptive estimation,Besov space,Gaussian sequence model,Gaussian white noise model,Lp-ball,Multiple testing,nosource,Thresholding,Wavelet estimation}
}

@article{adhikariInferencePredictionOptimization2020,
  title = {Inference, Prediction and Optimization of Non-Pharmaceutical Interventions Using Compartment Models: The {{PyRoss}} Library},
  shorttitle = {Inference, Prediction and Optimization of Non-Pharmaceutical Interventions Using Compartment Models},
  author = {Adhikari, R. and Bolitho, Austen and Caballero, Fernando and Cates, Michael E. and Dolezal, Jakub and Ekeh, Timothy and Guioth, Jules and Jack, Robert L. and Kappler, Julian and Kikuchi, Lukas and Kobayashi, Hideki and Li, Yuting I. and Peterson, Joseph D. and Pietzonka, Patrick and Remez, Benjamin and Rohrbach, Paul B. and Singh, Rajesh and Turk, G{\"u}nther},
  year = {2020},
  month = may,
  journal = {arXiv:2005.09625 [physics, q-bio]},
  eprint = {2005.09625},
  primaryclass = {physics, q-bio},
  urldate = {2021-03-23},
  abstract = {PyRoss is an open-source Python library that offers an integrated platform for inference, prediction and optimisation of NPIs in age- and contact-structured epidemiological compartment models. This report outlines the rationale and functionality of the PyRoss library, with various illustrations and examples focusing on well-mixed, age-structured populations. The PyRoss library supports arbitrary structured models formulated stochastically (as master equations) or deterministically (as ODEs) and allows mid-run transitioning from one to the other. By supporting additional compartmental subdivision ad libitum, PyRoss can emulate time-since-infection models and allows medical stages such as hospitalization or quarantine to be modelled and forecast. The PyRoss library enables fitting to epidemiological data, as available, using Bayesian parameter inference, so that competing models can be weighed by their evidence. PyRoss allows fully Bayesian forecasts of the impact of idealized NPIs by convolving uncertainties arising from epidemiological data, model choice, parameters, and intrinsic stochasticity. Algorithms to optimize time-dependent NPI scenarios against user-defined cost functions are included. PyRoss's current age-structured compartment framework for well-mixed populations will in future reports be extended to include compartments structured by location, occupation, use of travel networks and other attributes relevant to assessing disease spread and the impact of NPIs. We argue that such compartment models, by allowing social data of arbitrary granularity to be combined with Bayesian parameter estimation for poorly-known disease variables, could enable more powerful and robust prediction than other approaches to detailed epidemic modelling. We invite others to use the PyRoss library for research to address today's COVID-19 crisis, and to plan for future pandemics.},
  archiveprefix = {arXiv},
  keywords = {nosource,Physics - Physics and Society,Quantitative Biology - Populations and Evolution,Quantitative Biology - Quantitative Methods}
}

@article{Aeschbacher2012,
  title = {A Novel Approach for Choosing Summary Statistics in Approximate {{Bayesian}} Computation},
  author = {Aeschbacher, Simon and Beaumont, Mark A. and Futschik, Andreas},
  year = {2012},
  journal = {Genetics},
  volume = {192},
  number = {3},
  pages = {1027--1047},
  issn = {00166731},
  doi = {10.1534/genetics.112.143164},
  abstract = {The choice of summary statistics is a crucial step in approximate Bayesian computation (ABC). Since statistics are often not sufficient, this choice involves a trade-off between loss of information and reduction of dimensionality. The latter may increase the efficiency of ABC. Here, we propose an approach for choosing summary statistics based on boosting, a technique from the machine-learning literature. We consider different types of boosting and compare them to partial least-squares regression as an alternative. To mitigate the lack of sufficiency, we also propose an approach for choosing summary statistics locally, in the putative neighborhood of the true parameter value. We study a demographic model motivated by the reintroduction of Alpine ibex (Capra ibex) into the Swiss Alps. The parameters of interest are the mean and standard deviation across microsatellites of the scaled ancestral mutation rate ({\texttheta}(anc) = 4N(e)u) and the proportion of males obtaining access to matings per breeding season ({$\omega$}). By simulation, we assess the properties of the posterior distribution obtained with the various methods. According to our criteria, ABC with summary statistics chosen locally via boosting with the L(2)-loss performs best. Applying that method to the ibex data, we estimate {\texttheta}(anc){$\approx$} 1.288 and find that most of the variation across loci of the ancestral mutation rate u is between 7.7 {\texttimes} 10(-4) and 3.5 {\texttimes} 10(-3) per locus per generation. The proportion of males with access to matings is estimated as {$\omega\approx$} 0.21, which is in good agreement with recent independent estimates.},
  isbn = {0016-6731},
  pmid = {22960215},
  keywords = {nosource}
}

@incollection{AHK16,
  title = {Symbolic Manipulation of Flows of Nonlinear Evolution Equations, with Application in the Analysis of Split-Step Time Integrators},
  booktitle = {Computer Algebra in Scientific Computing: 18th International Workshop, {{CASC}} 2016, Bucharest, Romania, September 19-23, 2016, Proceedings},
  author = {Auzinger, Winfried and Hofst{\"a}tter, Harald and Koch, Othmar},
  editor = {Gerdt, P Vladimir and Koepf, Wolfram and Seiler, M Werner and Vorozhtsov, V Evgenii},
  year = {2016},
  series = {Lecture Notes in Computer Science},
  volume = {9890},
  pages = {30--42},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-319-45641-6_4},
  abstract = {We describe a package realized in the Julia programming language which performs symbolic manipulations applied to nonlinear evolution equations, their flows, and commutators of such objects. This tool was employed to perform contrived computations arising in the analysis of the local error of operator splitting methods. It enabled the proof of the convergence of the basic method and of the asymptotical correctness of a defect-based error estimator. The performance of our package is illustrated on several examples.},
  isbn = {978-3-319-45641-6},
  keywords = {nosource}
}

@article{aitchison1982statistical,
  title = {The Statistical Analysis of Compositional Data},
  author = {Aitchison, J},
  year = {1982},
  journal = {Journal of the Royal Statistical Society, B},
  volume = {44},
  pages = {139--177},
  publisher = {JSTOR},
  isbn = {0412280604},
  keywords = {nosource}
}

@article{aitchison1985kernel,
  title = {Kernel Density Estimation for Compositional Data},
  author = {Aitchison, J and Lauder, I J},
  year = {1985},
  journal = {Applied statistics},
  pages = {129--137},
  publisher = {JSTOR},
  keywords = {nosource}
}

@article{akaike1974new,
  title = {A New Look at the Statistical Model Identification},
  author = {Akaike, Hirotugu A I},
  year = {1974},
  journal = {Automatic Control, IEEE Transactions on},
  volume = {19},
  number = {6},
  pages = {716--723},
  publisher = {Ieee},
  keywords = {nosource}
}

@article{alamichel2024bayesian,
  title = {Bayesian Mixture Models (in)Consistency for the Number of Clusters},
  author = {Alamichel, Louise and Bystrova, Daria and Arbel, Julyan and Kon Kam King, Guillaume},
  year = {2024},
  journal = {Scandinavian Journal of Statistics},
  volume = {in press},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/sjos.12739},
  doi = {10.1111/sjos.12739},
  abstract = {Abstract Bayesian nonparametric mixture models are common for modeling complex data. While these models are well-suited for density estimation, recent results proved posterior inconsistency of the number of clusters when the true number of components is finite, for the Dirichlet process and Pitman--Yor process mixture models. We extend these results to additional Bayesian nonparametric priors such as Gibbs-type processes and finite-dimensional representations thereof. The latter include the Dirichlet multinomial process, the recently proposed Pitman--Yor, and normalized generalized gamma multinomial processes. We show that mixture models based on these processes are also inconsistent in the number of clusters and discuss possible solutions. Notably, we show that a postprocessing algorithm introduced for the Dirichlet process can be extended to more general models and provides a consistent method to estimate the number of components.},
  keywords = {clustering,finite mixtures,finite-dimensional BNP representations,Gibbs-type process},
  file = {/home/gkonkamking/pCloudDrive/papers/Alamichel et al_Bayesian mixture models (in)consistency for the number of clusters.pdf}
}

@article{Aldenberg1993,
  ids = {Aldenberg1993d},
  title = {Confidence Limits for Hazardous Concentrations Based on Logistically Distributed {{NOEC}} Toxicity Data},
  author = {Aldenberg, T and Slob, W},
  year = {1993},
  journal = {Ecotoxicology and Environmental Safety},
  isbn = {0147-6513},
  pmid = {7682918},
  keywords = {nosource}
}

@article{Aldenberg2000,
  ids = {Aldenberg2000d},
  title = {Uncertainty of the Hazardous Concentration and Fraction Affected for Normal Species Sensitivity Distributions.},
  author = {Aldenberg, T and Jaworska, J S},
  year = {2000},
  month = may,
  journal = {Ecotoxicology and environmental safety},
  volume = {46},
  number = {1},
  eprint = {10805987},
  eprinttype = {pubmed},
  pages = {1--18},
  issn = {0147-6513},
  doi = {10.1006/eesa.1999.1869},
  abstract = {Species in the environment vary according to their sensitivity to a toxicant. Because these differences in sensitivity are unique to the toxicant at consideration and laboratory data sets to assess this variability are very small due to cost, it is important to provide uncertainty estimates of (1) environmental quality objectives (hazardous concentrations) derived from these laboratory data and (2) fraction of species affected at given, or predicted, laboratory or environmental concentrations. This article focuses on the normal (Gaussian) distribution of species sensitivity. It examines and compares results of Problems (1) and (2) from two opposing statistical philosophies, Bayesian and Classical, leading to vastly different numerical approaches. For the normal model, both approaches lead to identical answers, numerically. Extrapolation factors for the lower, median, and upper estimates of the hazardous concentration at six levels of protection are derived. Furthermore, upper, median, and lower estimates of the fraction affected at given, standardized, logarithmic concentrations have been tabulated. This table can be used directly for risk assessment without reference to protection levels or hazardous concentrations. The confidence limits for hazardous concentration and fraction affected depend heavily on the number of species tested and are independent of the toxic substance involved (provided the model is right), due to correction for the mean and standard deviation of the toxicity data. The equivalence of confidence limits for hazardous concentration and fraction affected is captured in the law of extrapolation: the upper (median, lower) confidence limit for the fraction affected at the lower (median, upper) confidence limit of the hazardous concentration is equal to the fraction affected (e.g., 5\%) used to define the hazardous concentration. The upper confidence limit for the fraction affected at the median estimate of the hazardous concentration for 5\% of the species is a fixed number depending on the sample size of the toxicity data only. It amounts to 46\% at n=3, down to 20\% at n=10, and still 12\% at n 30.},
  isbn = {0147-6513},
  pmid = {10805987},
  keywords = {Animals,Bayes Theorem,Bayesian statistics,confidence limit.,environmental quality objective,extrapolation,fraction affected,hazardous concentration,Humans,Normal Distribution,nosource,Probability,risk assessment,Risk Assessment,Species Specificity,Toxicology}
}

@incollection{aldenberg2002normal,
  title = {Normal Species Sensitivity Distributions and Probalistic Ecological Risk Assessment},
  booktitle = {Species Sensitivity Distribution in Ecotoxicology},
  author = {Aldenberg, Tom and Jaworska, Joanna S and Traas, Theo P},
  editor = {Posthuma, Leo and Suter, GW II and Traas, Theo P},
  year = {2002},
  pages = {49--102},
  publisher = {Lewis Publishers},
  address = {Boca Raton, FL},
  isbn = {1-56670-578-9},
  keywords = {nosource}
}

@article{Aldenberg2013,
  title = {Species Sensitivity Distribution Estimation from Uncertain ({{QSAR-based}}) Effects Data},
  author = {Aldenberg, Tom and Rorije, Emiel},
  year = {2013},
  month = mar,
  journal = {ATLA Alternatives to Laboratory Animals},
  volume = {41},
  number = {1},
  eprint = {23614542},
  eprinttype = {pubmed},
  pages = {19--31},
  issn = {02611929},
  abstract = {In environmental risk assessment, Species Sensitivity Distributions (SSDs) can be applied to estimate a PNEC (Predicted No-Effect Concentration) for a chemical substance, when sufficient data on species toxicities are available. The European Chemicals Agency (ECHA) recommendation is 10 biological species. The question addressed in this paper, is whether QSAR-predicted toxicities can be included in SSD based PNEC estimates, and whether any modifications need to be made to account for the uncertainty in the QSAR-model estimates. This problem is addressed from a probabilistic modelling point of view. From classical analysis of variation (ANOVA), we review how the error-in-data SSD problem is similar to separation into between-group and within-group variance. ECHA guidance suggests averaging similar endpoint data for a species, which is consistent with group means, as in ANOVA. This exercise reveals that error-in data reduces the estimation of the between species variation, i.e. the SSD variance, rather than enlarging it. A Bayesian analysis permits the assessment of the uncertainty of the SSD mean and variance parameters for given values of mean species toxicity error. This requires a hierarchical model. Prototyping this model for an artificial five-species data set seems to suggest that the influence of data error is relatively minor. Moreover, when neglecting this data error, a slightly conservative estimate of the SSD results. Hence, we suggest including (model-predicted) data as model point estimates and handling the SSD as usual. The Bayesian simulation of the error-in-data SSD leads to predictive distributions, being an average of posterior spaghetti plot densities or cumulative distributions. We derive new predictive extrapolation constants with several improvements over previous median uncertainty log10HC5 estimates, in that they are easily calculable from spreadsheet Student-t functions and based on a more realistic uniform prior for the SSD standard deviation. Other advantages are that they are single-number extrapolation constants and they are more sensitive to small sample size.},
  pmid = {23614542},
  keywords = {Bayesian hierarchical model,Extrapolation constants,nosource,Predictive uncertainty,Probabilistic risk assessment,QSAR and read-across prediction,Species sensitivity distribution}
}

@article{alessandretti2016user,
  title = {User-Based Representation of Time-Resolved Multimodal Public Transportation Networks},
  author = {Alessandretti, Laura and Karsai, M{\'a}rton and Gauvin, Laetitia},
  year = {2016},
  journal = {Royal Society open science},
  volume = {3},
  number = {7},
  pages = {160156},
  publisher = {The Royal Society},
  doi = {10.1098/rsos.160156},
  keywords = {nosource}
}

@article{alexandrovRepertoireMutationalSignatures2020,
  title = {The Repertoire of Mutational Signatures in Human Cancer},
  author = {Alexandrov, Ludmil B. and Kim, Jaegil and Haradhvala, Nicholas J. and Huang, Mi Ni and Tian Ng, Alvin Wei and Wu, Yang and Boot, Arnoud and Covington, Kyle R. and Gordenin, Dmitry A. and Bergstrom, Erik N. and Islam, S. M. Ashiqul and {Lopez-Bigas}, Nuria and Klimczak, Leszek J. and McPherson, John R. and Morganella, Sandro and Sabarinathan, Radhakrishnan and Wheeler, David A. and Mustonen, Ville and {PCAWG Mutational Signatures Working Group} and Alexandrov, Ludmil B. and Bergstrom, Erik N. and Boot, Arnoud and Boutros, Paul and Chan, Kin and Covington, Kyle R. and Fujimoto, Akihiro and Getz, Gad and Gordenin, Dmitry A. and Haradhvala, Nicholas J. and Huang, Mi Ni and Islam, S. M. Ashiqul and Kazanov, Marat and Kim, Jaegil and Klimczak, Leszek J. and {Lopez-Bigas}, Nuria and Lawrence, Michael and Martincorena, I{\~n}igo and McPherson, John R. and Morganella, Sandro and Mustonen, Ville and Nakagawa, Hidewaki and Tian Ng, Alvin Wei and Polak, Paz and Prokopec, Stephenie and Roberts, Steven A. and Rozen, Steven G. and Sabarinathan, Radhakrishnan and Saini, Natalie and Shibata, Tatsuhiro and Shiraishi, Yuichi and Stratton, Michael R. and Teh, Bin Tean and {V{\'a}zquez-Garc{\'i}a}, Ignacio and Wheeler, David A. and Wu, Yang and Yousif, Fouad and Yu, Willie and Getz, Gad and Rozen, Steven G. and Stratton, Michael R. and {PCAWG Consortium} and Aaltonen, Lauri A. and Abascal, Federico and Abeshouse, Adam and Aburatani, Hiroyuki and Adams, David J. and Agrawal, Nishant and Ahn, Keun Soo and Ahn, Sung-Min and Aikata, Hiroshi and Akbani, Rehan and Akdemir, Kadir C. and {Al-Ahmadie}, Hikmat and {Al-Sedairy}, Sultan T. and {Al-Shahrour}, Fatima and Alawi, Malik and Albert, Monique and Aldape, Kenneth and Alexandrov, Ludmil B. and Ally, Adrian and Alsop, Kathryn and Alvarez, Eva G. and Amary, Fernanda and Amin, Samirkumar B. and Aminou, Brice and Ammerpohl, Ole and Anderson, Matthew J. and Ang, Yeng and Antonello, Davide and Anur, Pavana and Aparicio, Samuel and Appelbaum, Elizabeth L. and Arai, Yasuhito and Aretz, Axel and Arihiro, Koji and Ariizumi, Shun-ichi and Armenia, Joshua and Arnould, Laurent and Asa, Sylvia and Assenov, Yassen and Atwal, Gurnit and Aukema, Sietse and Auman, J. Todd and Aure, Miriam R. R. and Awadalla, Philip and Aymerich, Marta and Bader, Gary D. and {Baez-Ortega}, Adrian and Bailey, Matthew H. and Bailey, Peter J. and Balasundaram, Miruna and Balu, Saianand and Bandopadhayay, Pratiti and Banks, Rosamonde E. and Barbi, Stefano and Barbour, Andrew P. and Barenboim, Jonathan and {Barnholtz-Sloan}, Jill and Barr, Hugh and Barrera, Elisabet and Bartlett, John and Bartolome, Javier and Bassi, Claudio and Bathe, Oliver F. and Baumhoer, Daniel and Bavi, Prashant and Baylin, Stephen B. and Bazant, Wojciech and Beardsmore, Duncan and Beck, Timothy A. and Behjati, Sam and Behren, Andreas and Niu, Beifang and Bell, Cindy and Beltran, Sergi and Benz, Christopher and Berchuck, Andrew and Bergmann, Anke K. and Bergstrom, Erik N. and Berman, Benjamin P. and Berney, Daniel M. and Bernhart, Stephan H. and Beroukhim, Rameen and Berrios, Mario and Bersani, Samantha and Bertl, Johanna and Betancourt, Miguel and Bhandari, Vinayak and Bhosle, Shriram G. and Biankin, Andrew V. and Bieg, Matthias and Bigner, Darell and Binder, Hans and Birney, Ewan and Birrer, Michael and Biswas, Nidhan K. and Bjerkehagen, Bodil and Bodenheimer, Tom and Boice, Lori and Bonizzato, Giada and De Bono, Johann S. and Boot, Arnoud and Bootwalla, Moiz S. and Borg, Ake and Borkhardt, Arndt and Boroevich, Keith A. and Borozan, Ivan and Borst, Christoph and Bosenberg, Marcus and Bosio, Mattia and Boultwood, Jacqueline and Bourque, Guillaume and Boutros, Paul C. and Bova, G. Steven and Bowen, David T. and Bowlby, Reanne and Bowtell, David D. L. and Boyault, Sandrine and Boyce, Rich and Boyd, Jeffrey and Brazma, Alvis and Brennan, Paul and Brewer, Daniel S. and Brinkman, Arie B. and Bristow, Robert G. and Broaddus, Russell R. and Brock, Jane E. and Brock, Malcolm and Broeks, Annegien and Brooks, Angela N. and Brooks, Denise and Brors, Benedikt and Brunak, S{\o}ren and Bruxner, Timothy J. C. and Bruzos, Alicia L. and Buchanan, Alex and Buchhalter, Ivo and Buchholz, Christiane and Bullman, Susan and Burke, Hazel and Burkhardt, Birgit and Burns, Kathleen H. and Busanovich, John and Bustamante, Carlos D. and Butler, Adam P. and Butte, Atul J. and Byrne, Niall J. and {B{\o}rresen-Dale}, Anne-Lise and {Caesar-Johnson}, Samantha J. and Cafferkey, Andy and Cahill, Declan and Calabrese, Claudia and Caldas, Carlos and Calvo, Fabien and Camacho, Niedzica and Campbell, Peter J. and Campo, Elias and Cant{\`u}, Cinzia and Cao, Shaolong and Carey, Thomas E. and {Carlevaro-Fita}, Joana and Carlsen, Rebecca and Cataldo, Ivana and Cazzola, Mario and Cebon, Jonathan and Cerfolio, Robert and Chadwick, Dianne E. and Chakravarty, Dimple and Chalmers, Don and Chan, Calvin Wing Yiu and Chan, Kin and {Chan-Seng-Yue}, Michelle and Chandan, Vishal S. and Chang, David K. and Chanock, Stephen J. and Chantrill, Lorraine A. and Chateigner, Aur{\'e}lien and Chatterjee, Nilanjan and Chayama, Kazuaki and Chen, Hsiao-Wei and Chen, Jieming and Chen, Ken and Chen, Yiwen and Chen, Zhaohong and Cherniack, Andrew D. and Chien, Jeremy and Chiew, Yoke-Eng and Chin, Suet-Feung and Cho, Juok and Cho, Sunghoon and Choi, Jung Kyoon and Choi, Wan and Chomienne, Christine and Chong, Zechen and Choo, Su Pin and Chou, Angela and Christ, Angelika N. and Christie, Elizabeth L. and Chuah, Eric and Cibulskis, Carrie and Cibulskis, Kristian and Cingarlini, Sara and Clapham, Peter and Claviez, Alexander and Cleary, Sean and Cloonan, Nicole and Cmero, Marek and Collins, Colin C. and Connor, Ashton A. and Cooke, Susanna L. and Cooper, Colin S. and Cope, Leslie and Corbo, Vincenzo and Cordes, Matthew G. and Cordner, Stephen M. and {Cort{\'e}s-Ciriano}, Isidro and Covington, Kyle and Cowin, Prue A. and Craft, Brian and Craft, David and Creighton, Chad J. and Cun, Yupeng and Curley, Erin and Cutcutache, Ioana and Czajka, Karolina and Czerniak, Bogdan and Dagg, Rebecca A. and Danilova, Ludmila and Davi, Maria Vittoria and Davidson, Natalie R. and Davies, Helen and Davis, Ian J. and {Davis-Dusenbery}, Brandi N. and Dawson, Kevin J. and De La Vega, Francisco M. and {De Paoli-Iseppi}, Ricardo and Defreitas, Timothy and Tos, Angelo P. Dei and Delaneau, Olivier and Demchok, John A. and Demeulemeester, Jonas and Demidov, German M. and Demircio{\u g}lu, Deniz and Dennis, Nening M. and Denroche, Robert E. and Dentro, Stefan C. and Desai, Nikita and Deshpande, Vikram and Deshwar, Amit G. and Desmedt, Christine and {Deu-Pons}, Jordi and Dhalla, Noreen and Dhani, Neesha C. and Dhingra, Priyanka and Dhir, Rajiv and DiBiase, Anthony and Diamanti, Klev and Ding, Li and Ding, Shuai and Dinh, Huy Q. and Dirix, Luc and Doddapaneni, HarshaVardhan and Donmez, Nilgun and Dow, Michelle T. and Drapkin, Ronny and Drechsel, Oliver and Drews, Ruben M. and Serge, Serge and Dudderidge, Tim and {Dueso-Barroso}, Ana and Dunford, Andrew J. and Dunn, Michael and Dursi, Lewis Jonathan and Duthie, Fraser R. and {Dutton-Regester}, Ken and Eagles, Jenna and Easton, Douglas F. and Edmonds, Stuart and Edwards, Paul A. and Edwards, Sandra E. and Eeles, Rosalind A. and Ehinger, Anna and Eils, Juergen and Eils, Roland and {El-Naggar}, Adel and Eldridge, Matthew and Ellrott, Kyle and Erkek, Serap and Escaramis, Georgia and Espiritu, Shadrielle M. G. and Estivill, Xavier and Etemadmoghadam, Dariush and Eyfjord, Jorunn E. and Faltas, Bishoy M. and Fan, Daiming and Fan, Yu and Faquin, William C. and Farcas, Claudiu and Fassan, Matteo and Fatima, Aquila and Favero, Francesco and Fayzullaev, Nodirjon and Felau, Ina and Fereday, Sian and Ferguson, Martin L. and Ferretti, Vincent and Feuerbach, Lars and Field, Matthew A. and Fink, J. Lynn and Finocchiaro, Gaetano and Fisher, Cyril and Fittall, Matthew W. and Fitzgerald, Anna and Fitzgerald, Rebecca C. and Flanagan, Adrienne M. and Fleshner, Neil E. and Flicek, Paul and Foekens, John A. and Fong, Kwun M. and Fonseca, Nuno A. and Foster, Christopher S. and Fox, Natalie S. and Fraser, Michael and Frazer, Scott and {Frenkel-Morgenstern}, Milana and Friedman, William and Frigola, Joan and Fronick, Catrina C. and Fujimoto, Akihiro and Fujita, Masashi and Fukayama, Masashi and Fulton, Lucinda A. and Fulton, Robert S. and Furuta, Mayuko and Futreal, P. Andrew and F{\"u}llgrabe, Anja and Gabriel, Stacey B. and Gallinger, Steven and {Gambacorti-Passerini}, Carlo and Gao, Jianjiong and Gao, Shengjie and Garraway, Levi and Garred, {\O}ystein and Garrison, Erik and Garsed, Dale W. and Gehlenborg, Nils and Gelpi, Josep L. L. and George, Joshy and Gerhard, Daniela S. and Gerhauser, Clarissa and Gershenwald, Jeffrey E. and Gerstein, Mark and Gerstung, Moritz and Getz, Gad and Ghori, Mohammed and Ghossein, Ronald and Giama, Nasra H. and Gibbs, Richard A. and Gibson, Bob and Gill, Anthony J. and Gill, Pelvender and Giri, Dilip D. and Glodzik, Dominik and Gnanapragasam, Vincent J. and Goebler, Maria Elisabeth and Goldman, Mary J. and Gomez, Carmen and Gonzalez, Santiago and {Gonzalez-Perez}, Abel and Gordenin, Dmitry A. and Gossage, James and Gotoh, Kunihito and Govindan, Ramaswamy and Grabau, Dorthe and Graham, Janet S. and Grant, Robert C. and Green, Anthony R. and Green, Eric and Greger, Liliana and Grehan, Nicola and Grimaldi, Sonia and Grimmond, Sean M. and Grossman, Robert L. and Grundhoff, Adam and Gundem, Gunes and Guo, Qianyun and Gupta, Manaswi and Gupta, Shailja and Gut, Ivo G. and Gut, Marta and G{\"o}ke, Jonathan and Ha, Gavin and Haake, Andrea and Haan, David and Haas, Siegfried and Haase, Kerstin and Haber, James E. and Habermann, Nina and Hach, Faraz and Haider, Syed and Hama, Natsuko and Hamdy, Freddie C. and Hamilton, Anne and Hamilton, Mark P. and Han, Leng and Hanna, George B. and Hansmann, Martin and Haradhvala, Nicholas J. and Harismendy, Olivier and Harliwong, Ivon and Harmanci, Arif O. and Harrington, Eoghan and Hasegawa, Takanori and Haussler, David and Hawkins, Steve and Hayami, Shinya and Hayashi, Shuto and Hayes, D. Neil and Hayes, Stephen J. and Hayward, Nicholas K. and Hazell, Steven and He, Yao and Heath, Allison P. and Heath, Simon C. and Hedley, David and Hegde, Apurva M. and Heiman, David I. and Heinold, Michael C. and Heins, Zachary and Heisler, Lawrence E. and {Hellstrom-Lindberg}, Eva and Helmy, Mohamed and Heo, Seong Gu and Hepperla, Austin J. and {Heredia-Genestar}, Jos{\'e} Mar{\'i}a and Herrmann, Carl and Hersey, Peter and Hess, Julian M. and Hilmarsdottir, Holmfridur and Hinton, Jonathan and Hirano, Satoshi and Hiraoka, Nobuyoshi and Hoadley, Katherine A. and Hobolth, Asger and Hodzic, Ermin and Hoell, Jessica I. and Hoffmann, Steve and Hofmann, Oliver and Holbrook, Andrea and Holik, Aliaksei Z. and Hollingsworth, Michael A. and Holmes, Oliver and Holt, Robert A. and Hong, Chen and Hong, Eun Pyo and Hong, Jongwhi H. and Hooijer, Gerrit K. and Hornsh{\o}j, Henrik and Hosoda, Fumie and Hou, Yong and Hovestadt, Volker and Howat, William and Hoyle, Alan P. and Hruban, Ralph H. and Hu, Jianhong and Hu, Taobo and Hua, Xing and Huang, Kuan-lin and Huang, Mei and Huang, Mi Ni and Huang, Vincent and Huang, Yi and Huber, Wolfgang and Hudson, Thomas J. and Hummel, Michael and Hung, Jillian A. and Huntsman, David and Hupp, Ted R. and Huse, Jason and Huska, Matthew R. and Hutter, Barbara and Hutter, Carolyn M. and H{\"u}bschmann, Daniel and {Iacobuzio-Donahue}, Christine A. and Imbusch, Charles David and Imielinski, Marcin and Imoto, Seiya and Isaacs, William B. and Isaev, Keren and Ishikawa, Shumpei and Iskar, Murat and Islam, S. M. Ashiqul and Ittmann, Michael and Ivkovic, Sinisa and Izarzugaza, Jose M. G. and Jacquemier, Jocelyne and Jakrot, Valerie and Jamieson, Nigel B. and Jang, Gun Ho and Jang, Se Jin and Jayaseelan, Joy C. and Jayasinghe, Reyka and Jefferys, Stuart R. and Jegalian, Karine and Jennings, Jennifer L. and Jeon, Seung-Hyup and Jerman, Lara and Ji, Yuan and Jiao, Wei and Johansson, Peter A. and Johns, Amber L. and Johns, Jeremy and Johnson, Rory and Johnson, Todd A. and Jolly, Clemency and Joly, Yann and Jonasson, Jon G. and Jones, Corbin D. and Jones, David R. and Jones, David T. W. and Jones, Nic and Jones, Steven J. M. and Jonkers, Jos and Ju, Young Seok and Juhl, Hartmut and Jung, Jongsun and Juul, Malene and Juul, Randi Istrup and Juul, Sissel and J{\"a}ger, Natalie and Kabbe, Rolf and Kahles, Andre and Kahraman, Abdullah and Kaiser, Vera B. and Kakavand, Hojabr and Kalimuthu, Sangeetha and Von Kalle, Christof and Kang, Koo Jeong and Karaszi, Katalin and Karlan, Beth and Karli{\'c}, Rosa and Karsch, Dennis and Kasaian, Katayoon and Kassahn, Karin S. and Katai, Hitoshi and Kato, Mamoru and Katoh, Hiroto and Kawakami, Yoshiiku and Kay, Jonathan D. and Kazakoff, Stephen H. and Kazanov, Marat D. and Keays, Maria and Kebebew, Electron and Kefford, Richard F. and Kellis, Manolis and Kench, James G. and Kennedy, Catherine J. and Kerssemakers, Jules N. A. and Khoo, David and Khoo, Vincent and Khuntikeo, Narong and Khurana, Ekta and Kilpinen, Helena and Kim, Hark Kyun and Kim, Hyung-Lae and Kim, Hyung-Yong and Kim, Hyunghwan and Kim, Jaegil and Kim, Jihoon and Kim, Jong K. and Kim, Youngwook and King, Tari A. and Klapper, Wolfram and Kleinheinz, Kortine and Klimczak, Leszek J. and Knappskog, Stian and Kneba, Michael and Knoppers, Bartha M. and Koh, Youngil and Komorowski, Jan and Komura, Daisuke and Komura, Mitsuhiro and Kong, Gu and Kool, Marcel and Korbel, Jan O. and Korchina, Viktoriya and Korshunov, Andrey and Koscher, Michael and Koster, Roelof and {Kote-Jarai}, Zsofia and Koures, Antonios and Kovacevic, Milena and Kremeyer, Barbara and Kretzmer, Helene and Kreuz, Markus and Krishnamurthy, Savitri and Kube, Dieter and Kumar, Kiran and Kumar, Pardeep and Kumar, Sushant and Kumar, Yogesh and Kundra, Ritika and K{\"u}bler, Kirsten and K{\"u}ppers, Ralf and Lagergren, Jesper and Lai, Phillip H. and Laird, Peter W. and Lakhani, Sunil R. and Lalansingh, Christopher M. and Lalonde, Emilie and Lamaze, Fabien C. and Lambert, Adam and Lander, Eric and Landgraf, Pablo and Landoni, Luca and Langer{\o}d, Anita and Lanz{\'o}s, Andr{\'e}s and Larsimont, Denis and Larsson, Erik and Lathrop, Mark and Lau, Loretta M. S. and Lawerenz, Chris and Lawlor, Rita T. and Lawrence, Michael S. and Lazar, Alexander J. and Lazic, Ana Mijalkovic and Le, Xuan and Lee, Darlene and Lee, Donghoon and Lee, Eunjung Alice and Lee, Hee Jin and Lee, Jake June-Koo and Lee, Jeong-Yeon and Lee, Juhee and Lee, Ming Ta Michael and {Lee-Six}, Henry and Lehmann, Kjong-Van and Lehrach, Hans and Lenze, Dido and Leonard, Conrad R. and Leongamornlert, Daniel A. and Leshchiner, Ignaty and Letourneau, Louis and Letunic, Ivica and Levine, Douglas A. and Lewis, Lora and Ley, Tim and Li, Chang and Li, Constance H. and Li, Haiyan Irene and Li, Jun and Li, Lin and Li, Shantao and Li, Siliang and Li, Xiaobo and Li, Xiaotong and Li, Xinyue and Li, Yilong and Liang, Han and Liang, Sheng-Ben and Lichter, Peter and Lin, Pei and Lin, Ziao and Linehan, W. M. and Lingj{\ae}rde, Ole Christian and Liu, Dongbing and Liu, Eric Minwei and Liu, Fei-Fei Fei and Liu, Fenglin and Liu, Jia and Liu, Xingmin and Livingstone, Julie and Livitz, Dimitri and Livni, Naomi and Lochovsky, Lucas and Loeffler, Markus and Long, Georgina V. and {Lopez-Guillermo}, Armando and Lou, Shaoke and Louis, David N. and Lovat, Laurence B. and Lu, Yiling and Lu, Yong-Jie and Lu, Youyong and Luchini, Claudio and Lungu, Ilinca and Luo, Xuemei and Luxton, Hayley J. and Lynch, Andy G. and Lype, Lisa and L{\'o}pez, Cristina and {L{\'o}pez-Ot{\'i}n}, Carlos and Ma, Eric Z. and Ma, Yussanne and MacGrogan, Gaetan and MacRae, Shona and Macintyre, Geoff and Madsen, Tobias and Maejima, Kazuhiro and Mafficini, Andrea and Maglinte, Dennis T. and Maitra, Arindam and Majumder, Partha P. and Malcovati, Luca and Malikic, Salem and Malleo, Giuseppe and Mann, Graham J. and {Mantovani-L{\"o}ffler}, Luisa and Marchal, Kathleen and Marchegiani, Giovanni and Mardis, Elaine R. and Margolin, Adam A. and Marin, Maximillian G. and Markowetz, Florian and Markowski, Julia and Marks, Jeffrey and {Marques-Bonet}, Tomas and Marra, Marco A. and Marsden, Luke and Martens, John W. M. and Martin, Sancha and {Martin-Subero}, Jose I. and Martincorena, I{\~n}igo and {Martinez-Fundichely}, Alexander and Maruvka, Yosef E. and Mashl, R. Jay and Massie, Charlie E. and Matthew, Thomas J. and Matthews, Lucy and Mayer, Erik and Mayes, Simon and Mayo, Michael and Mbabaali, Faridah and McCune, Karen and McDermott, Ultan and McGillivray, Patrick D. and McLellan, Michael D. and McPherson, John D. and McPherson, John R. and McPherson, Treasa A. and Meier, Samuel R. and Meng, Alice and Meng, Shaowu and Menzies, Andrew and Merrett, Neil D. and Merson, Sue and Meyerson, Matthew and Meyerson, William and Mieczkowski, Piotr A. and Mihaiescu, George L. and Mijalkovic, Sanja and Mikkelsen, Tom and Milella, Michele and Mileshkin, Linda and Miller, Christopher A. and Miller, David K. and Miller, Jessica K. and Mills, Gordon B. and Milovanovic, Ana and Minner, Sarah and Miotto, Marco and Arnau, Gisela Mir and Mirabello, Lisa and Mitchell, Chris and Mitchell, Thomas J. and Miyano, Satoru and Miyoshi, Naoki and Mizuno, Shinichi and {Moln{\'a}r-G{\'a}bor}, Fruzsina and Moore, Malcolm J. and Moore, Richard A. and Morganella, Sandro and Morris, Quaid D. and Morrison, Carl and Mose, Lisle E. and Moser, Catherine D. and Mui{\~n}os, Ferran and Mularoni, Loris and Mungall, Andrew J. and Mungall, Karen and Musgrove, Elizabeth A. and Mustonen, Ville and Mutch, David and Muyas, Francesc and Muzny, Donna M. and Mu{\~n}oz, Alfonso and Myers, Jerome and Myklebost, Ola and M{\"o}ller, Peter and Nagae, Genta and Nagrial, Adnan M. and {Nahal-Bose}, Hardeep K. and Nakagama, Hitoshi and Nakagawa, Hidewaki and Nakamura, Hiromi and Nakamura, Toru and Nakano, Kaoru and Nandi, Tannistha and Nangalia, Jyoti and Nastic, Mia and Navarro, Arcadi and Navarro, Fabio C. P. and Neal, David E. and Nettekoven, Gerd and Newell, Felicity and Newhouse, Steven J. and Newton, Yulia and Ng, Alvin Wei Tian and Ng, Anthony and Nicholson, Jonathan and Nicol, David and Nie, Yongzhan and Nielsen, G. Petur and Nielsen, Morten Muhlig and {Nik-Zainal}, Serena and Noble, Michael S. and Nones, Katia and Northcott, Paul A. and Notta, Faiyaz and O'Connor, Brian D. and O'Donnell, Peter and O'Donovan, Maria and O'Meara, Sarah and O'Neill, Brian Patrick and O'Neill, J. Robert and Ocana, David and Ochoa, Angelica and Oesper, Layla and Ogden, Christopher and Ohdan, Hideki and Ohi, Kazuhiro and {Ohno-Machado}, Lucila and Oien, Karin A. and Ojesina, Akinyemi I. and Ojima, Hidenori and Okusaka, Takuji and Omberg, Larsson and Ong, Choon Kiat and Ossowski, Stephan and Ott, German and Ouellette, B. F. Francis and P'ng, Christine and Paczkowska, Marta and Paiella, Salvatore and Pairojkul, Chawalit and Pajic, Marina and {Pan-Hammarstr{\"o}m}, Qiang and Papaemmanuil, Elli and Papatheodorou, Irene and Paramasivam, Nagarajan and Park, Ji Wan and Park, Joong-Won and Park, Keunchil and Park, Kiejung and Park, Peter J. and Parker, Joel S. and Parsons, Simon L. and Pass, Harvey and Pasternack, Danielle and Pastore, Alessandro and Patch, Ann-Marie and Pauport{\'e}, Iris and Pea, Antonio and Pearson, John V. and Pedamallu, Chandra Sekhar and Pedersen, Jakob Skou and Pederzoli, Paolo and Peifer, Martin and Pennell, Nathan A. and Perou, Charles M. and Perry, Marc D. and Petersen, Gloria M. and Peto, Myron and Petrelli, Nicholas and Petryszak, Robert and Pfister, Stefan M. and Phillips, Mark and Pich, Oriol and Pickett, Hilda A. and Pihl, Todd D. and Pillay, Nischalan and Pinder, Sarah and Pinese, Mark and Pinho, Andreia V. and Pitk{\"a}nen, Esa and Pivot, Xavier and {Pi{\~n}eiro-Y{\'a}{\~n}ez}, Elena and Planko, Laura and Plass, Christoph and Polak, Paz and Pons, Tirso and Popescu, Irinel and Potapova, Olga and Prasad, Aparna and Preston, Shaun R. and Prinz, Manuel and Pritchard, Antonia L. and Prokopec, Stephenie D. and Provenzano, Elena and Puente, Xose S. and Puig, Sonia and Puiggr{\`o}s, Montserrat and {Pulido-Tamayo}, Sergio and Pupo, Gulietta M. and Purdie, Colin A. and Quinn, Michael C. and Rabionet, Raquel and Rader, Janet S. and Radlwimmer, Bernhard and Radovic, Petar and Raeder, Benjamin and Raine, Keiran M. and Ramakrishna, Manasa and Ramakrishnan, Kamna and Ramalingam, Suresh and Raphael, Benjamin J. and Rathmell, W. Kimryn and Rausch, Tobias and Reifenberger, Guido and Reimand, J{\"u}ri and {Reis-Filho}, Jorge and Reuter, Victor and {Reyes-Salazar}, Iker and Reyna, Matthew A. and Reynolds, Sheila M. and Rheinbay, Esther and Riazalhosseini, Yasser and Richardson, Andrea L. and Richter, Julia and Ringel, Matthew and Ringn{\'e}r, Markus and Rino, Yasushi and Rippe, Karsten and Roach, Jeffrey and Roberts, Lewis R. and Roberts, Nicola D. and Roberts, Steven A. and Robertson, A. Gordon and Robertson, Alan J. and Rodriguez, Javier Bartolom{\'e} and {Rodriguez-Martin}, Bernardo and {Rodr{\'i}guez-Gonz{\'a}lez}, F. Germ{\'a}n and Roehrl, Michael H. A. and Rohde, Marius and Rokutan, Hirofumi and Romieu, Gilles and Rooman, Ilse and Roques, Tom and Rosebrock, Daniel and Rosenberg, Mara and Rosenstiel, Philip C. and Rosenwald, Andreas and Rowe, Edward W. and Royo, Romina and Rozen, Steven G. and Rubanova, Yulia and Rubin, Mark A. and {Rubio-Perez}, Carlota and Rudneva, Vasilisa A. and Rusev, Borislav C. and Ruzzenente, Andrea and R{\"a}tsch, Gunnar and Sabarinathan, Radhakrishnan and Sabelnykova, Veronica Y. and Sadeghi, Sara and Sahinalp, S. Cenk and Saini, Natalie and {Saito-Adachi}, Mihoko and Saksena, Gordon and Salcedo, Adriana and Salgado, Roberto and Salichos, Leonidas and Sallari, Richard and Saller, Charles and Salvia, Roberto and Sam, Michelle and Samra, Jaswinder S. and {Sanchez-Vega}, Francisco and Sander, Chris and Sanders, Grant and Sarin, Rajiv and Sarrafi, Iman and {Sasaki-Oku}, Aya and Sauer, Torill and Sauter, Guido and Saw, Robyn P. M. and Scardoni, Maria and Scarlett, Christopher J. and Scarpa, Aldo and Scelo, Ghislaine and Schadendorf, Dirk and Schein, Jacqueline E. and Schilhabel, Markus B. and Schlesner, Matthias and Schlomm, Thorsten and Schmidt, Heather K. and Schramm, Sarah-Jane and Schreiber, Stefan and Schultz, Nikolaus and Schumacher, Steven E. and Schwarz, Roland F. and Scolyer, Richard A. and Scott, David and Scully, Ralph and Seethala, Raja and Segre, Ayellet V. and Selander, Iris and Semple, Colin A. and Senbabaoglu, Yasin and Sengupta, Subhajit and Sereni, Elisabetta and Serra, Stefano and Sgroi, Dennis C. and Shackleton, Mark and Shah, Nimish C. and Shahabi, Sagedeh and Shang, Catherine A. and Shang, Ping and Shapira, Ofer and Shelton, Troy and Shen, Ciyue and Shen, Hui and Shepherd, Rebecca and Shi, Ruian and Shi, Yan and Shiah, Yu-Jia and Shibata, Tatsuhiro and Shih, Juliann and Shimizu, Eigo and Shimizu, Kiyo and Shin, Seung Jun and Shiraishi, Yuichi and Shmaya, Tal and Shmulevich, Ilya and Shorser, Solomon I. and Short, Charles and Shrestha, Raunak and Shringarpure, Suyash S. and Shriver, Craig and Shuai, Shimin and Sidiropoulos, Nikos and Siebert, Reiner and Sieuwerts, Anieta M. and Sieverling, Lina and Signoretti, Sabina and Sikora, Katarzyna O. and Simbolo, Michele and Simon, Ronald and Simons, Janae V. and Simpson, Jared T. and Simpson, Peter T. and Singer, Samuel and {Sinnott-Armstrong}, Nasa and Sipahimalani, Payal and Skelly, Tara J. and Smid, Marcel and Smith, Jaclyn and {Smith-McCune}, Karen and Socci, Nicholas D. and Sofia, Heidi J. and Soloway, Matthew G. and Song, Lei and Sood, Anil K. and Sothi, Sharmila and Sotiriou, Christos and Soulette, Cameron M. and Span, Paul N. and Spellman, Paul T. and Sperandio, Nicola and Spillane, Andrew J. and Spiro, Oliver and Spring, Jonathan and Staaf, Johan and Stadler, Peter F. and Staib, Peter and Stark, Stefan G. and Stebbings, Lucy and Stef{\'a}nsson, {\'O}lafur Andri and Stegle, Oliver and Stein, Lincoln D. and Stenhouse, Alasdair and Stewart, Chip and Stilgenbauer, Stephan and Stobbe, Miranda D. and Stratton, Michael R. and Stretch, Jonathan R. and Struck, Adam J. and Stuart, Joshua M. and Stunnenberg, Henk G. and Su, Hong and Su, Xiaoping and Sun, Ren X. and Sungalee, Stephanie and Susak, Hana and Suzuki, Akihiro and Sweep, Fred and Szczepanowski, Monika and S{\"u}ltmann, Holger and Yugawa, Takashi and Tam, Angela and Tamborero, David and Tan, Benita Kiat Tee and Tan, Donghui and Tan, Patrick and Tanaka, Hiroko and Taniguchi, Hirokazu and Tanskanen, Tomas J. and Tarabichi, Maxime and Tarnuzzer, Roy and Tarpey, Patrick and Taschuk, Morgan L. and Tatsuno, Kenji and Tavar{\'e}, Simon and Taylor, Darrin F. and {Taylor-Weiner}, Amaro and Teague, Jon W. and Teh, Bin Tean and Tembe, Varsha and Temes, Javier and Thai, Kevin and Thayer, Sarah P. and Thiessen, Nina and Thomas, Gilles and Thomas, Sarah and Thompson, Alan and Thompson, Alastair M. and Thompson, John F. F. and Thompson, R. Houston and Thorne, Heather and Thorne, Leigh B. and Thorogood, Adrian and Tiao, Grace and Tijanic, Nebojsa and Timms, Lee E. and Tirabosco, Roberto and Tojo, Marta and Tommasi, Stefania and Toon, Christopher W. and Toprak, Umut H. and Torrents, David and Tortora, Giampaolo and Tost, J{\"o}rg and Totoki, Yasushi and Townend, David and Traficante, Nadia and Treilleux, Isabelle and Trotta, Jean-R{\'e}mi and Tr{\"u}mper, Lorenz H. P. and Tsao, Ming and Tsunoda, Tatsuhiko and Tubio, Jose M. C. and Tucker, Olga and Turkington, Richard and Turner, Daniel J. and Tutt, Andrew and Ueno, Masaki and Ueno, Naoto T. and Umbricht, Christopher and Umer, Husen M. and Underwood, Timothy J. and Urban, Lara and Urushidate, Tomoko and Ushiku, Tetsuo and {Uusk{\"u}la-Reimand}, Liis and Valencia, Alfonso and Van Den Berg, David J. and Van Laere, Steven and Van Loo, Peter and Van Meir, Erwin G. and Van Den Eynden, Gert G. and Van Der Kwast, Theodorus and Vasudev, Naveen and Vazquez, Miguel and Vedururu, Ravikiran and Veluvolu, Umadevi and Vembu, Shankar and Verbeke, Lieven P. C. and Vermeulen, Peter and Verrill, Clare and Viari, Alain and Vicente, David and Vicentini, Caterina and VijayRaghavan, K. and Viksna, Juris and Vilain, Ricardo E. and Villasante, Izar and {Vincent-Salomon}, Anne and Visakorpi, Tapio and Voet, Douglas and Vyas, Paresh and {V{\'a}zquez-Garc{\'i}a}, Ignacio and Waddell, Nick M. and Waddell, Nicola and Wadelius, Claes and Wadi, Lina and Wagener, Rabea and Wala, Jeremiah A. and Wang, Jian and Wang, Jiayin and Wang, Linghua and Wang, Qi and Wang, Wenyi and Wang, Yumeng and Wang, Zhining and Waring, Paul M. and Warnatz, Hans-J{\"o}rg and Warrell, Jonathan and Warren, Anne Y. and Waszak, Sebastian M. and Wedge, David C. and Weichenhan, Dieter and Weinberger, Paul and Weinstein, John N. and Weischenfeldt, Joachim and Weisenberger, Daniel J. and Welch, Ian and Wendl, Michael C. and Werner, Johannes and Whalley, Justin P. and Wheeler, David A. and Whitaker, Hayley C. and Wigle, Dennis and Wilkerson, Matthew D. and Williams, Ashley and Wilmott, James S. and Wilson, Gavin W. and Wilson, Julie M. and Wilson, Richard K. and Winterhoff, Boris and Wintersinger, Jeffrey A. and Wiznerowicz, Maciej and Wolf, Stephan and Wong, Bernice H. and Wong, Tina and Wong, Winghing and Woo, Youngchoon and Wood, Scott and Wouters, Bradly G. and Wright, Adam J. and Wright, Derek W. and Wright, Mark H. and Wu, Chin-Lee and Wu, Dai-Ying and Wu, Guanming and Wu, Jianmin and Wu, Kui and Wu, Yang and Wu, Zhenggang and Xi, Liu and Xia, Tian and Xiang, Qian and Xiao, Xiao and Xing, Rui and Xiong, Heng and Xu, Qinying and Xu, Yanxun and Xue, Hong and Yachida, Shinichi and Yakneen, Sergei and Yamaguchi, Rui and Yamaguchi, Takafumi N. and Yamamoto, Masakazu and Yamamoto, Shogo and Yamaue, Hiroki and Yang, Fan and Yang, Huanming and Yang, Jean Y. and Yang, Liming and Yang, Lixing and Yang, Shanlin and Yang, Tsun-Po and Yang, Yang and Yao, Xiaotong and Yaspo, Marie-Laure and Yates, Lucy and Yau, Christina and Ye, Chen and Ye, Kai and Yellapantula, Venkata D. and Yoon, Christopher J. and Yoon, Sung-Soo and Yousif, Fouad and Yu, Jun and Yu, Kaixian and Yu, Willie and Yu, Yingyan and Yuan, Ke and Yuan, Yuan and Yuen, Denis and Yung, Christina K. and Zaikova, Olga and Zamora, Jorge and Zapatka, Marc and Zenklusen, Jean C. and Zenz, Thorsten and Zeps, Nikolajs and Zhang, Cheng-Zhong and Zhang, Fan and Zhang, Hailei and Zhang, Hongwei and Zhang, Hongxin and Zhang, Jiashan and Zhang, Jing and Zhang, Junjun and Zhang, Xiuqing and Zhang, Xuanping and Zhang, Yan and Zhang, Zemin and Zhao, Zhongming and Zheng, Liangtao and Zheng, Xiuqing and Zhou, Wanding and Zhou, Yong and Zhu, Bin and Zhu, Hongtu and Zhu, Jingchun and Zhu, Shida and Zou, Lihua and Zou, Xueqing and {deFazio}, Anna and Van As, Nicholas and Van Deurzen, Carolien H. M. and Van De Vijver, Marc J. and Van'T Veer, L. and Von Mering, Christian},
  year = {2020},
  month = feb,
  journal = {Nature},
  volume = {578},
  number = {7793},
  pages = {94--101},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-1943-3},
  urldate = {2024-04-12},
  abstract = {Abstract                            Somatic mutations in cancer genomes are caused by multiple mutational processes, each of which generates a characteristic mutational signature               1               . Here, as part of the Pan-Cancer Analysis of Whole Genomes (PCAWG) Consortium               2               of the International Cancer Genome Consortium (ICGC) and The Cancer Genome Atlas (TCGA), we characterized mutational signatures using 84,729,690~somatic mutations from 4,645~whole-genome and 19,184~exome sequences that encompass most types of cancer. We identified 49~single-base-substitution, 11~doublet-base-substitution, 4~clustered-base-substitution and 17~small insertion-and-deletion signatures. The substantial size of our dataset, compared with previous analyses               3--15               , enabled the discovery of new signatures, the separation of overlapping signatures and the decomposition of signatures into components that may represent associated---but distinct---DNA damage, repair and/or replication mechanisms. By estimating the contribution of each signature to the mutational catalogues of individual cancer genomes, we revealed associations of signatures to exogenous or endogenous exposures, as well as to defective DNA-maintenance processes. However, many signatures are of unknown cause. This analysis provides a systematic perspective on the repertoire of mutational processes that contribute to the development of human cancer.},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Alexandrov et al_2020_The repertoire of mutational signatures in human cancer.pdf}
}

@article{alickeCausationNormViolation2011,
  title = {Causation, {{Norm Violation}}, and {{Culpable Control}}},
  author = {Alicke, Mark D. and Rose, David and Bloom, Dori},
  year = {2011},
  month = nov,
  journal = {The Journal of Philosophy},
  volume = {108},
  number = {12},
  pages = {670--696},
  doi = {10.5840/jphil20111081238},
  urldate = {2020-05-04},
  keywords = {nosource}
}

@article{alkemaEstimatingTrendsTotal2012,
  title = {Estimating Trends in the Total Fertility Rate with Uncertainty Using Imperfect Data: {{Examples}} from {{West Africa}}},
  shorttitle = {Estimating Trends in the Total Fertility Rate with Uncertainty Using Imperfect Data},
  author = {Alkema, Leontine and Raftery, Adrian E. and Gerland, Patrick and Clark, Samuel J. and Pelletier, Fran{\c c}ois},
  year = {2012},
  month = apr,
  journal = {Demographic research},
  volume = {26},
  number = {15},
  pages = {10.4054/DemRes.2012.26.15},
  issn = {1435-9871},
  doi = {10.4054/DemRes.2012.26.15},
  urldate = {2023-02-20},
  abstract = {Background Estimating the total fertility rate is challenging for many developing countries because of limited data and varying data quality. A standardized, reproducible approach to produce estimates that include an uncertainty assessment is desired. Methods We develop a method to estimate and assess uncertainty in the total fertility rate over time, based on multiple imperfect observations from different data sources, including surveys and censuses. We take account of measurement error in observations by decomposing it into bias and variance, and assess both by linear regression on a variety of data quality covariates. We estimate the total fertility rate using a local smoother, and assess uncertainty using the weighted likelihood bootstrap. Results We apply our method to data from seven countries in West Africa and construct estimates and uncertainty intervals for the total fertility rate. Based on cross-validation exercises, we find that accounting for differences in data quality between observations gives better calibrated confidence intervals and reduces bias. Conclusions When working with multiple imperfect observations from different data sources to estimate the total fertility rate, or demographic indicators in general, potential biases and differences in error variance should be taken into account to improve the estimates and their uncertainty assessment.},
  pmcid = {PMC3837539},
  pmid = {24273449},
  file = {/home/gkonkamking/pCloudDrive/papers/Alkema et al_2012_Estimating trends in the total fertility rate with uncertainty using imperfect.pdf}
}

@article{alterEffectsFluencyPsychological2008,
  title = {Effects of {{Fluency}} on {{Psychological Distance}} and {{Mental Construal}} (or {{Why New York Is}} a {{Large City}}, but {{New York Is}} a {{Civilized Jungle}}):},
  shorttitle = {Effects of {{Fluency}} on {{Psychological Distance}} and {{Mental Construal}} (or {{Why New York Is}} a {{Large City}}, but {{New York Is}} a {{Civilized Jungle}})},
  author = {Alter, Adam L. and Oppenheimer, Daniel M.},
  year = {2008},
  month = feb,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {1467-9280},
  urldate = {2020-08-18},
  abstract = {People construe the world along a continuum from concretely (focusing on specific, local details) to abstractly (focusing on global essences). We show that peop...},
  copyright = {{\copyright} 2008 Association for Psychological Science},
  langid = {english},
  keywords = {nosource}
}

@article{alterOvercomingIntuitionMetacognitive2007,
  title = {Overcoming Intuition: {{Metacognitive}} Difficulty Activates Analytic Reasoning},
  shorttitle = {Overcoming Intuition},
  author = {Alter, Adam L. and Oppenheimer, Daniel M. and Epley, Nicholas and Eyre, Rebecca N.},
  year = {2007},
  journal = {Journal of Experimental Psychology: General},
  volume = {136},
  number = {4},
  pages = {569--576},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/0096-3445.136.4.569},
  abstract = {Humans appear to reason using two processing styles: System 1 processes that are quick, intuitive, and effortless and System 2 processes that are slow, analytical, and deliberate that occasionally correct the output of System 1. Four experiments suggest that System 2 processes are activated by metacognitive experiences of difficulty or disfluency during the process of reasoning. Incidental experiences of difficulty or disfluency--receiving information in a degraded font (Experiments 1 and 4), in difficult-to-read lettering (Experiment 2), or while furrowing one's brow (Experiment 3)--reduced the impact of heuristics and defaults in judgment (Experiments 1 and 3), reduced reliance on peripheral cues in persuasion (Experiment 2), and improved syllogistic reasoning (Experiment 4). Metacognitive experiences of difficulty or disfluency appear to serve as an alarm that activates analytic forms of reasoning that assess and sometimes correct the output of more intuitive forms of reasoning. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Processes,Cues,Judgment,Metacognition,nosource,Reasoning}
}

@article{altmejdPredictingReplicabilitySocial2019,
  title = {Predicting the Replicability of Social Science Lab Experiments},
  author = {Altmejd, Adam and Dreber, Anna and Forsell, Eskil and Huber, Juergen and Imai, Taisuke and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Camerer, Colin},
  editor = {Wicherts, Jelte M.},
  year = {2019},
  month = dec,
  journal = {PLOS ONE},
  volume = {14},
  number = {12},
  pages = {e0225826},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0225826},
  urldate = {2020-03-10},
  abstract = {We measure how accurately replication of experimental results can be predicted by blackbox statistical models. With data from four large-scale replication projects in experimental psychology and economics, and techniques from machine learning, we train predictive models and study which variables drive predictable replication. The models predicts binary replication with a cross-validated accuracy rate of 70\% (AUC of 0.77) and estimates of relative effect sizes with a Spearman {$\rho$} of 0.38. The accuracy level is similar to market-aggregated beliefs of peer scientists [1, 2]. The predictive power is validated in a pre-registered out of sample test of the outcome of [3], where 71\% (AUC of 0.73) of replications are predicted correctly and effect size correlations amount to {$\rho$} = 0.25. Basic features such as the sample and effect sizes in original papers, and whether reported effects are single-variable main effects or two-variable interactions, are predictive of successful replication. The models presented in this paper are simple tools to produce cheap, prognostic replicability metrics. These models could be useful in institutionalizing the process of evaluation of new findings and guiding resources to those direct replications that are likely to be most informative.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/KC7WYPTS/Altmejd et al. - 2019 - Predicting the replicability of social science lab.pdf}
}

@article{Altshuler1981,
  title = {Modeling of Dose-Response Relationships},
  author = {Altshuler, B.},
  year = {1981},
  month = dec,
  journal = {Environmental Health Perspectives},
  volume = {Vol. 42},
  number = {December},
  pages = {23--27},
  issn = {00916765},
  doi = {10.1289/ehp.814223},
  abstract = {The main dose-response models for chronic toxicity are considered. For dichotomous response, the log probit, multi-hit, and multistage models are presented. For time-to-occurrence response, the log-normal and three variations of multistage models are presented. Finally, the Cornfield hockey-stick model is considered, and, for low-dose extrapolation, it is suggested that response be taken to be proportional to dose and to a power of time determined by background response.},
  pmid = {7333256},
  keywords = {Animals,Dose-Response Relationship,Drug,Models,nosource,Statistics as Topic,Theoretical,Time Factors,Toxicology}
}

@article{amodio2008individual,
  title = {Individual Differences in the Regulation of Intergroup Bias: {{The}} Role of Conflict Monitoring and Neural Signals for Control.},
  author = {Amodio, David M and Devine, Patricia G and {Harmon-Jones}, Eddie},
  year = {2008},
  journal = {Journal of personality and social psychology},
  volume = {94},
  number = {1},
  pages = {60},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{amorim2010predicted,
  title = {Predicted No Effect Concentration ({{PNEC}}) for Triclosan to Terrestrial Species (Invertebrates and Plants)},
  author = {Amorim, M{\'o}nica J B and Oliveira, Eva and Soares, Amadeu M V M and {Scott-Fordsmand}, Janeck J},
  year = {2010},
  journal = {Environment international},
  volume = {36},
  number = {4},
  pages = {338--343},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{Anderson1952,
  title = {Asymptotic {{Theory}} of {{Certain}} "{{Goodness}} of {{Fit}}" {{Criteria Based}} on {{Stochastic Processes}}},
  author = {Anderson, T. W. and Darling, D. A.},
  year = {1952},
  journal = {The Annals of Mathematical Statistics},
  volume = {23},
  number = {2},
  eprint = {2236446},
  eprinttype = {jstor},
  pages = {193--212},
  issn = {0003-4851},
  urldate = {2020-01-15},
  abstract = {The statistical problem treated is that of testing the hypothesis that n independent, identically distributed random variables have a specified continuous distribution function F(x). If Fn(x) is the empirical cumulative distribution function and {$\psi$}(t) is some nonnegative weight function (0 {$\leq$} t {$\leq$} 1), we consider \$n{\textasciicircum}\{{\textbackslash}frac\{1\}\{2\}\} {\textbackslash}sup\_\{-{\textbackslash}infty\vphantom\}},
  keywords = {nosource}
}

@article{anderson1994model,
  title = {Model Comparisons and r 2},
  author = {{Anderson-Sprecher}, Richard},
  year = {1994},
  journal = {The American Statistician},
  volume = {48},
  number = {2},
  pages = {113--117},
  publisher = {Taylor \& Francis},
  keywords = {nosource}
}

@book{anderson2012optimal,
  title = {Optimal Filtering},
  author = {Anderson, Brian D O and Moore, John B},
  year = {2012},
  publisher = {Dover},
  address = {Mineola, New York},
  isbn = {0-486-43938-0},
  keywords = {nosource}
}

@article{andersonLocalLadderEffectSocial2012,
  title = {The {{Local-Ladder Effect}}: {{Social Status}} and {{Subjective Well-Being}}},
  shorttitle = {The {{Local-Ladder Effect}}},
  author = {Anderson, Cameron and Kraus, Michael W. and Galinsky, Adam D. and Keltner, Dacher},
  year = {2012},
  month = may,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  doi = {10.1177/0956797611434537},
  urldate = {2020-06-09},
  abstract = {Dozens of studies in different nations have revealed that socioeconomic status only weakly predicts an individual's subjective well-being (SWB). These results i...},
  langid = {english},
  keywords = {nosource}
}

@article{andrieuParticleMarkovChain2010,
  title = {Particle {{Markov}} Chain {{Monte Carlo}} Methods: {{Particle Markov Chain Monte Carlo Methods}}},
  shorttitle = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  year = {2010},
  month = jun,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {72},
  number = {3},
  pages = {269--342},
  issn = {13697412, 14679868},
  doi = {10.1111/j.1467-9868.2009.00736.x},
  urldate = {2020-09-03},
  abstract = {Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a L{\'e}vy-driven stochastic volatility model.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/J4IZ4BQQ/Andrieu et al. - 2010 - Particle Markov chain Monte Carlo methods Particl.pdf}
}

@article{andrieuTutorialAdaptiveMCMC2008,
  title = {A Tutorial on Adaptive {{MCMC}}},
  author = {Andrieu, Christophe and Thoms, Johannes},
  year = {2008},
  month = dec,
  journal = {Statistics and Computing},
  volume = {18},
  number = {4},
  pages = {343--373},
  issn = {1573-1375},
  doi = {10.1007/s11222-008-9110-y},
  urldate = {2020-11-08},
  abstract = {We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental properties are not satisfied. This leads to guidelines concerning the design of correct algorithms. We then review criteria and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria, but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove to be robust and reliable in practice. These algorithms are applied to artificial and high dimensional scenarios, but also to the classic mine disaster dataset inference problem.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/DZILSLWU/Andrieu and Thoms - 2008 - A tutorial on adaptive MCMC.pdf}
}

@article{angwinMachineBias,
  title = {Machine {{Bias}}},
  author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  journal = {ProPublica},
  urldate = {2023-10-18},
  abstract = {There's software used across the country to predict future criminals. And it's biased against blacks.},
  langid = {english}
}

@article{antoniak1974mixtures,
  title = {Mixtures of {{Dirichlet}} Processes with Applications to {{Bayesian}} Nonparametric Problems},
  author = {Antoniak, Charles E},
  year = {1974},
  journal = {The Annals of Statistics},
  pages = {1152--1174},
  publisher = {JSTOR},
  keywords = {nosource}
}

@article{aragamIdentifiabilityNonparametricMixture2020,
  title = {Identifiability of Nonparametric Mixture Models and {{Bayes}} Optimal Clustering},
  author = {Aragam, Bryon and Dan, Chen and Xing, Eric P. and Ravikumar, Pradeep},
  year = {2020},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {48},
  number = {4},
  pages = {2277--2302},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/19-AOS1887},
  urldate = {2024-01-19},
  abstract = {Motivated by problems in data clustering, we establish general conditions under which families of nonparametric mixture models are identifiable by introducing a novel framework involving clustering overfitted parametric (i.e., misspecified) mixture models. These identifiability conditions generalize existing conditions in the literature and are flexible enough to include, for example, mixtures of infinite Gaussian mixtures. In contrast to the recent literature, we allow for general nonparametric mixture components and instead impose regularity assumptions on the underlying mixing measure. As our primary application we apply these results to partition-based clustering, generalizing the notion of a Bayes optimal partition from classical parametric model-based clustering to nonparametric settings. Furthermore, this framework is constructive, so that it yields a practical algorithm for learning identified mixtures, which is illustrated through several examples on real data. The key conceptual device in the analysis is the convex, metric geometry of probability measures on metric spaces and its connection to the Wasserstein convergence of mixing measures. The result is a flexible framework for nonparametric clustering with formal consistency guarantees.},
  keywords = {62G05,62H12,62H30,Bayes optimal partition,clustering,Identifiability,Mixture models,nonparametric statistics},
  file = {/home/gkonkamking/pCloudDrive/papers/Aragam et al_2020_Identifiability of nonparametric mixture models and Bayes optimal clustering.pdf}
}

@article{arbel2013applied,
  title = {Ecotoxicological Data Study of Diversity Using a Dependent {{Bayesian}} Nonparametric Model},
  author = {Arbel, Julyan and Mengersen, Kerrie and Raymond, Ben and King, Catherine K},
  year = {2013},
  journal = {Manuscript under preparation},
  keywords = {nosource}
}

@article{arbel2013bayesian,
  title = {Bayesian Optimal Adaptive Estimation Using a Sieve Prior},
  author = {Arbel, Julyan and Gayraud, Ghislaine and Rousseau, Judith},
  year = {2013},
  journal = {Scandinavian Journal of Statistics},
  volume = {40},
  number = {3},
  eprint = {1204.2392v2},
  pages = {549--570},
  publisher = {Wiley Online Library},
  issn = {03036898},
  doi = {10.1002/sjos.12002},
  abstract = {We derive rates of contraction of posterior distributions on nonparametric models resulting from sieve priors. The aim of the paper is to provide general conditions to get posterior rates when the parameter space has a general structure, and rate adaptation when the parameter space is, e.g., a Sobolev class. The conditions employed, although standard in the literature, are combined in a novel way. The results are applied to density, regression, nonlinear autoregression and Gaussian white noise models. In the latter we have also considered a loss function which is different from the usual L2 norm, namely the pointwise loss. In this case it is possible to prove that the adaptive Bayesian approach for the L2 loss is strongly suboptimal and we provide a lower bound on the rate.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1204.2392v2},
  keywords = {Adaptation,Minimax criteria,Non-parametric models,nosource,Rate of contraction,Sieve prior,White noise model}
}

@article{arbel2013comment,
  title = {Comment on {\textbackslash}citet\{mullerbayesian\}},
  author = {Arbel, Julyan and Nipoti, Bernardo},
  year = {2013},
  journal = {Bayesian Analysis},
  volume = {8},
  number = {2},
  pages = {326--328},
  publisher = {Bayesian Analysis},
  keywords = {nosource}
}

@article{arbel2013methodo,
  title = {Bayesian Nonparametric Dependent Model for the Study of Diversity for Species Data},
  author = {Rousseau, J and Mengersen, Kerrie and Arbel, Julyan},
  year = {2014},
  journal = {Manuscript under preparation},
  eprint = {1402.3093v1},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1402.3093v1},
  keywords = {Bayesian nonparametrics,Covariate-dependent model,duplicate-citation-key,Gaussian processes,Griffiths-Engen-McCloskey distribution,nosource,Stick-breaking representation}
}

@article{arbel2013methodo,
  title = {Bayesian Nonparametric Dependent Models for the Study of Diversity in Species Data},
  author = {Arbel, Julyan and Mengersen, Kerrie and Rousseau, Judith},
  year = {2013},
  journal = {Manuscript under preparation},
  keywords = {duplicate-citation-key,nosource}
}

@phdthesis{arbel2013thesis,
  title = {Contributions {\`a} La Statistique {{Bay{\'e}sienne}} Non-Param{\'e}trique},
  author = {Arbel, Julyan},
  year = {2013},
  school = {Universit{\'e} Paris-Dauphine},
  keywords = {duplicate-citation-key,nosource}
}

@phdthesis{arbel2013thesis,
  title = {Contributions to {{Bayesian}} Nonparametric Statistics},
  author = {Arbel, Julyan},
  year = {2013},
  school = {Universit{\'e} Paris-Dauphine},
  keywords = {duplicate-citation-key,nosource}
}

@article{arbel2014full,
  title = {Full {{Bayesian}} Inference with Hazard Mixture Models},
  author = {Arbel, Julyan and Lijoi, Antonio and Nipoti, Bernardo},
  year = {2016},
  journal = {Computational Statistics \& Data Analysis},
  volume = {93},
  pages = {359--372},
  publisher = {Elsevier},
  issn = {01679473},
  doi = {10.1016/j.csda.2014.12.003},
  abstract = {Bayesian nonparametric inferential procedures based on Markov chain Monte Carlo marginal methods typically yield point estimates in the form of posterior expectations. Though very useful and easy to implement in a variety of statistical problems, these methods may suffer from some limitations if used to estimate non-linear functionals of the posterior distribution. The main goal is to develop a novel methodology that extends a well-established marginal procedure designed for hazard mixture models, in order to draw approximate inference on survival functions that is not limited to the posterior mean but includes, as remarkable examples, credible intervals and median survival time. The proposed approach relies on a characterization of the posterior moments that, in turn, is used to approximate the posterior distribution by means of a technique based on Jacobi polynomials. The inferential performance of this methodology is analyzed by means of an extensive study of simulated data and real data consisting of leukemia remission times. Although tailored to the survival analysis context, the proposed procedure can be adapted to a range of other models for which moments of the posterior distribution can be estimated.},
  keywords = {Bayesian nonparametrics,Completely random measures,Hazard mixture models,Median survival time,Moment-based approximations,nosource,Survival analysis}
}

@article{arbel2015application,
  title = {Application of a {{Bayesian}} Nonparametric Model to Derive Toxicity Estimates Based on the Response of {{Antarctic}} Microbial Communities to Fuel-Contaminated Soil},
  author = {Arbel, Julyan and King, Catherine K and Raymond, Ben and Winsley, Tristrom and Mengersen, Kerrie L},
  year = {2015},
  journal = {Ecology and evolution},
  volume = {5},
  number = {13},
  pages = {2633--2645},
  publisher = {Wiley Online Library},
  keywords = {nosource}
}

@article{arbel2016bayesian,
  title = {Bayesian Nonparametric Dependent Model for Partially Replicated Data: The Influence of Fuel Spills on Species Diversity},
  author = {Arbel, Julyan and Mengersen, Kerrie and Rousseau, Judith},
  year = {2016},
  journal = {The Annals of Applied Statistics},
  volume = {10},
  number = {3},
  pages = {1496--1516},
  publisher = {Institute of Mathematical Statistics},
  keywords = {nosource}
}

@article{arbel2017moment,
  title = {A Moment-Matching Ferguson \& Klass Algorithm},
  author = {Arbel, Julyan and Pr{\"u}nster, Igor},
  year = {2017},
  journal = {Statistics and Computing},
  volume = {27},
  number = {1},
  pages = {3--17},
  doi = {10.1007/s11222-016-9676-8},
  file = {/home/gkonkamking/Zotero/storage/3Q4EGZ8D/Arbel and Prünster - 2017 - A moment-matching ferguson & klass algorithm.pdf}
}

@article{arbelApproximatingPredictiveProbabilities2017,
  title = {Approximating Predictive Probabilities of {{Gibbs-type}} Priors},
  author = {Arbel, Julyan and Favaro, Stefano},
  year = {2017},
  month = jul,
  journal = {arXiv:1707.08053 [stat]},
  eprint = {1707.08053},
  primaryclass = {stat},
  urldate = {2020-03-18},
  abstract = {Gibbs-type random probability measures, or Gibbs-type priors, are arguably the most "natural" generalization of the celebrated Dirichlet prior. Among them the two parameter Poisson-Dirichlet prior certainly stands out for the mathematical tractability and interpretability of its predictive probabilities, which made it the natural candidate in several applications. Given a sample of size \$n\$, in this paper we show that the predictive probabilities of any Gibbs-type prior admit a large \$n\$ approximation, with an error term vanishing as \$o(1/n)\$, which maintains the same desirable features as the predictive probabilities of the two parameter Poisson-Dirichlet prior.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/gkonkamking/Zotero/storage/HDQELPFC/Arbel and Favaro - 2017 - Approximating predictive probabilities of Gibbs-ty.pdf}
}

@article{arbelBNPdensityBayesianNonparametric2021,
  title = {{{BNPdensity}}: {{Bayesian}} Nonparametric Mixture Modelling in {{R}}},
  author = {Arbel, Julyan and Kon Kam King, Guillaume and Lijoi, Antonio and {Nieto-Barajas}, Luis and Pr{\"u}nster, Igor},
  year = {2021},
  journal = {Australian \& New Zealand Journal of Statistics},
  volume = {63},
  number = {3},
  pages = {542--564},
  publisher = {Wiley Online Library},
  doi = {10.1111/anzs.12342},
  file = {/home/gkonkamking/Zotero/storage/JTV35B2D/Arbel et al. - 2021 - BNPdensity Bayesian nonparametric mixture modelli.pdf}
}

@article{arbelBNPdensityPackageBayesian,
  title = {{{BNPdensity}}: A Package for {{Bayesian Nonparametric}} Density Estimation Using {{Normalised Random Measures}} with {{Independent Increments}}.},
  author = {Arbel, Julyan and Kon Kam King, Guillaume and Lijoi, Antonio and {Nieto-barajas}, Luis Ernesto and Pr{\"u}nster, Igor},
  abstract = {Robust statistical data modeling under potential model mis-specification often requires leaving the parametric world for the nonparametric. In the latter, parameters are infinite dimensional objects such as functions, probability distributions or infinite vectors. In the Bayesian nonparametric approach, prior distributions are designed for these parameters, which provide a handle to manage the complexity of nonparametric models in practice. However, most modern Bayesian nonparametric models seem often out of reach to practitioners, as inference algorithms need careful design to deal with the infinite number of parameters. The aim of this work is to facilitate the journey by providing computational tools for inference. The article describes a set of functions available in the R package BNPdensity in order to carry out Bayesian nonparametric density estimation including all types of censored data. The package provides access to a large class of infinite mix- ture models based on normalised random measures, which form a generalization of the Dirichlet process mixture. One advantage of this generalization is that it offers much more flexibility for specifying the prior number of clusters than the Dirichlet. Another crucial advantage is the complete flexibility in specifying the prior for the scale and location parameters of the clusters, because conjugacy is not required. Inference is performed using a theoretically grounded approximate sampling methodology known as the Ferguson \& Klass algorithm. The package also offers several goodness of fit diagnostics such as QQ-plots, including a cross-validation criterion, the conditional predictive ordinate. The proposed methodology is illustrated on an extensive ecological risk assessment task called the Species Sensitivity Distribution (SSD) problem, showcasing the benefits of the Bayesian nonparametric framework.},
  keywords = {nosource}
}

@article{arbelStochasticApproximationsPitman2019,
  title = {Stochastic {{Approximations}} to the {{Pitman}}--{{Yor Process}}},
  author = {Arbel, Julyan and De Blasi, Pierpaolo and Pr{\"u}nster, Igor},
  year = {2019},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {14},
  number = {4},
  pages = {1201--1219},
  issn = {1936-0975},
  doi = {10.1214/18-BA1127},
  urldate = {2020-02-18},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/CT7UQ56Y/Arbel et al. - 2019 - Stochastic Approximations to the Pitman–Yor Proces.pdf}
}

@article{Archer:2013aa,
  title = {Bayesian Entropy Estimation for Countable Discrete Distributions},
  author = {Archer, Evan and Park, Il Memming and Pillow, Jonathan},
  year = {2013},
  eprint = {1302.0328v2},
  abstract = {We consider the problem of estimating Shannon's entropy \$H\$ from discrete data, in cases where the number of possible symbols is unknown or even countably infinite. The Pitman-Yor process, a generalization of Dirichlet process, provides a tractable prior distribution over the space of countably infinite discrete distributions, and has found major applications in Bayesian non-parametric statistics and machine learning. Here we show that it also provides a natural family of priors for Bayesian entropy estimation, due to the fact that moments of the induced posterior distribution over \$H\$ can be computed analytically. We derive formulas for the posterior mean (Bayes' least squares estimate) and variance under Dirichlet and Pitman-Yor process priors. Moreover, we show that a fixed Dirichlet or Pitman-Yor process prior implies a narrow prior distribution over \$H\$, meaning the prior strongly determines the entropy estimate in the under-sampled regime. We derive a family of continuous mixing measures such that the resulting mixture of Pitman-Yor processes produces an approximately flat prior over \$H\$. We show that the resulting Pitman-Yor Mixture (PYM) entropy estimator is consistent for a large class of distributions. We explore the theoretical properties of the resulting estimator, and show that it performs well both in simulation and in application to real data.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1302.0328v2},
  howpublished = {{\textbackslash}url\{http://arxiv.org/abs/1302.0328\}},
  isbn = {1532-4435},
  keywords = {bayesian estimation,bayesian nonparametrics,dirichlet,duplicate-citation-key,entropy,information theory,neural coding,nosource,pitman,process,yor process}
}

@article{Argiento2014,
  ids = {argientoEstimationPredictionInterpretation2014},
  title = {Estimation, Prediction and Interpretation of {{NGG}} Random Effects Models: {{An}} Application to {{Kevlar}} Fibre Failure Times},
  author = {Argiento, Raffaele and Guglielmi, Alessandra and Pievatolo, Antonio},
  year = {2014},
  journal = {Statistical Papers},
  volume = {55},
  number = {3},
  pages = {805--826},
  issn = {09325026},
  doi = {10.1007/s00362-013-0528-8},
  abstract = {We propose a class of Bayesian semiparametric mixed-effects models; its distinctive feature is the randomness of the grouping of observations, which can be inferred from the data. The model can be viewed under a more natural perspective, as a Bayesian semiparametric regression model on the log-scale; hence, in the original scale, the error is a mixture of Weibull densities mixed on both parameters by a normalized generalized gamma random measure, encompassing the Dirichlet process. As an estimate of the posterior distribution of the clustering of the random-effects parameters, we consider the partition minimizing the posterior expectation of a suitable class of loss functions. As a merely illustrative application of our model we consider a Kevlar fibre lifetime dataset (with censoring). We implement an MCMC scheme, obtaining posterior credibility intervals for the predictive distributions and for the quantiles of the failure times under different stress levels. Compared to a previous parametric Bayesian analysis, we obtain narrower credibility intervals and a better fit to the data. We found that there are three main clusters among the random-effects parameters, in accordance with previous frequentist analysis. {\copyright} 2013 Springer-Verlag Berlin Heidelberg.},
  keywords = {Bayesian nonparametrics,Clustering,Generalized linear mixed models,Hierarchical models,Mixture models,Nonparametric models},
  file = {/home/gkonkamking/Zotero/storage/E9639DS3/Argiento et al. - 2014 - Estimation, prediction and interpretation of NGG r.pdf}
}

@article{argiento2014bayesian,
  title = {A Priori Truncation Method for Posterior Sampling from Homogeneous {{NRMI}} Mixture Models},
  author = {Argiento, Raffaele and Bianchini, Ilaria and Guglielmi, Alessandra},
  year = {2015},
  journal = {Manuscript under preparation},
  keywords = {nosource}
}

@article{argiento2015blocked,
  title = {A Blocked {{Gibbs}} Sampler for {{NGG-mixture}} Models via a Priori Truncation},
  author = {Argiento, Raffaele and Bianchini, Ilaria and Guglielmi, Alessandra},
  year = {2015},
  journal = {Statistics and Computing},
  issn = {09603174},
  doi = {10.1007/s11222-015-9549-6},
  keywords = {A priori truncation method,Bayesian nonparametric mixture models,Blocked Gibbs sampler,Finite dimensional approximation,Normalized generalized gamma process,nosource}
}

@article{argientoSemiparametricBayesianGeneralized2013,
  title = {A Semiparametric {{Bayesian}} Generalized Linear Mixed Model for the Reliability of {{Kevlar}} Fibers},
  author = {Argiento, Raffaele and Guglielmi, Alessandra and Soriano, Jacopo},
  year = {2013},
  journal = {Applied Stochastic Models in Business and Industry},
  volume = {29},
  number = {5},
  pages = {410--423},
  issn = {1526-4025},
  doi = {10.1002/asmb.1936},
  urldate = {2020-03-13},
  abstract = {We analyze the reliability of NASA composite pressure vessels by using a new Bayesian semiparametric model. The data set consists of lifetimes of pressure vessels, wrapped with a Kevlar fiber, grouped by spool, subject to different stress levels; 10\% of the data are right censored. The model that we consider is a regression on the log-scale for the lifetimes, with fixed (stress) and random (spool) effects. The prior of the spool parameters is nonparametric, namely they are a sample from a normalized generalized gamma process, which encompasses the well-known Dirichlet process. The nonparametric prior is assumed to robustify inferences to misspecification of the parametric prior. Here, this choice of likelihood and prior yields a new Bayesian model in reliability analysis. Via a Bayesian hierarchical approach, it is easy to analyze the reliability of the Kevlar fiber by predicting quantiles of the failure time when a new spool is selected at random from the population of spools. Moreover, for comparative purposes, we review the most interesting frequentist and Bayesian models analyzing this data set. Our credibility intervals of the quantiles of interest for a new random spool are narrower than those derived by previous Bayesian parametric literature, although the predictive goodness-of-fit performances are similar. Finally, as an original feature of our model, by means of the discreteness of the random-effects distribution, we are able to cluster the spools into three different groups. Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {accelerated failure time regression model,Bayesian clustering,Bayesian nonparametrics,nosource,random-effects model,reliability}
}

@article{arias-castroEstimationLatentDistances2021,
  title = {On the Estimation of Latent Distances Using Graph Distances},
  author = {{Arias-Castro}, Ery and Channarond, Antoine and Pelletier, Bruno and Verzelen, Nicolas},
  year = {2021},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {15},
  number = {1},
  pages = {722--747},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1935-7524, 1935-7524},
  doi = {10.1214/21-EJS1801},
  urldate = {2021-05-19},
  abstract = {We are given the adjacency matrix of a geometric graph and the task of recovering the latent positions. We study one of the most popular approaches which consists in using the graph distances and derive error bounds under various assumptions on the link function. In the simplest case where the link function is proportional to an indicator function, the bound matches an information lower bound that we derive.},
  keywords = {graph distances,graph embedding,Latent positions,multidimensional scaling,Random geometric graphs},
  file = {/home/gkonkamking/Zotero/storage/TDY4PKHK/Arias-Castro et al. - 2021 - On the estimation of latent distances using graph .pdf}
}

@techreport{armcanz2000australian,
  title = {Australian and {{New Zealand}} Guidelines for Fresh and Marine Water Quality},
  author = {{ANZECC}},
  year = {2000},
  volume = {4},
  address = {Canberra, Australia},
  institution = {{Australian and New Zealand Environmental and Conservation Council Agriculture and Resource Management Council of Australia and New Zealand}},
  abstract = {risk assessment. Washington, DC. Australian and New Zealand Environmental and Conservation Council and Agriculture and Resource Management Council of Australia and New Zealand. 2000. Australian and New Zealand guidelines for fresh and marine water quality. Canberra, Australia.},
  keywords = {nosource}
}

@incollection{ArmitagePeterandColton2005,
  title = {Maximum Likelihood},
  booktitle = {Encyclopedia of Biostatistics},
  author = {{Armitage, Peter {and} Colton}, Theodore},
  year = {2005},
  edition = {2},
  pages = {3058--3059},
  publisher = {John Wiley \& Sons},
  isbn = {0-470-84907-X},
  keywords = {nosource}
}

@article{arnaudon2018duality,
  title = {A Duality Formula and a Particle {{Gibbs}} Sampler for Continuous Time {{Feynman-Kac}} Measures on Path Spaces},
  author = {Arnaudon, Marc and Del Moral, Pierre},
  year = {2018},
  journal = {arXiv preprint arXiv:1805.05044},
  eprint = {1805.05044},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@article{Artigas2012,
  title = {Towards a Renewed Research Agenda in Ecotoxicology},
  author = {Artigas, J and Arts, G. H P and Babut, Marc},
  year = {2012},
  month = jan,
  journal = {Environmental {\dots}},
  volume = {160},
  number = {1},
  pages = {201--206},
  address = {Cemagref, UR MALY, 3 Quai Chauveau, F-69336 Lyon Cedex 09, France.},
  issn = {1873-6424},
  abstract = {New concerns about biodiversity, ecosystem services and human health triggered several new regulations increasing the need for sound ecotoxicological risk assessment. The PEER network aims to share its view on the research issues that this challenges. PEER scientists call for an improved biologically relevant exposure assessment. They promote comprehensive effect assessment at several biological levels. Biological traits should be used for Environmental risk assessment (ERA) as promising tools to better understand relationships between structure and functioning of ecosystems. The use of modern high throughput methods could also enhance the amount of data for a better risk assessment. Improved models coping with multiple stressors or biological levels are necessary to answer for a more scientifically based risk assessment. Those methods must be embedded within life cycle analysis or economical models for efficient regulations. Joint research programmes involving humanities with ecological sciences should be developed for a sound risk management.},
  keywords = {Ecotoxicology,nosource,Research policy,Risk assessment,Stress ecology}
}

@article{ascasibar2005numerical,
  title = {Numerical Estimation of Densities},
  author = {Ascasibar, Y. and Binney, J.},
  year = {2005},
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {356},
  number = {3},
  pages = {872--882},
  publisher = {Oxford University Press},
  issn = {00358711},
  doi = {10.1111/j.1365-2966.2004.08480.x},
  abstract = {We present a novel technique, dubbed FIESTAS, to estimate the underlying density field from a discrete set of sample points in an arbitrary multidimensional space. FIESTAS assigns a volume to each point by means of a binary tree. Density is then computed by integrating over an adaptive kernel. As a first test, we construct several Monte Carlo realizations of a Hernquist profile and recover the particle density in both real and phase space. At a given point, Poisson noise causes the unsmoothed estimates to fluctuate by a factor of{\~ }2 regardless of the number of particles. This spread can be reduced to about 1dex {\~(}26 per cent) by our smoothing procedure. The density range over which the estimates are unbiased widens as the particle number increases. Our tests show that real-space densities obtained with an SPH kernel are significantly more biased than those yielded by FIESTAS. In phase space, about 10 times more particles are required in order to achieve a similar accuracy. As a second application we have estimated phase-space densities in a dark matter halo from a cosmological simulation. We confirm the results of Arad, Dekel \& Klypin that the highest values of f are all associated with substructure rather than the main halo, and that the volume function v(f){\~ }f-2.5 over about four orders of magnitude in f. We show that a modified version of the toy model proposed by Arad et al. explains this result and suggests that the departures of v(f) from power-law form are not mere numerical artefacts. We conclude that our algorithm accurately measures the phase-space density up to the limit where discreteness effects render the simulation itself unreliable. Computationally, FIESTAS is orders of magnitude faster than the method based on Delaunay tessellation that Arad et al. employed, making it practicable to recover smoothed density estimates for sets of 109 points in six dimensions.},
  arxiv = {0409233v1 [arXiv:astro-ph]},
  arxivid = {arXiv:astro-ph/0409233v1},
  keywords = {Dark matter,Galaxies: haloes,Galaxies: kinematics and dynamics,Methods: data analysis,Methods: numerical,nosource}
}

@article{ascolaniClusteringConsistencyDirichlet2023,
  title = {Clustering Consistency with {{Dirichlet}} Process Mixtures},
  author = {Ascolani, Filippo and Lijoi, Antonio and Rebaudo, Giovanni and Zanella, Giacomo},
  year = {2023},
  journal = {Biometrika},
  volume = {110},
  number = {2},
  eprint = {2205.12924},
  primaryclass = {math, stat},
  pages = {551--558},
  urldate = {2022-08-05},
  abstract = {Dirichlet process mixtures are flexible non-parametric models, particularly suited to density estimation and probabilistic clustering. In this work we study the posterior distribution induced by Dirichlet process mixtures as the sample size increases, and more specifically focus on consistency for the unknown number of clusters when the observed data are generated from a finite mixture. Crucially, we consider the situation where a prior is placed on the concentration parameter of the underlying Dirichlet process. Previous findings in the literature suggest that Dirichlet process mixtures are typically not consistent for the number of clusters if the concentration parameter is held fixed and data come from a finite mixture. Here we show that consistency for the number of clusters can be achieved if the concentration parameter is adapted in a fully Bayesian way, as commonly done in practice. Our results are derived for data coming from a class of finite mixtures, with mild assumptions on the prior for the concentration parameter and for a variety of choices of likelihood kernels for the mixture.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/gkonkamking/pCloudDrive/papers/Ascolani et al_2022_Clustering consistency with Dirichlet process mixtures.pdf;/home/gkonkamking/Zotero/storage/Q5CUHTUV/MainFinal.pdf;/home/gkonkamking/Zotero/storage/WBY4NLV4/SuppFinal.pdf}
}

@article{ascolaniPredictiveInferenceFleming2021,
  title = {Predictive Inference with {{Fleming}}--{{Viot-driven}} Dependent {{Dirichlet}} Processes},
  author = {Ascolani, Filippo and Lijoi, Antonio and Ruggiero, Matteo},
  year = {2021},
  journal = {Bayesian Analysis},
  volume = {16},
  number = {2},
  pages = {371--395},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1206},
  urldate = {2020-05-21},
  abstract = {We consider predictive inference using a class of temporally dependent Dirichlet processes driven by Fleming--Viot diffusions, which have a natural bearing in Bayesian nonparametrics and lend the resulting family of random probability measures to analytical posterior analysis. Formulating the implied statistical model as a hidden Markov model, we fully describe the predictive distribution induced by these Fleming--Viot-driven dependent Dirichlet processes, for a sequence of observations collected at a certain time given another set of draws collected at several previous times. This is identified as a mixture of P{\'o}lya urns, whereby the observations can be values from the baseline distribution or copies of previous draws collected at the same time as in the usual P{\'o}lya urn, or can be sampled from a random subset of the data collected at previous times. We characterize the time-dependent weights of the mixture which select such subsets and discuss the asymptotic regimes. We describe the induced partition by means of a Chinese restaurant process metaphor with a conveyor belt, whereby new customers who do not sit at an occupied table open a new table by picking a dish either from the baseline distribution or from a time-varying offer available on the conveyor belt. We lay out explicit algorithms for exact and approximate posterior sampling of both observations and partitions, and illustrate our results on predictive problems with synthetic and real data.},
  langid = {english},
  keywords = {Chinese restaurant,conveyor belt,generalized P{\'o}lya urn,hidden Markov model,predictive distribution,random partition},
  file = {/home/gkonkamking/Zotero/storage/X29FH55L/Ascolani et al. - 2020 - Predictive inference with Fleming–Viot-driven depe.pdf}
}

@article{ascolaniSmoothingDistributionsConditional2022,
  title = {Smoothing Distributions for Conditional {{Fleming-Viot}} and {{Dawson-Watanabe}} Diffusions},
  author = {Ascolani, Filippo and Lijoi, Antonio and Ruggiero, Matteo},
  year = {2022},
  journal = {Bernoulli},
  volume = {to appear},
  keywords = {nosource}
}

@article{Ashauer2006,
  title = {Predicting Effects on Aquatic Organisms from Fluctuating or Pulsed Exposure to Pesticides},
  author = {Shauer, R Oman A and Oxall, A Listair B and Rown, C Olin B},
  year = {2006},
  journal = {Environmental Toxicology},
  volume = {25},
  number = {7},
  pages = {1899--1912},
  issn = {0730-7268},
  doi = {10.1897/05-393R.1},
  abstract = {Exposure of aquatic nontarget organisms to pesticides almost always occurs as pulses or fluctuating concentrations. Extrapolation from laboratory to field thus depends on an understanding and ability to simulate effects resulting from these types of exposure. This paper reviews models that may be used to predict effects on aquatic organisms resulting from time-varying exposure to pesticides. We evaluate and compare the theoretical basis of these models and their applicability to the simulation of effects from fluctuating exposures. The many different models rest on only a few basic concepts with differing degrees of mechanistic character. Building on this critical review, we select the most appropriate models and propose modifications. Two process-based models, the threshold hazard model and the modified damage assessment model, represent the optimum descriptions that are available at present. They could facilitate a better understanding of the ecotoxicity of different compound and species combinations and even mixtures of noninteracting compounds. The possibility to model lethal and sublethal effects allows applications in risk assessment, standard setting, and ecological modeling.},
  pmid = {16833153},
  keywords = {nosource,toxicity}
}

@article{Ashauer2007,
  title = {New Ecotoxicological Model to Simulate Survival of Aquatic Invertebrates after Exposure to Fluctuating and Sequential Pulses of Pesticides},
  author = {Ashauer, Roman and Boxall, Alistair B A and Brown, Colin D.},
  year = {2007},
  journal = {Environmental Science and Technology},
  volume = {41},
  number = {4},
  pages = {1480--1486},
  issn = {0013936X},
  doi = {10.1021/es061727b},
  abstract = {Aquatic nontarget organisms are exposed to fluctuating concentrations or sequential pulses of contaminants, so we need to predict effects resulting from such patterns of exposure. We present a process-based model, the Threshold Damage Model (TDM), that links exposure with effects and demonstrate how to simulate the survival of the aquatic invertebrate Gammarus pulex. Based on survival experiments of up to 28 days duration with three patterns of repeated exposure pulses and fluctuating concentrations of two pesticides with contrasting modes of action (pentachlorophenol and chlorpyrifos) we evaluate the new model and compare it to two approaches based on time-weighted averages. Two models, the Threshold Damage Model and the time-weighted averages fitted to pulses, are able to simulate the observed survival (mean errors 15\% or less, r2 between 0.77 and 0.96). The models are discussed with respect to their theoretical base, data needs, and potential for extrapolation to different scenarios. The Threshold Damage Model is particularly useful because its parameters can be used to calculate recovery times, toxicokinetics are separated from toxicodynamics, and parameter values reflect the mode of action.},
  isbn = {0013-936X (Print){\textbackslash}r0013-936X (Linking)},
  pmid = {17593760},
  keywords = {nosource}
}

@article{ashauer2008toxicodynamic,
  title = {Toxicodynamic Assumptions in Ecotoxicological Hazard Models},
  author = {Ashauer, Roman and Brown, Colin D.},
  year = {2009},
  journal = {Environmental toxicology and chemistry},
  volume = {27},
  number = {8},
  pages = {1817--1821},
  issn = {0730-7268},
  doi = {10.1897/07-642.1},
  abstract = {Existing toxicokinetic and toxicodynamic models and dynamic formulations of popular ecotoxicological concepts (e.g., the critical body residue concept) are examined. Their underlying assumptions about speed of recovery and thresholds are clarified, and a rigorous mathematical treatment shows that they can all be placed within a unifying framework. Such analysis aids in the selection of appropriate ecotoxicological models},
  isbn = {0730-7268},
  pmid = {18318593},
  keywords = {Hazard models Risk assessment Tissue residue Toxic,nosource}
}

@article{Ashauer2010,
  title = {Toxicokinetic and Toxicodynamic Modeling Explains Carry-over Toxicity from Exposure to Diazinon by Slow Organism Recovery},
  author = {Ashauer, Roman and Hintermeister, Anita and Caravatti, Ivo and Kretschmann, Andreas and Escher, Beate I.},
  year = {2010},
  journal = {Environmental Science and Technology},
  volume = {44},
  number = {10},
  pages = {3963--3971},
  issn = {0013936X},
  doi = {10.1021/es903478b},
  abstract = {Carry-over toxicity occurs when organisms exposed to an environmental toxicant survive but carry some damage resulting in reduced fitness. Upon subsequently encountering another exposure event stronger effects are possible if the organisms have not yet fully recovered. Carry-over toxicity was observed after exposure of the freshwater amphipod Gammarus pulex to repeated pulses of diazinon with varying intervals. Uptake, biotransformation and depuration kinetics were determined. Metabolites were identified and quantified (diazoxon, 2-isopropyl-6-methyl-4-pyrimidinol, one nonidentified metabolite). Parameters of a process-based toxicokinetic-toxicodynamic model were determined by least-squares fitting followed by Markov Chain Monte Carlo parameter estimation. Model parametrization was based on the time-course of measured internal concentrations of diazinon and its metabolite diazoxon in combination with the pulsed toxicity experiment. Prediction intervals, which take the covariation between parameters into account, were calculated for bioaccumulation factors, organism recovery time and simulations of internal concentrations as well as the time-course of survival under variable exposure. Organism recovery time was 28 days (95\% prediction interval 25-31 days), indicating the possibility for carry-over toxicity from exposure events several weeks apart. The slow organism recovery and carry-over toxicity was caused by slow toxicodynamic recovery; toxicokinetic processes alone would have resulted in a recovery time of only 1-2 days.},
  isbn = {0013-936X},
  pmid = {20397634},
  keywords = {nosource}
}

@article{Ashauer2011,
  title = {Environmental Risk Assessment of Fluctuating Diazinon Concentrations in an Urban and Agricultural Catchment Using Toxicokinetic-Toxicodynamic Modeling},
  author = {Ashauer, Roman and Wittmer, Irene and Stamm, Christian and Escher, Beate I.},
  year = {2011},
  journal = {Environmental Science and Technology},
  volume = {45},
  number = {22},
  pages = {9783--9792},
  issn = {0013936X},
  doi = {10.1021/es202413a},
  abstract = {Temporally resolved environmental risk assessment of fluctuating concentrations of micropollutants is presented. We separated the prediction of toxicity over time from the extrapolation from one to many species and from acute to sublethal effects. A toxicokinetic?toxicodynamic (TKTD) model predicted toxicity caused by fluctuating concentrations of diazinon, measured by time-resolved sampling over 108 days from three locations in a stream network, representing urban, agricultural and mixed land use. We calculated extrapolation factors to quantify variation in toxicity among species and effect types based on available toxicity data, while correcting for different test durations with the TKTD model. Sampling from the distribution of extrapolation factors and prediction of time-resolved toxicity with the TKTD model facilitated subsequent calculation of the risk of undesired toxic events. Approximately one-fifth of aquatic organisms were at risk and fluctuating concentrations were more toxic than their averages. Contribution of urban and agricultural sources of diazinon to the overall risk varied. Thus using fixed concentrations as water quality criteria appears overly simplistic because it ignores the temporal dimension of toxicity. However, the improved prediction of toxicity for fluctuating concentrations may be small compared to uncertainty due to limited diversity of toxicity data to base the extrapolation factors on.{\textbackslash}nTemporally resolved environmental risk assessment of fluctuating concentrations of micropollutants is presented. We separated the prediction of toxicity over time from the extrapolation from one to many species and from acute to sublethal effects. A toxicokinetic?toxicodynamic (TKTD) model predicted toxicity caused by fluctuating concentrations of diazinon, measured by time-resolved sampling over 108 days from three locations in a stream network, representing urban, agricultural and mixed land use. We calculated extrapolation factors to quantify variation in toxicity among species and effect types based on available toxicity data, while correcting for different test durations with the TKTD model. Sampling from the distribution of extrapolation factors and prediction of time-resolved toxicity with the TKTD model facilitated subsequent calculation of the risk of undesired toxic events. Approximately one-fifth of aquatic organisms were at risk and fluctuating concentrations were more toxic than their averages. Contribution of urban and agricultural sources of diazinon to the overall risk varied. Thus using fixed concentrations as water quality criteria appears overly simplistic because it ignores the temporal dimension of toxicity. However, the improved prediction of toxicity for fluctuating concentrations may be small compared to uncertainty due to limited diversity of toxicity data to base the extrapolation factors on.},
  isbn = {0013-936X},
  pmid = {21958042},
  keywords = {nosource}
}

@article{Ashauer2011a,
  title = {Toxicokinetic-Toxicodynamic Modeling of Quantal and Graded Sublethal Endpoints: {{A}} Brief Discussion of Concepts},
  author = {Ashauer, Roman and Agatz, Annika and Albert, Carlo and Ducrot, Virginie and Galic, Nika and Hendriks, Jan and Jager, Tjalling and Kretschmann, Andreas and O'Connor, Isabel and Rubach, Mascha Nadine and Nyman, Anna Maija and Schmitt, Walter and Stadnicka, Julita and {van den Brink}, Paul J. and Preuss, Thomas G.},
  year = {2011},
  journal = {Environmental Toxicology and Chemistry},
  volume = {30},
  number = {11},
  pages = {2519--2524},
  issn = {07307268},
  doi = {10.1002/etc.639},
  abstract = {We report on the advantages and problems of using toxicokinetic-toxicodynamic (TKTD) models for the analysis, understanding, and simulation of sublethal effects. Only a few toxicodynamic approaches for sublethal effects are available. These differ in their effect mechanism and emphasis on linkages between endpoints. We discuss how the distinction between quantal and graded endpoints and the type of linkage between endpoints can guide model design and selection. Strengths and limitations of two main approaches and possible ways forward are outlined.},
  isbn = {0730-7268},
  pmid = {21805502},
  keywords = {Adverse outcome pathway,Dose-response,Dynamic energy budget,Mechanistic effect model,nosource,Toxicity}
}

@article{Ashauer2013,
  title = {A Method to Predict and Understand Fish Survival under Dynamic Chemical Stress Using Standard Ecotoxicity Data},
  author = {Ashauer, Roman and Thorbek, Pernille and Warinton, Jacqui S. and Wheeler, James R. and Maund, Steve},
  year = {2013},
  journal = {Environmental Toxicology and Chemistry},
  volume = {32},
  number = {4},
  pages = {954--965},
  issn = {07307268},
  doi = {10.1002/etc.2144},
  abstract = {The authors present a method to predict fish survival under exposure to fluctuating concentrations and repeated pulses of a chemical stressor. The method is based on toxicokinetic-toxicodynamic modeling using the general unified threshold model of survival (GUTS) and calibrated using raw data from standard fish acute toxicity tests. The model was validated by predicting fry survival in a fish early life stage test. Application of the model was demonstrated by using Forum for Co-ordination of Pesticide Fate Models and Their Use surface water (FOCUS-SW) exposure patterns as model input and predicting the survival of fish over 485 d. Exposure patterns were also multiplied by factors of five and 10 to achieve higher exposure concentrations for fish survival predictions. Furthermore, the authors quantified how far the exposure profiles were below the onset of mortality by finding the corresponding exposure multiplication factor for each scenario. The authors calculated organism recovery times as additional characteristic of toxicity as well as number of peaks, interval length between peaks, and mean duration as additional characteristics of the exposure pattern. The authors also calculated which of the exposure patterns had the smallest and largest inherent potential toxicity. Sensitivity of the model to parameter changes depends on the exposure pattern and differs between GUTS individual tolerance and GUTS stochastic death. Possible uses of the additional information gained from modeling to inform risk assessment are discussed.},
  isbn = {1552-8618},
  pmid = {23365017},
  keywords = {Aquatic toxicity,Carry-over toxicity,Dose-response model,nosource,Pesticide fate model,Time-variable exposure}
}

@article{Ashauer2016,
  title = {Modelling Survival: Exposure Pattern, Species Sensitivity and Uncertainty},
  author = {Ashauer, Roman and Albert, Carlo and Augustine, Starrlight and Cedergreen, Nina and Charles, Sandrine and Ducrot, Virginie and Focks, Andreas and Gabsi, Faten and Gergs, Andr{\'e} and Goussen, Benoit and Jager, Tjalling and Kramer, Nynke I. and Nyman, Anna-Maija and Poulsen, Veronique and Reichenberger, Stefan and Sch{\"a}fer, Ralf B. and {Van den Brink}, Paul J. and Veltman, Karin and Vogel, S{\"o}ren and Zimmer, Elke I. and Preuss, Thomas G.},
  year = {2016},
  journal = {Scientific Reports},
  volume = {6},
  pages = {29178},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/srep29178},
  keywords = {nosource}
}

@article{ashauerAdvantagesToxicokineticToxicodynamic2010,
  title = {Advantages of Toxicokinetic and Toxicodynamic Modelling in Aquatic Ecotoxicology and Risk Assessment},
  author = {Ashauer, Roman and Escher, Beate I.},
  year = {2010},
  journal = {Journal of Environmental Monitoring},
  volume = {12},
  number = {11},
  pages = {2056},
  issn = {1464-0325, 1464-0333},
  doi = {10.1039/c0em00234h},
  urldate = {2021-11-17},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/G7CHS2RG/Ashauer and Escher - 2010 - Advantages of toxicokinetic and toxicodynamic mode.pdf}
}

@misc{assembly2015sustainable,
  title = {Transforming Our World: The 2030 Agenda for Sustainable Development},
  author = {United Nations},
  year = {2015},
  publisher = {United Nations, New York},
  keywords = {nosource}
}

@article{australianpancreaticcancergenomeinitiativeSignaturesMutationalProcesses2013,
  title = {Signatures of Mutational Processes in Human Cancer},
  author = {{Australian Pancreatic Cancer Genome Initiative} and {ICGC Breast Cancer Consortium} and {ICGC MMML-Seq Consortium} and {ICGC PedBrain} and Alexandrov, Ludmil B. and {Nik-Zainal}, Serena and Wedge, David C. and Aparicio, Samuel A. J. R. and Behjati, Sam and Biankin, Andrew V. and Bignell, Graham R. and Bolli, Niccol{\`o} and Borg, Ake and {B{\o}rresen-Dale}, Anne-Lise and Boyault, Sandrine and Burkhardt, Birgit and Butler, Adam P. and Caldas, Carlos and Davies, Helen R. and Desmedt, Christine and Eils, Roland and Eyfj{\"o}rd, J{\'o}runn Erla and Foekens, John A. and Greaves, Mel and Hosoda, Fumie and Hutter, Barbara and Ilicic, Tomislav and Imbeaud, Sandrine and Imielinski, Marcin and J{\"a}ger, Natalie and Jones, David T. W. and Jones, David and Knappskog, Stian and Kool, Marcel and Lakhani, Sunil R. and {L{\'o}pez-Ot{\'i}n}, Carlos and Martin, Sancha and Munshi, Nikhil C. and Nakamura, Hiromi and Northcott, Paul A. and Pajic, Marina and Papaemmanuil, Elli and Paradiso, Angelo and Pearson, John V. and Puente, Xose S. and Raine, Keiran and Ramakrishna, Manasa and Richardson, Andrea L. and Richter, Julia and Rosenstiel, Philip and Schlesner, Matthias and Schumacher, Ton N. and Span, Paul N. and Teague, Jon W. and Totoki, Yasushi and Tutt, Andrew N. J. and {Vald{\'e}s-Mas}, Rafael and {van Buuren}, Marit M. and {van 't Veer}, Laura and {Vincent-Salomon}, Anne and Waddell, Nicola and Yates, Lucy R. and {Zucman-Rossi}, Jessica and Andrew Futreal, P. and McDermott, Ultan and Lichter, Peter and Meyerson, Matthew and Grimmond, Sean M. and Siebert, Reiner and Campo, El{\'i}as and Shibata, Tatsuhiro and Pfister, Stefan M. and Campbell, Peter J. and Stratton, Michael R.},
  year = {2013},
  month = aug,
  journal = {Nature},
  volume = {500},
  number = {7463},
  pages = {415--421},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature12477},
  urldate = {2021-03-31},
  langid = {english}
}

@article{Awkerman2008,
  title = {Development of Species Sensitivity Distributions for Wildlife Using Interspecies Toxicity Correlation Models.},
  author = {a Awkerman, Jill and Raimondo, Sandy and Barron, Mace G},
  year = {2008},
  month = may,
  journal = {Environmental science \& technology},
  volume = {42},
  number = {9},
  eprint = {18522132},
  eprinttype = {pubmed},
  pages = {3447--52},
  issn = {0013-936X},
  abstract = {Species sensitivity distributions (SSD) are probability distributions of chemical toxicity of multiple species and have had limited application in wildlife risk assessment because of relatively small data sets of wildlife toxicity values. Interspecies correlation estimation (ICE) models predict the acute toxicity to untested taxa from known toxicity of a single surrogate species. ICE models were used to predict toxicity values to wildlife species and generate SSDs for 23 chemicals using four avian surrogates. The hazard levels associated with the fifth percentile of the distribution (HD5) were compared for ICE SSDs and independent SSDs created with measured data. SSDs were composed of either avian only or avian and mammalian taxa. ICE HD5s were within 5-fold of 90\% of measured HD5s and were generally higher than measured HD5s. The first percentile of the distribution (HD1) and the fifth percentile of the lower confidence limit (HDL) of ICE SSDs produced values that were not significantly different from measured HD5s. Using a bird surrogate to predicttoxicity to birds and the Norway rat to predict toxicity to mammals improved some estimates of ICE HD5s compared with those generated using only bird surrogates. These results indicate that ICE models can be used to generate SSDs comparable to those derived from measured wildlife toxicity data and provide robust estimates of the HD5.},
  isbn = {0013-936X},
  pmid = {18522132},
  keywords = {Animals,Birds,Databases,duplicate-citation-key,Ecosystem,Environmental Monitoring,Environmental Monitoring: methods,Environmental Pollutants,Environmental Pollutants: analysis,Factual,Models,nosource,Probability,Rats,Risk Assessment,Sample Size,Sensitivity and Specificity,Species Specificity,Theoretical,Water Pollutants,Water Pollutants: analysis}
}

@article{Baas2009,
  title = {Estimation of No Effect Concentrations from Exposure Experiments When Values Scatter among Individuals},
  author = {Baas, Jan and Jager, T. and Kooijman, S. A L M},
  year = {2009},
  journal = {Ecological Modelling},
  volume = {220},
  number = {3},
  pages = {411--418},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2008.10.008},
  abstract = {The parameters that are most commonly used in risk assessment, LCx values or no observed effect concentrations, both have serious drawbacks. As an alternative, No effect concentrations (NEC) as a parameter in a process-based model, offer great potential in risk assessment. So far estimates of the NEC assume that all individuals have the same NEC, but it is to be expected that organism differ in their physiology and therefore individuals in a cohort do not all have the same value for the NEC. We investigated how much variation in the NEC is allowed before an estimate of a NEC from a survival experiment fails. We therefore assumed that each individual organism has its own NEC, drawn independently from a log-normal distribution around a mean NEC. In addition we also investigated if the standard deviation in the log-normal distribution itself could be estimated from a survival experiment. It showed that for a wide range of individual differences in the NEC the estimates of the NEC are accurate and precise. Only if the differences between individuals become much larger than what could be derived from survival experiments reported in the open literature the estimated NEC becomes unreliable. The standard deviation in the log-normal distribution of the NEC can also be estimated but with a high uncertainty. When a standard model is used where all exposed individuals have the same NEC on data where there is a different NEC for individuals, the NEC can still be estimated with high accuracy and precision. ?? 2008 Elsevier B.V. All rights reserved.},
  keywords = {Individual tolerance distribution,Monte Carlo simulation,No effect concentration,nosource,Survival,Toxicity}
}

@article{Baas2015,
  title = {Sensitivity of Animals to Chemical Compounds Links to Metabolic Rate},
  author = {Baas, Jan and Kooijman, Sebastiaan A L M},
  year = {2015},
  journal = {Ecotoxicology},
  volume = {24},
  number = {3},
  pages = {657--663},
  issn = {15733017},
  doi = {10.1007/s10646-014-1413-5},
  abstract = {Ecotoxicological studies have shown considerable variation in species sensitivity for chemical compounds, but general patterns in sensitivity are still not known. A better understanding of this sensitivity is important in the context of environmental risk assessment but also in a more general ecological and evolutionary one. We investigated the metabolic rate or more precise the specific somatic maintenance (expressed in J cm(-3) d(-1), at a standardised body temperature of 20 {$^\circ$}C) on the sensitivity of a species to chemical poisoning. The sensitivity of a species was expressed in terms of its threshold concentration for survival, the no effect concentrations (NEC, in {\textmu}mol/L). Somatic maintenance data were based on the 'add-my-pet' database hosted by the VU University of Amsterdam. NECs were derived from the US-EPA ECOTOX database. We focussed on four pesticides; two that need a metabolic activation, Chlorpyrifos and Malathion, and two without metabolic activation, carbofuran and carbaryl. All four pesticides showed a similar response: a strong negative correlation between the specific somatic maintenance and the NEC. We discuss possible explanations, deviations and ecological implications.},
  pmid = {25564013},
  keywords = {Biological traits,No effect concentration,nosource,Pesticides,Species sensitivity,Specific somatic maintenance}
}

@article{Babenko:2009p2,
  title = {On the Posterior Pointwise Convergence Rate of a {{Gaussian}} Signal under a Conjugate Prior},
  author = {Babenko, Alexandra and Belitser, Eduard},
  year = {2009},
  month = mar,
  journal = {Statistics \& Probability Letters},
  volume = {79},
  number = {5},
  pages = {670--675},
  issn = {01677152},
  doi = {10.1016/j.spl.2008.10.019},
  keywords = {nosource}
}

@article{babenko2010oracle,
  title = {Oracle Convergence Rate of Posterior under Projection Prior and {{Bayesian}} Model Selection},
  author = {Babenko, Alexandra and Belitser, Eduard},
  year = {2010},
  journal = {Math. Methods Statist.},
  volume = {19},
  number = {3},
  pages = {219--245},
  publisher = {Springer},
  issn = {1066-5307},
  keywords = {nosource}
}

@article{Babut2011,
  title = {{{BSAFs}} for Freshwater Fish and Derivation of a Sediment Quality Guideline for {{PCBs}} in the {{Rhone}} Basin, {{France}}},
  author = {Babut, Marc and Lopes, Christelle and Pradelle, S{\'e}bastien and Persat, Henri and Badot, Pierre Marie},
  year = {2012},
  month = dec,
  journal = {Journal of Soils and Sediments},
  volume = {12},
  number = {2},
  pages = {241--251},
  issn = {14390108},
  doi = {10.1007/s11368-011-0448-y},
  abstract = {Purpose Since 2005, freshwater fish contamination by polychlorobiphenyls (PCBs), polychlorodibenzodioxins, and polychlorodibenzofurans has been assessed in the Rhone River basin (France). A large database of surface sediment contamination by PCBs is also available, opening the way to the study of biota-to-sediment accumulation factor (BSAF) distribution throughout this basin. The ultimate goal of the study was to determine a sediment quality guideline (SQG) corresponding to the regulatory fish consumption limit. Materials and methods A bootstrapping procedure for determining BSAFs was applied to a data set matching the available databases of sediment and fish contamination by PCBs in the Rhone River basin. The SQG was obtained by combining the current tissue-based regulatory threshold with a characteristic BSAF value, for a species particularly prone to accumulating these compounds. As the current regulatory threshold refers to dioxins and related com- pounds, while most available sediment data deal with non- dioxin-like PCBs, a set of correlations was used to derive an SQG for the sum of seven indicator PCBs. To assess the reliability of these SQG pairs, actual fish and sediment concentrations were classified into four categories, accord- ing to a comparison with the regulatory threshold for fish data and the calculated SQG. Results and discussion BSAFs were determined for 11 species. The barbel and European eel had the highest BSAFs. The common carp, a benthic species, had surpris- ingly low BSAFs, as low as pelagic and omnivorous species such as chub. The data set was also split into two parts, one comprising fish samples at or above the regulatory limit and the other samples below this threshold. Sediment concentrations and BSAFs were higher in the former group. An SQG was derived on the basis of the 75th percentile of barbel's BSAF, equaling 26.6 ng g-1 dry weight (range 15.6--38.6 ng g-1) for the sum of seven indicator PCBs. When tested against the same database, this SQG displayed an overall efficiency of about 60\% resulting from the limited reliability of sediment data and the factors influencing PCB bioavailability. Conclusions From the perspective of regulatory frame- works such as the European Water Framework Directive, a tiered monitoring strategy combining sediments (first tier) and biota (second tier) could be more relevant than a single- compartment approach. In this context, an SQG such as the one determined herein would trigger the second tier.},
  isbn = {1439-0108},
  keywords = {BSAF,Fish,nosource,PCB,River,Sediment,SQG}
}

@article{bacalladoSufficientnessPostulatesGibbstype2017,
  title = {Sufficientness Postulates for {{Gibbs-type}} Priors and Hierarchical Generalizations},
  author = {Bacallado, Sergio and Battiston, Marco and Favaro, Stefano and Trippa, Lorenzo},
  year = {2017},
  journal = {Statistical Science},
  pages = {487--500},
  publisher = {JSTOR},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/Zotero/storage/8BF36AH6/Bacallado et al. - 2017 - Sufficientness postulates for Gibbs-type priors an.pdf}
}

@article{baczkowski1998range,
  title = {Range of Validity of \${$\alpha\$$} and \${$\beta\$$} for a Generalized Diversity Index \{\$\vphantom\}{{H}}({$\alpha$},{$\beta$})\$\vphantom\{\} Due to \{\vphantom\}{{G}}\vphantom\{\}ood},
  author = {Baczkowski, A J and Joanes, D N and Shamia, G M},
  year = {1998},
  journal = {Mathematical biosciences},
  volume = {148},
  number = {2},
  pages = {115--128},
  publisher = {Elsevier},
  keywords = {duplicate-citation-key,nosource}
}

@article{baczkowski1998range,
  title = {Range of Validity of [Alpha] and [Beta] for a Generalized Diversity Index {{H}}([Alpha],[Beta]) Due to {{Good}}},
  author = {Baczkowski, a J and Joanes, D N and Shamia, G M},
  year = {1998},
  journal = {Mathematical Biosciences},
  volume = {148},
  number = {2},
  pages = {115--128},
  publisher = {Elsevier},
  abstract = {Good (I.J. Good, Biometrika 40 (1953) 237; J. Am. Statist. Assoc. 77 (1982) 561) proposed a diversity index H([alpha],[beta]), defined for non-negative integer [alpha] and [beta], which generalized the well-known indices of Shannon and Simpson. In this paper we further generalize Good's index by allowing the parameters [alpha] and [beta] to take values in the real plane. For two simple properties which a \&lsquo;good' diversity index should possess, we determine the range of valid [alpha] and [beta] values for this generalized index.},
  keywords = {Diversity indices,duplicate-citation-key,nosource,Schur-concave}
}

@article{bae2005prediction,
  title = {Prediction of Protein Interdomain Linker Regions by a Hidden {{Markov}} Model},
  author = {Bae, Kyounghwa and Mallick, Bani K and Elsik, Christine G},
  year = {2005},
  journal = {Bioinformatics},
  volume = {21},
  number = {10},
  pages = {2264--2270},
  publisher = {Oxford University Press},
  keywords = {nosource}
}

@article{bailleulRClonePackageIdentify2016,
  title = {{{RClone}}: A Package to Identify {{MultiLocus Clonal Lineages}} and Handle Clonal Data Sets in r.},
  shorttitle = {{{RClone}}},
  author = {Bailleul, Diane and Stoeckel, Solenn and Arnaud-Haond, Sophie},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {8},
  pages = {966--970},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12550},
  urldate = {2021-05-02},
  abstract = {Partially, clonal species are common in the Tree of Life. And yet, population genetic models still mostly focus on the extremes: strictly sexual versus purely asexual reproduction. Here, we present an R package built upon genclone software including new functions and several improvements. The RClone package includes functions to handle clonal data sets, allowing (i) checking for data set reliability to discriminate multilocus genotypes (MLGs), (ii) ascertainment of MLG and semi-automatic determination of clonal lineages (MLL), (iii) genotypic richness and evenness indices calculation based on MLGs or MLLs and (iv) describing several spatial components of clonality. RClone allows the one-shot analysis of multipopulation data sets without size limitation, suitable for data sets now increasingly produced through next-generation sequencing. A major improvement compared to existing software is the ability to determine the threshold to cluster similar MLGs into MLLs, based on implemented simulations of sexual events. Several functions allow data importation, conversion and exportation with adegenet, Genetix or Arlequin. RClone is provided with two vignettes to handle analysis on one (RClone\_quickmanual) or several populations (RClone\_qmsevpops).},
  copyright = {{\copyright} 2016 The Authors. Methods in Ecology and Evolution {\copyright} 2016 British Ecological Society},
  langid = {english},
  keywords = {clonal diversity,clonal population,clonality,multilocus genotypes,multilocus lineages,software,spatial autocorrelation},
  file = {/home/gkonkamking/Zotero/storage/9GCGE9KU/Bailleul et al. - 2016 - RClone a package to identify MultiLocus Clonal Li.pdf}
}

@article{Baird2007,
  title = {Using Biological Traits to Predict Species Sensitivity to Toxic Substances.},
  author = {Baird, Donald J and {Van den Brink}, Paul J},
  year = {2007},
  month = jun,
  journal = {Ecotoxicology and environmental safety},
  volume = {67},
  number = {2},
  eprint = {16996132},
  eprinttype = {pubmed},
  pages = {296--301},
  issn = {0147-6513},
  doi = {10.1016/j.ecoenv.2006.07.001},
  abstract = {Species sensitivity distributions (SSD) assume that sensitivity to toxicants within target species is random. While the SSD approach has shown promise, it is limited by the fact that data are sparse for most compounds, and that these data are largely based on the lethal responses of a small group of testing lab species. Here we present an alternative approach, based on the hypothesis that organisms' sensitivity to stress is a function of their biology, and can be predicted from species traits such as morphology, life history, physiology and feeding ecology. Using data from the US EPA's AQUIRE database, we found that four species traits explained 71\% of the variability in sensitivity to toxicants within a group of 12 species exposed to 15 chemicals. Our results indicate that this approach has promise, but effort is needed to compile species trait information to increase the power, precision and taxonomic representativeness of this approach.},
  isbn = {0147-6513},
  pmid = {16996132},
  keywords = {Animals,Biological,Databases,duplicate-citation-key,Ecological relevance,Environmental Monitoring,Environmental Monitoring: methods,Environmental Pollutants,Environmental Pollutants: toxicity,Factual,Models,nosource,Species Specificity,Species traits,Species-sensitivity distribution}
}

@article{Baird2008,
  title = {Trait-Based Ecological Risk Assessment ({{TERA}}): The New Frontier?},
  author = {Baird, Donald J. and Rubach, M. N. and {Van den Brink}, P. J.},
  year = {2008},
  journal = {Integrated environmental assessment and management},
  volume = {4},
  number = {1},
  pages = {2--3},
  issn = {1551-3793},
  doi = {10.1897/ieam_2007-063.1},
  abstract = {Traits describe the physical characteristics, ecological niche, and functional role of species within ecosystems, and trait-based approaches are now being introduced into the field of Ecological Risk Assessment (ERA). The costs and benefits arising from the adoption of these approaches in the assessment of risks from toxic substances are described, and the path forward for this new frontier in risk assessment science is presented. In particular, the necessity for more open collaboration and web-based data-sharing to facilitate the development of these exciting new tools is stressed, and the role of scientific organizations such as SETAC as promoters of this ambitious program is highlighted.},
  isbn = {1551-3777},
  pmid = {18260202},
  keywords = {nosource}
}

@article{Baird2011,
  title = {Toward a Knowledge Infrastructure for Traits-Based Ecological Risk Assessment},
  author = {Baird, Donald J. and Baker, Christopher J O and Brua, Robert B. and Hajibabaei, Mehrdad and McNicol, Kearon and Pascoe, Timothy J. and {de Zwart}, Dick},
  year = {2011},
  month = apr,
  journal = {Integrated Environmental Assessment and Management},
  volume = {7},
  number = {2},
  eprint = {21442733},
  eprinttype = {pubmed},
  pages = {209--215},
  issn = {15513793},
  doi = {10.1002/ieam.129},
  abstract = {The trait approach has already indicated significant potential as a tool in understanding natural variation among species in sensitivity to contaminants in the process of ecological risk assessment. However, to realize its full potential, a defined nomenclature for traits is urgently required, and significant effort is required to populate databases of species-trait relationships. Recently, there have been significant advances in the area of information management and discovery in the area of the semantic web. Combined with continuing progress in biological trait knowledge, these suggest that the time is right for a reevaluation of how trait information from divergent research traditions is collated and made available for end users in the field of environmental management. Although there has already been a great deal of work on traits, the information is scattered throughout databases, literature, and undiscovered sources. Further progress will require better leverage of this existing data and research to fill in the gaps. We review and discuss a number of technical and social challenges to bringing together existing information and moving toward a new, collaborative approach. Finally, we outline a path toward enhanced knowledge discovery within the traits domain space, showing that, by linking knowledge management infrastructure, semantic metadata (trait ontologies), and Web 2.0 and 3.0 technologies, we can begin to construct a dedicated platform for TERA science.},
  isbn = {1551-3793 (Electronic){\textbackslash}n1551-3777 (Linking)},
  pmid = {21442733},
  keywords = {Ecological risk assesssment,Knowledge infrastructure,nosource,Traits}
}

@incollection{baiSpikeSlabMeetsLASSO2021,
  title = {Spike-and-{{Slab Meets LASSO}}: {{A Review}} of the {{Spike-and-Slab LASSO}}},
  shorttitle = {Spike-and-{{Slab Meets LASSO}}},
  booktitle = {Handbook of {{Bayesian Variable Selection}}},
  author = {Bai, Ray and Ro{\v c}kov{\'a}, Veronika and George, Edward I.},
  year = {2021},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {High-dimensional data sets have become ubiquitous in the past few decades, often with many more covariates than observations. In the frequentist setting, penalized likelihood methods are the most popular approach for variable selection and estimation in high-dimensional data. In the Bayesian framework, spike-and-slab methods are commonly used as probabilistic constructs for high-dimensional modeling. Within the context of linear regression, Rockov{\'a} and George (2018) introduced the spike-and-slab LASSO (SSL), an approach based on a prior which provides a continuum between the penalized likelihood LASSO and the Bayesian point-mass spike-and-slab formulations. Since its inception, the spike-and-slab LASSO has been extended to a variety of contexts, including generalized linear models, factor analysis, graphical models, and nonparametric regression. The goal of this chapter is to survey the landscape surrounding spike-and-slab LASSO methodology. First we elucidate the attractive properties and the computational tractability of SSL priors in high dimensions. We then review methodological developments of the SSL and outline several theoretical developments. We illustrate the methodology on both simulated and real datasets.},
  isbn = {978-1-00-308901-8},
  file = {/home/gkonkamking/Zotero/storage/ECXQ3NNN/Bai et al. - 2021 - Spike-and-Slab Meets LASSO A Review of the Spike-.pdf}
}

@article{banerjeeBayesianInferenceHighdimensional2021,
  title = {Bayesian Inference in High-Dimensional Models},
  author = {Banerjee, Sayantan and Castillo, Isma{\"e}l and Ghosal, Subhashis},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.04491 [math, stat]},
  eprint = {2101.04491},
  primaryclass = {math, stat},
  urldate = {2021-05-31},
  abstract = {Models with dimension more than the available sample size are now commonly used in various applications. A sensible inference is possible using a lower-dimensional structure. In regression problems with a large number of predictors, the model is often assumed to be sparse, with only a few predictors active. Interdependence between a large number of variables is succinctly described by a graphical model, where variables are represented by nodes on a graph and an edge between two nodes is used to indicate their conditional dependence given other variables. Many procedures for making inferences in the high-dimensional setting, typically using penalty functions to induce sparsity in the solution obtained by minimizing a loss function, were developed. Bayesian methods have been proposed for such problems more recently, where the prior takes care of the sparsity structure. These methods have the natural ability to also automatically quantify the uncertainty of the inference through the posterior distribution. Theoretical studies of Bayesian procedures in high-dimension have been carried out recently. Questions that arise are, whether the posterior distribution contracts near the true value of the parameter at the minimax optimal rate, whether the correct lower-dimensional structure is discovered with high posterior probability, and whether a credible region has adequate frequentist coverage. In this paper, we review these properties of Bayesian and related methods for several high-dimensional models such as many normal means problem, linear regression, generalized linear models, Gaussian and non-Gaussian graphical models. Effective computational approaches are also discussed.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/gkonkamking/Zotero/storage/QK2GJ9UP/Banerjee et al. - 2021 - Bayesian inference in high-dimensional models.pdf}
}

@article{baptistaNetworkExtractionRouting2020,
  title = {Network Extraction by Routing Optimization},
  author = {Baptista, Diego and Leite, Daniela and Facca, Enrico and Putti, Mario and De Bacco, Caterina},
  year = {2020},
  month = nov,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {20806},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-77064-4},
  urldate = {2025-01-27},
  abstract = {Routing optimization is a relevant problem in many contexts. Solving directly this type of optimization problem is often computationally intractable. Recent studies suggest that one can instead turn this problem into one of solving a dynamical system of equations, which can instead be solved efficiently using numerical methods. This results in enabling the acquisition of optimal network topologies from a variety of routing problems. However, the actual extraction of the solution in terms of a final network topology relies on numerical details which can prevent an accurate investigation of their topological properties. In fact, in this context, theoretical results are fully accessible only to an expert audience and ready-to-use implementations for non-experts are rarely available or insufficiently documented. In particular, in this framework, final graph acquisition is a challenging problem in-and-of-itself. Here we introduce a method to extract network topologies from dynamical equations related to routing optimization under various parameters' settings. Our method is made of three steps: first, it extracts an optimal trajectory by solving a dynamical system, then it pre-extracts a network, and finally, it filters out potential redundancies. Remarkably, we propose a principled model to address the filtering in the last step, and give a quantitative interpretation in terms of a transport-related cost function. This principled filtering can be applied to more general problems such as network extraction from images, thus going beyond the scenarios envisioned in the first step. Overall, this novel algorithm allows practitioners to easily extract optimal network topologies by combining basic tools from numerical methods, optimization and network theory. Thus, we provide an alternative to manual graph extraction which allows a grounded extraction from a large variety of optimal topologies. The analysis of these may open up the possibility to gain new insights into the structure and function of optimal networks. We provide an open source implementation of the code online.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Applied mathematics,Complex networks,Computer science},
  file = {/home/gkonkamking/pCloudDrive/papers/Baptista et al. - 2020 - Network extraction by routing optimization.pdf}
}

@article{barbieriOptimalPredictiveModel2004,
  title = {Optimal Predictive Model Selection},
  author = {Barbieri, Maria Maddalena and Berger, James O.},
  year = {2004},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {32},
  number = {3},
  pages = {870--897},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053604000000238},
  urldate = {2022-05-03},
  abstract = {Often the goal of model selection is to choose a model for future prediction, and it is natural to measure the accuracy of a future prediction by squared error loss. Under the Bayesian approach, it is commonly perceived that the optimal predictive model is the model with highest posterior probability, but this is not necessarily the case. In this paper we show that, for selection among normal linear models, the optimal predictive model is often the median probability model, which is defined as the model consisting of those variables which have overall posterior probability greater than or equal to 1/2 of being in a model. The median probability model often differs from the highest probability model.},
  keywords = {62C10,62F15,Bayesian linear models,predictive distribution,squared error loss,Variable selection},
  file = {/home/gkonkamking/Zotero/storage/3Z83WMYU/Barbieri and Berger - 2004 - Optimal predictive model selection.pdf}
}

@article{barbosa2021uncovering,
  title = {Uncovering the Socioeconomic Facets of Human Mobility},
  author = {Barbosa, Hugo and Hazarie, Surendra and Dickinson, Brian and Bassolas, Aleix and Frank, Adam and Kautz, Henry and Sadilek, Adam and Ramasco, Jos{\'e} J and Ghoshal, Gourab},
  year = {2021},
  journal = {Scientific reports},
  volume = {11},
  number = {1},
  pages = {1--13},
  publisher = {Nature Publishing Group},
  keywords = {nosource}
}

@article{barbourTransitionFunctionExpansion2000,
  title = {A Transition Function Expansion for a Diffusion Model with Selection},
  author = {Barbour, A. D. and Ethier, S. N. and Griffiths, R. C.},
  year = {2000},
  month = feb,
  journal = {The Annals of Applied Probability},
  volume = {10},
  number = {1},
  pages = {123--162},
  issn = {1050-5164},
  doi = {10.1214/aoap/1019737667},
  urldate = {2020-06-15},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Barbour et al_2000_A transition function expansion for a diffusion model with selection.pdf}
}

@article{bardenetMarkovChainMonte,
  title = {On {{Markov}} Chain {{Monte Carlo}} Methods for Tall Data},
  author = {Bardenet, Remi and Doucet, Arnaud and Holmes, Chris},
  pages = {43},
  abstract = {Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number n of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the MetropolisHastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach relying on a control variate method which samples under regularity conditions from a distribution provably close to the posterior distribution of interest, yet can require less than O(n) data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we emphasize that we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/FVIZZLGF/Bardenet et al. - On Markov chain Monte Carlo methods for tall data.pdf}
}

@article{barnard2000modeling,
  title = {Modeling Covariance Matrices in Terms of Standard Deviations and Correlations, with Application to Shrinkage},
  author = {Barnard, John and McCulloch, Robert and Meng, Xiao-Li},
  year = {2000},
  journal = {Statistica Sinica},
  volume = {10},
  number = {4},
  pages = {1281--1311},
  publisher = {C/O DR HC HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN},
  issn = {10170405},
  abstract = {The covariance matrix plays an important role in statistical inference, yet modeling a covariance matrix is often a difficult task in practice due to its dimensionality and the non-negative definite constraint. In order to model a co- variance matrix effectively, it is typically broken down into components based on modeling considerations or mathematical convenience. Decompositions that have received recent research attention include variance components, spectral decompo- sition, Cholesky decomposition, and matrix logarithm. In this paper we study a statistically motivated decomposition which appears to be relatively unexplored for the purpose of modeling. We model a covariance matrix in terms of its correspond- ing standard deviations and correlation matrix. We discuss two general modeling situations where this approach is useful: shrinkage estimation of regression co- efficients, and a general location-scale model for both categorical and continuous variables. We present some simple choices for priors in terms of standard deviations and the correlation matrix, and describe a straightforward computational strategy for obtaining the posterior of the covariance matrix. We apply our method to real and simulated data sets in the context of shrinkage estimation.},
  isbn = {1017-0405},
  keywords = {and phrases,general location model,general location-scale model,gibbs sampler,hierarchical models,markov chain monte carlo,nosource,wishart distri-},
  file = {/home/gkonkamking/pCloudDrive/papers/Barnard et al. - 2000 - Modeling covariance matrices in terms of standard deviations and correlations, with application to s.pdf}
}

@article{barrientos2012fully,
  title = {Fully Nonparametric Regression for Bounded Data Using Dependent {{Bernstein}} Polynomials},
  author = {Barrientos, Andres F. and Jara, Alejandro and Quintana, Fernando A},
  year = {2012},
  journal = {Manuscript under preparation},
  pages = {1--38},
  abstract = {We propose a novel class of probability models for sets of predictor-dependent prob- ability distributions with bounded domain. The proposal extends the Dirichlet-Bernstein prior for single density estimation, by using dependent stick-breaking processes. A gen- eral model class and two simplified versions of the general class are discussed in detail. Appealing theoretical properties such as continuity, association structure, marginal distri- bution, large support and consistency of the posterior distribution are established for all models. The behavior of the models is illustrated using simulated and real-life data. The simulated data is also used to compare the proposed methodology to existing methods.},
  keywords = {bayesian nonparametrics,dependent dirichlet processes,dependent processes,dirichlet process,duplicate-citation-key,nosource,random bernstein polynomials}
}

@article{barrientos2012fully,
  title = {Fully Nonparametric Regression for Bounded Data Using Dependent \{\vphantom\}{{B}}\vphantom\{\}ernstein Polynomials},
  author = {Barrientos, A and Jara, A and Quintana, Fernando A},
  year = {2012},
  journal = {Manuscript under preparation},
  keywords = {duplicate-citation-key,nosource}
}

@article{barrientos2012support,
  title = {On the Support of {{MacEachern}} ' s Dependent {{Dirichlet}} Processes and Extensions},
  author = {Barrientos, Andres F. and Jara, Alejandro and Quintana, Fernando A},
  year = {2012},
  journal = {Analysis},
  volume = {7},
  number = {1},
  pages = {1--34},
  publisher = {International Society for Bayesian Analysis},
  keywords = {bayesian nonparametrics,breaking pro-,copulas,hellinger support,kullback,leibler support,nosource,related probability distributions,stick,weak support}
}

@article{barrios2013modeling,
  title = {Modeling with Normalized Random Measure Mixture Models},
  author = {Barrios, Ernesto and Lijoi, Antonio and {Nieto-Barajas}, Luis E. and Pr{\"u}nster, Igor},
  year = {2013},
  journal = {Statistical Science},
  volume = {28},
  number = {3},
  eprint = {1310.0260v1},
  pages = {313--334},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  doi = {10.1214/13-sts416},
  abstract = {The Dirichlet process mixture model and more general mixtures based on discrete random probability measures have been shown to be flexible and accurate models for density estimation and clustering. The goal of this paper is to illustrate the use of normalized random measures as mixing measures in nonparametric hierarchical mixture models and point out how possible computational issues can be successfully addressed. To this end, we first provide a concise and accessible introduction to normalized random measures with independent increments. Then, we explain in detail a particular way of sampling from the posterior using the Ferguson-Klass representation. We develop a thorough comparative analysis for location-scale mixtures that considers a set of alternatives for the mixture kernel and for the nonparametric component. Simulation results indicate that normalized random measure mixtures potentially represent a valid default choice for density estimation problems. As a byproduct of this study an R package to fit these models was produced and is available in the Comprehensive R Archive Network (CRAN). {\copyright} Institute of Mathematical Statistics, 2013.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1310.0260v1},
  keywords = {Bayesian nonparametrics,clustering,completely random measure,density estimation,Dirichlet process,increasing additive process,latent variables,mixture model,normalized generalized gamma process,normalized inverse Gaussian process,normalized random measure,normalized stable process},
  file = {/home/gkonkamking/pCloudDrive/papers/Barrios et al_2013_Modeling with normalized random measure mixture models.pdf}
}

@article{Barron:1999p213,
  title = {Problems},
  author = {Wasserman, Larry and Schervish, Mark J. and Barron, Andrew R},
  year = {1999},
  journal = {The Annals of Statistics},
  volume = {27},
  number = {2},
  pages = {536--561},
  issn = {0090-5364},
  doi = {10.1214/aos/1018031206},
  abstract = {We give conditions that guarantee that the posterior probability of every Hellinger neighborhood of the true distribution tends to 1 almost surely. The conditions are (1) a requirement that the prior not put high mass near distributions with very rough densities and (2) a requirement that the prior put positive mass in Kullback-Leibler neighborhoods of the true distribution. The results are based on the idea of approximating the set of distributions with a finite-dimensional set of distributions with sufficiently small Hellinger bracketing metric entropy. We apply the results to some examples.},
  isbn = {00905364},
  keywords = {duplicate-citation-key,Exponential families,Hellinger distance,Nonparametric Bayesian inference,nosource,P??lya trees}
}

@article{Barron:1999p213,
  title = {The Consistency of Posterior Distributions in Nonparametric Problems},
  author = {Barron, A and Schervish, M and Wasserman, L},
  year = {1999},
  journal = {Ann. Statist.},
  volume = {17},
  number = {2},
  eprint = {120103},
  eprinttype = {jstor},
  pages = {536--561},
  abstract = {IN NONPARAMETRIC PROBLEMS BY ANDREW BARRON,1 MARK J. SCHERVISH 2 AND 3 Yale University and Carnegie Mellon University We give conditions that guarantee that the posterior probability of},
  keywords = {duplicate-citation-key,nosource}
}

@article{Barron2013,
  title = {Development of Aquatic Toxicity Benchmarks for Oil Products Using Species Sensitivity Distributions.},
  author = {Barron, Mace G and Hemmer, Michael J and Jackson, Crystal R},
  year = {2013},
  month = apr,
  journal = {Integrated environmental assessment and management},
  volume = {9999},
  number = {9999},
  eprint = {23554001},
  eprinttype = {pubmed},
  pages = {1--6},
  issn = {1551-3793},
  doi = {10.1002/ieam.1420},
  abstract = {Determining the sensitivity of a diversity of species to spilled oil and chemically dispersed oil continues to be a significant challenge in spill response and impact assessment. We used standardized tests from the literature to develop species sensitivity distributions (SSDs) of acute aquatic toxicity values for several petroleum products and 2 Corexit oil dispersants. Fifth percentile hazard concentrations (HC5s) were computed from the SSDs and used to assess relative oil product toxicity and in evaluating the feasibility of establishing toxicity benchmarks for a community of species. The sensitivity of mysids (Americamysis bahia) and silversides (Menidia beryllina) were evaluated within the SSDs to determine if these common test species were appropriate surrogates for a broader range of species. In general, SSD development was limited by the availability of acute toxicity values that met standardization criteria for a diversity of species. Pooled SSDs were also developed for crude oil and Corexit dispersants because there was only small variability in the HC5s among the individual oil or dispersant products. The sensitivity of mysids and silversides varied across the oil and dispersant products, with the majority of toxicity values greater than the HC5. Application of SSDs appears to be a reasonable approach to developing oil product toxicity benchmarks, but additional toxicity data are needed for a larger range of species conducted under standardized test conditions. Integr Environ Assess Manag 2013;9999:XX-XX. {\copyright} 2013 SETAC.},
  isbn = {1551-3777},
  pmid = {23554001},
  keywords = {acute toxicity,aquatic oil species sensitivity,distribution,duplicate-citation-key,nosource}
}

@techreport{barron88,
  title = {The Exponential Convergence of Posterior Probabilities with Implications for Bayes Estimators of Density Functions},
  author = {Barron, Andrew R},
  year = {1988},
  volume = {7},
  institution = {University of Illinois at Urbana-Campaign},
  abstract = {Necessary and sufficient conditions are determined for sequences of posterior probabilities of parameter sets to converge to one at an exponential rate, assumpiong that the prior assigns positive probability to the relative entropy rate neighborhoods of the distribution of the process \{X\_n\}. The result is applied to the case of independent random variables to determine conditions on the prior such that Bayes estimators {\textbackslash}hat\{p\}\_n(x) of the probability density function p(x) converge in the L{\textasciicircum}1 sense, i.e., {\textbackslash}int {\textbar}{\textbackslash}hat\{p\}\_n-p{\textbar} tends to zero, with probability one. Also, some useful bounds are obtained for all N for the expected value of th esum of relative entropies {\textbackslash}sum\_\{n {\textbackslash}leq N\} {\textbackslash}int p {\textbackslash}log(p/{\textbackslash}hat\{p\}\_n). The proof uses a frequentist approximation to the Bayesian's joint law for the parameter and the data. The results are applied to a variety of interesting priors.},
  keywords = {duplicate-citation-key,nosource}
}

@article{barry2014burrlioz,
  title = {Burrlioz 2.0},
  author = {Barry, {\relax SAH} and Henderson, B},
  year = {2014},
  journal = {Commonwealth Science and Industrial Research Organisation, Canberra, Australia},
  volume = {24},
  keywords = {nosource}
}

@article{Baty2013,
  title = {A Toolbox for Nonlinear Regression in r: {{The}} Package Nlstools},
  author = {Baty, F and Ritz, C and Charles, Sandrine and Brutsche, M and Flandrois, J P and {Delignette-Muller}, Marie Laure},
  year = {2015},
  journal = {accepted for publication at the Journal of Statistical Software},
  number = {1988},
  pages = {2008},
  issn = {15487660},
  keywords = {2008,agricultural science,already dedicated to the,and microbiology,bootstrap,chemistry,confidence regions,diagnostics,fields,fit of nonlinear models,in a broad diversity,incl,interest in the use,jackknife,nonlinear regression,nosource,of nonlinear regression models,of scientific,pharmacology,provides,ritz,streibig,the basic routine that,there is an increasing,various r functions are}
}

@article{baudrotFitReducedGUTS2018,
  title = {Fit {{Reduced GUTS Models Online}}: {{From Theory}} to {{Practice}}: {{GUTS Online}}: {{From Theory}} to {{Practice}}},
  shorttitle = {Fit {{Reduced GUTS Models Online}}},
  author = {Baudrot, Virgile and Veber, Philippe and Gence, Guillaume and Charles, Sandrine},
  year = {2018},
  month = sep,
  journal = {Integrated Environmental Assessment and Management},
  volume = {14},
  number = {5},
  pages = {625--630},
  issn = {15513777},
  doi = {10.1002/ieam.4061},
  urldate = {2021-11-17},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/BZU5AJCE/Baudrot et al. - 2018 - Fit Reduced GUTS Models Online From Theory to Pra.pdf}
}

@article{baudrotNewInsightsCompare2018,
  title = {New {{Insights}} to {{Compare}} and {{Choose TKTD Models}} for {{Survival Based}} on an {{Interlaboratory Study}} for {{{\emph{Lymnaea}}}}{\emph{ Stagnalis}} {{Exposed}} to {{Cd}}},
  author = {Baudrot, Virgile and Preux, Sara and Ducrot, Virginie and Pave, Alain and Charles, Sandrine},
  year = {2018},
  month = feb,
  journal = {Environmental Science \& Technology},
  volume = {52},
  number = {3},
  pages = {1582--1590},
  issn = {0013-936X, 1520-5851},
  doi = {10.1021/acs.est.7b05464},
  urldate = {2021-11-17},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/FC3FF7PN/Baudrot et al. - 2018 - New Insights to Compare and Choose TKTD Models for.pdf}
}

@article{bauerCuingConsumerismSituational2012,
  title = {Cuing {{Consumerism}}: {{Situational Materialism Undermines Personal}} and {{Social Well-Being}}},
  shorttitle = {Cuing {{Consumerism}}},
  author = {Bauer, Monika A. and Wilkie, James E. B. and Kim, Jung K. and Bodenhausen, Galen V.},
  year = {2012},
  month = mar,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  doi = {10.1177/0956797611429579},
  urldate = {2020-06-09},
  abstract = {Correlational evidence indicates that materialistic individuals experience relatively low levels of well-being. Across four experiments, we found that situational cuing can also trigger materialist...},
  langid = {english},
  keywords = {nosource}
}

@article{BBCILSZ16,
  title = {Unreasonable Effectiveness of Learning Neural Networks: \{\vphantom\}{{From}}\vphantom\{\} Accessible States and Robust Ensembles to Basic Algorithmic Schemes},
  shorttitle = {Unreasonable Effectiveness of Learning Neural Netw},
  author = {Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer T and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
  year = {2016},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {48},
  pages = {E7655----E7662},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1608103113},
  abstract = {In artificial neural networks, learning from data is a computationally demanding task in which a large number of connection weights are iteratively tuned through stochastic-gradient-based heuristic processes over a cost function. It is not well understood how learning occurs in these systems, in particular how they avoid getting trapped in configurations with poor computational performance. Here, we study the difficult case of networks with discrete weights, where the optimization landscape is very rough even for simple architectures, and provide theoretical and numerical evidence of the existence of rare---but extremely dense and accessible---regions of configurations in the network weight space. We define a measure, the robust ensemble (RE), which suppresses trapping by isolated configurations and amplifies the role of these dense regions. We analytically compute the RE in some exactly solvable models and also provide a general algorithmic scheme that is straightforward to implement: define a cost function given by a sum of a finite number of replicas of the original cost function, with a constraint centering the replicas around a driving assignment. To illustrate this, we derive several powerful algorithms, ranging from Markov Chains to message passing to gradient descent processes, where the algorithms target the robust dense states, resulting in substantial improvements in performance. The weak dependence on the number of precision bits of the weights leads us to conjecture that very similar reasoning applies to more conventional neural networks. Analogous algorithmic schemes can also be applied to other optimization problems.},
  keywords = {nosource}
}

@inproceedings{BCKSE14,
  title = {Array Operators Using Multiple Dispatch: A Design Methodology for Array Implementations in Dynamic Languages},
  booktitle = {{{ARRAY}}'14 Proceedings of {{ACM SIGPLAN}} International Workshop on Libraries, Languages, and Compilers for Array Programming},
  author = {Bezanson, Jeff and Chen, Jiahao and Karpinski, Stefan and Shah, Viral and Edelman, Alan},
  year = {2014},
  pages = {56--61},
  publisher = {ACM},
  address = {New York, \{NY\}, \{USA\}},
  doi = {10.1145/2627373.2627383},
  abstract = {Arrays are such a rich and fundamental data type that they tend to be built into a language, either in the compiler or in a large low-level library. Defining this functionality at the user level instead provides greater flexibility for application domains not envisioned by the language designer. Only a few languages, such as C++ and Haskell, provide the necessary power to define n-dimensional arrays, but these systems rely on compile-time abstraction, sacrificing some flexibility. In contrast, dynamic languages make it straightforward for the user to define any behavior they might want, but at the possible expense of performance. As part of the Julia language project, we have developed an approach that yields a novel trade-off between flexibility and compile-time analysis. The core abstraction we use is multiple dispatch. We have come to believe that while multiple dispatch has not been especially popular in most kinds of programming, technical computing is its killer application. By expressing key functions such as array indexing using multi-method signatures, a surprising range of behaviors can be obtained, in a way that is both relatively easy to write and amenable to compiler analysis. The compact factoring of concerns provided by these methods makes it easier for user-defined types to behave consistently with types in the standard library.},
  keywords = {nosource}
}

@inproceedings{beal2002infinite,
  title = {The Infinite Hidden {{Markov}} Model},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Beal, Matthew J and Ghahramani, Zoubin and Rasmussen, Carl E},
  year = {2002},
  pages = {577--584},
  file = {/home/gkonkamking/Zotero/storage/7MNGS5WZ/Beal et al. - 2002 - The infinite hidden Markov model.pdf}
}

@article{beaneyExcessMortalityGold2020,
  title = {Excess Mortality: The Gold Standard in Measuring the Impact of {{COVID-19}} Worldwide?},
  shorttitle = {Excess Mortality},
  author = {Beaney, Thomas and Clarke, Jonathan M and Jain, Vageesh and Golestaneh, Amelia Kataria and Lyons, Gemma and Salman, David and Majeed, Azeem},
  year = {2020},
  month = sep,
  journal = {Journal of the Royal Society of Medicine},
  volume = {113},
  number = {9},
  pages = {329--334},
  publisher = {SAGE Publications},
  issn = {0141-0768},
  doi = {10.1177/0141076820956802},
  urldate = {2021-10-20},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/N5EWDXE4/Beaney et al. - 2020 - Excess mortality the gold standard in measuring t.pdf}
}

@article{Beaudouin2013,
  title = {Comparison of Species Sensitivity Distributions Based on Population or Individual Endpoints},
  author = {Beaudouin, R{\'e}my and P{\'e}ry, Alexandre R R},
  year = {2013},
  journal = {Environmental Toxicology and Chemistry},
  volume = {32},
  number = {5},
  pages = {1173--1177},
  issn = {07307268},
  doi = {10.1002/etc.2148},
  abstract = {Species sensitivity distributions (SSDs) developed from individual and population endpoints were compared based on simulations and a case study. The simulations were performed with five invertebrate species accounting for the diversity of benthic macroinvertebrate communities in large European lowland rivers and for five benthic invertebrates used as laboratory species. Population growth rate 10\% effective concentration (EC10) values were, in most of the simulations, higher than the lowest of the EC10 values at the individual level. However, for the set of ecologically representative species, the fifth percentile level of this distribution (HC5) was more protective for population endpoints than for individual endpoints. This was the opposite for the set of laboratory species. Population and individual SSDs were also compared based on existing data on Cu for the five laboratory invertebrate species. In this case, the calculated population HC5 value was almost twice the individual value, and the authors showed much reduced variability between species sensitivities at population level compared with individual level. They conclude that population-based HC5 would generally be more protective than individual-based HC5. However, the change of level could reveal higher homogeneity at population level than at individual level, supporting the use of population-based HC5 to avoid overprotection. The authors thus advise the derivation of population-based HC5, as soon as it is possible, to derive such value with a relevant panel of species.},
  pmid = {23377887},
  keywords = {Computer modeling,Ecological risk assessment,Invertebrates,nosource,Population modeling,Species sensitivity distributions}
}

@article{Beaumont2002,
  title = {Approximate {{Bayesian}} Computation in Population Genetics},
  author = {Beaumont, Mark A. and Zhang, Wenyang and Balding, David J.},
  year = {2002},
  journal = {Genetics},
  volume = {162},
  number = {4},
  pages = {2025--2035},
  issn = {00166731},
  doi = {Genetics December 1, 2002 vol. 162 no. 4 2025-2035},
  abstract = {We propose a new method for approximate Bayesian statistical inference on the basis of summary statistics. The method is suited to complex problems that arise in population genetics, extending ideas developed in this setting by earlier authors. Properties of the posterior distribution of a parameter, such as its mean or density curve, are approximated without explicit likelihood calculations. This is achieved by fitting a local-linear regression of simulated parameter values on simulated summary statistics, and then substituting the observed summary statistics into the regression equation. The method combines many of the advantages of Bayesian statistical inference with the computational efficiency of methods based on summary statistics. A key advantage of the method is that the nuisance parameters are automatically integrated out in the simulation step, so that the large numbers of nuisance parameters that arise in population genetics problems can be handled without difficulty. Simulation results indicate computational and statistical efficiency that compares favorably with those of alternative methods previously proposed in the literature. We also compare the relative efficiency of inferences obtained using methods based on summary statistics with those obtained directly from the data using MCMC.},
  isbn = {0016-6731},
  pmid = {12524368},
  keywords = {nosource}
}

@article{Beckon2008,
  title = {A General Approach to Modeling Biphasic Relationships},
  author = {Beckon, William N. and Parkins, Cary and Maximovich, Alexey and Beckon, Angela V.},
  year = {2008},
  month = feb,
  journal = {Environmental Science and Technology},
  volume = {42},
  number = {4},
  eprint = {18351110},
  eprinttype = {pubmed},
  pages = {1308--1314},
  issn = {0013936X},
  doi = {10.1021/es071148m},
  abstract = {Biphasic relationships can be found throughout the sciences, especially in the dose-response relationships of pharmacology, toxicology, agriculture, and nutrition. Accurate modeling of biphasic dose-response is an essential step in establishing effective guidelines for the protection of human and ecosystem health, yet currently-used biphasic mathematical models lack biological rationale and fit only limited sets of biphasic data. To model biphasic relationships more closely over wider ranges of exposures, we suggest a simple, general, biologically reasonable modeling approach leading to a family of mathematical models that combine log-logistic functions: at least one for the upslope and one forthe downslope of the biphasic relationship. All parameters employed are meaningfully interpretable. These models can be used to test for the presence of biphasic effects, and they simplify to a standard log-logistic model in the special case where no biphasic effect can be detected. They offer the promise of improvement in assessment of the safety and efficacy of pharmaceuticals and nutrients as well as in determination of the toxicity of contaminants. Additionally, they may be useful in modeling nonmonotonic cause-effect relationships in other scientific disciplines.},
  isbn = {0013-936X},
  pmid = {18351110},
  keywords = {Dose-Response Relationship,Drug,Models,nosource,Theoretical}
}

@article{beebeEpistemicSideEffectEffect2010,
  title = {The {{Epistemic Side-Effect Effect}}},
  author = {Beebe, James R. and Buckwalter, Wesley},
  year = {2010},
  journal = {Mind \& Language},
  volume = {25},
  number = {4},
  pages = {474--498},
  issn = {1468-0017},
  doi = {10.1111/j.1468-0017.2010.01398.x},
  urldate = {2020-05-04},
  abstract = {Knobe (2003a, 2003b, 2004b) and others have demonstrated the surprising fact that the valence of a side-effect action can affect intuitions about whether that action was performed intentionally. Here we report the results of an experiment that extends these findings by testing for an analogous effect regarding knowledge attributions. Our results suggest that subjects are less likely to find that an agent knows an action will bring about a side-effect when the effect is good than when it is bad. It is further argued that these findings, while preliminary, have important implications for recent debates within epistemology about the relationship between knowledge and action.},
  copyright = {{\copyright} 2010 Blackwell Publishing Ltd},
  langid = {english}
}

@article{beebeGettierizedKnobeEffects2013,
  title = {Gettierized {{Knobe Effects}}},
  author = {Beebe, James R. and Shea, Joseph},
  year = {2013},
  month = sep,
  journal = {Episteme},
  volume = {10},
  number = {3},
  pages = {219--240},
  publisher = {Cambridge University Press},
  issn = {1742-3600, 1750-0117},
  doi = {10.1017/epi.2013.23},
  urldate = {2020-05-04},
  abstract = {We report experimental results showing that participants are more likely to attribute knowledge in familiar Gettier cases when the would-be knowers are performing actions that are negative in some way (e.g. harmful, blameworthy, norm-violating) than when they are performing positive or neutral actions. Our experiments bring together important elements from the Gettier case literature in epistemology and the Knobe effect literature in experimental philosophy and reveal new insights into folk patterns of knowledge attribution.},
  langid = {english},
  keywords = {nosource}
}

@article{BEKS14,
  title = {Julia: {{A}} Fresh Approach to Numerical Computing},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  year = {2017},
  journal = {SIAM Review},
  volume = {59},
  pages = {65--98},
  abstract = {The Julia programming language is gaining enormous popularity. Julia was designed to be easy and fast. Most importantly, Julia shatters deeply established notions widely held in the applied community: 1. High-level, dynamic code has to be slow by some sort of law of nature. 2. It is sensible to prototype in one language and then recode in another language. 3. There are parts of a system for the programmer, and other parts best left untouched as they are built by the experts. Julia began with a deep understanding of the needs of the scientific programmer and the needs of the computer in mind. Bridging cultures that have often been distant, Julia combines expertise from computer science and computational science creating a new approach to scientific computing. This note introduces the programmer to the language and the underlying design theory. It invites the reader to rethink the fundamental foundations of numerical computing systems. In particular, there is the fascinating dance between specialization and abstraction. Specialization allows for custom treatment. We can pick just the right algorithm for the right circumstance and this can happen at runtime based on argument types (code selection via multiple dispatch). Abstraction recognizes what remains the same after differences are stripped away and ignored as irrelevant. The recognition of abstraction allows for code reuse (generic programming). A simple idea that yields incredible power. The Julia design facilitates this interplay in many explicit and subtle ways for machine performance and, most importantly, human convenience.},
  keywords = {nosource}
}

@article{Belanger2016,
  title = {Future Needs and Recommendations in the Development of Species Sensitivity Distributions: {{Estimating}} Toxicity Thresholds for Aquatic Ecological Communities and Assessing Impacts of Chemical Exposures},
  author = {Belanger, Scott and Barron, Mace and Craig, Peter and Dyer, Scott and {Galay-Burgos}, Malyka and Hamer, Mick and Marshall, Stuart and Posthuma, Leo and Raimondo, Sandy and Whitehouse, Paul},
  year = {2016},
  journal = {Integrated Environmental Assessment and Management},
  issn = {15513777},
  doi = {10.1002/ieam.1841},
  keywords = {be addressed,belanger,com,impact,nosource,pg,probabilistic,research needs,risk assessment,scott e,se,to whom correspondence may}
}

@article{Belitser:2003p168,
  title = {Adaptive {{Bayesian}} Inference on the Mean of an Infinite-Dimensional Normal Distribution},
  author = {Belitser, Eduard and Ghosal, Subhashis},
  year = {2003},
  journal = {Annals of Statistics},
  volume = {31},
  number = {2},
  eprint = {3448405},
  eprinttype = {jstor},
  pages = {536--559},
  issn = {00905364},
  doi = {10.1214/aos/1051027880},
  abstract = {Ann. Statist. 2003, Vol. 31, No. 2, 536-559 ? Institute of Mathematical Statistics, 2003 ADAPTIVE BAYESIAN INFERENCE ON THE MEAN OF AN INFINITE-DIMENSIONAL NORMAL DISTRIBUTION Utrecht University and North},
  keywords = {Adaptive Bayes procedure,Convergence rate,Minimax risk,Model selection,nosource,Posterior distribution}
}

@article{belitser2013adaptive,
  title = {Adaptive Priors Based on Splines with Random Knots},
  author = {Belitser, Eduard and Serra, Paulo},
  year = {2014},
  journal = {Bayesian Analysis},
  volume = {9},
  number = {4},
  eprint = {1303.3365v1},
  pages = {859--882},
  issn = {19316690},
  doi = {10.1214/14-BA879},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1303.3365v1},
  keywords = {Adaptive prior,Bayesian non-parametric,nosource,Optimal contraction rate,Random knots,Spline}
}

@incollection{benard1955plotting,
  title = {The Plotting of Observations on Probability},
  booktitle = {Statistica Neerlandica},
  author = {Benard, A and {Bos-Levenbach}, E-C},
  year = {1953},
  volume = {7},
  pages = {163--173},
  publisher = {Stichting Mathematisch Centrum. Statistische Afdeling},
  keywords = {nosource}
}

@article{bergerBayesianAnalysisCovariance2020,
  title = {Bayesian {{Analysis}} of the {{Covariance Matrix}} of a {{Multivariate Normal Distribution}} with a {{New Class}} of {{Priors}}},
  author = {Berger, James O. and Sun, Dongchu and Song, Chengyuan},
  year = {2020},
  journal = {The Annals of Statistics},
  volume = {48},
  number = {4},
  eprint = {26931562},
  eprinttype = {jstor},
  pages = {2381--2403},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  urldate = {2024-10-24},
  abstract = {Bayesian analysis for the covariance matrix of a multivariate normal distribution has received a lot of attention in the last two decades. In this paper, we propose a new class of priors for the covariance matrix, including both inverse Wishart and reference priors as special cases. The main motivation for the new class is to have available priors---both subjective and objective---that do not ``force eigenvalues apart,'' which is a criticism of inverse Wishart and Jeffreys priors. Extensive comparison of these ``shrinkage priors'' with inverse Wishart and Jeffreys priors is undertaken, with the new priors seeming to have considerably better performance. A number of curious facts about the new priors are also observed, such as that the posterior distribution will be proper with just three vector observations from the multivariate normal distribution---regardless of the dimension of the covariance matrix---and that useful inference about features of the covariance matrix can be possible. Finally, a new MCMC algorithm is developed for this class of priors and is shown to be computationally effective for matrices of up to 100 dimensions.},
  file = {/home/gkonkamking/pCloudDrive/papers/Berger et al. - 2020 - Bayesian Analysis of the Covariance Matrix of a Multivariate Normal Distribution with a New Class of.pdf}
}

@article{bergReducedBiasNonparametric2019,
  title = {Reduced Bias Nonparametric Lifetime Density and Hazard Estimation},
  author = {Berg, Arthur and Politis, Dimitris and Suaray, Kagba and Zeng, Hui},
  year = {2019},
  month = aug,
  journal = {TEST},
  issn = {1863-8260},
  doi = {10.1007/s11749-019-00677-z},
  urldate = {2020-07-16},
  abstract = {Kernel-based nonparametric hazard rate estimation is considered with a special class of infinite-order kernels that achieves favorable bias and mean square error properties. A fully automatic and adaptive implementation of a density and hazard rate estimator is proposed for randomly right censored data. Careful selection of the bandwidth in the proposed estimators yields estimates that are more efficient in terms of overall mean square error performance, and in some cases, a nearly parametric convergence rate is achieved. Additionally, rapidly converging bandwidth estimates are presented for use in second-order kernels to supplement such kernel-based methods in hazard rate estimation. Simulations illustrate the improved accuracy of the proposed estimator against other nonparametric estimators of the density and hazard function. A real data application is also presented on survival data from 13,166 breast carcinoma patients.},
  langid = {english},
  keywords = {nosource}
}

@article{berke2022generating,
  title = {Generating Synthetic Mobility Data for a Realistic Population with {{RNNs}} to Improve Utility and Privacy},
  author = {Berke, Alex and Doorley, Ronan and Larson, Kent and Moro, Esteban},
  year = {2022},
  journal = {arXiv preprint arXiv:2201.01139},
  eprint = {2201.01139},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@article{bernardo1996concept,
  title = {The Concept of Exchangeability and Its Applications},
  author = {Bernardo, Jos{\'e} M.},
  year = {1996},
  journal = {Far East Journal of Mathematical Sciences},
  volume = {4},
  pages = {1--7},
  publisher = {SHIROCH REPROGRAPHICS},
  issn = {0971-4332},
  abstract = {The general concept of exchangeability allows the more flexible modelling of most experimental setups. The representation theorems for exchangeable sequences of random variables establish that any coherent analysis of the information thus modelled requires the specification of a joint probability distribution on all the parameters involved, hence forcing a Bayesian approach. The concept of partial exchangeability provides a further refinement, by permitting appropriate modelling of related experimental setups, leading to coherent information integration by means of so-called hierarchical models. Recent applications of hierarchical models for combining information from similar experiments in education, medicine and psychology have been produced under the name of meta-analysis.},
  keywords = {bayesian inference,exchangeability,hierarchical models,meta-analysis,nosource,reference distributions,representation theorems}
}

@incollection{bernardo2009bayesian,
  title = {Bayesian Theory},
  booktitle = {Wiley Series in Probability and Mathematical Statistics},
  author = {Bernardo, Jos{\'e} M. and Smith, Adrian F. M.},
  year = {2009},
  edition = {2nd},
  volume = {405},
  pages = {221--222},
  publisher = {Wiley},
  issn = {0957-0233},
  doi = {10.1088/0957-0233/12/2/702},
  abstract = {Within metrology the evaluation of measurement uncertainty plays a special and very important role. In the past the generally accepted procedure for establishing the uncertainty was the so-called error analysis . Within this procedure probability is interpreted strictly in the statistical (frequentist) sense. Since the available information needed to infer the measurand almost never consists of only observed statistical data, the inference on the basis of error analysis showed difficulties that in 1993 were overcome by an international recommendation to apply, at least within the various branches of metrology, Bayesian Statistics to measurement data. But the price to pay for the resulting consistent and satisfying evaluation procedure is to change one's view of probability and to accept the consequences. This needs guidance, particularly for those educated in frequency-based statistics. The book Bayesian Theory , first published in hardback in 1994, provides in about 600 pages a very clear, careful and well structured description of the foundations and key theoretical concepts of Bayesian Statistics. Although not written especially for metrologists and their needs to evaluate measurement data, the text to a large extent can be recommended without restriction to this community as an extensive and excellent introduction and guide to the world of Bayesian coherent reasoning. The detailed comparison with non-Bayesian theories, such as the frequentist procedures, is particularly enlightening in understanding the basic Bayesian concepts. In the volume the authors concentrate on the answer to the question of why Bayesian Statistics should be used at all. Only in future volumes will they deal with analytical and numerical techniques to implement Bayesian procedures and the study of methods of analysis for various types of models and problems. The contents of the book go well beyond the problem of statistical inference, which is viewed as a special case of decision theory. The interpretation of probability in Bayesian Statistics as well as the foundations of probability theory and decision theory are presented. Apart from modelling and model comparison the central Bayesian problem of the prior distribution, particularly in case of ignorance, is addressed at length. It is discussed in conjunction with the introduction of information-theoretic concepts. Critical issues are explicitly mentioned and discussed. Numerous examples illustrate the clearly expounded theoretical considerations. There are about 1500 references (only a few later than 1994) including a list of other Bayesian textbooks. A list of abbreviations and symbols used is unfortunately missing. Whereas the authors throughout focus on statistical concepts rather than rigorous mathematics the reader should be prepared for a mathematical presentation on the level of advanced calculus. The book will not be the primary source for an actual evaluation of the uncertainty of measurement given the model of evaluation and incomplete information about random and systematic effects occurring in measurement. But it certainly is an excellent primary source for those who wish to learn about the learning and decision process in a situation of uncertainty. It is this situation the metrologist faces after measurement when having to state what he has learned about the measurand. Wolfgang W{\"o}ger},
  isbn = {0-471-49464-X},
  pmid = {2871217},
  keywords = {nosource}
}

@article{Bernton2017,
  title = {Inference in Generative Models Using the {{Wasserstein}} Distance},
  author = {Bernton, Espen and Jacob, Pierre E and Gerber, Mathieu and Robert, Christian P},
  year = {2017},
  eprint = {1701.05146},
  pages = {1--30},
  abstract = {A growing range of generative statistical models are such the numerical evaluation of their likelihood functions is intractable. Approximate Bayesian computation and indirect inference have become popular approaches to overcome this issue, simulating synthetic data given parameters and comparing summaries of these simulations with the corresponding observed values. We propose to avoid these summaries and the ensuing loss of information through the use of Wasserstein distances between empirical distributions of observed and synthetic data. We describe how the approach can be used in the setting of dependent data such as time series, and how approximations of the Wasserstein distance allow the method to scale to large data sets. In particular, we propose a new approximation to the optimal assignment problem using the Hilbert space-filling curve. We provide an in-depth theoretical study, including consistency in the number of simulated data sets for a fixed number of observations and posterior concentration rates. The approach is illustrated with various examples, including a multivariate g-and-k distribution, a toggle switch model from systems biology, a queueing model, and a L{\textbackslash}'evy-driven stochastic volatility model.},
  archiveprefix = {arXiv},
  arxivid = {1701.05146},
  file = {/home/gkonkamking/Zotero/storage/GAZVTE25/Bernton et al. - 2017 - Inference in generative models using the Wasserste.pdf}
}

@article{berntonApproximateBayesianComputation2019,
  title = {Approximate {{Bayesian}} Computation with the {{Wasserstein}} Distance},
  author = {Bernton, Espen and Jacob, Pierre E. and Gerber, Mathieu and Robert, Christian P.},
  year = {2019},
  month = apr,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {81},
  number = {2},
  eprint = {1905.03747},
  pages = {235--269},
  issn = {13697412},
  doi = {10.1111/rssb.12312},
  urldate = {2020-07-21},
  abstract = {A growing number of generative statistical models do not permit the numerical evaluation of their likelihood functions. Approximate Bayesian computation (ABC) has become a popular approach to overcome this issue, in which one simulates synthetic data sets given parameters and compares summaries of these data sets with the corresponding observed values. We propose to avoid the use of summaries and the ensuing loss of information by instead using the Wasserstein distance between the empirical distributions of the observed and synthetic data. This generalizes the well-known approach of using order statistics within ABC to arbitrary dimensions. We describe how recently developed approximations of the Wasserstein distance allow the method to scale to realistic data sizes, and propose a new distance based on the Hilbert space-filling curve. We provide a theoretical study of the proposed method, describing consistency as the threshold goes to zero while the observations are kept fixed, and concentration properties as the number of observations grows. Various extensions to time series data are discussed. The approach is illustrated on various examples, including univariate and multivariate g-and-k distributions, a toggle switch model from systems biology, a queueing model, and a L{\textbackslash}'evy-driven stochastic volatility model.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/gkonkamking/Zotero/storage/IHL39MIT/Bernton et al. - 2019 - Approximate Bayesian computation with the Wasserst.pdf}
}

@article{bertolettiChoosingNumberClusters2015,
  title = {Choosing the Number of Clusters in a Finite Mixture Model Using an Exact Integrated Completed Likelihood Criterion},
  author = {Bertoletti, Marco and Friel, Nial and Rastelli, Riccardo},
  year = {2015},
  month = aug,
  journal = {METRON},
  volume = {73},
  number = {2},
  pages = {177--199},
  issn = {2281-695X},
  doi = {10.1007/s40300-015-0064-5},
  urldate = {2020-05-07},
  abstract = {The integrated completed likelihood (ICL) criterion has proven to be a very popular approach in model-based clustering through automatically choosing the number of clusters in a mixture model. This approach effectively maximises the complete data likelihood, thereby including the allocation of observations to clusters in the model selection criterion. However for practical implementation one needs to introduce an approximation in order to estimate the ICL. Our contribution here is to illustrate that through the use of conjugate priors one can derive an exact expression for ICL and so avoiding any approximation. Moreover, we illustrate how one can find both the number of clusters and the best allocation of observations in one algorithmic framework. The performance of our algorithm is presented on several simulated and real examples.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/US592J8M/Bertoletti et al. - 2015 - Choosing the number of clusters in a finite mixtur.pdf}
}

@article{bertrand2013multiple,
  title = {Multiple Single Nucleotide Polymorphism Analysis Using Penalized Regression in Nonlinear Mixed-Effect Pharmacokinetic Models},
  author = {Bertrand, Julie and Balding, David J},
  year = {2013},
  journal = {Pharmacogenetics and genomics},
  volume = {23},
  number = {3},
  pages = {167--174},
  publisher = {LWW},
  doi = {10.1097/FPC.0b013e32835dd22c}
}

@article{beskos2005exact,
  title = {Exact Simulation of Diffusions},
  author = {Beskos, Alexandros and Roberts, Gareth O and others},
  year = {2005},
  journal = {The Annals of Applied Probability},
  volume = {15},
  number = {4},
  pages = {2422--2444},
  publisher = {Institute of Mathematical Statistics},
  keywords = {nosource}
}

@article{beskosExactComputationallyEfficient2006,
  title = {Exact and {{Computationally Efficient Likelihood-Based Estimation}} for {{Discretely Observed Diffusion Processes}} (with {{Discussion}})},
  author = {Beskos, Alexandros and Papaspiliopoulos, Omiros and Roberts, Gareth O. and Fearnhead, Paul},
  year = {2006},
  month = jun,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {68},
  number = {3},
  pages = {333--382},
  issn = {1369-7412},
  doi = {10.1111/j.1467-9868.2006.00552.x},
  urldate = {2024-07-29},
  abstract = {The objective of the paper is to present a novel methodology for likelihood-based inference for discretely observed diffusions. We propose Monte Carlo methods, which build on recent advances on the exact simulation of diffusions, for performing maximum likelihood and Bayesian estimation.},
  file = {/home/gkonkamking/pCloudDrive/papers/Beskos et al_2006_Exact and Computationally Efficient Likelihood-Based Estimation for Discretely.pdf}
}

@article{beskosRetrospectiveExactSimulation2006,
  title = {Retrospective Exact Simulation of Diffusion Sample Paths with Applications},
  author = {Beskos, Alexandros and Papaspiliopoulos, Omiros and Roberts, Gareth O.},
  year = {2006},
  month = dec,
  journal = {Bernoulli},
  volume = {12},
  number = {6},
  pages = {1077--1098},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  doi = {10.3150/bj/1165269151},
  urldate = {2020-05-28},
  abstract = {We present an algorithm for exact simulation of a class of It{\^o}'s diffusions. We demonstrate that when the algorithm is applicable, it is also straightforward to simulate diffusions conditioned to hit specific values at predetermined time instances. We also describe a method that exploits the properties of the algorithm to carry out inference on discretely observed diffusions without resorting to any kind of approximation apart from the Monte Carlo error.},
  langid = {english},
  mrnumber = {MR2274855},
  zmnumber = {1129.60073},
  keywords = {conditioned diffusion processes,discretely observed diffusions,exact simulation,Monte Carlo maximum likelihood,rejection sampling},
  file = {/home/gkonkamking/Zotero/storage/SL95QEIK/Beskos et al. - 2006 - Retrospective exact simulation of diffusion sample.pdf}
}

@article{Besse2013,
  title = {Caged {{Gammarus}} Fossarum ({{Crustacea}}) as a Robust Tool for the Characterization of Bioavailable Contamination Levels in Continental Waters: {{Towards}} the Determination of Threshold Values},
  author = {Besse, Jean Philippe and Coquery, Marina and Lopes, Christelle and Chaumot, Arnaud and Budzinski, H{\'e}l{\`e}ne and Labadie, Pierre and Geffard, Olivier},
  year = {2013},
  month = feb,
  journal = {Water Research},
  volume = {47},
  number = {2},
  eprint = {23182666},
  eprinttype = {pubmed},
  pages = {650--660},
  issn = {00431354},
  doi = {10.1016/j.watres.2012.10.024},
  abstract = {We investigated the suitability of an active biomonitoring approach, using the ecologically relevant species Gammarus fossarum, to assess trends of bioavailable contamination in continental waters. Gammarids were translocated into cages at 27 sites, in the Rh??ne-Alpes region (France) during early autumn 2009. Study sites were chosen to represent different physico-chemical characteristics and various anthropic pressures. Biotic factors such as sex, weight and food availability were controlled in order to provide robust and comparable results. After one week of exposure, concentrations of 11 metals/metalloids (Cd, Pb, Hg, Ni, Zn, Cr, Co, Cu, As, Se and Ag) and 38 hydrophobic organic substances including polycyclic aromatic hydrocarbons (PAHs), polychlorobiphenyles (PCBs), pentabromodiphenylethers (PBDEs) and organochlorine pesticides, were measured in gammarids. All metals except Ag, and 33 organic substances among 38 were quantified in G. fossarum, showing that this species is relevant for chemical biomonitoring. The control of biotic factors allowed a robust and direct inter-site comparison of the bioavailable contamination levels. Overall, our results show the interest and robustness of the proposed methodological approach for assessing trends of bioavailable contamination, notably for metals and hydrophobic organic contaminants, in continental waters.Furthermore, we built threshold values of bioavailable contamination in gammarids, above which measured concentrations are expected to reveal a bioavailable contamination at the sampling site. Two ways to define such values were investigated, a statistical approach and a model fit. Threshold values were determined for almost all the substances investigated in this study and similar values were generally derived from the two approaches. Then, levels of contaminants measured in G. fossarum at the 27 study sites were compared to the threshold values obtained using the model fit. These threshold values could serve as a basis for further implementation of quality grids to rank sites according to the extent of the bioavailable contamination, with regard to the applied methodology. ?? 2012 Elsevier Ltd.},
  isbn = {00431354 (ISSN)},
  pmid = {23182666},
  keywords = {Active biomonitoring,Bioaccumulation,Freshwaters,Gammarids,nosource,Priority substances,Threshold values}
}

@article{Betancourt2017,
  title = {A Conceptual Introduction to Hamiltonian Monte Carlo},
  author = {Betancourt, Michael},
  year = {2017},
  eprint = {1701.02434},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous under- standing of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is con- fined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any ex- haustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  arxivid = {1701.02434},
  keywords = {nosource}
}

@article{betti2021detecting,
  title = {Detecting Adherence to the Recommended Childhood Vaccination Schedule from User-Generated Content in a {{US}} Parenting Forum},
  author = {Betti, Lorenzo and De Francisci Morales, Gianmarco and Gauvin, Laetitia and Kalimeri, Kyriaki and Mejova, Yelena and Paolotti, Daniela and Starnini, Michele},
  year = {2021},
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {4},
  pages = {e1008919},
  publisher = {Public Library of Science San Francisco, CA USA},
  doi = {10.1371/journal.pcbi.1008919},
  keywords = {nosource}
}

@article{bhadra2017horseshoe+,
  title = {The Horseshoe+ Estimator of Ultra-Sparse Signals},
  author = {Bhadra, Anindya and Datta, Jyotishka and Polson, Nicholas G and Willard, Brandon},
  year = {2017},
  journal = {Bayesian Analysis},
  volume = {12},
  number = {4},
  pages = {1105--1131},
  publisher = {International Society for Bayesian Analysis},
  keywords = {nosource},
  file = {/home/gkonkamking/pCloudDrive/papers/Bhadra et al_2017_The horseshoe+ estimator of ultra-sparse signals.pdf}
}

@techreport{bhattach:pati:dunson:12,
  title = {Anisotropic Function Estimation Using Multi-Bandwidth {{Gaussian}} Processes},
  booktitle = {The Annals of Statistics},
  author = {Bhattacharya, Anirban and Pati, Debdeep and Dunson, David B.},
  year = {2013},
  volume = {42},
  number = {1},
  eprint = {1111.1044v4},
  pages = {352--381},
  institution = {Duke University},
  issn = {0090-5364},
  doi = {10.1214/13-AOS1192},
  abstract = {In nonparametric regression problems involving multiple predictors, there is typically interest in estimating an anisotropic multivariate regression surface in the important predictors while discarding the unimportant ones. Our focus is on defining a Bayesian procedure that leads to the minimax optimal rate of posterior contraction (up to a log factor) adapting to the unknown dimension and anisotropic smoothness of the true surface. We propose such an approach based on a Gaussian process prior with dimension-specific scalings, which are assigned carefully-chosen hyperpriors. We additionally show that using a homogenous Gaussian process with a single bandwidth leads to a sub-optimal rate in anisotropic cases.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1111.1044v4},
  keywords = {60K35,62G07,62G20,adaptive,Adaptive,and phrases,anisotropic,Bayesi,bayesian nonparametrics,function es-,gaussian process,nosource,rate of convergence,timation}
}

@article{bhattacharya2010nonparametric,
  title = {Nonparametric {{Bayesian}} Density Estimation on Manifolds},
  author = {Science, Statistical and Carolina, North},
  year = {2010},
  journal = {Biometrika},
  volume = {97},
  number = {September},
  pages = {851--865},
  publisher = {Biometrika Trust},
  issn = {0006-3444},
  doi = {10.1093/biomet/asq044},
  keywords = {duplicate-citation-key,nosource}
}

@article{bhattacharya2010nonparametric,
  title = {Nonparametric {{Bayesian}} Density Estimation on Manifolds with Applications to Planar Shapes},
  author = {Bhattacharya, A and Dunson, D B},
  year = {2010},
  journal = {Biometrika},
  volume = {97},
  number = {4},
  pages = {851},
  publisher = {Biometrika Trust},
  issn = {0006-3444},
  keywords = {duplicate-citation-key,nosource}
}

@article{bhattacharyaBayesianShrinkage2012,
  title = {Bayesian Shrinkage},
  author = {Bhattacharya, Anirban and Pati, Debdeep and Pillai, Natesh S. and Dunson, David B.},
  year = {2012},
  month = dec,
  journal = {arXiv:1212.6088 [math, stat]},
  eprint = {1212.6088},
  primaryclass = {math, stat},
  urldate = {2021-10-01},
  abstract = {Penalized regression methods, such as \$L\_1\$ regularization, are routinely used in high-dimensional applications, and there is a rich literature on optimality properties under sparsity assumptions. In the Bayesian paradigm, sparsity is routinely induced through two-component mixture priors having a probability mass at zero, but such priors encounter daunting computational problems in high dimensions. This has motivated an amazing variety of continuous shrinkage priors, which can be expressed as global-local scale mixtures of Gaussians, facilitating computation. In sharp contrast to the corresponding frequentist literature, very little is known about the properties of such priors. Focusing on a broad class of shrinkage priors, we provide precise results on prior and posterior concentration. Interestingly, we demonstrate that most commonly used shrinkage priors, including the Bayesian Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet Laplace (DL) priors are proposed, which are optimal and lead to efficient posterior computation exploiting results from normalized random measure theory. Finite sample performance of Dirichlet Laplace priors relative to alternatives is assessed in simulations.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/gkonkamking/Zotero/storage/G74DHS59/Bhattacharya et al. - 2012 - Bayesian shrinkage.pdf}
}

@article{biernackiAssessingMixtureModel2000,
  title = {Assessing a Mixture Model for Clustering with the Integrated Completed Likelihood},
  author = {Biernacki, C. and Celeux, G. and Govaert, G.},
  year = {2000},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {22},
  number = {7},
  pages = {719--725},
  issn = {1939-3539},
  doi = {10.1109/34.865189},
  abstract = {We propose an assessing method of mixture model in a cluster analysis setting with integrated completed likelihood. For this purpose, the observed data are assigned to unknown clusters using a maximum a posteriori operator. Then, the integrated completed likelihood (ICL) is approximated using the Bayesian information criterion (BIC). Numerical experiments on simulated and real data of the resulting ICL criterion show that it performs well both for choosing a mixture model and a relevant number of clusters. In particular, ICL appears to be more robust than BIC to violation of some of the mixture model assumptions and it can select a number of dusters leading to a sensible partitioning of the data.},
  keywords = {Bayes methods,Bayesian information criterion,Bayesian methods,clustering,Context modeling,Gaussian distribution,information theory,maximum likelihood estimation,mixture model assessment,nosource,Numerical simulation,pattern recognition,probability,Probability distribution,Robustness}
}

@article{Biips,
  title = {Biips: {{Software}} for Bayesian Inference with Interacting Particle Systems},
  author = {Todeschini, Adrien and Caron, Fran{\c c}ois and Fuentes, Marc and Legrand, Pierrick and Del Moral, Pierre},
  year = {2014},
  journal = {arXiv},
  keywords = {duplicate-citation-key,nosource}
}

@article{Biips,
  title = {Rbiips: {{Bayesian}} Inference with Interacting Particle Systems},
  author = {Todeschini, Adrien and Caron, Fran{\c c}ois and Fuentes, Marc},
  year = {2014},
  journal = {arXiv},
  keywords = {duplicate-citation-key,nosource}
}

@article{Billoir2008,
  title = {A {{Bayesian}} Approach to Analyzing Ecotoxicological Data.},
  author = {Billoir, Elise and {Delignette-Muller}, Marie Laure and P{\'e}ry, Alexandre R R and Charles, Sandrine},
  year = {2008},
  month = dec,
  journal = {Environmental science \& technology},
  volume = {42},
  number = {23},
  eprint = {19192828},
  eprinttype = {pubmed},
  pages = {8978--84},
  issn = {0013-936X},
  abstract = {Standardized chronic toxicity tests are usually analyzed using a NOEC (no observed effect concentration) or ECx (x\% effect concentration) calculation. However,these methods provide very little information for the material cost they entail. It has been proposed that biology-based methods, such as the DEBtox approach, would make better use of the data available. DEBtox deals with the energy balance between physiological processes, and gives insight on how a compound disturbs it. We propose that data analysis can be further improved by estimating the DEBtox parameters using the considerable expertise available in laboratories and/or the literature. The Bayesian inference appears to be an appropriate estimation method for this purpose, as this technique takes expertise into account as prior probability distribution for each parameter, and provides the corresponding posterior distributions given the data. From these posterior distributions, point estimates can easily be deduced, but also credible intervals which are ideal for use in risk assessment. In this paper, we demonstrate this approach through the analysis of two 21-day Daphnia reproduction tests.},
  isbn = {0013-936X (Print) 0013-936X (Linking)},
  pmid = {19192828},
  keywords = {*Bayes Theorem,*Databases as Topic,*Ecotoxicology,Animals,Bayes Theorem,Biological,Copper,Copper: toxicity,Copper/toxicity,Daphnia,Daphnia: drug effects,Daphnia: physiology,Daphnia/drug effects/physiology,Databases as Topic,duplicate-citation-key,Ecotoxicology,Models,nosource,Reproduction,Reproduction: drug effects,Reproduction/drug effects,Zinc,Zinc: toxicity,Zinc/toxicity}
}

@article{Billoir2009,
  title = {{{DEBtox}} Theory and Matrix Population Models as Helpful Tools in Understanding the Interaction between Toxic Cyanobacteria and Zooplankton},
  author = {Billoir, Elise and {da Silva Ferr{\~a}o-Filho}, Aloysio and {Laure Delignette-Muller}, Marie and Charles, Sandrine},
  year = {2009},
  month = jun,
  journal = {Journal of Theoretical Biology},
  volume = {258},
  number = {3},
  eprint = {18706427},
  eprinttype = {pubmed},
  pages = {380--388},
  issn = {00225193},
  doi = {10.1016/j.jtbi.2008.07.029},
  abstract = {Bioassays were performed to find out how field samples of the toxic cyanobacteria Microcystis aeruginosa affect Moina micrura, a cladoceran found in the tropical Jacarepagua Lagoon (Rio de Janeiro, Brazil). The DEBtox (Dynamic Energy Budget theory applied to toxicity data) approach has been proposed for use in analysing chronic toxicity tests as an alternative to calculating the usual safety parameters (NOEC, ECx). DEBtox theory deals with the energy balance between physiological processes (assimilation, maintenance, growth and reproduction), and it can be used to investigate and compare various hypotheses concerning the mechanism of action of a toxicant. Even though the DEBtox framework was designed for standard toxicity bioassays carried out with standard species (fish, daphnids), we applied the growth and reproduction models to M. micrura, by adapting the data available using a weight-length allometric relationship. Our modelling approach appeared to be very relevant at the individual level, and confirmed previous conclusions about the toxic mechanism. In our study we also wanted to assess the toxic effects at the population level, which is a more relevant endpoint in risk assessment. We therefore incorporated both lethal and sublethal toxic effects in a matrix population model used to calculate the finite rate of population change as a continuous function of the exposure concentration. Alongside this calculation, we constructed a confidence band to predict the critical exposure concentration for population health. Finally, we discuss our findings with regard to the prospects for further refining the analysis of ecotoxicological data. {\copyright} 2008 Elsevier Ltd. All rights reserved.},
  isbn = {0022-5193},
  pmid = {18706427},
  keywords = {Allometry,Bayesian inference,DEBtox models,Matrix population model,Microcystis aeruginosa,Moina micrura,nosource}
}

@article{Billoir2011,
  title = {Bayesian Modelling of Daphnid Responses to Time-Varying Cadmium Exposure in Laboratory Aquatic Microcosms},
  author = {Billoir, Elise and Delhaye, H{\'e}l{\`e}ne and Cl{\'e}ment, Bernard and {Delignette-Muller}, Marie Laure and Charles, Sandrine},
  year = {2011},
  month = may,
  journal = {Ecotoxicology and Environmental Safety},
  volume = {74},
  number = {4},
  eprint = {21056469},
  eprinttype = {pubmed},
  pages = {693--702},
  issn = {01476513},
  doi = {10.1016/j.ecoenv.2010.10.023},
  abstract = {Experiments were carried out to test the effects of cadmium on five aquatic species in 2-L indoor freshwater/sediment microcosms. Experimental data were collected over 21 days in static conditions, i.e. the microcosms evolved without water renewal. Because of speciation, the total cadmium concentration in water decreased with time. Here we present a focus on Daphnia magna responses. For the three life history traits we considered (survival, growth and reproduction), mathematical effect models were built based on threshold stress functions involving no effect concentrations (NECs). These models took the time-varying conditions of exposure into account through a time-recurrent formalism. Within a Bayesian framework, four kinds of data were fitted simultaneously (exposure, survival, growth and reproduction), using an appropriate error model for each endpoint. Hence, NECs were determined as well as their associated estimation uncertainty. Through this modelling approach, we demonstrate that thresholds for stress functions can be successfully inferred even in experimental setup more complex than standard bioassays. ?? 2010 Elsevier Inc.},
  pmid = {21056469},
  keywords = {Bayesian modelling,No effect concentration,nosource,Simultaneous fitting,Threshold effect model,Time-varying concentration}
}

@article{Billoir2012,
  title = {Comparison of Bioassays with Different Exposure Time Patterns: {{The}} Added Value of Dynamic Modelling in Predictive Ecotoxicology},
  author = {Billoir, Elise and Delhaye, H{\'e}l{\`e}ne and Forfait, Carole and Cl{\'e}ment, Bernard and {Triffault-Bouchet}, Ga{\"e}lle and Charles, Sandrine and {Delignette-Muller}, Marie Laure},
  year = {2012},
  month = jan,
  journal = {Ecotoxicology and Environmental Safety},
  volume = {75},
  number = {1},
  eprint = {21889211},
  eprinttype = {pubmed},
  pages = {80--86},
  issn = {01476513},
  doi = {10.1016/j.ecoenv.2011.08.006},
  abstract = {The purpose of this study was to compare Daphnia magna responses to cadmium between two toxicity experiments performed in static and flow-through conditions. As a consequence of how water was renewed, the two experiments were characterised by two different exposure time patterns for daphnids, time-varying and constant, respectively. Basing on survival, growth and reproduction, we addressed the questions of organism development and sensitivity to cadmium. Classical analysis methods are not designed to deal with the time dimension and therefore not suitable to compare effects of different exposure time patterns. We used instead a dynamic modelling framework taking all timepoints and the time course of exposure into account, making comparable the results obtained from our two experiments. This modelling framework enabled us to detect an improvement of organism development in flow-through conditions compared to static ones and infer similar sensitivity to cadmium for both exposure time patterns. {\copyright} 2011 Elsevier Inc.},
  pmid = {21889211},
  keywords = {Bayesian analysis,Cadmium,Daphnia magna,No effect concentration,nosource,Time-varying concentration,TKTD models (toxicokinetic-toxicodynamic)}
}

@article{binder1978bayesian,
  title = {Bayesian Cluster Analysis},
  author = {Binder, David A},
  year = {1978},
  journal = {Biometrika},
  volume = {65},
  number = {1},
  pages = {31--38},
  publisher = {Biometrika Trust},
  keywords = {nosource}
}

@incollection{bingham2010probability:chap15,
  title = {Diffusion Processes and Coalescent Trees},
  booktitle = {Probability and Mathematical Genetics: Papers in Honour of {{Sir John Kingman}}},
  author = {Griffiths, R. C. and Span{\`o}, Dario},
  editor = {Bingham, Nicholas H and Goldie, Charles M},
  year = {2010},
  volume = {378},
  pages = {358--379},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9781139107174},
  chapter = {15},
  keywords = {nosource}
}

@article{Biron2011,
  title = {Population-Level Modeling to Account for Multigenerational Effects of Uranium in Daphnia Magna},
  author = {Biron, Pierre Albin and Massarin, Sandrine and Alonzo, Fr{\'e}d{\'e}ric and {Garcia-Sanchez}, Laurent and Charles, Sandrine and Billoir, Elise},
  year = {2012},
  journal = {Environmental Science and Technology},
  volume = {46},
  number = {2},
  pages = {1136--1143},
  publisher = {ACS Publications},
  issn = {0013936X},
  doi = {10.1021/es202658b},
  abstract = {As part of the ecological risk assessment associated with radionuclides in freshwater ecosystems, toxicity of waterborne uranium was recently investigated in the microcrustacean Daphnia magna over a three-generation exposure (F0, F1, and F2). Toxic effects on daphnid life history and physiology, increasing over generations, were demonstrated at the organism level under controlled laboratory conditions. These effects were modeled using an approach based on the dynamic energy budget (DEB). For each of the three successive generations, DEBtox (dynamic energy budget applied to toxicity data) models were fitted to experimental data. Lethal and sublethal DEBtox outcomes and their uncertainty were projected to the population level using population matrix techniques. To do so, we compared two modeling approaches in which experimental results from F0, F1, and F2 generations were either considered separately (F0-, F1-, and F2-based simulations) or together in the actual succession of F0, F1, and F2 generations (multi-F-based simulation). The first approach showed that considering results from F0 only (equivalent to a standard toxicity test) would lead to a severe underestimation of uranium toxicity at the population level. Results from the second approach showed that combining effects in successive generations cannot generally be simplified to the worst case among F0-, F1-, and F2-based population dynamics.},
  isbn = {0013-936X},
  pmid = {22118338},
  keywords = {nosource}
}

@article{bisbee_2019,
  title = {{{BARP}}: {{Improving}} Mister {{P}} Using Bayesian Additive Regression Trees},
  author = {Bisbee, James},
  year = {2019},
  journal = {American Political Science Review},
  volume = {113},
  number = {4},
  pages = {1060--1065},
  publisher = {Cambridge University Press},
  doi = {10.1017/S0003055419000480}
}

@article{BKSE12,
  title = {\{\vphantom\}{{J}}\vphantom\{\}ulia: {{A}} Fast Dynamic Language for Technical Computing},
  author = {Bezanson, Jeff and Karpinski, Stefan and Shah, Viral B and Edelman, Alan},
  year = {2012},
  abstract = {Dynamic languages have become popular for scientific computing. They are generally considered highly productive, but lacking in performance. This paper presents Julia, a new dynamic language for technical computing, designed for performance from the beginning by adapting and extending modern programming language techniques. A design based on generic functions and a rich type system simultaneously enables an expressive programming model and successful type inference, leading to good performance for a wide range of programs. This makes it possible for much of the Julia library to be written in Julia itself, while also incorporating best-of-breed C and Fortran libraries.},
  keywords = {nosource}
}

@article{blangiardoEstimatingWeeklyExcess2020,
  title = {Estimating Weekly Excess Mortality at Sub-National Level in {{Italy}} during the {{COVID-19}} Pandemic},
  author = {Blangiardo, Marta and Cameletti, Michela and Pirani, Monica and Corsetti, Gianni and Battaglini, Marco and Baio, Gianluca},
  year = {2020},
  month = oct,
  journal = {PLOS ONE},
  volume = {15},
  number = {10},
  pages = {e0240286},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0240286},
  urldate = {2021-05-07},
  abstract = {In this study we present the first comprehensive analysis of the spatio-temporal differences in excess mortality during the COVID-19 pandemic in Italy. We used a population-based design on all-cause mortality data, for the 7,904 Italian municipalities. We estimated sex-specific weekly mortality rates for each municipality, based on the first four months of 2016--2019, while adjusting for age, localised temporal trends and the effect of temperature. Then, we predicted all-cause weekly deaths and mortality rates at municipality level for the same period in 2020, based on the modelled spatio-temporal trends. Lombardia showed higher mortality rates than expected from the end of February, with 23,946 (23,013 to 24,786) total excess deaths. North-West and North-East regions showed one week lag, with higher mortality from the beginning of March and 6,942 (6,142 to 7,667) and 8,033 (7,061 to 9,044) total excess deaths respectively. We observed marked geographical differences also at municipality level. For males, the city of Bergamo (Lombardia) showed the largest percent excess, 88.9\% (81.9\% to 95.2\%), at the peak of the pandemic. An excess of 84.2\% (73.8\% to 93.4\%) was also estimated at the same time for males in the city of Pesaro (Central Italy), in stark contrast with the rest of the region, which does not show evidence of excess deaths. We provided a fully probabilistic analysis of excess mortality during the COVID-19 pandemic at sub-national level, suggesting a differential direct and indirect effect in space and time. Our model can be used to help policy-makers target measures locally to contain the burden on the health-care system as well as reducing social and economic consequences. Additionally, this framework can be used for real-time mortality surveillance, continuous monitoring of local temporal trends and to flag where and when mortality rates deviate from the expected range, which might suggest a second wave of the pandemic.},
  langid = {english},
  keywords = {COVID 19,Death rates,Disease surveillance,Geography,Italian people,Italy,Medical risk factors,Pandemics},
  file = {/home/gkonkamking/Zotero/storage/GR24255A/Blangiardo et al. - 2020 - Estimating weekly excess mortality at sub-national.pdf}
}

@article{blankenship2008opening,
  title = {Opening the Mind to Close It: {{Considering}} a Message in Light of Important Values Increases Message Processing and Later Resistance to Change.},
  author = {Blankenship, Kevin L and Wegener, Duane T},
  year = {2008},
  journal = {Journal of Personality and Social Psychology},
  volume = {94},
  number = {2},
  pages = {196},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{blei2003latent,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  year = {2003},
  journal = {Journal of machine Learning research},
  volume = {3},
  number = {Jan},
  pages = {993--1022},
  keywords = {nosource}
}

@article{Blei2007,
  title = {The Nested {{Chinese}} Restaurant Process and {{Bayesian}} Nonparametric Inference of Topic Hierarchies},
  author = {Blei, David M. and Griffiths, Thomas L. and Jordan, Michael I.},
  year = {2010},
  journal = {Journal of the ACM},
  volume = {57},
  number = {2},
  eprint = {0710.0845},
  pages = {1--30},
  issn = {00045411},
  doi = {10.1145/1667053.1667056},
  abstract = {We present the nested Chinese restaurant process (nCRP), a stochastic process which assigns probability distributions to infinitely-deep, infinitely-branching trees. We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections. Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction. Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree. We demonstrate this algorithm on collections of scientific abstracts from several journals. This model exemplifies a recent trend in statistical machine learning--the use of Bayesian nonparametric methods to infer distributions on flexible data structures.},
  archiveprefix = {arXiv},
  arxivid = {0710.0845},
  isbn = {00045411},
  keywords = {Bayesian nonparametric statistics,nosource,unsupervised le}
}

@article{blei2017variational,
  ids = {bleiVariationalInferenceReview2017},
  title = {Variational Inference: {{A}} Review for Statisticians},
  author = {Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
  year = {2017},
  journal = {Journal of the American Statistical Association},
  number = {just-accepted},
  publisher = {Taylor \& Francis},
  keywords = {Algorithms,Computationally intensive methods,Statistical computing},
  file = {/home/gkonkamking/Zotero/storage/8WU5S8GA/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf}
}

@article{bleiVariationalInferenceDirichlet2006,
  title = {Variational Inference for {{Dirichlet}} Process Mixtures},
  author = {Blei, David M. and Jordan, Michael I.},
  year = {2006},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {1},
  number = {1},
  issn = {1936-0975},
  doi = {10.1214/06-BA104},
  urldate = {2021-06-08},
  abstract = {Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of nonparametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to explore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/78XF9IFE/Blei and Jordan - 2006 - Variational inference for Dirichlet process mixtur.pdf}
}

@article{blokzijlMutationalPatternsComprehensiveGenomewide2018,
  title = {{{MutationalPatterns}}: Comprehensive Genome-Wide Analysis of Mutational Processes},
  shorttitle = {{{MutationalPatterns}}},
  author = {Blokzijl, Francis and Janssen, Roel and Van Boxtel, Ruben and Cuppen, Edwin},
  year = {2018},
  month = dec,
  journal = {Genome Medicine},
  volume = {10},
  number = {1},
  pages = {33},
  issn = {1756-994X},
  doi = {10.1186/s13073-018-0539-0},
  urldate = {2024-04-19},
  abstract = {Background: Base substitution catalogues represent historical records of mutational processes that have been active in a cell. Such processes can be distinguished by various characteristics, like mutation type, sequence context, transcriptional and replicative strand bias, genomic distribution and association with (epi)-genomic features. Results: We have created MutationalPatterns, an R/Bioconductor package that allows researchers to characterize a broad range of patterns in base substitution catalogues to dissect the underlying molecular mechanisms. Furthermore, it offers an efficient method to quantify the contribution of known mutational signatures within single samples. This analysis can be used to determine whether certain DNA repair mechanisms are perturbed and to further characterize the processes underlying known mutational signatures. Conclusions: MutationalPatterns allows for easy characterization and visualization of mutational patterns. These analyses willsupport fundamental research into mutational mechanisms and may ultimately improve cancer diagnosis and treatment strategies. MutationalPatterns is freely available at http://bioconductor.org/packages/ MutationalPatterns.},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Blokzijl et al_2018_MutationalPatterns.pdf}
}

@article{blondel2012data,
  title = {Data for Development: The D4d Challenge on Mobile Phone Data},
  author = {Blondel, Vincent D and Esch, Markus and Chan, Connie and Cl{\'e}rot, Fabrice and Deville, Pierre and Huens, Etienne and Morlot, Fr{\'e}d{\'e}ric and Smoreda, Zbigniew and Ziemlicki, Cezary},
  year = {2012},
  journal = {arXiv preprint arXiv:1210.0137},
  eprint = {1210.0137},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{blondel2015survey,
  title = {A Survey of Results on Mobile Phone Datasets Analysis},
  author = {Blondel, Vincent D and Decuyper, Adeline and Krings, Gautier},
  year = {2015},
  journal = {EPJ data science},
  volume = {4},
  number = {1},
  pages = {10},
  publisher = {Springer},
  keywords = {nosource}
}

@article{blondelWeightedNonnegativeMatrix,
  title = {Weighted {{Nonnegative Matrix Factorization}} and {{Face Feature Extraction}}},
  author = {Blondel, Vincent D and Ho, Ngoc-Diep and {van Dooren}, Paul},
  pages = {17},
  abstract = {In this paper we consider weighted nonnegative matrix factorizations and we show that the popular algorithms of Lee and Seung can incorporate such a weighting. We then prove that for appropriately chosen weighting matrices, the weighted Euclidean distance function and the weighted generalized Kullback-Leibler divergence function are essentially identical. We finally show that the weighting can be chosen to emphasize parts of the data matrix to be approximated and we can apply this to the low rank fitting of a face image database.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/SYAUXQ6J/Blondel et al. - Weighted Nonnegative Matrix Factorization and Face.pdf}
}

@article{Blum2013,
  title = {A Comparative Review of Dimension Reduction Methods in Approximate Bayesian Computation},
  author = {Blum, M G B and Nunes, M A and Prangle, D and Sisson, S A},
  year = {2013},
  journal = {Statistical Science},
  volume = {28},
  number = {2},
  eprint = {1202.3819v3},
  pages = {189--208},
  issn = {0883-4237},
  doi = {10.1214/12-STS406},
  abstract = {Approximate Bayesian computation (ABC) methods make use of comparisons between simulated and observed summary statistics to over-come the problem of computationally intractable likelihood functions. As the practical implementation of ABC requires computations based on vectors of summary statistics, rather than full data sets, a central question is how to derive low-dimensional summary statistics from the observed data with min-imal loss of information. In this article we provide a comprehensive review and comparison of the performance of the principal methods of dimension reduction proposed in the ABC literature. The methods are split into three nonmutually exclusive classes consisting of best subset selection methods, projection techniques and regularization. In addition, we introduce two new methods of dimension reduction. The first is a best subset selection method based on Akaike and Bayesian information criteria, and the second uses ridge regression as a regularization procedure. We illustrate the performance of these dimension reduction techniques through the analysis of three challeng-ing models and data sets.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1202.3819v3},
  isbn = {0883-4237},
  keywords = {Approximate Bayesian computation,dimension reduction,likelihood-free inference,nosource,regularization,variable selection}
}

@article{BM73,
  title = {Ferguson Distributions via {{P{\'o}lya}} Urn Schemes},
  author = {Blackwell, David and MacQueen, James B},
  year = {1973},
  journal = {The annals of statistics},
  pages = {353--355},
  publisher = {JSTOR},
  keywords = {nosource}
}

@article{bocarejo2012transport,
  title = {Transport Accessibility and Social Inequities: A Tool for Identification of Mobility Needs and Evaluation of Transport Investments},
  author = {Bocarejo S, Juan Pablo and Oviedo H, Daniel Ricardo},
  year = {2012},
  journal = {Journal of transport geography},
  volume = {24},
  pages = {142--154},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{Boeckman2016,
  title = {Use of Species Sensitivity Distributions to Characterize Hazard for Insecticidal Traits},
  author = {Boeckman, Chad J. and Layton, Raymond},
  year = {2016},
  journal = {Journal of Invertebrate Pathology},
  pages = {8--10},
  publisher = {Elsevier Inc.},
  issn = {00222011},
  doi = {10.1016/j.jip.2016.08.006},
  keywords = {environmental risk assessment,nosource,species sensitivity distribution}
}

@article{boender1987multinomial,
  title = {A Multinomial {{Bayesian}} Approach to the Estimation of Population and Vocabulary Size},
  author = {Boender, C G E and Kan, A H G Rinnooy},
  year = {1987},
  journal = {Biometrika},
  volume = {74},
  number = {4},
  pages = {849--856},
  publisher = {Biometrika Trust},
  keywords = {nosource}
}

@article{Bonassi2015,
  title = {Sequential Monte Carlo with Adaptive Weights for Approximate Bayesian Computation},
  author = {Bonassi, Fernando V. and West, Mike},
  year = {2015},
  journal = {Bayesian Analysis},
  volume = {10},
  number = {1},
  eprint = {1503.07791v1},
  pages = {171--187},
  issn = {19316690},
  doi = {10.1214/14-BA891},
  abstract = {Methods of approximate Bayesian computation (ABC) are increas-ingly used for analysis of complex models. A major challenge for ABC is over-coming the often inherent problem of high rejection rates in the accept/reject methods based on prior:predictive sampling. A number of recent developments aim to address this with extensions based on sequential Monte Carlo (SMC) strategies. We build on this here, introducing an ABC SMC method that uses data-based adaptive weights. This easily implemented and computationally trivial extension of ABC SMC can very substantially improve acceptance rates, as is demonstrated in a series of examples with simulated and real data sets, including a currently topical example from dynamic modelling in systems biology applications.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1503.07791v1},
  keywords = {Adaptive simulation,Complex modelling,Dynamic bionetwork models,Importance sampling,Mixture model emulators,nosource}
}

@book{book:313129,
  type = {Pdf},
  title = {Nonnegative Matrix and Tensor Factorizations: {{Applications}} to Exploratory Multi-Way Data Analysis and Blind Source Separation},
  author = {Cichocki, Andrzej and Zdunek, Rafal and Phan, Anh Huy and Amari, Shun-ichi},
  year = {2009},
  publisher = {John Wiley \& Sons, Ltd},
  keywords = {nosource}
}

@book{book:54642,
  type = {Pdf},
  title = {Inference in Hidden Markov Models},
  author = {Capp{\'e}, Olivier and Moulines, Eric and Ryden, Tobias},
  year = {2005},
  publisher = {Springer},
  address = {New York, NY, USA},
  keywords = {nosource}
}

@article{Boone2014,
  title = {A {{Hellinger}} Distance Approach to {{MCMC}} Diagnostics},
  author = {Boone, Edward L. and Merrick, Jason R.W. and Krachey, Matthew J.},
  year = {2014},
  journal = {Journal of Statistical Computation and Simulation},
  volume = {84},
  number = {4},
  pages = {833--849},
  issn = {00949655},
  doi = {10.1080/00949655.2012.729588},
  abstract = {Bayesian analysis often requires the researcher to employ Markov Chain Monte Carlo (MCMC) techniques to draw samples from a posterior distribution which in turn is used to make inferences. Currently, several approaches to determine convergence of the chain as well as sensitivities of the resulting inferences have been developed. This work develops a Hellinger distance approach to MCMC diagnostics. An approximation to the Hellinger distance between two distributions f and g based on sampling is introduced. This approximation is studied via simulation to determine the accuracy. A criterion for using this Hellinger distance for determining chain convergence is proposed as well as a criterion for sensitivity studies. These criteria are illustrated using a dataset concerning the Anguilla australis, an eel native to New Zealand.},
  keywords = {Bayesian robustness,Hellinger distance,kernel density estimation,Markov chain Monte Carlo,nosource}
}

@article{boppHierarchicalMaxInfinitelyDivisible2021,
  title = {A {{Hierarchical Max-Infinitely Divisible Spatial Model}} for {{Extreme Precipitation}}},
  author = {Bopp, Gregory P. and Shaby, Benjamin A. and Huser, Rapha{\"e}l},
  year = {2021},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {116},
  number = {533},
  pages = {93--106},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1750414},
  urldate = {2022-03-03},
  abstract = {Understanding the spatial extent of extreme precipitation is necessary for determining flood risk and adequately designing infrastructure (e.g., stormwater pipes) to withstand such hazards. While environmental phenomena typically exhibit weakening spatial dependence at increasingly extreme levels, limiting max-stable process models for block maxima have a rigid dependence structure that does not capture this type of behavior. We propose a flexible Bayesian model from a broader family of (conditionally) max-infinitely divisible processes that allows for weakening spatial dependence at increasingly extreme levels, and due to a hierarchical representation of the likelihood in terms of random effects, our inference approach scales to large datasets. Therefore, our model not only has a flexible dependence structure, but it also allows for fast, fully Bayesian inference, prediction and conditional simulation in high dimensions. The proposed model is constructed using flexible random basis functions that are estimated from the data, allowing for straightforward inspection of the predominant spatial patterns of extremes. In addition, the described process possesses (conditional) max-stability as a special case, making inference on the tail dependence class possible. We apply our model to extreme precipitation in North-Eastern America, and show that the proposed model adequately captures the extremal behavior of the data. Interestingly, we find that the principal modes of spatial variation estimated from our model resemble observed patterns in extreme precipitation events occurring along the coast (e.g., with localized tropical cyclones and convective storms) and mountain range borders. Our model, which can easily be adapted to other types of environmental datasets, is therefore useful to identify extreme weather patterns and regions at risk. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  keywords = {Block maxima,Max-infinitely divisible process,Max-stable process,Subasymptotic extremes},
  file = {/home/gkonkamking/Zotero/storage/Q5Q3XV2E/Bopp et al. - 2021 - A Hierarchical Max-Infinitely Divisible Spatial Mo.pdf}
}

@article{borges1998family,
  title = {A Family of Nonextensive Entropies},
  author = {Borges, E P and Roditi, I},
  year = {1998},
  journal = {Physics Letters A},
  volume = {246},
  number = {5},
  pages = {399--402},
  publisher = {Elsevier},
  issn = {0375-9601},
  doi = {http://dx.doi.org/10.1016/S0375-9601(98)00572-6},
  arxiv = {S0375960198005726},
  arxivid = {S0375960198005726},
  isbn = {0375-9601},
  keywords = {nosource}
}

@article{BorjaHaigh2007,
  title = {The Birthday Problem},
  author = {Borja, Mario Cortina and Haigh, John},
  year = {2007},
  month = aug,
  journal = {Significance},
  volume = {4},
  number = {3},
  eprint = {https://academic.oup.com/jrssig/article-pdf/4/3/124/49109395/sign{\textbackslash}\_4{\textbackslash}\_3{\textbackslash}\_124.pdf},
  pages = {124--127},
  issn = {1740-9705},
  doi = {10.1111/j.1740-9713.2007.00246.x},
  abstract = {Happy Birthday! If it is not your birthday today, it may well be the birthday of someone in your office, or in your class at school. Possibly, it is the birthday of two people in your office or your class. Just how big does a group have to be before its members are likely to share a birthday? Mario Cortina Borja and John Haigh explain the birthday problem.}
}

@article{boroditskyMetaphoricStructuringUnderstanding2000,
  ids = {boroditskyMetaphoricStructuringUnderstanding2000a},
  title = {Metaphoric Structuring: Understanding Time through Spatial Metaphors},
  shorttitle = {Metaphoric Structuring},
  author = {Boroditsky, Lera},
  year = {2000},
  month = apr,
  journal = {Cognition},
  volume = {75},
  number = {1},
  pages = {1--28},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(99)00073-6},
  urldate = {2020-05-11},
  abstract = {The present paper evaluates the claim that abstract conceptual domains are structured through metaphorical mappings from domains grounded directly in experience. In particular, the paper asks whether the abstract domain of time gets its relational structure from the more concrete domain of space. Relational similarities between space and time are outlined along with several explanations of how these similarities may have arisen. Three experiments designed to distinguish between these explanations are described. The results indicate that (1) the domains of space and time do share conceptual structure, (2) spatial relational information is just as useful for thinking about time as temporal information, and (3) with frequent use, mappings between space and time come to be stored in the domain of time and so thinking about time does not necessarily require access to spatial schemas. These findings provide some of the first empirical evidence for Metaphoric Structuring. It appears that abstract domains such as time are indeed shaped by metaphorical mappings from more concrete and experiential domains such as space.},
  langid = {english},
  keywords = {Metaphoric structuring,nosource,Spatial metaphors,Understanding time}
}

@article{boulesteix2005predicting,
  title = {Predicting Transcription Factor Activities from Combined Analysis of Microarray and {{ChIP}} Data: A Partial Least Squares Approach},
  author = {Boulesteix, Anne-Laure and Strimmer, Korbinian},
  year = {2005},
  journal = {Theoretical Biology and Medical Modelling},
  volume = {2},
  number = {1},
  pages = {23},
  publisher = {BioMed Central},
  keywords = {nosource}
}

@inproceedings{boyle2004dependent,
  title = {Dependent Gaussian Processes},
  booktitle = {In Advances in Neural Information Processing Systems 17},
  author = {Boyle, Phillip and Frean, Marcus},
  year = {2005},
  pages = {217--224},
  abstract = {Gaussian processes are usually parameterised in terms of their covari- ance functions. However, this makes it difficult to deal with multiple outputs, because ensuring that the covariance matrix is positive definite is problematic. An alternative formulation is to treat Gaussian processes as white noise sources convolved with smoothing kernels, and to param- eterise the kernel instead. Using this, we extend Gaussian processes to handle multiple, coupled outputs.},
  keywords = {nosource}
}

@article{boysBayesianInferenceDiscretely2008,
  title = {Bayesian Inference for a Discretely Observed Stochastic Kinetic Model},
  author = {Boys, R. J. and Wilkinson, D. J. and Kirkwood, T. B. L.},
  year = {2008},
  month = jun,
  journal = {Statistics and Computing},
  volume = {18},
  number = {2},
  pages = {125--135},
  issn = {1573-1375},
  doi = {10.1007/s11222-007-9043-x},
  urldate = {2020-04-07},
  abstract = {The ability to infer parameters of gene regulatory networks is emerging as a key problem in systems biology. The biochemical data are intrinsically stochastic and tend to be observed by means of discrete-time sampling systems, which are often limited in their completeness. In this paper we explore how to make Bayesian inference for the kinetic rate constants of regulatory networks, using the stochastic kinetic Lotka-Volterra system as a model. This simple model describes behaviour typical of many biochemical networks which exhibit auto-regulatory behaviour. Various MCMC algorithms are described and their performance evaluated in several data-poor scenarios. An algorithm based on an approximating process is shown to be particularly efficient.},
  langid = {english},
  keywords = {nosource}
}

@article{bradley1986dependence,
  title = {Dependence in Probability and Statistics},
  author = {Bertail, P and Doukhan, Paul and Soulier, P},
  year = {2006},
  journal = {Eberlin Tassu},
  eprint = {1011.1669v3},
  pages = {490},
  issn = {1098-6596},
  doi = {10.1007/978-3-642-14104-1},
  abstract = {This book gives an account of recent developments in the field of probability and statistics for dependent data. It covers a wide range of topics from Markov chain theory and weak dependence with an emphasis on some recent developments on dynamical systems, to strong dependence in times series and random fields. There is a section on statistical estimation problems and specific applications. The book is written as a succession of papers by field specialists, alternating general surveys, mostly at a level accessible to graduate students in probability and statistics, and more general research papers mainly suitable to researchers in the field.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {9780387317410},
  pmid = {25246403},
  keywords = {duplicate-citation-key,nosource}
}

@article{bradley1986dependence,
  title = {Dependence in Probability and Statistics},
  author = {Bradley, Richard and Dehling, Herold and Doukhan, Paul and Neumann, Michael H},
  year = {1986},
  journal = {Eberlin Tassu},
  pages = {165--192},
  keywords = {duplicate-citation-key,nosource}
}

@article{bradleyComputationallyEfficientMultivariate2018,
  title = {Computationally {{Efficient Multivariate Spatio-Temporal Models}} for {{High-Dimensional Count-Valued Data}} (with {{Discussion}})},
  author = {Bradley, Jonathan R. and Holan, Scott H. and Wikle, Christopher K.},
  year = {2018},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {13},
  number = {1},
  pages = {253--310},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/17-BA1069},
  urldate = {2022-03-03},
  abstract = {We introduce a computationally efficient Bayesian model for predicting high-dimensional dependent count-valued data. In this setting, the Poisson data model with a latent Gaussian process model has become the de facto model. However, this model can be difficult to use in high dimensional settings, where the data may be tabulated over different variables, geographic regions, and times. These computational difficulties are further exacerbated by acknowledging that count-valued data are naturally non-Gaussian. Thus, many of the current approaches, in Bayesian inference, require one to carefully calibrate a Markov chain Monte Carlo (MCMC) technique. We avoid MCMC methods that require tuning by developing a new conjugate multivariate distribution. Specifically, we introduce a multivariate log-gamma distribution and provide substantial methodological development of independent interest including: results regarding conditional distributions, marginal distributions, an asymptotic relationship with the multivariate normal distribution, and full-conditional distributions for a Gibbs sampler. To incorporate dependence between variables, regions, and time points, a multivariate spatio-temporal mixed effects model (MSTM) is used. To demonstrate our methodology we use data obtained from the US Census Bureau's Longitudinal Employer-Household Dynamics (LEHD) program. In particular, our approach is motivated by the LEHD's Quarterly Workforce Indicators (QWIs), which constitute current estimates of important US economic variables.},
  keywords = {62H11,62P12,Aggregation,American Community Survey,Bayesian hierarchical model,big data,Longitudinal Employer-Household Dynamics (LEHD) program,Markov chain Monte Carlo,non-Gaussian,Quarterly Workforce Indicators},
  file = {/home/gkonkamking/Zotero/storage/UXMRP7GN/Bradley et al. - 2018 - Computationally Efficient Multivariate Spatio-Temp.pdf}
}

@article{Bradshaw2014,
  title = {Using an {{Ecosystem Approach}} to Complement Protection Schemes Based on Organism-Level Endpoints},
  author = {Bradshaw, Clare and Kapustka, Lawrence and Barnthouse, Lawrence W and Brown, Justin and Ciffroy, Philippe and Forbes, Valery E. and Geras'kin, Stanislav and Kautsky, Ulrik and Br{\'e}chignac, Fran{\c c}ois},
  year = {2014},
  journal = {Journal of Environmental Radioactivity},
  volume = {136},
  pages = {98--104},
  issn = {18791700},
  doi = {10.1016/j.jenvrad.2014.05.017},
  abstract = {Radiation protection goals for ecological resources are focussed on ecological structures and functions at population-, community-, and ecosystem-levels. The current approach to radiation safety for non-human biota relies on organism-level endpoints, and as such is not aligned with the stated overarching protection goals of international agencies. Exposure to stressors can trigger non-linear changes in ecosystem structure and function that cannot be predicted from effects on individual organisms. From the ecological sciences, we know that important interactive dynamics related to such emergent properties determine the flows of goods and services in ecological systems that human societies rely upon. A previous Task Group of the IUR (International Union of Radioecology) has presented the rationale for adding an Ecosystem Approach to the suite of tools available to manage radiation safety. In this paper, we summarize the arguments for an Ecosystem Approach and identify next steps and challenges ahead pertaining to developing and implementing a practical Ecosystem Approach to complement organism-level endpoints currently used in radiation safety. ?? 2014 Elsevier Ltd.},
  isbn = {0265-931x},
  pmid = {24929504},
  keywords = {Complex ecological systems,Ecological dynamics,Indirect effects,Non-linearity,nosource,Species interactions,Wildlife}
}

@article{brain1989equation,
  title = {An Equation to Describe Dose Response Where There Is Stimulation of Growth at Low Doses},
  author = {Brain, P and Cousens, R},
  year = {1989},
  journal = {Weed Research},
  volume = {29},
  number = {2},
  pages = {93--96},
  publisher = {Wiley Online Library},
  issn = {0043-1737},
  doi = {10.1111/j.1365-3180.1989.tb00845.x},
  keywords = {nosource}
}

@article{braunLocalLikelihoodDensity2005,
  title = {Local Likelihood Density Estimation for Interval Censored Data},
  author = {Braun, John and Duchesne, Thierry and Stafford, James E.},
  year = {2005},
  journal = {Canadian Journal of Statistics},
  volume = {33},
  number = {1},
  pages = {39--60},
  issn = {1708-945X},
  doi = {10.1002/cjs.5540330104},
  urldate = {2020-07-24},
  abstract = {The authors propose a class of procedures for local likelihood estimation from data that are either interval-censored or that have been aggregated into bins. One such procedure relies on an algorithm that generalizes existing self-consistency algorithms by introducing kernel smoothing at each step of the iteration. The entire class of procedures yields estimates that are obtained as solutions of fixed point equations. By discretizing and applying numerical integration, the authors use fixed point theory to study convergence of algorithms for the class. Rapid convergence is effected by the implementation of a local EM algorithm as a global Newton iteration. The latter requires an explicit solution of the local likelihood equations which can be found by using the symbolic Newton-Raphson algorithm, if necessary.},
  langid = {english},
  keywords = {EM algorithm,HIV,kernel density estimation,likelihood cross-validation,non-parametric maximum likelihood,nosource,self-consistency,smoothed histograms,symbolic computation}
}

@article{bray2018biological,
  title = {Biological Interactions Mediate Context and Species-Specific Sensitivities to Salinity},
  author = {Bray, J P and Reich, J and Nichols, S J and Kon Kam King, Guillaume and Mac Nally, R and Thompson, R and {O'Reilly-Nugent}, A and Kefford, B J},
  year = {2018},
  journal = {Philosophical Transactions of the Royal Society B},
  volume = {374},
  number = {1764},
  pages = {20180020},
  publisher = {The Royal Society},
  copyright = {All rights reserved},
  file = {/home/gkonkamking/Zotero/storage/CID24GX3/Bray et al. - 2018 - Biological interactions mediate context and specie.pdf}
}

@article{brayCanSPEciesRisk2021,
  title = {Can {{SPEcies At Risk}} of Pesticides ({{SPEAR}}) Indices Detect Effects of Target Stressors amongst Multiple Interacting Stressors?},
  author = {Bray, Jonathan P and {O'Reilly-Nugent}, Andrew and Kon Kam King, Guillaume and Kaserzon, Sarit and Nichols, Susan J and Mac Nally, Ralph and Thomson, Ross M and Kefford, Ben J},
  year = {2021},
  journal = {Science of The Total Environment},
  volume = {763},
  pages = {142997},
  keywords = {nosource}
}

@article{Breen2012,
  title = {Epistasis as the Primary Factor in Molecular Evolution},
  author = {Breen, Michael S and Kemena, Carsten and Vlasov, Peter K and Notredame, Cedric and Kondrashov, Fyodor A},
  year = {2012},
  journal = {Nature},
  volume = {490},
  number = {7421},
  pages = {535--538},
  publisher = {Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/nature11510},
  abstract = {The main forces directing long-term molecular evolution remain obscure. A sizable fraction of amino-acid substitutions seem to be fixed by positive selection, but it is unclear to what degree long-term protein evolution is constrained by epistasis, that is, instances when substitutions that are accepted in one genotype are deleterious in another. Here we obtain a quantitative estimate of the prevalence of epistasis in long-term protein evolution by relating data on amino-acid usage in 14 organelle proteins and 2 nuclear-encoded proteins to their rates of short-term evolution. We studied multiple alignments of at least 1,000 orthologues for each of these 16 proteins from species from a diverse phylogenetic background and found that an average site contained approximately eight different amino acids. Thus, without epistasis an average site should accept two-fifths of all possible amino acids, and the average rate of amino-acid substitutions should therefore be about three-fifths lower than the rate of neutral evolution. However, we found that the measured rate of amino-acid substitution in recent evolution is 20 times lower than the rate of neutral evolution and an order of magnitude lower than that expected in the absence of epistasis. These data indicate that epistasis is pervasive throughout protein evolution: about 90 per cent of all amino-acid substitutions have a neutral or beneficial impact only in the genetic backgrounds in which they occur, and must therefore be deleterious in a different background of other species. Our findings show that most amino-acid substitutions have different fitness effects in different species and that epistasis provides the primary conceptual framework to describe the tempo and mode of long-term protein evolution.},
  isbn = {1476-4687 (Electronic){\textbackslash}r0028-0836 (Linking)},
  pmid = {23064225},
  keywords = {nosource}
}

@article{bresler1986two,
  title = {Two-Filter Formulae for Discrete-Time Non-Linear {{Bayesian}} Smoothing},
  author = {Bresler, Yoram},
  year = {1986},
  journal = {International Journal of Control},
  volume = {43},
  number = {2},
  pages = {629--641},
  publisher = {Taylor \& Francis},
  keywords = {nosource}
}

@article{bressanBestMenAre2008,
  title = {The {{Best Men Are}} ({{Not Always}}) {{Already Taken}}: {{Female Preference}} for {{Single Versus Attached Males Depends}} on {{Conception Risk}}},
  shorttitle = {The {{Best Men Are}} ({{Not Always}}) {{Already Taken}}},
  author = {Bressan, Paola and Stranieri, Debora},
  year = {2008},
  month = feb,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {1467-9280},
  urldate = {2020-08-18},
  abstract = {Because men of higher genetic quality tend to be poorer partners and parents than men of lower genetic quality, women may profit from securing a stable investme...},
  copyright = {{\copyright} 2008 Association for Psychological Science},
  langid = {english},
  keywords = {nosource}
}

@article{briers2010smoothing,
  title = {Smoothing Algorithms for State-Space Models},
  author = {Briers, Mark and Doucet, Arnaud and Maskell, Simon},
  year = {2010},
  journal = {Annals of the Institute of Statistical Mathematics},
  volume = {62},
  number = {1},
  pages = {61--89},
  publisher = {Springer},
  issn = {00203157},
  doi = {10.1007/s10463-009-0236-2},
  abstract = {Two-filter smoothing is a principled approach for performing optimal smoothing in non-linear non-Gaussian state--space models where the smoothing dis-tributions are computed through the combination of 'forward' and 'backward' time filters. The 'forward' filter is the standard Bayesian filter but the 'backward' filter, generally referred to as the backward information filter, is not a probability measure on the space of the hidden Markov process. In cases where the backward information filter can be computed in closed form, this technical point is not important. However, for general state--space models where there is no closed form expression, this prohibits the use of flexible numerical techniques such as Sequential Monte Carlo (SMC) to approximate the two-filter smoothing formula. We propose here a generalised two-filter smoothing formula which only requires approximating probability distributions and applies to any state--space model, removing the need to make restrictive assump-tions used in previous approaches to this problem. SMC algorithms are developed to implement this generalised recursion and we illustrate their performance on various problems.},
  keywords = {Non-linear diffusion,nosource,Parameter estimation,Rao-blackwellisation,Sequential monte carlo,State-space models,Two-filter smoothing}
}

@article{Brink2008,
  title = {Ecological Risk Assessment: {{From}} Book-Keeping to Chemical Stress Ecology},
  author = {Van Den Brink, Paul J.},
  year = {2008},
  journal = {Environmental Science and Technology},
  volume = {42},
  number = {24},
  pages = {8999--9004},
  issn = {0013936X},
  doi = {10.1021/es801991c},
  abstract = {To best address the effect of chemicals in the environment extrapolation from single species to ecosystems must be understood and modeled.},
  isbn = {0013-936X},
  pmid = {19174864},
  keywords = {nosource}
}

@article{briscoeSNVFEASTMicrobialSource2023,
  title = {{{SNV-FEAST}}: Microbial Source Tracking with Single Nucleotide Variants},
  shorttitle = {{{SNV-FEAST}}},
  author = {Briscoe, Leah and Halperin, Eran and Garud, Nandita R.},
  year = {2023},
  month = apr,
  journal = {Genome Biology},
  volume = {24},
  number = {1},
  pages = {101},
  issn = {1474-760X},
  doi = {10.1186/s13059-023-02927-8},
  urldate = {2024-01-23},
  abstract = {Elucidating the sources of a microbiome can provide insight into the ecological dynamics responsible for the formation of these communities. Source tracking approaches to date leverage species abundance information; however, single nucleotide variants (SNVs) may be more informative because of their high specificity to certain sources. To overcome the computational burden of utilizing all SNVs for a given sample, we introduce a novel method to identify signature SNVs for source tracking. Signature SNVs used as input into a previously designed source tracking algorithm, FEAST, can more accurately estimate contributions than species and provide novel insights, demonstrated in three case studies.},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Briscoe et al_2023_SNV-FEAST.pdf}
}

@article{brix1999generalized,
  title = {Generalized Gamma Measures and Shot-Noise Cox Processes},
  author = {Brix, Anders},
  year = {1999},
  journal = {Advances in Applied Probability},
  volume = {31},
  number = {4},
  pages = {929--953},
  publisher = {JSTOR},
  issn = {00018678},
  doi = {10.1239/aap/1029955251},
  file = {/home/gkonkamking/Zotero/storage/7AMMBPER/Brix - 1999 - Generalized gamma measures and shot-noise cox proc.pdf}
}

@article{Brix2001,
  title = {Assessing Acute and Chronic Copper Risks to Freshwater Aquatic Life Using Species Sensitivity Distributions for Different Taxonomic Groups},
  author = {Brix, {\relax KV} and DeForest, {\relax DK} and Adams, {\relax WJ}},
  year = {2001},
  journal = {{\dots} Toxicology and Chemistry},
  volume = {20},
  number = {8},
  pages = {1846--1856},
  isbn = {0730-7268},
  pmid = {11491571},
  keywords = {copper,duplicate-citation-key,nosource,probabilistic risk assessment}
}

@article{Bro2003,
  title = {A New Efficient Method for Determining the Number of Components in {{PARAFAC}} Models},
  author = {Bro, Rasmus and Kiers, Henk A L},
  year = {2003},
  journal = {Journal of Chemometrics},
  volume = {17},
  number = {5},
  pages = {274--286},
  issn = {08869383},
  doi = {10.1002/cem.801},
  abstract = {A new diagnostic called the core consistency diagnostic (CORCONDIA) is suggested for determining the proper number of components for multiway models. It applies especially to the parallel factor analysis (PARAFAC) model, but also to other models that can be considered as restricted Tucker3 models. It is based on scrutinizing the `appropriateness' of the structural model based on the data and the estimated parameters of gradually augmented models. A PARAFAC model (employing dimension-wise combinations of components for all modes) is called appropriate if adding other combinations of the same components does not improve the fit considerably. It is proposed to choose the largest model that is still sufficiently appropriate. Using examples from a range of different types of data, it is shown that the core consistency diagnostic is an effective tool for determining the appropriate number of components in e.g. PARAFAC models. However, it is also shown, using simulated data, that the theoretical understanding of CORCONDIA is not yet complete. Copyright {\copyright} 2003 John Wiley \& Sons, Ltd.},
  isbn = {0886-9383},
  keywords = {Cross-validation,Loss function,nosource,Number of components,PARAFAC,Scree plots,Validation},
  file = {/home/gkonkamking/pCloudDrive/papers/Bro_Kiers_2003_A new efficient method for determining the number of components in PARAFAC.pdf}
}

@article{Brock2012,
  title = {Acute Toxicity Tests with {{Daphnia}} Magna, {{Americamysis}} Bahia, {{Chironomus}} Riparius and {{Gammarus}} Pulex and Implications of New {{EU}} Requirements for the Aquatic Effect Assessment of Insecticides},
  author = {Brock, Theo C. M. and Van Wijngaarden, R. P A},
  year = {2012},
  journal = {Environmental Science and Pollution Research},
  volume = {19},
  number = {8},
  pages = {3610--3618},
  issn = {09441344},
  doi = {10.1007/s11356-012-0930-0},
  abstract = {Threshold concentrations for treatment related effects of 31 insecticides, as derived from aquatic micro-/mesocosm tests, were used to calibrate the predictive value of the European Tier-1 acute effect assessment on basis of laboratory toxicity tests with Daphnia magna, Chironomus spp., Americamysis bahia and Gammarus pulex. The acute Tier-1 effect assessment on basis of Daphnia (EC(50)/100) overall was protective for organophosphates, carbamates and most pyrethroids but not for neonicotinoids and the majority of insect growth regulators (IGRs) in the database. By including the 28-day water-spiked Chironomus riparius test, the effect assessment improves but selecting the lowest value on basis of the 48-h Daphnia test (EC50/100) and the 28-day Chironomus test (NOEC/10) is not fully protective for 4 out of 23 insecticide cases. An assessment on basis of G. pulex (EC(50)/100) is sufficiently protective for 15 out of 19 insecticide cases. The Tier-1 procedure on basis of acute toxicity data (EC(50)/100) for the combination of Daphnia and A. bahia and/or Chironomus (new EU dossier requirements currently under discussion) overall is protective to pulsed insecticide exposures in micro-/mesocosms. For IGRs that affect moulting, the effect assessment on basis of the 48-h Chironomus test (EC(50)/100) may not always be protective enough to replace that of the water-spiked 28-day C. riparius test (NOEC/10) because of latency of effects.},
  pmid = {22562347},
  keywords = {Aquatic invertebrates,Calibration,Ecotoxicology,Micro-/mesocosms,nosource,Tier I}
}

@article{Brockmann2006,
  title = {The Scaling Laws of Human Travel.},
  author = {Brockmann, D and Hufnagel, L and Geisel, T},
  year = {2006},
  month = jan,
  journal = {Nature},
  volume = {439},
  number = {7075},
  pages = {462--5},
  issn = {1476-4687},
  doi = {10.1038/nature04292},
  abstract = {The dynamic spatial redistribution of individuals is a key driving force of various spatiotemporal phenomena on geographical scales. It can synchronize populations of interacting species, stabilize them, and diversify gene pools. Human travel, for example, is responsible for the geographical spread of human infectious disease. In the light of increasing international trade, intensified human mobility and the imminent threat of an influenza A epidemic, the knowledge of dynamical and statistical properties of human travel is of fundamental importance. Despite its crucial role, a quantitative assessment of these properties on geographical scales remains elusive, and the assumption that humans disperse diffusively still prevails in models. Here we report on a solid and quantitative assessment of human travelling statistics by analysing the circulation of bank notes in the United States. Using a comprehensive data set of over a million individual displacements, we find that dispersal is anomalous in two ways. First, the distribution of travelling distances decays as a power law, indicating that trajectories of bank notes are reminiscent of scale-free random walks known as L{\'e}vy flights. Second, the probability of remaining in a small, spatially confined region for a time T is dominated by algebraically long tails that attenuate the superdiffusive spread. We show that human travelling behaviour can be described mathematically on many spatiotemporal scales by a two-parameter continuous-time random walk model to a surprising accuracy, and conclude that human travel on geographical scales is an ambivalent and effectively superdiffusive process.},
  arxiv = {0605511 [cond-mat]},
  arxivid = {cond-mat/0605511},
  isbn = {1476-4687},
  pmid = {16437114},
  keywords = {Diffusion,Disease Transmission,Economics,Human,Human: epidemiology,Human: transmission,Humans,Infectious,Influenza,Locomotion,Models,Movement,nosource,Theoretical,Travel,United States}
}

@article{broderick2013feature,
  title = {Feature Allocations , Probability Functions , and Paintboxes},
  author = {Broderick, Tamara and Pitman, Jim and Jordan, Michael I},
  year = {2013},
  journal = {arXiv preprint arXiv:1301.6647},
  number = {4},
  eprint = {1301.6647},
  pages = {801--836},
  doi = {10.1214/13-BA823},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1301.6647v2},
  keywords = {beta process,efpf,feature,feature allocation,feature frequency model,indian buffet process,nosource,paintbox}
}

@article{Brooks1998,
  title = {General Methods for Monitoring Convergence of Iterative Simulations},
  author = {Brooks, Stephen P B and Gelman, Andrew G},
  year = {1998},
  journal = {Journal of computational and graphical statistics},
  volume = {7},
  number = {4},
  pages = {434--455},
  publisher = {Taylor {\textbackslash}\& Francis},
  issn = {10618600},
  doi = {10.2307/1390675},
  abstract = {algorithms have made a; can be applied; convergence diagnosis; inference; introduction and background; markov chain monte carlo; mcmc; richardson; see gilks; significant impact on; the range of problems; to which bayesian analyses},
  isbn = {10618600},
  pmid = {1390675},
  keywords = {1,nosource}
}

@article{Brown:1996p21,
  title = {Asymptotic Equivalence of Nonparametric Regression and White Noise},
  author = {Brown, Lawrence and Low, Mark},
  year = {1996},
  journal = {Ann. Statist.},
  volume = {24},
  number = {6},
  eprint = {2242689},
  eprinttype = {jstor},
  pages = {2384--2398},
  abstract = {The principal result is that, under conditions, to any nonparametric regression problem there corresponds an asymptotically equivalent sequence of white noise with drift problems, and conversely. This asymptotic equivalence is in a global and uniform sense. Any normalized risk function attainable in one problem is asymptotically attainable in the other, with the difference in normalized risks converging to zero uniformly over the entire parameter space. The results are constructive. A recipe is provided for producing these asymptotically equivalent procedures. Some implications and generalizations of the principal result are also discussed.},
  pmid = {2242689},
  keywords = {62M05,nosource}
}

@inproceedings{brown1993using,
  title = {Using {{Dirichlet}} Mixture Priors to Derive Hidden {{Markov}} Models for Protein Families.},
  booktitle = {International Conference on Intelligent Systems for Molecular Biology},
  author = {Brown, M and Hughey, R and Krogh, A and Mian, I S and Sj{\"o}lander, K and Haussler, D},
  year = {1993},
  volume = {1},
  pages = {47--55},
  issn = {1553-0833},
  abstract = {A Bayesian method for estimating the amino acid distributions in the states of a hidden Markov model (HMM) for a protein family or the columns of a multiple alignment of that family is introduced. This method uses Dirichlet mixture densities as priors over amino acid distributions. These mixture densities are determined from examination of previously constructed HMMs or multiple alignments. It is shown that this Bayesian method can improve the quality of HMMs produced from small training sets. Specific experiments on the EF-hand motif are reported, for which these priors are shown to produce HMMs with higher likelihood on unseen data, and fewer false positives and false negatives in a database search task.},
  isbn = {1553-0833 (Print)},
  pmid = {7584370},
  keywords = {nosource}
}

@techreport{brown2009critical,
  title = {Critical Comparison of Available and Potential Higher Tier Testing Approaches for the Risk Assessment of Plant Protection Products, Considering at Least Field and Semi-Field Experimental Designs, Extrapolation from Dose-Response Relationships, and Increas},
  author = {Brown, Kevin and Tomlinson, Josie and Dungan, Jennifer L. and Hinchcliffe, Amelia and Palmquist, Katherine},
  year = {2009},
  pages = {1--192},
  institution = {EFSA/PPR/2008/01},
  abstract = {This final report constitutes the final deliverable for the assignment contracted by the European Food Safety Authority (EFSA) to Exponent International Limited to perform a literature review on ecotoxicology of chemicals with special focus on plant protection products in order to perform a ``critical comparison of available and potential higher tier testing approaches for the risk assessment of plant protection products, considering at least field and semi-field experimental designs, extrapolation from dose-response relationships, and increased dosages (aquatic and terrestrial)'' (Lot 4 of CFT/EFSA/PPR/2008/01).},
  chapter = {Literature},
  keywords = {nosource}
}

@article{buckwalterKnowledgeStakesMistakes2015,
  title = {Knowledge, {{Stakes}}, and {{Mistakes}}},
  author = {Buckwalter, Wesley and Schaffer, Jonathan},
  year = {2015},
  journal = {No{\^u}s},
  volume = {49},
  number = {2},
  pages = {201--234},
  issn = {1468-0068},
  doi = {10.1111/nous.12017},
  urldate = {2020-05-11},
  copyright = {{\copyright} 2013 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {nosource}
}

@article{burbidgeAlternativeTransformationsHandle1988,
  title = {Alternative {{Transformations}} to {{Handle Extreme Values}} of the {{Dependent Variable}}},
  author = {Burbidge, John B. and Magee, Lonnie and Robb, A. Leslie},
  year = {1988},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {83},
  number = {401},
  pages = {123--127},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1988.10478575},
  urldate = {2020-05-12},
  langid = {english},
  keywords = {nosource}
}

@book{burden,
  ids = {burdenNumericalAnalysis1993},
  title = {Numerical Analysis},
  author = {Burden, R L and Faires, J D},
  year = {1993},
  publisher = {PWS Publishing Company, Boston},
  keywords = {nosource}
}

@incollection{burden1993numerical,
  title = {Numerical Analysis},
  booktitle = {Igarss 2014},
  author = {Faires, Richard L. Burden;J.Douglas},
  year = {2014},
  number = {1},
  eprint = {1502.06106v1},
  pages = {1--5},
  publisher = {PWS Publishing Company, Boston},
  issn = {978-0-538-73351-9},
  doi = {10.1007/s13398-014-0173-7.2},
  abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1502.06106v1},
  isbn = {978-0-87421-656-1},
  pmid = {20815475},
  keywords = {high resolution images,nosource,research,risks management,sustainable reconstruction}
}

@article{Burkner2017,
  title = {Brms: {{An R Package}} for {{Bayesian Multilevel Models Using Stan}}},
  shorttitle = {Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2017},
  month = aug,
  journal = {Journal of Statistical Software},
  volume = {80},
  number = {1},
  pages = {1--28},
  issn = {1548-7660},
  doi = {10.18637/jss.v080.i01},
  urldate = {2020-01-15},
  copyright = {Copyright (c) 2017 Paul-Christian B{\"u}rkner},
  langid = {english},
  keywords = {Bayesian inference,MCMC,multilevel model,nosource,ordinal data,R,Stan}
}

@article{Burkner2017,
  title = {Brms: {{An R Package}} for {{Bayesian Multilevel Models Using Stan}}},
  shorttitle = {Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2017},
  month = aug,
  journal = {Journal of Statistical Software},
  volume = {80},
  number = {1},
  pages = {1--28},
  issn = {1548-7660},
  doi = {10.18637/jss.v080.i01},
  urldate = {2020-01-15},
  copyright = {Copyright (c) 2017 Paul-Christian B{\"u}rkner},
  langid = {english},
  keywords = {Bayesian inference,MCMC,multilevel model,nosource,ordinal data,R,Stan}
}

@article{Burnham2011,
  title = {{{AIC}} Model Selection and Multimodel Inference in Behavioral Ecology: {{Some}} Background, Observations, and Comparisons},
  author = {Burnham, Kenneth P. and Anderson, David R. and Huyvaert, Kathryn P.},
  year = {2011},
  journal = {Behavioral Ecology and Sociobiology},
  volume = {65},
  number = {1},
  pages = {23--35},
  issn = {03405443},
  doi = {10.1007/s00265-010-1029-6},
  abstract = {We briefly outline the information-theoretic (I-T) approaches to valid inference including a review of some simple methods for making formal inference from all the hypotheses in the model set (multimodel inference). The I-T approaches can replace the usual t tests and ANOVA tables that are so inferentially limited, but still commonly used. The I-T methods are easy to compute and understand and provide formal measures of the strength of evidence for both the null and alternative hypotheses, given the data. We give an example to highlight the importance of deriving alternative hypotheses and representing these as probability models. Fifteen technical issues are addressed to clarify various points that have appeared incorrectly in the recent literature. We offer several remarks regarding the future of empirical science and data analysis under an I-T framework.},
  isbn = {0340-5443},
  pmid = {3527},
  keywords = {AIC,Evidence,Kullback-Leibler information,Model averaging,Model likelihoods,Model probabilities,Model selection,Multimodel inference,nosource}
}

@article{burr2010bspmma,
  title = {Bspmma: {{An R}} Package for {{Bayesian}} Semi-Parametric Models for Metaanalysis},
  author = {Burr, Deborah},
  year = {2012},
  journal = {Journal of Statistical Software},
  volume = {50},
  number = {4},
  pages = {1--23},
  issn = {1548-7660},
  abstract = {We introduce an R package, bspmma, which implements a Dirichlet-based{\textbackslash}nrandom effects model specific to meta-analysis. In meta-analysis,{\textbackslash}nwhen combining effect estimates from several heterogeneous studies,{\textbackslash}nit is common to use a random-effects model. The usual frequentist{\textbackslash}nor Bayesian models specify a normal distribution for the true effects.{\textbackslash}nHowever, in many situations, the effect distribution is not normal,{\textbackslash}ne. g., it can have thick tails, be skewed, or be multi-modal. A Bayesian{\textbackslash}nnonparametric model based on mixtures of Dirichlet process priors{\textbackslash}nhas been proposed in the literature, for the purpose of accommodating{\textbackslash}nthe non-normality. We review this model and then describe a competitor,{\textbackslash}na semiparametric version which has the feature that it allows for{\textbackslash}na well-defined centrality parameter convenient for determining whether{\textbackslash}nthe overall effect is significant. This second Bayesian model is{\textbackslash}nbased on a different version of the Dirichlet process prior, and{\textbackslash}nwe call it the ``conditional Dirichlet model.\{''\} The package contains{\textbackslash}nfunctions to carry out analyses based on either the ordinary or the{\textbackslash}nconditional Dirichlet model, functions for calculating certain Bayes{\textbackslash}nfactors that provide a check on the appropriateness of the conditional{\textbackslash}nDirichlet model, and functions that enable an empirical Bayes selection{\textbackslash}nof the precision parameter of the Dirichlet process. We illustrate{\textbackslash}nthe use of the package on two examples, and give an interpretation{\textbackslash}nof the results in these two different scenarios.},
  keywords = {conditional Dirichlet process,Dirichlet process,nosource}
}

@article{burraFolkConceptsIntention2006,
  title = {The {{Folk Concepts}} of {{Intention}} and {{Intentional Action}}: {{A Cross-Cultural Study}}},
  shorttitle = {The {{Folk Concepts}} of {{Intention}} and {{Intentional Action}}},
  author = {Burra, Arudra and Knobe, Joshua},
  year = {2006},
  month = jan,
  journal = {Journal of Cognition and Culture},
  volume = {6},
  number = {1-2},
  pages = {113--132},
  publisher = {Brill},
  issn = {1568-5373, 1567-7095},
  doi = {10.1163/156853706776931222},
  urldate = {2020-04-16},
  abstract = {{$<$}section class="abstract"{$><$}div id="" class="section"{$><$}h3 class="abstractTitle text-title my-1" id="d90e3"{$>$}Abstract{$<$}/h3{$><$}p{$>$}Recent studies point to a surprising divergence between people's use of the concept of intention and their use of the concept of acting intentionally. It seems that people's application of the concept of intention is determined by their beliefs about the agent's psychological states whereas their use of the concept of acting intentionally is determined at least in part by their beliefs about the moral status of the behavior itself (i.e., by their beliefs about whether the behavior is morally good or morally bad). These findings raise a number of difficult questions about the relationship between the concept of intention and the concept of acting intentionally. The present paper addresses those questions using a variety of different methods, including conceptual analysis, psychological experimentation, and an examination of people's use of certain expressions in other languages.{$<$}/p{$><$}/div{$><$}/section{$>$}},
  chapter = {Journal of Cognition and Culture},
  langid = {english}
}

@article{bush,
  title = {A Semiparametric {{Bayesian}} Model for Randomised Block Designs},
  author = {Bush, Christopher A. and MacEachern, Steven N.},
  year = {1996},
  journal = {Biometrika},
  volume = {83},
  number = {2},
  pages = {275--285},
  publisher = {Biometrika Trust},
  issn = {0006-3444},
  doi = {10.1093/biomet/83.2.275},
  abstract = {A model is proposed for a Bayesian semiparametric analysis of randomised block experiments. The model is a hierarchical model in which a Dirichlet process is inserted at the middle stage for the distribution of the block effects. This model allows an arbitrary distribution of block effects, and it results in effective estimates of treatment contrasts, block effects and the distribution of block effects. An effective computational strategy is presented for describing the posterior distribution.},
  keywords = {nosource}
}

@inproceedings{bystrova2021approximating,
  title = {Approximating the Clusters' Prior Distribution in {{Bayesian}} Nonparametric Models},
  booktitle = {Third Symposium on Advances in Approximate Bayesian Inference},
  author = {Bystrova, Daria and Arbel, Julyan and Kon Kam King, Guillaume and Deslandes, Fran{\c c}ois},
  year = {2021},
  keywords = {nosource},
  file = {/home/gkonkamking/pCloudDrive/papers/Bystrova et al_2021_Approximating the clusters' prior distribution in Bayesian nonparametric models.pdf}
}

@article{BZFPZWP14,
  title = {Fast and Accurate Multivariate {{Gaussian}} Modeling of Protein Families: {{Predicting}} Residue Contacts and Protein-Interaction Partners},
  author = {Baldassi, Carlo and Zamparo, Marco and Feinauer, Christoph and Procaccini, Andrea and Zecchina, Riccardo and Weigt, Martin and Pagnani, Andrea},
  year = {2014},
  journal = {PLoS ONE},
  volume = {9},
  number = {3},
  pages = {e92721},
  publisher = {Public Library of Science},
  issn = {19326203},
  doi = {10.1371/journal.pone.0092721},
  abstract = {In the course of evolution, proteins show a remarkable conservation of their three-dimensional structure and their biological function, leading to strong evolutionary constraints on the sequence variability between homologous proteins. Our method aims at extracting such constraints from rapidly accumulating sequence data, and thereby at inferring protein structure and function from sequence information alone. Recently, global statistical inference methods (e.g. direct-coupling analysis, sparse inverse covariance estimation) have achieved a breakthrough towards this aim, and their predictions have been successfully implemented into tertiary and quaternary protein structure prediction methods. However, due to the discrete nature of the underlying variable (amino-acids), exact inference requires exponential time in the protein length, and efficient approximations are needed for practical applicability. Here we propose a very efficient multivariate Gaussian modeling approach as a variant of direct-coupling analysis: the discrete amino-acid variables are replaced by continuous Gaussian random variables. The resulting statistical inference problem is efficiently and exactly solvable. We show that the quality of inference is comparable or superior to the one achieved by mean-field approximations to inference with discrete variables, as done by direct-coupling analysis. This is true for (i) the prediction of residue-residue contacts in proteins, and (ii) the identification of protein-protein interaction partner in bacterial signal transduction. An implementation of our multivariate Gaussian approach is available at the website http://areeweb.polito.it/ricerca/cmp/code.},
  isbn = {10.1371/journal.pone.0092721},
  pmid = {24663061},
  keywords = {biocomp,nosource}
}

@article{Cai:2007p90,
  title = {Trade-Offs between Global and Local Risks in Nonparametric Function Estimation},
  author = {Cai, T Tony and Low, Mark G and Zhao, Linda H and Ao, Z H},
  year = {2007},
  journal = {Bernoulli},
  volume = {13},
  number = {1},
  pages = {1--19},
  issn = {1350-7265},
  doi = {10.3150/07-BEJ5001},
  abstract = {The problem of loss adaptation is investigated: given a fixed parameter, the goal is to construct an estimator that adapts to the loss function in the sense that the estimator is optimal both globally and locally at every point.},
  keywords = {besov class,constrained risk inequality,loss adaptation,nonparametric function estimation,nonparametric regression,normal location,nosource,scale model,superefficiency,wavelets,white noise}
}

@article{Calabrese2003,
  title = {{{HORMESIS}}: {{The}} Dose-Response Revolution},
  author = {Calabrese, Edward J. and Baldwin, Linda A.},
  year = {2003},
  month = jan,
  journal = {Annual Review of Pharmacology and Toxicology},
  volume = {43},
  number = {1},
  pages = {175--197},
  issn = {0362-1642},
  doi = {10.1146/annurev.pharmtox.43.100901.140223},
  abstract = {Hormesis, a dose-response relationship phenomenon characterized by low-dose stimulation and high-dose inhibition, has been frequently observed in properly designed studies and is broadly generalizable as being independent of chemical/physical agent, biological model, and endpoint measured. This under-recognized and -appreciated concept has the potential to profoundly change toxicology and its related disciplines with respect to study design, animal model selection, endpoint selection, risk assessment methods, and numerous other aspects, including chemotherapeutics. This article indicates that as a result of hormesis, fundamental changes in the concept and conduct of toxicology and risk assessment should be made, including (a) the definition of toxicology, (b) the process of hazard (e.g., including study design, selection of biological model, dose number and distribution, endpoint measured, and temporal sequence) and risk assessment [e.g., concept of NOAEL (no observed adverse effect level), low dose modeling, recognition of beneficial as well as harmful responses] for all agents, and (c) the harmonization of cancer and noncancer risk assessment.},
  pmid = {12195028},
  keywords = {Animals,Dose-Response Relationship,Drug,Humans,nosource,Risk Assessment,Xenobiotics,Xenobiotics: toxicity}
}

@article{Calabrese2005,
  title = {Paradigm Lost, Paradigm Found: {{The}} Re-Emergence of Hormesis as a Fundamental Dose Response Model in the Toxicological Sciences},
  author = {Calabrese, Edward J.},
  year = {2005},
  month = dec,
  journal = {Environmental Pollution},
  volume = {138},
  number = {3},
  eprint = {16098930},
  eprinttype = {pubmed},
  pages = {379--412},
  issn = {02697491},
  doi = {10.1016/j.envpol.2004.10.001},
  abstract = {This paper provides an assessment of the toxicological basis of the hormetic dose-response relationship including issues relating to its reproducibility, frequency, and generalizability across biological models, endpoints measured and chemical class/physical stressors and implications for risk assessment. The quantitative features of the hormetic dose response are described and placed within toxicological context that considers study design, temporal assessment, mechanism, and experimental model/population heterogeneity. Particular emphasis is placed on an historical evaluation of why the field of toxicology rejected hormesis in favor of dose response models such as the threshold model for assessing non-carcinogens and linear no threshold (LNT) models for assessing carcinogens. The paper argues that such decisions were principally based on complex historical factors that emerged from the intense and protracted conflict between what is now called traditional medicine and homeopathy and the overly dominating influence of regulatory agencies on the toxicological intellectual agenda. Such regulatory agency influence emphasized hazard/risk assessment goals such as the derivation of no observed adverse effect levels (NOAELs) and the lowest observed adverse effect levels (LOAELs) which were derived principally from high dose studies using few doses, a feature which restricted perceptions and distorted judgments of several generations of toxicologists concerning the nature of the dose-response continuum. Such historical and technical blind spots lead the field of toxicology to not only reject an established dose-response model (hormesis), but also the model that was more common and fundamental than those that the field accepted. ?? 2004 Elsevier Ltd. All rights reserved.},
  isbn = {0269-7491},
  pmid = {16098930},
  keywords = {Biphasic,Dose-response,History of medicine,History of science,Homeopathy,Hormesis,J-shaped,Linearity,nosource,Risk assessment,Threshold,Toxicology,U-shaped}
}

@article{Calabrese2007,
  title = {Threshold Dose - {{Response}} Model - {{RIP}}: 1911 to 2006},
  author = {Calabrese, Edward J.},
  year = {2007},
  month = jul,
  journal = {BioEssays},
  volume = {29},
  number = {7},
  eprint = {17563088},
  eprinttype = {pubmed},
  pages = {686--688},
  issn = {02659247},
  doi = {10.1002/bies.20590},
  abstract = {This essay represents a serious but fictional obituary of a scientific concept called the Threshold Dose-Response Model, which has long dominated the fields of toxicology and the broader biomedical sciences. Recent evidence indicates that the Threshold Dose-Response Model has long outlived its utility to predict low-dose effects. In fact, so poorly does this model predict low-dose responses that the idea arose that it should receive a symbolic burial recounting its achievements and failings, hence this obituary.},
  pmid = {17563088},
  keywords = {20th Century,21st Century,Biological,Dose-Response Relationship,Drug,History,Models,nosource,Pharmacology,Pharmacology: history,Pharmacology: methods,Risk Assessment,Risk Assessment: history,Risk Assessment: methods,Toxicology,Toxicology: history,Toxicology: methods}
}

@article{Calabrese2009,
  title = {Getting the Dose-Response Wrong: {{Why}} Hormesis Became Marginalized and the Threshold Model Accepted},
  author = {Calabrese, Edward J.},
  year = {2009},
  journal = {Archives of Toxicology},
  volume = {83},
  number = {3},
  pages = {227--247},
  issn = {03405761},
  doi = {10.1007/s00204-009-0411-5},
  abstract = {The dose-response relationship is central to the biological and biomedical sciences. During the early decades of the twentieth century consensus emerged that the most fundamental dose-response relationship was the threshold model, upon which scientific, health and medical research/clinical practices have been based. This paper documents that the scientific community made a fundamental error on the nature of the dose response in accepting the threshold model and in rejecting the hormetic-biphasic model, principally due to conflicts with homeopathy. Not only does this paper detail the underlying factors leading to this dose response decision, but it reveals that the scientific community never validated the threshold model throughout the twentieth century. Recent findings indicate that the threshold model poorly predicts responses in the low dose zone whereas its dose response "rival", the hormesis model, has performed very well. This analysis challenges a key foundation upon which biological, biomedical and clinical science rest.},
  isbn = {1432-0738 (Electronic){\textbackslash}n0340-5761 (Linking)},
  pmid = {19234688},
  keywords = {Biphasic,Hormesis,Hormetic,J-shaped,nosource,Threshold,U-shaped}
}

@article{Calabrese2011,
  title = {The Hormesis Database: {{The}} Occurrence of Hormetic Dose Responses in the Toxicological Literature},
  author = {Calabrese, Edward J. and Blain, Robyn B.},
  year = {2011},
  month = oct,
  journal = {Regulatory Toxicology and Pharmacology},
  volume = {61},
  number = {1},
  eprint = {21699952},
  eprinttype = {pubmed},
  pages = {73--81},
  publisher = {Elsevier Inc.},
  issn = {02732300},
  doi = {10.1016/j.yrtph.2011.06.003},
  abstract = {In 2005 we published an assessment of dose responses that satisfied a priori evaluative criteria for inclusion within the relational retrieval hormesis database (Calabrese and Blain, 2005). The database included information on study characteristics (e.g., biological model, gender, age and other relevant aspects, number of doses, dose distribution/range, quantitative features of the dose response, temporal features/repeat measures, and physical/chemical properties of the agents). The 2005 article covered information for about 5000 dose responses; the present article has been expanded to cover approximately 9000 dose responses. This assessment extends and strengthens the conclusion of the 2005 paper that the hormesis concept is broadly generalizable, being independent of biological model, endpoint measured and chemical class/physical agent. It also confirmed the definable quantitative features of hormetic dose responses in which the strong majority of dose responses display maximum stimulation less than twice that of the control group and a stimulatory width that is within approximately 10-20-fold of the estimated toxicological or pharmacological threshold. The remarkable consistency of the quantitative features of the hormetic dose response suggests that hormesis may provide an estimate of biological plasticity that is broadly generalized across plant, microbial and animal (invertebrate and vertebrate) models. ?? 2011 Elsevier Inc.},
  isbn = {1096-0295 (Electronic){\textbackslash}r0273-2300 (Linking)},
  pmid = {21699952},
  keywords = {Adaptive response,Biphasic,Dose-response,Hormesis,Hormetic,Inverted U-shaped,J-shaped,nosource,Plasticity}
}

@article{Caldwell2008,
  title = {Derivation of an Aquatic Predicted No-Effect Concentration for the Synthetic Hormone, 17 Alpha-Ethinyl Estradiol.},
  author = {Caldwell, Daniel J and Mastrocco, Frank and Hutchinson, Thomas H and L{\"a}nge, Reinhard and Heijerick, Dagobert and Janssen, Colin and Anderson, Paul D and Sumpter, John P},
  year = {2008},
  month = oct,
  journal = {Environmental science \& technology},
  volume = {42},
  number = {19},
  eprint = {18939525},
  eprinttype = {pubmed},
  pages = {7046--54},
  issn = {0013-936X},
  abstract = {17alpha-Ethinyl estradiol (EE2) is a synthetic estrogen widely used in combination with other steroid hormones in oral contraceptives and in the contraceptive patch. EE2 has been detected in sewage treatment plant effluents in the low nanogram -per-liter range and occasionally in surface waters in the U.S., U.K., Canada, Brazil, Germany, and elsewhere. The mode of action is receptor-mediated, and estrogen receptors exist in mammals and other vertebrates. A large number of studies on the effects of EE2 on aquatic organisms exist. One hundred English language studies published between 1994 and 2007, one as yet unpublished study, and findings published in conference proceedings (in German) were compared to published data quality criteria to identify the most relevant studies for deriving a predicted no-effect concentration (PNEC). Reproduction in fish was identified as the most sensitive end point in aquatic species. A species sensitivity distribution was constructed using no observed effect concentrations (NOECs) for reproductive effects from 39 papers in 26 species, resulting in a median hazardous concentration at which 5\% of the species tested are affected (HC5,50) of 0.35 ng/L. After comparing this HC5,50 to all of the laboratory and field-derived toxicity information available for EE2, we recommend using 0.35 ng/L as the PNEC for EE2 in surface water. This PNEC is below 95\% of the existing NOECs for effects on reproduction and is also below virtually all of the NOECs for vitellogenin induction in the key fish reproduction studies.},
  isbn = {0013-936X},
  pmid = {18939525},
  keywords = {Animals,duplicate-citation-key,Environment,Ethinyl Estradiol,Ethinyl Estradiol: toxicity,Fishes,Fishes: growth \& development,Fishes: physiology,Life Cycle Stages,Life Cycle Stages: drug effects,No-Observed-Adverse-Effect Level,nosource,Population Dynamics,Reproduction,Reproduction: drug effects,Species Specificity,Vitellogenins,Vitellogenins: metabolism}
}

@article{camererEvaluatingReplicabilityLaboratory2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  year = {2016},
  month = mar,
  journal = {Science},
  volume = {351},
  number = {6280},
  pages = {1433--1436},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaf0918},
  urldate = {2020-04-14},
  abstract = {Another social science looks at itself Experimental economists have joined the reproducibility discussion by replicating selected published experiments from two top-tier journals in economics. Camerer et al. found that two-thirds of the 18 studies examined yielded replicable estimates of effect size and direction. This proportion is somewhat lower than unaffiliated experts were willing to bet in an associated prediction market, but roughly in line with expectations from sample sizes and P values. Science, this issue p. 1433 The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61\%); on average, the replicated effect size is 66\% of the original. The replicability rate varies between 67\% and 78\% for four additional replicability indicators, including a prediction market measure of peer beliefs. By several metrics, economics experiments do replicate, although not as often as predicted. By several metrics, economics experiments do replicate, although not as often as predicted.},
  chapter = {Report},
  copyright = {Copyright {\copyright} 2016, American Association for the Advancement of Science},
  langid = {english},
  pmid = {26940865},
  file = {/home/gkonkamking/Zotero/storage/GIEPYDAU/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf}
}

@article{camererEvaluatingReplicabilitySocial2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637--644},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  urldate = {2020-04-14},
  abstract = {Camerer et al. carried out replications of 21 Science and Nature social science experiments, successfully replicating 13 out of 21 (62\%). Effect sizes of replications were about half of the size of the originals.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/Z4M4LUA8/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf}
}

@article{Campbell2000,
  title = {{{BurrliOZ}}: {{A}} Computer Program for Calculating Toxicant Trigger Values for the {{ANZECC}} and {{ARMCANZ}} Water Quality Guidelines},
  booktitle = {Perth, Western Australia, {\dots}},
  author = {Campbell, E and Palmer, {\relax MJ} and Shao, Q and Warne, {\relax MSJ} and Wilson, D},
  year = {2000},
  journal = {Perth, Western Australia, {\dots}},
  keywords = {duplicate-citation-key,nosource}
}

@misc{campbellOnlineVariationalFiltering2022,
  title = {Online {{Variational Filtering}} and {{Parameter Learning}}},
  author = {Campbell, Andrew and Shi, Yuyang and Rainforth, Tom and Doucet, Arnaud},
  year = {2022},
  month = jun,
  number = {arXiv:2110.13549},
  eprint = {2110.13549},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-10-05},
  abstract = {We present a variational method for online state estimation and parameter learning in state-space models (SSMs), a ubiquitous class of latent variable models for sequential data. As per standard batch variational techniques, we use stochastic gradients to simultaneously optimize a lower bound on the log evidence with respect to both model parameters and a variational approximation of the states' posterior distribution. However, unlike existing approaches, our method is able to operate in an entirely online manner, such that historic observations do not require revisitation after being incorporated and the cost of updates at each time step remains constant, despite the growing dimensionality of the joint posterior distribution of the states. This is achieved by utilizing backward decompositions of this joint posterior distribution and of its variational approximation, combined with Bellman-type recursions for the evidence lower bound and its gradients. We demonstrate the performance of this methodology across several examples, including high-dimensional SSMs and sequential Variational Auto-Encoders.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/gkonkamking/pCloudDrive/papers/Campbell et al_2022_Online Variational Filtering and Parameter Learning.pdf}
}

@article{campbellTruncatedRandomMeasures2019,
  title = {Truncated Random Measures},
  author = {Campbell, Trevor and Huggins, Jonathan H. and How, Jonathan P. and Broderick, Tamara},
  year = {2019},
  month = may,
  journal = {Bernoulli},
  volume = {25},
  number = {2},
  pages = {1256--1288},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  doi = {10.3150/18-BEJ1020},
  urldate = {2023-10-29},
  abstract = {Completely random measures (CRMs) and their normalizations are a rich source of Bayesian nonparametric priors. Examples include the beta, gamma, and Dirichlet processes. In this paper, we detail two major classes of sequential CRM representations---series representations and superposition representations---within which we organize both novel and existing sequential representations that can be used for simulation and posterior inference. These two classes and their constituent representations subsume existing ones that have previously been developed in an ad hoc manner for specific processes. Since a complete infinite-dimensional CRM cannot be used explicitly for computation, sequential representations are often truncated for tractability. We provide truncation error analyses for each type of sequential representation, as well as their normalized versions, thereby generalizing and improving upon existing truncation error bounds in the literature. We analyze the computational complexity of the sequential representations, which in conjunction with our error bounds allows us to directly compare representations and discuss their relative efficiency. We include numerous applications of our theoretical results to commonly-used (normalized) CRMs, demonstrating that our results enable a straightforward representation and analysis of CRMs that has not previously been available in a Bayesian nonparametric context.},
  keywords = {Bayesian nonparametrics,completely random measure,normalized completely random measure,Poisson point process,truncation},
  file = {/home/gkonkamking/pCloudDrive/papers/Campbell et al_2019_Truncated random measures.pdf}
}

@incollection{canada_guidelines,
  title = {A Protocol for the Derivation of Water Quality Guidelines for the Protection of Aquatic Life},
  booktitle = {Canadian Environmental Quality Guidelines},
  author = {{CCME}},
  year = {2007},
  number = {Ccme 1991},
  publisher = {Canadian Council of Ministers of the Environment},
  address = {Winnipeg},
  keywords = {nosource}
}

@article{canale2013posterior,
  title = {Posterior Consistency of Nonparametric Location-Scale Mixtures for Multivariate Density Estimation},
  author = {Canale, Antonio and De Blasi, Pierpaolo},
  year = {2013},
  journal = {arXiv preprint arXiv:1306.2671},
  eprint = {1306.2671},
  primaryclass = {math.ST},
  archiveprefix = {arXiv},
  arxivid = {math.ST/1306.2671},
  keywords = {Mathematics - Statistics Theory,nosource}
}

@article{Canale2016,
  title = {Bayesian Nonparametric Forecasting of Monotonic Functional Time Series},
  author = {Canale, Antonio and Ruggiero, Matteo},
  year = {2016},
  journal = {Electronic Journal of Statistics},
  volume = {10},
  number = {2},
  eprint = {1608.08056v1},
  pages = {3265--3286},
  issn = {19357524},
  doi = {10.1214/16-EJS1190},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1608.08056v1},
  keywords = {Approximate bayesian computation,Dependent processes,Dirichlet process,Interacting particle system,Moran model,nosource,Polya urn,Prediction}
}

@article{Canale2016a,
  title = {Constrained Functional Time Series: {{Applications}} to the {{Italian}} Gas Market},
  author = {Canale, Antonio and Vantini, Simone},
  year = {2016},
  journal = {International Journal of Forecasting},
  volume = {32},
  number = {4},
  pages = {1340--1351},
  publisher = {Elsevier B.V.},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2016.05.002},
  abstract = {Motivated by market dynamic modelling in the Italian Natural Gas Balancing Platform, we propose a model for analyzing time series of functions, subject to equality and inequality constraints at the two edges of the domain, respectively, such as daily demand and offer curves. Specifically, we provide the constrained functions with suitable pre-Hilbert structures, and introduce a useful isometric bijective map that associates each possible bounded and monotonic function to an unconstrained one. We introduce a functional-to-functional autoregressive model that is used to forecast future demand/offer functions, and estimate the model via the minimization of a penalized mean squared error of prediction, with a penalty term based on the Hilbert???Schmidt squared norm of autoregressive lagged operators. The approach is of general interest and could be generalized to any situation in which one has to deal with functions that are subject to the above constraints which evolve over time.},
  isbn = {0169-2070},
  keywords = {Autoregressive model,Demand and offer model,Energy forecasting,Functional data analysis,Functional ridge regression,nosource},
  file = {/home/gkonkamking/pCloudDrive/papers/Canale_Vantini_2016_Constrained functional time series2.pdf}
}

@article{canaleBayesianKernelMixtures2011,
  title = {Bayesian {{Kernel Mixtures}} for {{Counts}}},
  author = {Canale, Antonio and Dunson, David B.},
  year = {2011},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {106},
  number = {496},
  pages = {1528--1539},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1198/jasa.2011.tm10552},
  urldate = {2022-05-25},
  abstract = {Although Bayesian nonparametric mixture models for continuous data are well developed, the literature on related approaches for count data is limited. A common strategy is to use a mixture of Poissons, which unfortunately is quite restrictive in not accounting for distributions with variance less than the mean. Other approaches include mixing multinomials, which requires finite support, and using a Dirichlet process prior with a Poisson base measure, which does not allow for smooth deviations from the Poisson. We propose broad class of alternative models, nonparametric mixtures of rounded continuous kernels. We develop an efficient Gibbs sampler for posterior computation, and perform a simulation study to assess performance. Focusing on the rounded Gaussian case, we generalize the modeling framework to account for multivariate count data, joint modeling with continuous and categorical variables, and other complications. We illustrate our methods through applications to a developmental toxicity study and marketing data. Supplemental material is available online.},
  keywords = {Bayesian nonparametrics,Dirichlet process mixtures,Kullback--Leibler condition,Large support,Multivariate count data,Posterior consistency,Rounded Gaussian distribution},
  file = {/home/gkonkamking/Zotero/storage/MMJ34NRK/Canale and Dunson - 2011 - Bayesian Kernel Mixtures for Counts.pdf}
}

@article{caoEfficientFormulationStochastic2004,
  title = {Efficient Formulation of the Stochastic Simulation Algorithm for Chemically Reacting Systems},
  author = {Cao, Yang and Li, Hong and Petzold, Linda},
  year = {2004},
  month = sep,
  journal = {The Journal of Chemical Physics},
  volume = {121},
  number = {9},
  pages = {4059--4067},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.1778376},
  urldate = {2022-07-06},
  file = {/home/gkonkamking/pCloudDrive/papers/Cao et al_2004_Efficient formulation of the stochastic simulation algorithm for chemically.pdf}
}

@article{Carinci2015,
  title = {Dualities in Population Genetics: {{A}} Fresh Look with New Dualities},
  author = {Carinci, Gioia and Giardin{\`a}, Cristian and Giberti, Claudio and Redig, Frank},
  year = {2015},
  journal = {Stochastic Processes and their Applications},
  volume = {125},
  number = {3},
  eprint = {1302.3206},
  pages = {941--969},
  issn = {03044149},
  doi = {10.1016/j.spa.2014.10.009},
  abstract = {We apply our general method of duality, introduced in Giardin{\`a} et al. (2007), to models of population dynamics. The classical dualities between forward and ancestral processes can be viewed as a change of representation in the classical creation and annihilation operators, both for diffusions dual to coalescents of Kingman's type, as well as for models with finite population size. Next, using SU(1,1) raising and lowering operators, we find new dualities between the Wright-Fisher diffusion with d types and the Moran model, both in presence and absence of mutations. These new dualities relates two forward evolutions. From our general scheme we also identify self-duality of the Moran model.},
  archiveprefix = {arXiv},
  arxivid = {1302.3206},
  keywords = {1) algebra,Duality,Heisenberg algebra,Mathematical population genetics,Moran model,nosource,SU (1,Wright-Fisher model}
}

@article{carlin1997bayes,
  title = {Bayes and Empirical {{Bayes}} Methods for Data Analysis},
  author = {Carlin, Bradley P and Louis, Thomas A},
  year = {1997},
  journal = {Statistics and Computing},
  volume = {7},
  number = {2},
  pages = {153--154},
  publisher = {Springer},
  issn = {0277-6715},
  isbn = {1584881704},
  keywords = {nosource}
}

@article{carlin2016comment,
  title = {Comment: {{Is}} Reform Possible without a Paradigm Shift?},
  author = {Carlin, John B},
  year = {2016},
  journal = {The American Statistician},
  volume = {1},
  file = {/home/gkonkamking/Zotero/storage/ZG3EM3HL/Carlin - 2016 - Comment Is reform possible without a paradigm shi.pdf}
}

@article{carlsonBsplinesHypergeometricFunctions1991,
  title = {B-Splines, Hypergeometric Functions, and {{Dirichlet}} Averages},
  author = {Carlson, B. C},
  year = {1991},
  month = dec,
  journal = {Journal of Approximation Theory},
  volume = {67},
  number = {3},
  pages = {311--325},
  issn = {0021-9045},
  doi = {10.1016/0021-9045(91)90006-V},
  urldate = {2021-10-18},
  abstract = {Properties of Dirichlet averages are used to derive some well-known and some new properties of univariate and multivariate simplex splines. A univariate B-spline is the jump discontinuity of a hypergeometric R-function, and the Fourier transform of a B-spline is a confluent hypergeometric S-function. A univariate or multivariate B-spline is a Dirichlet average of a Dirac delta function. Its dependence on the knots is governed by a system of Euler-Poisson partial differential equations.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/NL3KLFM6/Carlson - 1991 - B-splines, hypergeometric functions, and Dirichlet.pdf;/home/gkonkamking/Zotero/storage/NBZXNTQT/002190459190006V.html}
}

@article{carltonFamilyDensitiesDerived2002,
  title = {A Family of Densities Derived from the Three-Parameter {{Dirichlet}} Process},
  author = {Carlton, Matthew A.},
  year = {2002},
  month = dec,
  journal = {Journal of Applied Probability},
  volume = {39},
  number = {4},
  pages = {764--774},
  publisher = {Cambridge University Press},
  issn = {0021-9002, 1475-6072},
  doi = {10.1239/jap/1037816017},
  urldate = {2022-01-20},
  abstract = {The traditional Dirichlet process is characterized by its distribution on a measurable partition of the state space - namely, the Dirichlet distribution. In this paper, we consider a generalization of the Dirichlet process and the family of multivariate distributions it induces, with particular attention to a special case where the multivariate density function is tractable.},
  langid = {english},
  keywords = {60G57,62E15,density function,Dirichlet process,random measure},
  file = {/home/gkonkamking/Zotero/storage/9JCKQIKK/Carlton - 2002 - A family of densities derived from the three-param.pdf}
}

@inproceedings{caron2006bayesian,
  title = {Bayesian Inference for Linear Dynamic Models with \{\vphantom\}{{Dirichlet}}\vphantom\{\} Process Mixtures},
  booktitle = {{{IEEE}} Trans. on Signal Processing},
  author = {Caron, Fran{\c c}ois and Davis, Mark A and Doucet, A and Duflos, E and Vangeeghe, P},
  year = {2006},
  volume = {42},
  number = {4},
  pages = {1264--1274},
  doi = {10.1109/TAES.2006.314571},
  organization = {Citeseer},
  keywords = {nosource}
}

@inproceedings{caron2007generalized,
  title = {Generalized {{Polya}} Urn for Time-Varying {{Dirichlet}} Process Mixtures},
  booktitle = {23rd Conference on Uncertainty in Artificial Intelligence ({{UAI}}'2007), Vancouver, Canada},
  author = {Caron, F and Davy, M and Doucet, A},
  year = {2007},
  number = {6},
  abstract = {Dirichlet Process Mixtures (DPMs) are a popular class of statistical models to perform density estimation and clustering. However, when the data available have a distribution evolving over time, such models are inade- quate. We introduce here a class of time- varying DPMs which ensures that at each time step the random distribution follows a DPM model. Our model relies on an intuitive and simple generalized Polya urn scheme. Inference is performed using Markov chain Monte Carlo and Sequential Monte Carlo. We demonstrate our model on various appli- cations.},
  isbn = {0-9749039-3-0},
  organization = {Citeseer},
  keywords = {nosource}
}

@article{caron2008bayesian,
  title = {Bayesian Inference for Linear Dynamic Models with {{Dirichlet}} Process Mixtures},
  author = {Caron, Fran{\c c}ois and Davy, Manuel and Doucet, Arnaud and Duflos, Emmanuel and Vanheeghe, Philippe},
  year = {2008},
  journal = {IEEE Transactions on Signal Processing},
  volume = {56},
  number = {1},
  pages = {71--84},
  publisher = {IEEE},
  keywords = {nosource}
}

@article{caron2014bayesian,
  title = {Bayesian Nonparametric Models of Sparse and Exchangeable Random Graphs},
  author = {Caron, Fran{\c c}ois and Fox, Emily B.},
  year = {2014},
  journal = {arXiv preprint},
  volume = {2014},
  eprint = {1401.1137},
  pages = {1--64},
  abstract = {Statistical network modeling has focused on representing the graph as a discrete structure, namely the adjacency matrix, and considering the exchangeability of this array. In such cases, the Aldous-Hoover representation theorem (Aldous, 1981; Hoover, 1979) applies and informs us that the graph is necessarily either dense or empty. In this paper, we instead consider representing the graph as a measure on the positive quadrant. For the associated definition of exchangeability in this continuous space, we rely on the Kallenberg representation theorem (Kallenberg, 2005). We show that for certain choices of the specified graph construction, our network process is both exchangeable and sparse with power-law degree distribution. In particular, we build on the framework of completely random measures (CRMs) and use the theory associated with such processes to derive important network properties, such as an urn representation for network simulation. The CRM framework also provides for interpretability of the network model in terms of node-specific sociability parameters, with properties such as sparsity and power-law behavior simply tuned by three hyperparameters. Our theoretical results are explored empirically and compared to common network models.},
  archiveprefix = {arXiv},
  arxivid = {1401.1137},
  keywords = {and phrases,evy measure,exchangeability,fc acknowledges the support,generalized gamma process,intra-,l,nosource,of the european commission,point process,random graphs,under the marie curie}
}

@article{Carra,
  title = {The Coalescing Colony Model: Mean-Field, Scaling, and Geometry},
  author = {Carra, Giulia and Mallick, Kirone and Barthelemy, Marc},
  eprint = {1709.08628},
  abstract = {We analyze the coalescing model where a 'primary' colony grows and randomly emits secondary colonies that spread and eventually coalesce with it. This model describes population proliferation in theoretical ecology, tumor growth and is also of great interest for modeling the development of cities. Assuming the primary colony to be always spherical of radius r(t) and the emission rate proportional to r(t) {\texttheta} where {\texttheta} {$>$} 0, we derive the mean-field equations governing the dynamics of the primary colony, calculate the scaling exponents versus {\texttheta} and compare our results with numerical simulations. We then critically test the validity of the circular approximation and show that it is sound for a constant emission rate ({\texttheta} = 0). However, when the emission rate is proportional to the perimeter, the circular approximation breaks down and the roughness of the primary colony can not be discarded, thus modifying the scaling exponents. Dispersal models have been used extensively to inves-tigate the proliferation of animal colonies in theoretical ecology [1, 2] and as a simplified model for the growth of cancerous tumors [3, 4]. Such models are also good candidates for describing the growth of the built-area of cities [5] for which we now have empirical data over long periods of time [6]. The main feature of dispersal models is the concomitant existence of two growth mechanisms. The first process is the growth of the main -- so-called primary -- colony, which occurs via a reaction-diffusion process (as described by a FKK-like equation [7, 8]) and leads to a constant growth with velocity c, depending on the details of the system. The second ingredient is ran-dom dispersal from the primary colony, which represents the emergence of secondary settlements in the framework of animal ecology, the development of metastatic tumors, or, in the urban sprawl case, the creation of small towns in the periphery of large cities. In the real world, dis-persion follows privileged directions under the effect of external forces such as blood vessels, winds and rivers, or transportation networks for cities but in a first ap-proach, these anisotropic effects will be neglected. We will assume that secondary colonies also grow at the ve-locity c and will eventually coalesce with the primary colony, leading to a larger primary colony whose time-dependent size will depend on the emission rate. A classical way to study dispersal is through the dis-persal kernel representing the probability distribution of dispersal distances and various forms for these kernels have been discussed [9]. A different approach has been introduced by Kawasaki and Shigesada in [1, 8] who pro-posed the use of simple models to tackle this challenging problem. We shall follow this point of view and study the coalescing colony model where a primary colony grows at radial velocity c and emits a secondary colony at a rate {$\lambda$} and at a distance from its border (long-range dis-persal). The variable can be drawn from a probability distribution P () but we consider here that the secondary colonies are emitted at a constant distance 0 from the boundary of the primary colony (i.e. P () = {$\delta$}(- 0)). Besides, we assume that each secondary colony also grows with the same radial speed c and does not emit tertiary colonies. The dependence of the emission rate on the colony size is taken into account by the functional form},
  archiveprefix = {arXiv},
  arxivid = {1709.08628},
  keywords = {Complex systems modeling,Dispersal problem ---,nosource,Physics ---,Statistical}
}

@article{carroll1970analysis,
  title = {Analysis of Individual Differences in Multidimensional Scaling via an {{N-way}} Generalization of ``{{Eckart-Young}}'' Decomposition},
  author = {Carroll, J Douglas and Chang, Jih-Jie},
  year = {1970},
  journal = {Psychometrika},
  volume = {35},
  number = {3},
  pages = {283--319},
  publisher = {Springer},
  keywords = {nosource}
}

@article{carterSingleExposureAmerican2011,
  title = {A {{Single Exposure}} to the {{American Flag Shifts Support Toward Republicanism}} up to 8 {{Months Later}}:},
  shorttitle = {A {{Single Exposure}} to the {{American Flag Shifts Support Toward Republicanism}} up to 8 {{Months Later}}},
  author = {Carter, Travis J. and Ferguson, Melissa J. and Hassin, Ran R.},
  year = {2011},
  month = jul,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  doi = {10.1177/0956797611414726},
  urldate = {2020-05-20},
  abstract = {There is scant evidence that incidental cues in the environment significantly alter people's political judgments and behavior in a durable way. We report that a brief exposure to the American flag ...},
  langid = {english},
  keywords = {nosource}
}

@article{carusoMereExposureMoney2013,
  title = {Mere Exposure to Money Increases Endorsement of Free-Market Systems and Social Inequality},
  author = {Caruso, Eugene M. and Vohs, Kathleen D. and Baxter, Brittani and Waytz, Adam},
  year = {2013},
  journal = {Journal of Experimental Psychology: General},
  volume = {142},
  number = {2},
  pages = {301--306},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/a0029288},
  abstract = {The present research tested whether incidental exposure to money affects people's endorsement of social systems that legitimize social inequality. We found that subtle reminders of the concept of money, relative to nonmoney concepts, led participants to endorse more strongly the existing social system in the United States in general (Experiment 1) and free-market capitalism in particular (Experiment 4), to assert more strongly that victims deserve their fate (Experiment 2), and to believe more strongly that socially advantaged groups should dominate socially disadvantaged groups (Experiment 3). We further found that reminders of money increased preference for a free-market system of organ transplants that benefited the wealthy at the expense of the poor even though this was not the prevailing system (Experiment 5) and that this effect was moderated by participants' nationality. These results demonstrate how merely thinking about money can influence beliefs about the social order and the extent to which people deserve their station in life. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Exposure,Money,nosource,Social Equality}
}

@article{carvalho2010horseshoe,
  title = {The Horseshoe Estimator for Sparse Signals},
  author = {Carvalho, Carlos M and Polson, Nicholas G and Scott, James G},
  year = {2010},
  journal = {Biometrika},
  volume = {97},
  number = {2},
  pages = {465--480},
  publisher = {Oxford University Press},
  file = {/home/gkonkamking/Zotero/storage/Y4VL78C6/Carvalho et al. - 2010 - The horseshoe estimator for sparse signals.pdf}
}

@book{casella2002statistical,
  title = {Statistical Inference},
  author = {Casella, George},
  year = {2002},
  edition = {2},
  volume = {2},
  publisher = {Duxbury Pacific Grove, CA},
  isbn = {0-495-39187-5},
  keywords = {nosource}
}

@article{casellaPenalizedRegressionStandard2010,
  title = {Penalized Regression, Standard Errors, and {{Bayesian}} Lassos},
  author = {Casella, George and Ghosh, Malay and Gill, Jeff and Kyung, Minjung},
  year = {2010},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {5},
  number = {2},
  pages = {369--411},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/10-BA607},
  urldate = {2021-11-11},
  abstract = {Penalized regression methods for simultaneous variable selection and coefficient estimation, especially those based on the lasso of Tibshirani (1996), have received a great deal of attention in recent years, mostly through frequentist models. Properties such as consistency have been studied, and are achieved by different lasso variations. Here we look at a fully Bayesian formulation of the problem, which is flexible enough to encompass most versions of the lasso that have been previously considered. The advantages of the hierarchical Bayesian formulations are many. In addition to the usual ease-of-interpretation of hierarchical models, the Bayesian formulation produces valid standard errors (which can be problematic for the frequentist lasso), and is based on a geometrically ergodic Markov chain. We compare the performance of the Bayesian lassos to their frequentist counterparts using simulations, data sets that previous lasso papers have used, and a difficult modeling problem for predicting the collapse of governments around the world. In terms of prediction mean squared error, the Bayesian lasso performance is similar to and, in some cases, better than, the frequentist lasso.},
  keywords = {geometric ergodicity,Gibbs sampling,hierarchical models,Variable selection},
  file = {/home/gkonkamking/Zotero/storage/689JY5RV/Casella et al. - 2010 - Penalized regression, standard errors, and Bayesia.pdf}
}

@article{castillo2013bayesian,
  title = {On {{Bayesian}} Supremum Norm Contraction Rates},
  author = {Castillo, Isma??l},
  year = {2014},
  journal = {Annals of Statistics},
  volume = {42},
  number = {5},
  eprint = {1304.1761v3},
  pages = {2058--2091},
  issn = {00905364},
  doi = {10.1214/14-AOS1253},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1304.1761v3},
  keywords = {Bayesian nonparametrics,Contraction rates,duplicate-citation-key,nosource,Supremum norm}
}

@article{castilloBayesianLinearRegression2015,
  title = {Bayesian Linear Regression with Sparse Priors},
  author = {Castillo, Isma{\"e}l and {Schmidt-Hieber}, Johannes and {van der Vaart}, Aad},
  year = {2015},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {43},
  number = {5},
  issn = {0090-5364},
  doi = {10.1214/15-AOS1334},
  urldate = {2022-03-18},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/YDMSWFXU/Castillo et al. - 2015 - Bayesian linear regression with sparse priors.pdf}
}

@article{castilloNeedlesStrawHaystack2012,
  title = {Needles and {{Straw}} in a {{Haystack}}: {{Posterior}} Concentration for Possibly Sparse Sequences},
  shorttitle = {Needles and {{Straw}} in a {{Haystack}}},
  author = {Castillo, Isma{\"e}l and van der Vaart, Aad},
  year = {2012},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {40},
  number = {4},
  pages = {2069--2101},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/12-AOS1029},
  urldate = {2024-10-17},
  abstract = {We consider full Bayesian inference in the multivariate normal mean model in the situation that the mean vector is sparse. The prior distribution on the vector of means is constructed hierarchically by first choosing a collection of nonzero means and next a prior on the nonzero values. We consider the posterior distribution in the frequentist set-up that the observations are generated according to a fixed mean vector, and are interested in the posterior distribution of the number of nonzero components and the contraction of the posterior distribution to the true mean vector. We find various combinations of priors on the number of nonzero coefficients and on these coefficients that give desirable performance. We also find priors that give suboptimal convergence, for instance, Gaussian priors on the nonzero coefficients. We illustrate the results by simulations.},
  keywords = {62G05,62G20,asymptotics,Bayesian estimators,contraction,Gaussian sequence model,mixture priors,Sparsity},
  file = {/home/gkonkamking/Zotero/storage/WGJQ29LP/Castillo and Vaart - 2012 - Needles and Straw in a Haystack Posterior concentration for possibly sparse sequences.pdf}
}

@article{Cattuto2007,
  title = {Network Properties of Folksonomies},
  author = {Cattuto, Ciro and Schmitz, Christoph and Baldassarri, Andrea and Servedio, Vito D P and Loreto, Vittorio and Hotho, Andreas and Grahl, Miranda and Stumme, Gerd},
  year = {2007},
  journal = {Ai Communications},
  volume = {20},
  pages = {245--262},
  issn = {09217126},
  doi = {citeulike-article-id:1473536},
  abstract = {Social resource sharing systems like YouTube and del.icio.us have acquired a large number of users within the last few years. They provide rich resources for data analysis, information retrieval, and knowledge discovery applications. A first step towards this end is to gain better insights into content and structure of these systems. In this paper, we will analyse the main network characteristics of two of these systems. We consider their underlying data structures - so-called folksonomies - as tri-partite hypergraphs, and adapt classical network measures like characteristic path length and clustering coefficient to them. Subsequently, we introduce a network of tag co-occurrence and investigate some of its statistical properties, focusing on correlations in node connectivity and pointing out features that reflect emergent semantics within the folksonomy. We show that simple statistical indicators unambiguously spot non-social behavior such as spam.},
  isbn = {09217126},
  keywords = {folksonomies,nosource,semiotic dynamics,semiotics,small worlds}
}

@inproceedings{CE14,
  title = {Parallel Prefix Polymorphism Permits Parallelization, Presentation \& Proof},
  booktitle = {{{HPTCDL}}'14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages},
  author = {Chen, Jiahao and Edelman, Alan},
  year = {2014},
  pages = {47--56},
  publisher = {ACM},
  address = {New York},
  doi = {10.1109/HPTCDL.2014.9},
  abstract = {Polymorphism in programming languages enables code reuse. Here, we show that polymorphism has broad applicability far beyond computations for technical computing: parallelism in distributed computing, presentation of visualizations of runtime data flow, and proofs for formal verification of correctness. The ability to reuse a single codebase for all these purposes provides new ways to understand and verify parallel programs.},
  keywords = {nosource}
}

@article{Cedergreen2009,
  title = {Improved Empirical Models Describing Hormesis.},
  author = {Cedergreen, Nina and Ritz, Christian and Streibig, Jens Carl},
  year = {2005},
  journal = {Environmental toxicology and chemistry / SETAC},
  volume = {24},
  number = {12},
  eprint = {16445100},
  eprinttype = {pubmed},
  pages = {3166--72},
  publisher = {Wiley Online Library},
  issn = {0730-7268},
  doi = {10.1897/05-014R.1},
  abstract = {During the past two decades, the phenomenon of hormesis has gained increased recognition. To promote research in hormesis, a sound statistical quantification of important parameters, such as the level and significance of the increase in response and the range of concentration where it occurs, is strongly needed. Here, we present an improved statistical model to describe hormetic dose-response curves and test for the presence of hormesis. Using the delta method and freely available software, any percentage effect dose or concentration can be derived with its associated standard errors. Likewise, the maximal response can be extracted and the growth stimulation calculated. The new model was tested on macrophyte data from multiple-species experiments and on laboratory data of Lemna minor. For the 51 curves tested, significant hormesis was detected in 18 curves, and for another 17 curves, the hormesis model described that data better than the logistic model did. The increase in response ranged from 5 to 109\%. The growth stimulation occurred at an average dose somewhere between zero and concentrations corresponding to approximately 20 to 25\% of the median effective concentration (EC50). Testing the same data with the hormesis model proposed by Brain and Cousens in 1989, we found no significant hormesis. Consequently, the new model is shown to be far more robust than previous models, both in terms of variation in data and in terms of describing hormetic effects ranging from small effects of a 10\% increase in response up to effects of an almost 100\% increase in response.},
  isbn = {0730-7268},
  pmid = {16445100},
  keywords = {Animals,Arylsulfonates,Arylsulfonates: toxicity,Biological,Computer Simulation,Dose-Response Relationship,Drug,Herbicides,Herbicides: toxicity,Humans,Medicinal,Medicinal: drug effects,Medicinal: metabolism,Models,Nitrobenzoates,Nitrobenzoates: toxicity,nosource,Plants,Risk Assessment,Statistical,Toxicity Tests,Toxicology,Toxicology: statistics \& numerical data,Triazines,Triazines: toxicity}
}

@article{centerbar2008affective,
  title = {Affective Incoherence: When Affective Concepts and Embodied Reactions Clash.},
  author = {Centerbar, David B and Schnall, Simone and Clore, Gerald L and Garvin, Erika D},
  year = {2008},
  journal = {Journal of personality and social psychology},
  volume = {94},
  number = {4},
  pages = {560},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{cerouNonasymptoticTheoremUnnormalized2011,
  title = {A Nonasymptotic Theorem for Unnormalized {{Feynman}}--{{Kac}} Particle Models},
  author = {C{\'e}rou, F. and Del Moral, P. and Guyader, A.},
  year = {2011},
  month = aug,
  journal = {Annales de l'Institut Henri Poincar{\'e}, Probabilit{\'e}s et Statistiques},
  volume = {47},
  number = {3},
  pages = {629--649},
  issn = {0246-0203},
  doi = {10.1214/10-AIHP358},
  urldate = {2021-02-09},
  abstract = {We present a nonasymptotic theorem for interacting particle approximations of unnormalized Feynman--Kac models. We provide an original stochastic analysis-based on Feynman--Kac semigroup techniques combined with recently developed coalescent tree-based functional representations of particle block distributions. We present some regularity conditions under which the L2-relative error of these weighted particle measures grows linearly with respect to the time horizon yielding what seems to be the first results of this type for this class of unnormalized models. We also illustrate these results in the context of particle absorption models, with a special interest in rare event analysis.},
  langid = {english},
  keywords = {nosource}
}

@article{cerquetti2010bayesian,
  title = {Bayesian Nonparametric Analysis for a Species Sampling Model with Finitely Many Types},
  author = {Cerquetti, Annalisa},
  year = {2010},
  journal = {arXiv preprint arXiv:1001.0245},
  eprint = {1001.0245},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@article{cerquetti2012bayesian,
  title = {Bayesian Nonparametric Estimation of \{\vphantom\}{{S}}\vphantom\{\}impson's Evenness Index under {{$\alpha$}}-\{\vphantom\}{{G}}\vphantom\{\}ibbs Priors},
  author = {Cerquetti, Annalisa},
  year = {2012},
  journal = {arXiv preprint arXiv:1203.1666},
  eprint = {1203.1666},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@article{cerquettiNoteBayesianNonparametric2007,
  title = {A Note on {{Bayesian}} Nonparametric Priors Derived from Exponentially Tilted {{Poisson}}--{{Kingman}} Models},
  author = {Cerquetti, Annalisa},
  year = {2007},
  journal = {Statistics \& probability letters},
  volume = {77},
  number = {18},
  pages = {1705--1711},
  publisher = {Elsevier},
  doi = {10.1016/j.spl.2007.04.011},
  file = {/home/gkonkamking/Zotero/storage/82V8HBV7/Cerquetti - 2007 - A note on Bayesian nonparametric priors derived fr.pdf}
}

@article{ceylanEstimationCOVID19Prevalence2020,
  title = {Estimation of {{COVID-19}} Prevalence in {{Italy}}, {{Spain}}, and {{France}}},
  author = {Ceylan, Zeynep},
  year = {2020},
  month = aug,
  journal = {Science of The Total Environment},
  volume = {729},
  pages = {138817},
  issn = {0048-9697},
  doi = {10.1016/j.scitotenv.2020.138817},
  urldate = {2021-11-23},
  abstract = {At the end of December 2019, coronavirus disease 2019 (COVID-19) appeared in Wuhan city, China. As of April 15, 2020, {$>$}1.9 million COVID-19 cases were confirmed worldwide, including {$>$}120,000 deaths. There is an urgent need to monitor and predict COVID-19 prevalence to control this spread more effectively. Time series models are significant in predicting the impact of the COVID-19 outbreak and taking the necessary measures to respond to this crisis. In this study, Auto-Regressive Integrated Moving Average (ARIMA) models were developed to predict the epidemiological trend of COVID-19 prevalence of Italy, Spain, and France, the most affected countries of Europe. The prevalence data of COVID-19 from 21 February 2020 to 15 April 2020 were collected from the World Health Organization website. Several ARIMA models were formulated with different ARIMA parameters. ARIMA (0,2,1), ARIMA (1,2,0), and ARIMA (0,2,1) models with the lowest MAPE values (4.7520, 5.8486, and 5.6335) were selected as the best models for Italy, Spain, and France, respectively. This study shows that ARIMA models are suitable for predicting the prevalence of COVID-19 in the future. The results of the analysis can shed light on understanding the trends of the outbreak and give an idea of the epidemiological stage of these regions. Besides, the prediction of COVID-19 prevalence trends of Italy, Spain, and France can help take precautions and policy formulation for this epidemic in other countries.},
  langid = {english},
  keywords = {ARIMA,COVID-19,Forecasting,Infection disease,Pandemic,Time series},
  file = {/home/gkonkamking/Zotero/storage/UHTGZRY7/Ceylan - 2020 - Estimation of COVID-19 prevalence in Italy, Spain,.pdf}
}

@phdthesis{chabert-liddellStatisticalLearningCollections2022,
  title = {Statistical {{Learning}} of {{Collections}} of {{Networks}} with {{Applications}} in {{Ecology}} and {{Sociology}}},
  author = {{Chabert-Liddell}, Saint-Clair},
  year = {2022},
  month = mar,
  urldate = {2024-04-23},
  abstract = {This thesis deals with the development of statistical methods for the analysis of collections of interaction networks through three original contributions. Interaction networks are a natural way to represent in graph form the exchanges or relationships existing between a set of nodes representing species or individuals. Considering collections of networks allows to study heterogeneous systems, composed of several kinds of interactions involving different types of nodes. When the different networks of the collection are linked by a hierarchical relationship, we speak of multilevel networks. The stochastic block model has proven its relevance to model the heterogeneity of the behavior of nodes in a single network. Extensions to collections of networks and to multilevel networks are proposed. They allow to obtain a clustering of the nodes of the networks according to their role in the ecosystem or social system, and to summarize the structure of the system at the mesoscopic scale through a small number of parameters. The inference of these models is complex and variational methods are adapted for this purpose. Model selection methods are also used to determine the dependence between levels for multilevel networks and the similarity between structures for collections of networks.A last part of this thesis proposes a new method to study the robustness of ecological interaction networks. Each network is modeled by a probabilistic model whose parameters represent the network structure. This allows to make the link between the structure of the ecosystem and its robustness, but also to compare the robustness of a collection of networks and to correct the robustness of a network whose sampling would be incomplete.The developed methods are implemented in R packages and applied on data from social sciences and ecology.},
  langid = {english},
  school = {Universit{\'e} Paris-Saclay},
  file = {/home/gkonkamking/pCloudDrive/papers/Chabert-Liddell_2022_Statistical Learning of Collections of Networks with Applications in Ecology.pdf}
}

@article{chae2013bayesian,
  title = {Bayesian Clustering of Multinomial Observations with Auxiliary Variables},
  author = {Chae, Minwoo},
  year = {2013},
  journal = {Manuscript in preparation},
  keywords = {nosource}
}

@article{chaleyat2006computable,
  title = {Computable Infinite-Dimensional Filters with Applications to Discretized Diffusion Processes},
  author = {{Chaleyat-Maurel}, Mireille and {Genon-Catalot}, Valentine},
  year = {2006},
  journal = {Stochastic processes and their applications},
  volume = {116},
  number = {10},
  pages = {1447--1467},
  publisher = {Elsevier},
  file = {/home/gkonkamking/Zotero/storage/8H9DPKFF/Chaleyat-Maurel and Genon-Catalot - 2006 - Computable infinite-dimensional filters with appli.pdf}
}

@article{chaleyat2009filtering,
  title = {Filtering the {{Wright-Fisher}} Diffusion},
  author = {{Chaleyat-Maurel}, Mireille and {Genon-Catalot}, Valentine},
  year = {2009},
  journal = {ESAIM: Probability and Statistics},
  volume = {13},
  pages = {197--217},
  publisher = {EDP Sciences},
  file = {/home/gkonkamking/Zotero/storage/5Y6RS4J6/Chaleyat-Maurel and Genon-Catalot - 2009 - Filtering the Wright-Fisher diffusion.pdf}
}

@article{chalmandrierEffectsSpeciesSimilarity2015,
  title = {Effects of Species' Similarity and Dominance on the Functional and Phylogenetic Structure of a Plant Meta-Community},
  author = {Chalmandrier, L. and M{\"u}nkem{\"u}ller, T. and Lavergne, S. and Thuiller, W.},
  year = {2015},
  journal = {Ecology},
  volume = {96},
  number = {1},
  pages = {143--153},
  issn = {1939-9170},
  doi = {10.1890/13-2153.1},
  urldate = {2021-05-02},
  abstract = {Different assembly processes drive the spatial structure of meta-communities ({$\beta$}-diversity). Recently, functional and phylogenetic diversities have been suggested as indicators of these assembly processes. Assuming that diversity is a good proxy for niche overlap, high {$\beta$}-diversity along environmental gradients should be the result of environmental filtering while low {$\beta$}-diversity should stem from competitive interactions. So far, studies trying to disentangle the relative importance of these assembly processes have provided mixed results. One reason for this may be that these studies often rely on a single measure of diversity and thus implicitly make a choice on how they account for species relative abundances and how species similarities are captured by functional traits or phylogeny. Here, we tested the effect of gradually scaling the importance of dominance (the weight given to dominant vs. rare species) and species similarity (the weight given to small vs. large similarities) on resulting {$\beta$}-diversity patterns of an alpine plant meta-community. To this end, we combined recent extensions of the Hill numbers framework with Pagel's phylogenetic tree transformation approach. We included functional (based on the leaf--height--seed spectrum) and phylogenetic facets of {$\beta$}-diversity in our analysis and explicitly accounted for effects of environmental and spatial covariates. We found that functional {$\beta$}-diversity was high when the same weight was given to dominant vs. rare species and to large vs. small species' similarities. In contrast, phylogenetic {$\beta$}-diversity was low when greater weight was given to dominant species and small species' similarities. Those results suggested that different environments along the gradients filtered different species according to their functional traits, while, the same competitive lineages dominated communities across the gradients. Our results highlight that functional vs. phylogenetic facets, presence-absence vs. abundance structure and different weights of species' dissimilarity provide complementary and important information on the drivers of meta-community structure. By utilizing the full extent of information provided by the flexible frameworks of Hill numbers and Pagel's tree transformation, we propose a new approach to disentangle the patterns resulting from different assembly processes.},
  copyright = {{\copyright} 2015 by the Ecological Society of America},
  langid = {english},
  keywords = {{$\beta$}-diversity,alpine communities,community assembly,functional diversity,Hill numbers,phylogenetic diversity},
  file = {/home/gkonkamking/Zotero/storage/FB2QR8X6/Chalmandrier et al. - 2015 - Effects of species' similarity and dominance on th.pdf}
}

@article{chambaz2008bounds,
  title = {Bounds for {{Bayesian}} Order Identification with Application to Mixtures},
  author = {Chambaz, Antoine and Rousseau, Judith},
  year = {2008},
  journal = {The Annals of Statistics},
  volume = {36},
  number = {2},
  pages = {938--962},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/009053607000000857},
  file = {/home/gkonkamking/pCloudDrive/papers/Chambaz_Rousseau_2008_Bounds for Bayesian order identification with application to mixtures.pdf}
}

@article{chapman1996warning,
  title = {A Warning: {{NOECs}} Are Inappropriate for Regulatory Use},
  author = {Chapman, Peter M. and Caldwell, Richard S and Chapman, Peter F},
  year = {1996},
  journal = {Environmental Toxicology and Chemistry},
  volume = {15},
  number = {2},
  pages = {77--79},
  publisher = {Wiley Online Library},
  issn = {07307268},
  doi = {10.1002/etc.5620150201},
  isbn = {0730-7268},
  keywords = {nosource}
}

@article{Chapman1998,
  title = {A Critical Evaluation of Safety (Uncertainty) Factors for Ecological Risk Assessment},
  author = {Chapman, {\relax PM}},
  year = {1998},
  journal = {{\dots} Toxicology and Chemistry},
  volume = {17},
  number = {1},
  pages = {99--108},
  keywords = {duplicate-citation-key,nosource,safety factors}
}

@article{Chapman1998,
  title = {Annual {{Review A CRITICAL EVALUATION OF SAFETY}} ( {{UNCERTAINTY}} ) {{FACTORS FOR ECOLOGICAL RISK ASSESSMENT}}},
  author = {Hapman, P Eter M C and Fairbrother, Anne and Rown, D Erek B},
  year = {1998},
  journal = {Environmental toxicology and chemistry / SETAC},
  volume = {17},
  number = {1},
  pages = {99--108},
  issn = {0730-7268},
  doi = {10.1897/1551-5028(1998)017<0099:ACEOSU>2.3.CO;2},
  abstract = {Evaluation of environmental risks posed by potentially hazardous substances requires achieving a balance between over- and underprotection, i.e., between societal benefits posed by the use of particular substances and their potential risks. Uncertainty (e.g., only laboratory data may be available, field or epidemiological data may be limited and less than clear-cut, etc.) will always exist and is often conservatively dealt with by the use of so-called ``safety'' or ``uncertainty'' factors, some of which remain relatively little changed since their origin in 1945. Extrapolations involving safety factors for both aquatic and terrestrialenvironments include inter- and intraspecies, acute-to-chronic, lowest- to no-observed-effect concentration (NOEC), and laboratory-to-field ex- trapolation (e.g., extrapolation of laboratory results to the field). To be realistic, such extrapolations need to have a clear relationship with the field effect of concern and to be based on good science. The end result is, in any case, simply an estimate of a field NOEC, not an actual NOEC. Science-based versus policy-driven safety factors, including their uses and limitations, are critically examined in the context of national and international legislation on risk assessment. Key recommendations include providing safety factors as a potential threshold effects range instead of a discrete number and using experimental results rather than defaulting to safety factors to compensate for lack of information. This latter recommendation has the additional value of rendering safety factors predictive rather than simply protective. We also consider the so-called ``Precautionary Principle,'' which originated in 1980 and effectively addresses risk by proposing that the safety factor should be infinitely large. Keywords---Safety},
  isbn = {1552-8618},
  keywords = {duplicate-citation-key,nosource,safety factors}
}

@article{Chapman1998a,
  title = {New and Emerging Issues in Ecotoxicology- {{The}} Shape of Testing to Come?},
  booktitle = {Australasian Journal of Ecotoxicology},
  author = {Chapman, Peter M.},
  year = {1998},
  volume = {4},
  pages = {1--7},
  keywords = {nosource}
}

@article{Chapman2002a,
  title = {Ecological Risk Assessment ({{ERA}}) and Hormesis},
  author = {Chapman, Peter M.},
  year = {2002},
  month = apr,
  journal = {Science of the Total Environment},
  volume = {288},
  number = {1-2},
  eprint = {12013541},
  eprinttype = {pubmed},
  pages = {131--140},
  issn = {00489697},
  doi = {10.1016/S0048-9697(01)01120-2},
  abstract = {Based on our current state of knowledge, the significance and importance of hormesis is likely to be greater for ecotoxicology, a component of ecological risk assessment (ERA), than for the overall process of ERA. Appropriately determining the role of hormesis in ERA will require extension of hormesis beyond chemical stressors to abiotic (e.g. habitat) and biotic stressors (e.g. species introductions, organism interactions). It will also require determining for all stressors whether at both individual and higher levels of organization, hormesis has positive, neutral or adverse effects. This determination must be made for model organisms, populations and communities. Adverse effects are the least likely, however, neutral effects cannot be ruled out. Presently, consideration of hormetic effects in ERA is most appropriate in a detailed level ecological risk assessment (DLERA), the most complex form of ERA. It is not appropriate in either problem formulation or a screening level ERA (SLERA). Further, for hormetic effects to be recognized and accepted fully into ERA may require a paradigm shift. Three on-going paradigm shifts to which hormesis could be linked are: recognition of the low utility of no-observed effects concentrations (NOECs); recognition of the need for special treatment of essential element dose/concentration-responses, which are similar to hormetic responses; and, the replacement of environmental toxicology with ecological toxicology (ecotoxicology). {\copyright} 2002 Elsevier Science B.V. All rights reserved.},
  isbn = {1604986433},
  pmid = {12013541},
  keywords = {Ecological risk assessment,Ecotoxicology,Exposure-response relationships,Hormesis,nosource,Subsidy-stress}
}

@article{Chapman2002b,
  title = {Integrating Toxicology and Ecology: {{Putting}} the 'eco' into Ecotoxicology},
  author = {Chapman, Peter M.},
  year = {2002},
  journal = {Marine Pollution Bulletin},
  volume = {44},
  number = {1},
  pages = {7--15},
  issn = {0025326X},
  doi = {10.1016/S0025-326X(01)00253-3},
  abstract = {Environmental toxicology has been and continues to be an important discipline (e.g., single-species testing for screening purposes). However, ecological toxicology (ecotoxicology - more realism in tests, test species and exposures) is required for predicting real world effects and for site-specific assessments. Ecotoxicology and ecology have shown similar developmental patterns over time; closer cooperation between ecologists and toxicologists would benefit both disciplines. Ecology can be incorporated into toxicology either extrinsically (separately, e.g., providing information on pre-selected test species) or intrinsically (e.g., as part of test species selection) - the latter is preferable. General guidelines for acute and chronic testing and criteria for species selection differ for ecotoxicology and environmental toxicology, and are outlined. An overall framework is proposed based on ecological risk assessment (ERA), for combining ecology and toxicology (environmental and ecological) for decision-making. Increased emphasis on ecotoxicology represents a shift from reductionist to holistic approaches. Copyright ?? 2002 .},
  isbn = {1604986433},
  pmid = {11883685},
  keywords = {Ecology,Ecotoxicology,nosource,Risk assessment,Sediments}
}

@book{charalambides2005combinatorial,
  title = {Combinatorial Methods in Discrete Distributions},
  author = {Charalambides, Charalambos A},
  year = {2005},
  volume = {600},
  publisher = {John Wiley \& Sons},
  keywords = {nosource}
}

@incollection{Charles2009,
  title = {Ecotoxicology Modeling},
  booktitle = {Springer},
  author = {Devillers, James},
  editor = {Devillers, James},
  year = {2009},
  series = {Emerging Topics in Ecotoxicology},
  volume = {2},
  pages = {412},
  publisher = {Springer US},
  address = {Boston, MA},
  issn = {1868-1344},
  doi = {10.1007/978-1-4419-0197-2},
  abstract = {Both by definition and by scope, ecotoxicology is an interdisciplinary and multidisciplinary science combining the disciplines of chemistry, biochemistry, toxicology, and ecology. Modeling is also increasingly used by ecotoxicologists to better understand the effects and fate of chemicals in the different compartments of the biosphere and to simulate them in the frame of predictive hazard and risk assessment schemes. Ecotoxicology Modeling is a comprehensive and well-documented text providing a collection of computational methods to the ecotoxicologists primarily interested in the study of the adverse effects of chemicals, their mechanisms of action and/or their environmental fate and behavior. Avoiding mathematical jargon, the book presents numerous case studies to enable the reader to understand the interest but also the limitations of linear and nonlinear models in ecotoxicology. Written by an international team of scientists, Ecotoxicology Modeling is of primary interest to those whose research or professional activity is directly concerned with the development and application of models in ecotoxicology. It is also intended to provide the graduate and post-graduate students with a clear and accessible text covering the main types of modeling approaches used in environmental sciences.},
  isbn = {978-1-4419-0196-5},
  pmid = {15828940},
  keywords = {nosource,PBTK,QSAR}
}

@article{chen2000probability,
  title = {Probability Density Function Estimation Using Gamma Kernels},
  author = {Chen, Song Xi},
  year = {2000},
  journal = {Annals of the Institute of Statistical Mathematics},
  volume = {52},
  number = {3},
  pages = {471--480},
  publisher = {Springer},
  keywords = {nosource}
}

@article{Chen2004a,
  ids = {Chen2004},
  title = {A Conservative, Nonparametric Estimator for the 5th Percentile of the Species Sensitivity Distributions},
  author = {Chen, Ling},
  year = {2004},
  month = jul,
  journal = {Journal of Statistical Planning and Inference},
  volume = {123},
  number = {2},
  pages = {243--258},
  issn = {03783758},
  doi = {10.1016/S0378-3758(03)00148-4},
  keywords = {logistic model,Logistic model,lognormal distribution,Lognormal distribution,nosource,risk assessment,Risk assessment,species sensitivity,Species sensitivity distributions,the p th percentile,The pth percentile}
}

@article{Chen2014a,
  title = {Bayesian Variable Selection for Multi-Response Linear Regression},
  author = {Chen, Wan-ping and Wu, Ying Nian and Chen, Ray-bin},
  year = {2014},
  number = {1996},
  pages = {74--88},
  keywords = {multi-task learning,nosource,simultaneous sparse,support union recovery}
}

@article{chen2014use,
  title = {The Use of Sampling Weights in {{Bayesian}} Hierarchical Models for Small Area Estimation},
  author = {Chen, Cici and Wakefield, Jon and Lumely, Thomas},
  year = {2014},
  journal = {Spatial and spatio-temporal epidemiology},
  volume = {11},
  pages = {33--43},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{chenBayesianHierarchicalFramework2015,
  title = {A {{Bayesian Hierarchical Framework}} for {{Modeling Brain Connectivity}} for {{Neuroimaging Data}}},
  author = {Chen, Shuo and Bowman, F. DuBois and Mayberg, Helen S.},
  year = {2015},
  month = oct,
  journal = {Biometrics},
  volume = {72},
  number = {2},
  pages = {596},
  doi = {10.1111/biom.12433},
  urldate = {2024-10-28},
  abstract = {We propose a novel Bayesian hierarchical model for brain imaging data that unifies voxel level (the most localized unit of measure) and region level brain connectivity analyses, and yields population level inferences. Functional connectivity ...},
  langid = {english},
  pmid = {26501687},
  file = {/home/gkonkamking/pCloudDrive/papers/Chen et al. - 2015 - A Bayesian Hierarchical Framework for Modeling Brain Connectivity for Neuroimaging Data.pdf}
}

@article{chendependent,
  title = {Dependent Normalized Random Measures},
  author = {Chen, Changyou and Rao, Vinayak and Buntine, Wray and Whye Teh, Yee},
  year = {2013},
  journal = {Proceedings of The 30th International Conference on Machine Learning},
  pages = {969--977},
  keywords = {nosource}
}

@article{chenReviewStudyFunctional2021,
  title = {A Review Study of Functional Autoregressive Models with Application to Energy Forecasting},
  author = {Chen, Ying and Koch, Thorsten and Lim, Kian Guan and Xu, Xiaofei and Zakiyeva, Nazgul},
  year = {2021},
  journal = {WIREs Computational Statistics},
  volume = {13},
  number = {3},
  pages = {e1525},
  issn = {1939-0068},
  doi = {10.1002/wics.1525},
  urldate = {2023-03-19},
  abstract = {In this data-rich era, it is essential to develop advanced techniques to analyze and understand large amounts of data and extract the underlying information in a flexible way. We provide a review study on the state-of-the-art statistical time series models for univariate and multivariate functional data with serial dependence. In particular, we review functional autoregressive (FAR) models and their variations under different scenarios. The models include the classic FAR model under stationarity; the FARX and pFAR model dealing with multiple exogenous functional variables and large-scale mixed-type exogenous variables; the vector FAR model and common functional principal component technique to handle multiple dimensional functional time series; and the warping FAR, varying coefficient-FAR and adaptive FAR models to handle seasonal variations, slow varying effects and the more challenging cases of structural changes or breaks respectively. We present the models' setup and detail the estimation procedure. We discuss the models' applicability and illustrate the numerical performance using real-world data of high-resolution natural gas flows in the high-pressure gas pipeline network of Germany. We conduct 1-day and 14-days-ahead out-of-sample forecasts of the daily gas flow curves. We observe that the functional time series models generally produce stable out-of-sample forecast accuracy. This article is categorized under: Statistical Models {$>$} Semiparametric Models Data: Types and Structure {$>$} Time Series, Stochastic Processes, and Functional Data},
  langid = {english},
  keywords = {energy forecast,functional autoregressive modeling,functional time series,sieve estimation},
  file = {/home/gkonkamking/pCloudDrive/papers/Chen et al_2021_A review study of functional autoregressive models with application to energy.pdf}
}

@article{cherian2023statistical,
  title = {Statistical Inference for Fairness Auditing},
  author = {Cherian, John J and Cand{\`e}s, Emmanuel J},
  year = {2023},
  journal = {arXiv preprint arXiv:2305.03712},
  eprint = {2305.03712},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/pCloudDrive/papers/Cherian_Candès_2023_Statistical inference for fairness auditing.pdf}
}

@article{chessman2020no,
  title = {No Support for Purported Effects of Salt-Tolerant Stream Invertebrates on the Salinity Responses of Salt-Sensitive Stream Invertebrates},
  author = {Chessman, Bruce C},
  year = {2020},
  journal = {Marine and Freshwater Research},
  publisher = {CSIRO},
  file = {/home/gkonkamking/Zotero/storage/GVHERXBC/Chessman - 2020 - No support for purported effects of salt-tolerant .pdf}
}

@article{Chevre2006,
  title = {Policy Analysis Including Mixtures in the Determination of Water Quality Criteria for Herbicides in Surface Water},
  author = {Chevre, Nathalie and Stamm, Christian and Fenner, Kathrin and Loepfe, Christian and Singer, Heinz and Escher, Beate I.},
  year = {2006},
  journal = {Environmental science \& technology},
  volume = {40},
  number = {2},
  isbn = {0013-936X},
  pmid = {16468385},
  keywords = {duplicate-citation-key,nosource}
}

@article{Chevre2008,
  title = {Cost-Effective Experimental Design to Support Modeling of Concentration-Response Functions},
  author = {Ch{\`e}vre, Nathalie and Brazzale, Alessandra R.},
  year = {2008},
  month = jun,
  journal = {Chemosphere},
  volume = {72},
  number = {5},
  eprint = {18436276},
  eprinttype = {pubmed},
  pages = {803--810},
  issn = {00456535},
  doi = {10.1016/j.chemosphere.2008.03.001},
  abstract = {Modeling concentration-response function became extremely popular in ecotoxicology during the last decade. Indeed, modeling allows determining the total response pattern of a given substance. However, reliable modeling is consuming in term of data, which is in contradiction with the current trend in ecotoxicology, which aims to reduce, for cost and ethical reasons, the number of data produced during an experiment. It is therefore crucial to determine experimental design in a cost-effective manner. In this paper, we propose to use the theory of locally D-optimal designs to determine the set of concentrations to be tested so that the parameters of the concentration-response function can be estimated with high precision. We illustrated this approach by determining the locally D-optimal designs to estimate the toxicity of the herbicide dinoseb on daphnids and algae. The results show that the number of concentrations to be tested is often equal to the number of parameters and often related to the their meaning, i.e. they are located close to the parameters. Furthermore, the results show that the locally D-optimal design often has the minimal number of support points and is not much sensitive to small changes in nominal values of the parameters. In order to reduce the experimental cost and the use of test organisms, especially in case of long-term studies, reliable nominal values may therefore be fixed based on prior knowledge and literature research instead of on preliminary experiments. ?? 2008 Elsevier Ltd. All rights reserved.},
  pmid = {18436276},
  keywords = {concentration-response function,D-optimality criterion,Ecotoxicology,Experimental design,Modeling,nosource}
}

@article{chi2022microestimates,
  title = {Microestimates of Wealth for All Low-and Middle-Income Countries},
  author = {Chi, Guanghua and Fang, Han and Chatterjee, Sourav and Blumenstock, Joshua E},
  year = {2022},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {3},
  publisher = {National Acad Sciences},
  doi = {10.1073/pnas.2113658119},
  keywords = {nosource}
}

@article{chib1996calculating,
  title = {Calculating Posterior Distributions and Modal Estimates in {{Markov}} Mixture Models},
  author = {Chib, Siddhartha},
  year = {1996},
  journal = {Journal of Econometrics},
  volume = {75},
  number = {1},
  pages = {79--97},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{chiMicroestimatesWealthAll2022,
  title = {Microestimates of Wealth for All Low- and Middle-Income Countries},
  author = {Chi, Guanghua and Fang, Han and Chatterjee, Sourav and Blumenstock, Joshua E.},
  year = {2022},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {3},
  pages = {e2113658119},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2113658119},
  urldate = {2023-10-25},
  abstract = {Many critical policy decisions, from strategic investments to the allocation of humanitarian aid, rely on data about the geographic distribution of wealth and poverty. Yet many poverty maps are out of date or exist only at very coarse levels of granularity. Here we develop microestimates of the relative wealth and poverty of the populated surface of all 135 low- and middle-income countries (LMICs) at 2.4 km resolution. The estimates are built by applying machine-learning algorithms to vast and heterogeneous data from satellites, mobile phone networks, and topographic maps, as well as aggregated and deidentified connectivity data from Facebook. We train and calibrate the estimates using nationally representative household survey data from 56 LMICs and then validate their accuracy using four independent sources of household survey data from 18 countries. We also provide confidence intervals for each microestimate to facilitate responsible downstream use. These estimates are provided free for public use in the hope that they enable targeted policy response to the COVID-19 pandemic, provide the foundation for insights into the causes and consequences of economic development and growth, and promote responsible policymaking in support of sustainable development.},
  file = {/home/gkonkamking/pCloudDrive/papers/Chi et al_2022_Microestimates of wealth for all low- and middle-income countries.pdf}
}

@article{chiquet2018variational,
  title = {Variational Inference for Sparse Network Reconstruction from Count Data},
  author = {Chiquet, Julien and Mariadassou, Mahendra and Robin, St{\'e}phane},
  year = {2018},
  journal = {arXiv preprint arXiv:1806.03120},
  eprint = {1806.03120},
  archiveprefix = {arXiv},
  file = {/home/gkonkamking/Zotero/storage/B4SZ4YW2/Chiquet et al. - 2018 - Variational inference for sparse network reconstru.pdf}
}

@article{Choi2009,
  title = {Integrated Stochastic Environmental Risk Assessment of the Harbour Area Treatment Scheme ({{HATS}}) in {{Hong Kong}}.},
  author = {Choi, K W and Lee, Joseph H W and Kwok, Kevin W H and Leung, Kenneth M Y},
  year = {2009},
  month = may,
  journal = {Environmental science \& technology},
  volume = {43},
  number = {10},
  eprint = {19544877},
  eprinttype = {pubmed},
  pages = {3705--11},
  issn = {0013-936X},
  abstract = {Submarine ocean outfalls are commonly used for the disposal of partially treated effluents in coastal cities. Typically, the greatest environmental risk caused by toxic substances occurs in the near field of the outfall discharge. The ecological impact of the effluent varies greatly under different discharge and environmental conditions that are characterized by both regular and stochastic variations. For a comprehensive environmental risk assessment of a coastal discharge, it is necessary to determine both the likelihood and severity of the adverse effects on the biological community. We present the first integrated stochastic (Monte Carlo) environmental risk assessment of a major coastal sewage outfall discharge--the Stonecutters Island outfall of the Harbour Area Treatment Scheme (HATS) in Hong Kong. Unionized ammonia (NH3) is used as the target pollutant. To accurately envisage the ambient concentrations of NH3, a Lagrangian jet model (JETLAG/VISJET) is used to analyze pollutant concentrations in the nearfield of the outfall. The environmental conditions are simulated from 3D hydrodynamic model simulations over a 4 month period for typical wet and dry seasons. Statistical characteristics of the effluent discharge and receiving water temperature are derived from field data. The probability distribution of predicted exposure concentrations (EC) is generated from this integrated simulation. A species sensitivity distribution, which represents a statistical distribution of threshold sublethal effects levels or benchmark concentrations (BC) for various marine organisms is constructed using available chronic toxicity data. The environmental risk of NH3 on the marine community is characterized by computing statistical distributions of Hazard Quotient (HO = EC/BC) using Monte Carlo simulation. It is found that the probability of HO {$>$} for HATS Stage 1 (1.6 million m3/day sewage treated with chemically enhanced primary treatment) is around 0.11 for wet season but just about 0.06 for the dry season. The risk increases by about 10\% to 0.08-0.13 with additional sewage loads of 0.8 million m3/day at the same level of treatment (HATS Stage 2A). With an upgrade to secondary treatment (HATS Stage 2B), the probability will be reduced to 0.03-0.05. Compared to the use of "worst case" scenarios or point pollution threshold estimates, the present method offers a more holistic ecological assessment, and is much less sensitive to arbitrary choice of model parameters. The present risk assessment approach can be readily extended to the accurate determination of mixing zones based on statistical evaluation of ecological risks.},
  isbn = {0013-936X},
  pmid = {19544877},
  keywords = {Ammonia,duplicate-citation-key,Environment,Geography,Hong Kong,nosource,Risk Assessment,Sewage,Species Specificity,Stochastic Processes,Tropical Climate,Water Pollution}
}

@book{chopinIntroductionSequentialMonte2020,
  title = {An {{Introduction}} to {{Sequential Monte Carlo}}},
  author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
  year = {2020},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer International Publishing},
  urldate = {2020-05-28},
  abstract = {This book provides a general introduction to Sequential Monte Carlo (SMC) methods, also known as particle filters. These methods have become a staple for the sequential analysis of data in such diverse fields as signal processing, epidemiology, machine learning, population ecology, quantitative finance, and robotics.The coverage is comprehensive, ranging from the underlying theory to computational implementation, methodology, and diverse applications in various areas of science. This is achieved by describing SMC algorithms as particular cases of a general framework, which involves concepts such as Feynman-Kac distributions, and tools such as importance sampling and resampling. This general framework is used consistently throughout the book.Extensive coverage is provided on sequential learning (filtering, smoothing) of state-space (hidden Markov) models, as this remains an important application of SMC methods. More recent applications, such as parameter estimation of these models (through e.g. particle Markov chain Monte Carlo techniques) and the simulation of challenging probability distributions (in e.g. Bayesian inference or rare-event problems), are also discussed.The book may be used either as a graduate text on Sequential Monte Carlo methods and state-space modeling, or as a general reference work on the area. Each chapter includes a set of exercises for self-study, a comprehensive bibliography, and a ``Python corner,'' which discusses the practical implementation of the methods covered. In addition, the book comes with an open source Python library, which implements all the algorithms described in the book, and contains all the programs that were used to perform the numerical experiments.},
  isbn = {978-3-030-47844-5},
  langid = {english},
  keywords = {nosource}
}

@article{chung2009local,
  title = {The Local Dirichlet Process},
  author = {Chung, Yeonseung and Dunson, David B.},
  year = {2011},
  journal = {Annals of the Institute of Statistical Mathematics},
  volume = {63},
  number = {1},
  pages = {59--80},
  publisher = {Springer},
  issn = {00203157},
  doi = {10.1007/s10463-008-0218-9},
  abstract = {As a generalization of the Dirichlet process (DP) to allow predictor dependence, we propose a local Dirichlet process (lDP). The lDP provides a prior distribution for a collection of random probability measures indexed by predictors. This is accomplished by assigning stick-breaking weights and atoms to random locations in a predictor space. The probability measure at a given predictor value is then formulated using the weights and atoms located in a neighborhood about that predictor value. This construction results in a marginal DP prior for the random measure at any specific predictor value. Dependence is induced through local sharing of random components. Theoretical properties are considered and a blocked Gibbs sampler is proposed for posterior computation in lDP mixture models. The methods are illustrated using simulated examples and an epidemiologic application.},
  isbn = {1046300802189},
  pmid = {23645935},
  keywords = {Blocked Gibbs sampler,Dependent Dirichlet process,duplicate-citation-key,Mixture model,Non-parametric Bayes,nosource,Stick-breaking representation}
}

@article{chung2009local,
  title = {The Local Dirichlet Process},
  author = {Chung, Y and Dunson, D B},
  year = {2009},
  journal = {Annals of the Institute of Statistical Mathematics},
  pages = {1--22},
  publisher = {Springer},
  issn = {0020-3157},
  keywords = {duplicate-citation-key,nosource}
}

@article{chung2009nonparametric,
  title = {Nonparametric Bayes Conditional Distribution Modeling with Variable Selection},
  author = {Chung, Yeonseung and Dunson, David B.},
  year = {2009},
  journal = {Journal of the American Statistical Association},
  volume = {104},
  number = {488},
  pages = {1646--1660},
  issn = {0162-1459},
  doi = {10.1198/jasa.2009.tm08302},
  abstract = {This article considers a methodology for flexibly characterizing the relationship between a response and multiple predictors. Goals are (1) to estimate the conditional response distribution addressing the distributional changes across the predictor space, and (2) to identify important predictors for the response distribution change both within local regions and globally. We first introduce the probit stick-breaking process (PSBP) as a prior for an uncountable collection of predictor-dependent random distributions and propose a PSBP mixture (PSBPM) of normal regressions for modeling the conditional distributions. A global variable selection structure is incorporated to discard unimportant predictors, while allowing estimation of posterior inclusion probabilities. Local variable selection is conducted relying on the conditional distribution estimates at different predictor points. An efficient stochastic search sampling algorithm is proposed for posterior computation. The methods are illustrated through simulation and applied to an epidemiologic study.},
  pmid = {23580793},
  keywords = {conditional distribution estimation,hypothesis testing,kernel stick-breaking process,mixture of experts,nosource,search variable selection,stochastic}
}

@article{Chung2015,
  title = {Weakly Informative Prior for Point Estimation of Covariance Matrices in Hierarchical Models},
  author = {Chung, Yeonseung and Gelman, Andrew G and {Rabe-Hesketh}, S. and Liu, J. and Dorie, V.},
  year = {2015},
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {40},
  number = {2},
  eprint = {1011.1669v3},
  pages = {136--157},
  issn = {1076-9986},
  doi = {10.3102/1076998615570945},
  abstract = {When fitting hierarchical regression models, maximum likelihood (ML) estimation has computational (and, for some users, philosophical) advantages compared to full Bayesian inference, but when the number of groups is small, estimates of the covariance matrix (\{Sigma\}) of group-level varying coefficients are often degenerate. One can do better, even from a purely point estimation perspective, by using a prior distribution or penalty function. In this article, we use Bayes modal estimation to obtain positive definite covariance matrix estimates. We recommend a class of Wishart (not inverse-Wishart) priors for \{Sigma\} with a default choice of hyperparameters, that is, the degrees of freedom are set equal to the number of varying coefficients plus 2, and the scale matrix is the identity matrix multiplied by a value that is large relative to the scale of the problem. This prior is equivalent to independent gamma priors for the eigenvalues of \{Sigma\} with shape parameter 1.5 and rate parameter close to 0. It is also equivalent to independent gamma priors for the variances with the same hyperparameters multiplied by a function of the correlation coefficients. With this default prior, the posterior mode for \{Sigma\} is always strictly positive definite. Furthermore, the resulting uncertainty for the fixed coefficients is less underestimated than under classical ML or restricted maximum likelihood estimation. We also suggest an extension of our method that can be used when stronger prior information is available for some of the variances or correlations.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {1076998615570},
  pmid = {25246403},
  keywords = {nosource}
}

@article{cifarelli1978,
  title = {Problemi Statistici Non Parametrici in Condizioni Di Scambiabilit{\`a} Parziale. {{Impiego}} Di Medie Associative},
  author = {Cifarelli, D M and Regazzini, Eugenio},
  year = {1978},
  journal = {Quaderni Istituto Matematica Finanziaria dell'Universit{\`a} di Torino},
  publisher = {Quaderni Istituto Matematica Finanziaria, Torino},
  keywords = {nosource}
}

@article{Ciffroy2012a,
  ids = {Ciffroy2012},
  title = {Estimating Hazardous Concentrations by an Informative Bayesian Approach.},
  author = {Ciffroy, Philippe and Keller, Merlin and Pasanisi, Alberto},
  year = {2012},
  month = dec,
  journal = {Environmental toxicology and chemistry / SETAC},
  number = {October},
  eprint = {23280589},
  eprinttype = {pubmed},
  issn = {1552-8618},
  doi = {10.1002/etc.2096},
  abstract = {The species sensitivity distribution (SSD) approach is recommended for assessing chemical risk. In practice, however, it can be used only for the few substances for which large-scale ecotoxicological results are available. Indeed, the statistical frequentist approaches used for building SSDs and for deriving hazardous concentrations (HC5) inherently require extensive data to guarantee goodness-of-fit. An alternative Bayesian approach to estimating HC5 from small data sets was developed. In contrast to the noninformative Bayesian approaches that have been tested to date, the authors' method used informative priors related to the expected species sensitivity variance. This method was tested on actual ecotoxicological data for 21 "well-informed" substances. A cross-validation compared the HC5 values calculated using frequentist approaches with the results of our Bayesian approach, using both complete and truncated data samples. The authors' informative Bayesian approach was compared with noninformative Bayesian methods published in the past, including those incorporating loss functions. The authors found that even for the truncated sample the HC5 values derived from the informative Bayesian approach were generally close to those obtained using the frequentist approach, which requires more data. In addition, the probability of overestimating an HC5 is rather limited. More robust HC5 estimates can be practically obtained from additional data without impairing regulatory protection levels, which will encourage collecting new ecotoxicological data. In conclusion, the Bayesian informative approach was shown to be relatively robust and could be a good surrogate approach for deriving HC5 values from small data sets. Environ. Toxicol. Chem. {\copyright} 2012 SETAC.},
  isbn = {1552-8618},
  pmid = {23280589},
  keywords = {Bayesian statistic,nosource,Predicted no-effect concentration,Species sensitivity distribution}
}

@article{Ciliberti2011,
  title = {The {{Nile}} Monitor ({{Varanus}} Niloticus; {{Squamata}}: {{Varanidae}}) as a Sentinel Species for Lead and Cadmium Contamination in Sub-{{Saharan}} Wetlands},
  author = {Ciliberti, Alexandre and Berny, Philippe and {Delignette-Muller}, Marie Laure and {de Buffr{\'e}nil}, Vivian},
  year = {2011},
  month = oct,
  journal = {Science of the Total Environment},
  volume = {409},
  number = {22},
  eprint = {21885092},
  eprinttype = {pubmed},
  pages = {4735--4745},
  publisher = {Elsevier B.V.},
  issn = {00489697},
  doi = {10.1016/j.scitotenv.2011.07.028},
  abstract = {Wetland pollution is a matter of concern in sub-Saharan Africa. Though regularly exploited, the Nile monitor (Varanus niloticus), a large amphibious lizard, is not threatened. This work aims at assessing the value of this varanid as a sentinel species in surveys of environmental contamination by metals. Lead and cadmium quantifications were performed by graphite furnace-atomic absorption spectrophotometry in bone, intestine, kidney, liver and muscle in 71 monitors from three unevenly polluted sites in Mali and Niger, plus a reference site. The effects of sex, size and fat reserves as well as factors related to the sampling strategy (tissue sampled, sampling site) were studied with a mixed linear model. Metal contamination is moderate at the four sites but clear differences nevertheless occur. Lead levels are generally maximal in bone, with a gender-independent median value 320ng.g -1. Median cadmium concentrations never exceed 70.2ng.g -1 in females (kidney) and 57.5ng.g -1 in males (intestine). Such levels should have no detrimental effects on the monitors. Lead and cadmium levels in muscles are generally below 200 and 20ng.g -1, respectively, and should provoke no health hazard to occasional consumers of monitor meat. Metal organotropisms are consistent with those observed in other studies about Squamates: for lead: bone{$>$}[kidney, intestine, liver]{$>$}muscle in males and [bone, kidney]{$>$}[intestine, liver]{$>$}muscle in females; for cadmium: [liver, intestine, kidney]{$>$}[bone, muscle] for both genders. Females are more contaminated, especially in their kidneys. In this tissue, median values in ng.g -1 are 129.7 and 344.0 for lead and 43.0 and 70.2 for cadmium, for males and females, respectively. Nile monitors can reveal subtle differences in local pollution by metals; moreover, the spatial resolution of the pollution indication that they give seems to be very sharp. The practical relevance of this new tool is thus validated. {\copyright} 2011 Elsevier B.V.},
  pmid = {21885092},
  keywords = {Africa,Cadmium,Lead,nosource,Sentinel species,Squamata,Wetland}
}

@article{Ciric2012,
  title = {Use of Sensitivity Analysis to Identify Influential and Non-Influential Parameters within an Aquatic Ecosystem Model},
  author = {Ciric, C and Ciffroy, Philippe and Charles, Sandrine},
  year = {2012},
  month = jan,
  journal = {Ecological Modelling},
  volume = {246},
  pages = {119--130},
  publisher = {Elsevier B.V.},
  issn = {0304-3800},
  abstract = {Food-web models can be powerful tools to assess effects of chemicals on ecosystem structure and functioning. Indeed, they do not only account for direct ecotoxi.},
  keywords = {Ecological modelling,EFAST,Food-web models,Morris method,nosource,Sensitivity analysis}
}

@article{Clark2001,
  title = {Ecological Forecasting: An Emerging Imperative},
  author = {Clark, J. S},
  year = {2001},
  journal = {Science},
  volume = {293},
  number = {2001},
  pages = {657{\dbend}660},
  issn = {0036-8075},
  doi = {10.1126/science.293.5530.657},
  abstract = {Planning and decision-making can be improved by access to reliable forecasts of ecosystem state, ecosystem services, and natural capital. Availability of new data sets, together with progress in computation and statistics, will increase our ability to forecast ecosystem change. An agenda that would lead toward a capacity to produce, evaluate, and communicate forecasts of critical ecosystem services requires a process that engages scientists and decision-makers. Interdisciplinary linkages are necessary because of the climate and societal controls on ecosystems, the feedbacks involving social change, and the decision-making relevance of forecasts.},
  isbn = {0036-8075},
  pmid = {11474103},
  keywords = {nosource}
}

@article{clarke1981error,
  title = {Error and Uncertainty in Travel Surveys},
  author = {Clarke, Mike and Dix, Martin and Jones, Peter},
  year = {1981},
  journal = {Transportation},
  volume = {10},
  number = {2},
  pages = {105--126},
  publisher = {Springer},
  keywords = {nosource}
}

@article{Clarke2002,
  title = {Dryland Salinity in South-Western {{Australia}}: {{Its}} Origins, Remedies, and Future Research Directions},
  author = {Clarke, C. J. and George, R. J. and Bell, R. W. and Hatton, T. J.},
  year = {2002},
  journal = {Australian Journal of Soil Research},
  volume = {40},
  number = {1},
  pages = {93--113},
  issn = {00049573},
  doi = {10.1071/SR01028},
  abstract = {Replacement of deep-rooted, perennial native vegetation with shallow-rooted, annual agricultural plants has resulted in increased recharge causing shallow saline water tables leading to dryland salinity and loss of agricultural production. Restoring the vegetation by regeneration or replanting lowers water levels locally but field evidence and computer modelling suggests this needs to be widespread for regional effects, which conflicts with the future of conventional agriculture. Alley farming allows agriculture to be continued in the bays between the rows, but needs as much perennial, preferably deep-rooted, vegetation as possible in the bays to achieve the required recharge reductions. Where the asset to be preserved is valuable and a means of safe saline effluent disposal exists, pumps and drains will be part of any salinity management system, but where these conditions are not met they will be of limited use on an economic basis. To limit the spread of dryland salinity substantial change in farming systems is required and farmers need assurance that the recommended strategies will have the desired effect. Computer modelling is the only timely way to do this. An operationally simple 1-dimensional model already exists, and a 2-dimensional one is under development and testing. Three-dimensional modelling is also probably required to support strategic, intensive interventions.},
  isbn = {0004-9573},
  keywords = {Computer modelling,Engineering,nosource,Perennial,Revegetation}
}

@article{clauset2009power,
  title = {Power-Law Distributions in Empirical Data},
  author = {Clauset, Aaron and Shalizi, Cosma Rohilla and Newman, Mark E J},
  year = {2009},
  journal = {SIAM review},
  volume = {51},
  number = {4},
  pages = {661--703},
  publisher = {SIAM},
  keywords = {nosource}
}

@article{coda,
  title = {{{CODA}}: Convergence Diagnosis and Output Analysis for {{MCMC}}},
  author = {Plummer, Martyn and Best, Nicky and Cowles, Kate and Vines, Karen},
  year = {2006},
  journal = {R News},
  volume = {6},
  number = {March},
  pages = {7--11},
  issn = {1662-4025},
  doi = {10.1159/000323281},
  abstract = {[1st paragraph] At first sight, Bayesian inference with Markov Chain Monte Carlo (MCMC) appears to be straightforward. The user defines a full probability model, perhaps using one of the programs discussed in this issue; an underlying sampling engine takes the model definition and returns a sequence of dependent samples from the posterior distribution of the model parameters, given the supplied data. The user can derive any summary of the posterior distribution from this sample. For example, to calculate a 95\% credible interval for a parameter {$\alpha$}, it suffices to take 1000 MCMC iterations of {$\alpha$} and sort them so that {$\alpha$}121000. The credible interval estimate is then ({$\alpha$}25, {$\alpha$}975). However, there is a price to be paid for this simplicity. Unlike most numerical methods used in statistical inference, MCMC does not give a clear indication of whether it has converged. The underlying Markov chain theory only guarantees that the distribution of the output will converge to the posterior in the limit as the number of iterations increases to infinity. The user is generally ignorant about how quickly convergence occurs, and therefore has to fall back on post hoc testing of the sampled output. By convention, the sample is divided into two parts: a ``burn in'' period during which all samples are discarded, and the remainder of the run in which the chain is considered to have converged sufficiently close to the limiting distribution to be used. Two questions then arise: 1. How long should the burn in period be? 2. How many samples are required to accurately estimate posterior quantities of interest? The coda package for R contains a set of functions designed to help the user answer these questions. Some of these convergence diagnostics are simple graphical ways of summarizing the data. Others are formal statistical tests.},
  isbn = {1609-3631},
  pmid = {21196786},
  keywords = {nosource}
}

@article{cohen1994earth,
  title = {The Earth Is Round (P{$<$} 05).},
  author = {Cohen, Jacob},
  year = {1994},
  journal = {American psychologist},
  volume = {49},
  number = {12},
  pages = {997},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{cohen2019efficient,
  title = {Efficient Candidate Screening under Multiple Tests and Implications for Fairness},
  author = {Cohen, Lee and Lipton, Zachary C and Mansour, Yishay},
  year = {2019},
  journal = {arXiv preprint arXiv:1905.11361},
  eprint = {1905.11361},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{cohen2019efficient,
  title = {Efficient Candidate Screening under Multiple Tests and Implications for Fairness},
  author = {Cohen, Lee and Lipton, Zachary C and Mansour, Yishay},
  year = {2019},
  journal = {arXiv preprint arXiv:1905.11361},
  eprint = {1905.11361},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{collaborationEstimatingReproducibilityPsychological2015,
  ids = {collaborationEstimatingReproducibilityPsychological2015a},
  title = {Estimating the Reproducibility of Psychological Science},
  author = {Collaboration, Open Science},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  urldate = {2020-03-10},
  abstract = {Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 Structured Abstract INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  chapter = {Research Article},
  copyright = {Copyright {\copyright} 2015, American Association for the Advancement of Science},
  langid = {english},
  pmid = {26315443},
  file = {/home/gkonkamking/Zotero/storage/CV3E72DI/Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf;/home/gkonkamking/Zotero/storage/GNHQEFRJ/Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@article{Commeau2012,
  title = {Fitting a Lognormal Distribution to Enumeration and Absence/Presence Data},
  author = {Commeau, Natalie and Parent, Eric and {Delignette-Muller}, Marie Laure and Cornu, Marie},
  year = {2012},
  month = apr,
  journal = {International Journal of Food Microbiology},
  volume = {155},
  number = {3},
  eprint = {22353674},
  eprinttype = {pubmed},
  pages = {146--152},
  publisher = {Elsevier B.V.},
  issn = {01681605},
  doi = {10.1016/j.ijfoodmicro.2012.01.023},
  abstract = {To fit a lognormal distribution to a complex set of microbial data, including detection data (e.g. presence or absence in 25. g) and enumeration data (e.g. 30. cfu/g), we compared two models: a model called MCLD based on data expressed as concentrations (in cfu/g) or censored concentrations (e.g {$<$} 10. cfu/g, or {$>$}. 1. cfu/25. g) versus a model called MRD that directly uses raw data (presence/absence in test portions, and plate colony counts). We used these two models to simulated data sets, under standard conditions (limit of detection (LOD) = 1. cfu/25. g; limit of quantification (LOQ) = 10. cfu/g) and used a maximum likelihood estimation method (directly for the model MCLD and via the Expectation-Maximisation (EM) algorithm for the model MRD. The comparison suggests that in most cases estimates provided by the proposed model MRD are similar to those obtained by model MCLD accounting for censorship. Nevertheless, in some cases, the proposed model MRD leads to less biased and more precise estimates than model MCLD. ?? 2012 Elsevier B.V.},
  pmid = {22353674},
  keywords = {EM algorithm,Limit of detection,Limit of quantification,Maximum likelihood estimation,Microbial contamination assessment,nosource}
}

@article{comte2011multiplicative,
  title = {Multiplicative {{Kalman}} Filtering},
  author = {Comte, Fabienne and {Genon-Catalot}, Valentine and Kessler, Mathieu},
  year = {2011},
  journal = {test},
  volume = {20},
  number = {2},
  pages = {389--411},
  publisher = {Springer},
  file = {/home/gkonkamking/Zotero/storage/MP7EWITG/Comte et al. - 2011 - Multiplicative kalman filtering.pdf}
}

@article{concepts1989superanova,
  title = {{{SuperANOVA}}},
  author = {Concepts, Abacus},
  year = {1989},
  journal = {Abacus Concepts, Inc., Berkeley, CA},
  volume = {322},
  keywords = {nosource}
}

@article{coolsAdvancesMultidimensionalIntegration2002,
  title = {Advances in Multidimensional Integration},
  author = {Cools, Ronald},
  year = {2002},
  month = dec,
  journal = {Journal of Computational and Applied Mathematics},
  series = {Scientific and {{Engineering Computations}} for the 21st {{Century}} - {{Me}} Thodologies and {{Applications Proceedings}} of the 15th {{Toyota Conference}}},
  volume = {149},
  number = {1},
  pages = {1--12},
  issn = {0377-0427},
  doi = {10.1016/S0377-0427(02)00517-4},
  urldate = {2023-05-14},
  abstract = {This paper gives a personal bird's eye view of some aspects of multivariate numerical integration. It will sketch what happened during the past 50 years and point the reader's attention to what did not happen.},
  langid = {english},
  keywords = {Cubature,Numerical integration,Quadrature},
  file = {/home/gkonkamking/pCloudDrive/papers/Cools_2002_Advances in multidimensional integration.pdf}
}

@article{corbet2018exploring,
  title = {Exploring the Dynamic Relationships between Cryptocurrencies and Other Financial Assets},
  author = {Corbet, Shaen and Meegan, Andrew and Larkin, Charles and Lucey, Brian and Yarovaya, Larisa},
  year = {2018},
  journal = {Economics Letters},
  volume = {165},
  pages = {28--34},
  publisher = {Elsevier},
  keywords = {nosource}
}

@misc{corenflosAuxiliaryMCMCParticle2023,
  title = {Auxiliary {{MCMC}} and Particle {{Gibbs}} Samplers for Parallelisable Inference in Latent Dynamical Systems},
  author = {Corenflos, Adrien and S{\"a}rkk{\"a}, Simo},
  year = {2023},
  month = mar,
  number = {arXiv:2303.00301},
  eprint = {2303.00301},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.00301},
  urldate = {2023-11-08},
  abstract = {We introduce two new classes of exact Markov chain Monte Carlo (MCMC) samplers for inference in latent dynamical models. The first one, which we coin auxiliary Kalman samplers, relies on finding a linear Gaussian state-space model approximation around the running trajectory corresponding to the state of the Markov chain. The second, that we name auxiliary particle Gibbs samplers corresponds to deriving good local proposals in an auxiliary Feynman--Kac model for use in particle Gibbs. Both samplers are controlled by augmenting the target distribution with auxiliary observations, resulting in an efficient Gibbs sampling routine. We discuss the relative statistical and computational performance of the samplers introduced, and show how to parallelise the auxiliary samplers along the time dimension. We illustrate the respective benefits and drawbacks of the resulting algorithms on classical examples from the particle filtering literature.},
  archiveprefix = {arXiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Statistics - Computation,Statistics - Machine Learning},
  file = {/home/gkonkamking/pCloudDrive/papers/Corenflos_Särkkä_2023_Auxiliary MCMC and particle Gibbs samplers for parallelisable inference in.pdf}
}

@techreport{cormier2011field,
  title = {A Field-Based Aquatic Life Benchmark for Conductivity in Central Appalachian Streams},
  booktitle = {Office},
  author = {Agency, U S Environmental Protection},
  year = {2010},
  number = {March},
  pages = {pp. 193},
  institution = {EPA/600/R-10},
  abstract = {This report uses field data to derive an aquatic life benchmark for conductivity that may be applied to waters in the Appalachian Region that are dominated by salts of SO4 2- and HCO3 - at circum-neutral to mildly alkaline pH. This benchmark is intended to protect the aquatic life in the region. It is derived by a method modeled on the U.S. EPA's standard methodology for deriving water quality criteria. In particular, the methodology was adapted for use of field data. Field data were used because sufficient and appropriate laboratory data were not available and because high quality field data were available to relate conductivity to effects on aquatic life. This report provides scientific evidence for a conductivity benchmark in a specific region rather than for the entire United States. The method used in this report is based on the standard methodology in that it used the 5th percentile of a species sensitivity distribution (SSD) as the benchmark value. SSDs represent the response of aquatic life as a distribution with respect to exposure. It is implicitly assumed that if the exposure level is kept below the 5th percentile of the SSD, at least 95\% of species will be protected. Data analysis followed the standard methodology in aggregating species to genera and using interpolation to estimate the percentile. It differs primarily in that the points in the SSDs are extirpation concentrations (XCs) rather than median lethal concentrations (LC50s) or chronic values. The XC is the level of exposure above which a genus is effectively absent from water bodies in a region. For this benchmark value, the 95th percentile of the distribution of the probability of occurrence of a genus with respect to conductivity was used as a 95th percentile extirpation concentration. Hence, this aquatic life benchmark for conductivity is expected to avoid the local extirpation of 95\% of native species (based on the 5th percentile of the SSD) due to neutral to alkaline effluents containing a mixture of dissolved ions dominated by salts of SO4 2- and HCO3 - . Because it is not protective of all genera and protects against extirpation rather than reduction in abundance, this level is not fully protective of rare species or waters designated by state and federal agencies as exceptional. This field-based method has several advantages. Because it is based on biological surveys, it is inherently relevant to the streams where the benchmark may be applied and represents the actual aquatic life use in these streams. Another advantage is that the method assesses all life stages and ecological interactions of many species. Further, it represents the actual exposure conditions for elevated conductivity in the region, the actual temporal variation in exposure, and the actual mixture of ions that contribute to salinity as measured by conductivity. The disadvantages of field data result from the fact that exposures are not controlled. As a result, the causal nature of the relationship between conductivity and the associated biological impairments must be assessed. Also, any variables that are correlated with conductivity or the biotic response may confound the relationship of biota to conductivity. Assessments of causation and confounding were performed and are presented in the appendices. They demonstrate that conductivity is a cause of impairment and the relationship between conductivity and biological responses apparently is not significantly confounded. The chronic aquatic life benchmark value for conductivity derived from all-year data It is applicable to parts of West Virginia and Kentucky. It is from West Virginia is 300 {$\mu$}S/cm. expected to be applicable to the same regions in Ohio, Pennsylvania, Tennessee, and Maryland, but data from those states have not been analyzed. It may also be appropriate for other nearby regions such as Ecoregions 67 but has only been validated for use in Ecoregions 68, 69, and 70 at this time. However, this level may not apply when the relative concentrations of dissolved ions are not dominated by salts of SO4 -2 and HCO3 -.},
  isbn = {EPA/600/R-10/023A},
  keywords = {nosource}
}

@article{Cormier2013,
  title = {A Method for Deriving Water-Quality Benchmarks Using Field Data.},
  author = {Cormier, Susan M and Suter, Glenn W},
  year = {2013},
  month = feb,
  journal = {Environmental toxicology and chemistry / SETAC},
  volume = {32},
  number = {2},
  eprint = {23147651},
  eprinttype = {pubmed},
  pages = {255--62},
  issn = {1552-8618},
  doi = {10.1002/etc.2057},
  abstract = {The authors describe a methodology that characterizes effects to individual genera observed in the field and estimate the concentration at which 5\% of genera are adversely affected. Ionic strength, measured as specific conductance, is used to illustrate the methodology. Assuming some resilience in the population, 95\% of the genera are afforded protection. The authors selected an unambiguous effect, the presence or absence of a genus from sampling locations. The absence of a genus, extirpation, is operationally defined as the point above which only 5\% of the observations of a genus occurs. The concentrations that cause extirpation of each genus are rank-ordered from least to greatest, and the benchmark is estimated at the 5th percentile of the distribution using two-point interpolation. When a full range of exposures and many taxa are included in the model of taxonomic sensitivity, the model broadly characterizes how species in general respond to a concentration gradient of the causal agent. This recognized U.S. Environmental Protection Agency methodology has many advantages. Observations from field studies include the full range of conditions, effects, species, and interactions that occur in the environment and can be used to model some causal relationships that laboratory studies cannot.},
  isbn = {1552-8618},
  pmid = {23147651},
  keywords = {Benthic invertebrate,duplicate-citation-key,Extirpation,Ionic strength,nosource,Species sensitivity distribution,water-quality criteria,Water-quality criteria}
}

@article{Cornu2011,
  title = {Modeling Microbial Competition in Food: {{Application}} to the Behavior of {{Listeria}} Monocytogenes and Lactic Acid Flora in Pork Meat Products},
  author = {Cornu, Marie and Billoir, Elise and Bergis, H. and Beaufort, A. and Zuliani, V.},
  year = {2011},
  month = jun,
  journal = {Food Microbiology},
  volume = {28},
  number = {4},
  eprint = {21511123},
  eprinttype = {pubmed},
  pages = {639--647},
  publisher = {Elsevier Ltd},
  issn = {07400020},
  doi = {10.1016/j.fm.2010.08.007},
  abstract = {Competition between background microflora and microbial pathogens raises questions about the application of predictive microbiology in situ, i.e., in non-sterile naturally contaminated foods. In this article, we present a review of the models developed in predictive microbiology to describe interactions between microflora in foods, with a special focus on two approaches: one based on the Jameson effect (simultaneous deceleration of all microbial populations) and one based on the Lotka-Volterra competition model. As an illustration of the potential of these models, we propose various modeling examples in estimation and in prediction of microbial growth curves, all related to the behavior of Listeria monocytogenes with lactic acid bacteria in three pork meat products (fresh pork meat and two types of diced bacon). ?? 2010 Elsevier Ltd.},
  isbn = {084931237X},
  pmid = {21511123},
  keywords = {Challenge testing,Diced bacon,Maximal population density,nosource}
}

@article{corradinBNPmixPackageBayesian,
  title = {{{BNPmix}}: An {{R}} Package for {{Bayesian}} Nonparametric Modelling via {{Pitman-Yor}} Mixtures},
  author = {Corradin, Riccardo and Canale, Antonio and Nipoti, Bernardo},
  journal = {Journal of Statistical Software},
  number = {to appear},
  keywords = {nosource}
}

@article{correll20081,
  title = {1/f Noise and Effort on Implicit Measures of Bias.},
  author = {Correll, Joshua},
  year = {2008},
  journal = {Journal of personality and social psychology},
  volume = {94},
  number = {1},
  pages = {48},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{costantini2017,
  title = {Wright--{{Fisher}} Construction of the Two-Parameter {{Poisson}}--{{Dirichlet}} Diffusion},
  author = {Costantini, Cristina and De Blasi, Pierpaolo and Ethier, Stewart N and Ruggiero, Matteo and Span{\`o}, Dario},
  year = {2017},
  journal = {Ann. Appl. Probab.},
  volume = {27},
  number = {3},
  pages = {1923--1950},
  publisher = {The Institute of Mathematical Statistics},
  doi = {10.1214/16-AAP1252},
  file = {/home/gkonkamking/Zotero/storage/8Q7BK6CC/Costantini et al. - 2017 - Wright–Fisher construction of the two-parameter Po.pdf}
}

@article{costeaMetaSNVToolMetagenomic2017,
  title = {{{metaSNV}}: {{A}} Tool for Metagenomic Strain Level Analysis},
  shorttitle = {{{metaSNV}}},
  author = {Costea, Paul Igor and Munch, Robin and Coelho, Luis Pedro and Paoli, Lucas and Sunagawa, Shinichi and Bork, Peer},
  year = {2017},
  month = jul,
  journal = {PLOS ONE},
  volume = {12},
  number = {7},
  pages = {e0182392},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0182392},
  urldate = {2021-05-02},
  abstract = {We present metaSNV, a tool for single nucleotide variant (SNV) analysis in metagenomic samples, capable of comparing populations of thousands of bacterial and archaeal species. The tool uses as input nucleotide sequence alignments to reference genomes in standard SAM/BAM format, performs SNV calling for individual samples and across the whole data set, and generates various statistics for individual species including allele frequencies and nucleotide diversity per sample as well as distances and fixation indices across samples. Using published data from 676 metagenomic samples of different sites in the oral cavity, we show that the results of metaSNV are comparable to those of MIDAS, an alternative implementation for metagenomic SNV analysis, while data processing is faster and has a smaller storage footprint. Moreover, we implement a set of distance measures that allow the comparison of genomic variation across metagenomic samples and delineate sample-specific variants to enable the tracking of specific strain populations over time. The implementation of metaSNV is available at: http://metasnv.embl.de/.},
  langid = {english},
  keywords = {Bacterial genomics,Caries,Genomics,Metagenomics,nosource,Nucleotide sequencing,Nucleotides,Species diversity,Specimen storage}
}

@article{Coulaud2011,
  title = {In Situ Feeding Assay with {{Gammarus}} Fossarum ({{Crustacea}}): {{Modelling}} the Influence of Confounding Factors to Improve Water Quality Biomonitoring},
  author = {Coulaud, R and Geffard, Olivier and Xuereb, B and Lacaze, E and Qu{\'e}au, H and Garric, Jeanne and Charles, Sandrine and Chaumot, Arnaud},
  year = {2011},
  month = jan,
  journal = {Water Research},
  volume = {45},
  pages = {6417--6429},
  publisher = {Elsevier Ltd},
  issn = {0043-1354},
  abstract = {... modelling the influence of confounding factors to improve water quality biomonitoring Authors: R. Coulaud, O. Geffard, B. Xuereb, E. Lacaze, H. Qu{\~A}{\copyright}au, J. Garric, S. Charles, A. Chaumot PII: S0043-1354(11)00565-3 DOI: 10.1016 / j . watres . 2011.09 . 035 Reference: WR 8822 To ...},
  keywords = {Biomonitoring,Feeding rate,Gammarus,In situ assay,Modelling,nosource,Temperature}
}

@article{coutris2011can,
  title = {Can We Predict Community-Wide Effects of Herbicides from Toxicity Tests on Macrophyte Species?},
  author = {Coutris, Claire and Merlina, Georges and Silvestre, J{\'e}r{\^o}me and Pinelli, Eric and Elger, Arnaud},
  year = {2011},
  journal = {Aquatic Toxicology},
  volume = {101},
  number = {1},
  pages = {49--56},
  publisher = {Elsevier},
  issn = {0166445X},
  doi = {10.1016/j.aquatox.2010.08.017},
  abstract = {Macrophyte communities play an essential role in the way freshwater ecosystems function. It is thus of great concern to understand how environmental factors, especially anthropogenic ones, influence their composition and diversity. The aim of this study was to examine whether the effects of a herbicide mixture (50\% atrazine, 35\% isoproturon, 15\% alachlor) on single macrophyte species can be used to predict its impact at a community level. In a first experiment we tested the sensitivity of six species (Azolla filiculoides, Ceratophyllum demersum, Elodea canadensis, Lemna minor, Myriophyllum spicatum and Vallisneria spiralis) grown separately and exposed to 0.6-600{$\mu$}gL-1 of the herbicide mixture. In a second experiment, conducted in microcosms, we tested the effects of herbicides on macrophyte assemblages composed of the same six species exposed to 0, 6 or 60{$\mu$}gL-1 of the herbicide mixture. Species grown separately exhibited growth inhibition at 60 and 600{$\mu$}gL-1. At 600{$\mu$}gL-1 the sensitivity differed significantly between species. V. spiralis was the most resistant species, C. demersum, M. spicatum and E. canadensis exhibited intermediate sensitivities, and A. filiculoides and L. minor were the most sensitive species. In microcosms, community biomass and Shannon evenness index were reduced after 8 weeks at 60{$\mu$}gL-1. Communities also exhibited changes in their composition: the relative and absolute abundance of C. demersum increased at 6{$\mu$}gL-1, while the relative abundance of V. spiralis increased at 60{$\mu$}gL-1. These results are in agreement with the individual responses of these species to the herbicides. It is therefore concluded that short-term effects of herbicides on simple macrophyte communities can be predicted from the sensitivity of individual species. However, further investigations are required to examine whether longer term effects can be predicted as well, especially in more complex communities. {\copyright} 2010 Elsevier B.V.},
  isbn = {0166-445X},
  pmid = {20926143},
  keywords = {Chemical stress,Community ecotoxicology,nosource,Pesticide mixture,Water plant assemblages}
}

@article{covaEstimatingReproducibilityExperimental2018,
  title = {Estimating the {{Reproducibility}} of {{Experimental Philosophy}}},
  author = {Cova, Florian and Strickland, Brent and Abatista, Angela and Allard, Aur{\'e}lien and Andow, James and Attie, Mario and Beebe, James and Berni{\=u}nas, Renatas and Boudesseul, Jordane and Colombo, Matteo and Cushman, Fiery and Diaz, Rodrigo and {N'Djaye Nikolai van Dongen}, Noah and Dranseika, Vilius and Earp, Brian D. and Torres, Antonio Gait{\'a}n and Hannikainen, Ivar and {Hern{\'a}ndez-Conde}, Jos{\'e} V. and Hu, Wenjia and Jaquet, Fran{\c c}ois and Khalifa, Kareem and Kim, Hanna and Kneer, Markus and Knobe, Joshua and Kurthy, Miklos and Lantian, Anthony and Liao, Shen-yi and Machery, Edouard and Moerenhout, Tania and Mott, Christian and Phelan, Mark and Phillips, Jonathan and Rambharose, Navin and Reuter, Kevin and Romero, Felipe and Sousa, Paulo and Sprenger, Jan and Thalabard, Emile and Tobia, Kevin and Viciana, Hugo and Wilkenfeld, Daniel and Zhou, Xiang},
  year = {2018},
  month = jun,
  journal = {Review of Philosophy and Psychology},
  issn = {1878-5166},
  doi = {10.1007/s13164-018-0400-9},
  urldate = {2020-04-14},
  abstract = {Responding to recent concerns about the reliability of the published literature in psychology and other disciplines, we formed the X-Phi Replicability Project (XRP) to estimate the reproducibility of experimental philosophy (osf.io/dvkpr). Drawing on a representative sample of 40 x-phi studies published between 2003 and 2015, we enlisted 20 research teams across 8 countries to conduct a high-quality replication of each study in order to compare the results to the original published findings. We found that x-phi studies -- as represented in our sample -- successfully replicated about 70\% of the time. We discuss possible reasons for this relatively high replication rate in the field of experimental philosophy and offer suggestions for best research practices going forward.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/6CUQKD4E/Cova et al. - 2018 - Estimating the Reproducibility of Experimental Phi.pdf}
}

@misc{COVID19CommunityMobility,
  title = {{{COVID-19 Community Mobility Report}}},
  journal = {COVID-19 Community Mobility Report},
  urldate = {2023-10-25},
  abstract = {See how your community moved differently due to COVID-19},
  howpublished = {https://www.google.com/covid19/mobility?hl=en}
}

@article{cowartMetagenomicSequencingEnvironmental2018,
  title = {Metagenomic Sequencing of Environmental {{DNA}} Reveals Marine Faunal Assemblages from the {{West Antarctic Peninsula}}},
  author = {Cowart, Dominique A. and Murphy, Katherine R. and Cheng, C.-H. Christina},
  year = {2018},
  month = feb,
  journal = {Marine Genomics},
  volume = {37},
  pages = {148--160},
  publisher = {Elsevier},
  issn = {1874-7787},
  doi = {10.1016/j.margen.2017.11.003},
  urldate = {2021-06-03},
  abstract = {The West Antarctic Peninsula (WAP) is the fastest warming region in Antarctica where climate impact on the cold-adapted marine ecosystem is already vi{\dots}},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/HYWEXPQW/2018 - Metagenomic sequencing of environmental DNA reveal.pdf}
}

@incollection{CPW14,
  title = {Compositional Security Modelling},
  booktitle = {Human Aspects of Information Security, Privacy, and Trust},
  author = {Caulfield, Tristan and Pym, David and Williams, Julian},
  editor = {Tryfonas, Theo and Askoxylakis, Ioannis},
  year = {2014},
  series = {Lecture Notes in Computer Science},
  volume = {8533},
  pages = {233--245},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-319-07620-1_21},
  abstract = {Security managers face the challenge of formulating and implementing policies that deliver their desired system security postures --- for example, their preferred balance of confidentiality, integrity, and availability --- within budget (monetary and otherwise). In this paper, we describe a security modelling methodology, grounded in rigorous mathematical systems modelling and economics, that captures the managers' policies and the behavioural choices of agents operating within the system. Models are executable, so allowing systematic experimental exploration of the system-policy co-design space, and compositional, so managing the complexity of large-scale systems.},
  isbn = {978-3-319-07619-5},
  keywords = {nosource}
}

@article{Craig2008,
  title = {Consequences of Increasing the Number of Species Tested},
  author = {Craig, Peter S.},
  year = {2008},
  keywords = {nosource}
}

@article{craig2012species,
  title = {Species Non-Exchangeability in Probabilistic Ecotoxicological Risk Assessment},
  author = {Craig, Peter S. and Hickey, Graeme L. and Luttik, Robert and Hart, Andy},
  year = {2012},
  journal = {Journal of the Royal Statistical Society. Series A: Statistics in Society},
  volume = {175},
  number = {1},
  eprint = {0811.2183v2},
  pages = {243--262},
  publisher = {Wiley Online Library},
  issn = {09641998},
  doi = {10.1111/j.1467-985X.2011.00716.x},
  abstract = {The design, measurement, and analysis of a range of artificial materials{\textbackslash}nfor use at terahertz frequencies are described. The chosen structures{\textbackslash}nconsist of arrays of cylindrical gold-plated pillars with period{\textbackslash}ncomparable to the wavelength of incident radiation. An ultraviolet (UV){\textbackslash}nmicromachining approach to the fabrication of these high aspect-ratio{\textbackslash}npillars is described using the negative epoxy-based resin SU8. Lattice{\textbackslash}nfence structures are also realized using the same method. Terahertz{\textbackslash}n(THz) frequency time domain spectroscopy is performed on these{\textbackslash}nstructures in the range 200 GHz to 3.0 THz and the relative transmission{\textbackslash}nof the structures is determined. The pass and stop bands are observed{\textbackslash}nwith peak transmission of up to 97\%. Finite difference time domain{\textbackslash}nsimulations and complex photonic band structure calculations are shown{\textbackslash}nto provide good descriptions of the electromagnetic properties of the{\textbackslash}nstructures and are used to interpret the observed transmission spectra.{\textbackslash}n(c) American Institute of Physics.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:0811.2183v2},
  isbn = {9783642121425},
  keywords = {Assessment factors,Ecotoxicology,Exchangeability,nosource,Risk assessment,Species sensitivity},
  file = {/home/gkonkamking/Zotero/storage/BGVB3J6N/Craig et al. - 2012 - Species non-exchangeability in probabilistic ecotoxicological risk assessment.pdf}
}

@book{cramer1927sannolikhetskalkylen,
  title = {Sannolikhetskalkylen Och N\{{\aa}\}gra Av Dess Anv\{{\"a}\}ndningar},
  author = {Cram{\'e}r, Harald},
  year = {1927},
  publisher = {Gjallarhornet},
  keywords = {nosource}
}

@article{crampin1999reaction,
  title = {Reaction and Diffusion on Growing Domains: Scenarios for Robust Pattern Formation},
  author = {Crampin, Edmund J and Gaffney, Eamonn A and Maini, Philip K},
  year = {1999},
  journal = {Bulletin of mathematical biology},
  volume = {61},
  number = {6},
  pages = {1093--1120},
  publisher = {Springer},
  keywords = {nosource}
}

@article{crane2017hidden,
  title = {A Hidden {{Markov}} Model for Latent Temporal Clustering with Application to Ideological Alignment in the {{US Supreme Court}}},
  author = {Crane, Harry},
  year = {2017},
  journal = {Computational Statistics \& Data Analysis},
  volume = {110},
  pages = {19--36},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{cresswell2007introduction,
  title = {Introduction to Gendered Mobilities},
  author = {Cresswell, T and Uteng Priya, T},
  year = {2007},
  journal = {Gendered Mobilities},
  publisher = {Aldershot Ashgate},
  keywords = {⛔ No DOI found,nosource}
}

@article{critcherIncidentalEnvironmentalAnchors2008,
  title = {Incidental Environmental Anchors},
  author = {Critcher, Clayton R. and Gilovich, Thomas},
  year = {2008},
  journal = {Journal of Behavioral Decision Making},
  volume = {21},
  number = {3},
  pages = {241--251},
  issn = {1099-0771},
  doi = {10.1002/bdm.586},
  urldate = {2020-06-09},
  abstract = {Three studies examined whether potential anchor values that are incidentally present in the environment can affect a person's numerical estimates. In Study 1, estimates of an athlete's performance were influenced by the number on his jersey. In Study 2, estimates of the proportion of sales in the domestic market were influenced by a product's model number. In Study 3, participants' estimates of how much they would spend at a restaurant were influenced by whether the restaurant was named ``Studio 17'' or ``Studio 97.'' These effects were not qualified by participants' expertise in the relevant domain (Study 1) or by their ability to subsequently recall the anchor value (Study 3). These findings document the existence of a new form of ``basic anchoring'' and suggest that not all basic anchoring effects are as fragile as the existing anchoring literature suggests. Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {accessibility,basic anchoring,incidental anchoring,nosource,numeric priming}
}

@article{crow1970introduction,
  title = {An Introduction to Population Genetics Theory},
  author = {Crow, James F. and Kimura, Motoo},
  year = {1970},
  journal = {Cours de l'University of Oslo Department of Informatics},
  pages = {591},
  publisher = {{New York, Evanston and London: Harper \& Row, Publishers}},
  issn = {00324663},
  doi = {10.2307/1529706},
  abstract = {Many of the ideas current in discussions of problems of evolution and of natural and artificial selection stem from Sewall Wright. Until recently, with the appearance of the first two volumes of Wright's own trilogy see A.B.A., 37, No. 2116 and 39, No. 1331, anyone working with these topics in detail needed to go to the original papers. The gap is further narrowed by this book by two well-known authors. It is aimed primarily at graduate students and the greater part does not require a background of advanced mathematics. The final two chapters, in which gene frequency distributions are introduced, are at a much higher level, and students may find them very heavy weather indeed. Nor are they properly integrated with the rest of the book. For instance, on page 383 the authors discuss, from the stand-point of differential equations, the problem of random drift, and derive expressions very similar to those found by quite different methods on page 337. But no cross reference is made. Nor indeed is the reader given any clue as to why a partial differential equation should possess eigen values and eigen vectors. This could well have been done by the inclusion of a section in which the gene frequency distribution for a population of size 2N is treated as discrete in being able to take 2N+ 1 possible values. Change from generation to generation is then specified by the transition probability matrix. After all, Feller dealt with drift without selection by this method in 1951, and many other workers have used it since. In the past term, I have, with some colleagues, taken a group of graduate students through the book. In doing so, I became aware that in several places the ends were not as neatly tied as I had hoped-very often because the initial assumptions of the treatment were not clearly stated. This is perhaps inevitable when topics are dealt with didactically for the first time, and occurs in discussions of the sub-division of populations, of assortative mating and of effective population size in bisexual populations. I have perhaps tended to take for granted that, by two such authors, the book was bound to be excellent, and have concentrated rather on its deficiencies. May I then say firmly that my original assumption was justified? It is about fifty years since Fisher, Haldane and Wright first began to develop a theory of natural and artificial selection. In view of this, it is perhaps surprising that, arising from evidence from completely new sources such as starch-gel electrophoresis or the sequence of amino acids in peptide chains of various species, arguments should still be so animated about such fundamental problems as the possibility that many of the substitutions which have taken place in evolution may have been non-selective, or as to the reason why so much genetic variation seems to be maintained within random-breeding populations. I suggest that this is because previous theory, at least in its more precise forms, has been too analytical has concentrated too much on the evolution of the parts while forgetting that an organism functions as a whole. Se we have at the moment only the rudiments of a theory of the evolution of the genotype as a unit, or of the phenotype as a unit, bearing in mind now that any particular measurement may be affected by a great many genes. And any satisfactory theory would of course have to deal with the environment as a whole. Perhaps a tall order! Alan Robertson.},
  isbn = {9780808729013},
  pmid = {14100248},
  keywords = {nosource}
}

@article{dadanehBNPSeqBayesianNonparametric2018,
  title = {{{BNP-Seq}}: {{Bayesian Nonparametric Differential Expression Analysis}} of {{Sequencing Count Data}}},
  shorttitle = {{{BNP-Seq}}},
  author = {Dadaneh, Siamak Zamani and Qian, Xiaoning and Zhou, Mingyuan},
  year = {2018},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {521},
  pages = {81--94},
  issn = {0162-1459},
  doi = {10.1080/01621459.2017.1328358},
  urldate = {2020-02-10},
  abstract = {We perform differential expression analysis of high-throughput sequencing count data under a Bayesian nonparametric framework, removing sophisticated ad hoc pre-processing steps commonly required in existing algorithms. We propose to use the gamma (beta) negative binomial process, which takes into account different sequencing depths using sample-specific negative binomial probability (dispersion) parameters, to detect differentially expressed genes by comparing the posterior distributions of gene-specific negative binomial dispersion (probability) parameters. These model parameters are inferred by borrowing statistical strength across both the genes and samples. Extensive experiments on both simulated and real-world RNA sequencing count data show that the proposed differential expression analysis algorithms clearly outperform previously proposed ones in terms of the areas under both the receiver operating characteristic and precision-recall curves. Supplementary materials for this article are available online.},
  file = {/home/gkonkamking/Zotero/storage/ZXP6ILPW/Dadaneh et al. - 2018 - BNP-Seq Bayesian Nonparametric Differential Expre.pdf}
}

@article{dahl2006model,
  title = {Model-Based Clustering for Expression Data via a {{Dirichlet}} Process Mixture Model},
  author = {Dahl, David B},
  year = {2006},
  journal = {Bayesian inference for gene expression and proteomics},
  pages = {201--218},
  publisher = {Citeseer},
  keywords = {nosource}
}

@article{Dahl2007,
  title = {Multiple Hypothesis Testing by Clustering Treatment Effects},
  author = {Dahl, David B and a Newton, Michael},
  year = {2007},
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {478},
  pages = {517--526},
  issn = {0162-1459},
  doi = {10.1198/016214507000000211},
  abstract = {Multiple hypothesis testing and clustering have been the subject of extensive research in high-dimensional inference, yet these problems usually have been treated separately. By defining true clusters in terms of shared parameter values, we could improve the sensitivity of individual tests, because more data bearing on the same parameter values are available. We develop and evaluate a hybrid methodology that uses clustering information to increase testing sensitivity and accommodates uncertainty in the true clustering. To investigate the potential efficacy of the hybrid approach, we first study a stylized example in which each object is evaluated with a standard z score but different objects are connected by shared parameter values. We show that there is increased testing power when the clustering is estimated sufficiently well. We next develop a model-based analysis using a conjugate Dirichlet process mixture model. The method is general, but for specificity we focus attention on microarray gene expression data, to which both clustering and multiple testing methods are actively applied. Clusters provide the means for sharing information among genes, and the hybrid methodology averages over uncertainty in these clusters through Markov chain sampling. Simulations show that the hybrid method performs substantially better than other methods when clustering is heavy or moderate and performs well even under weak clustering. The proposed method is illustrated on microarray data from a study of the effects of aging on gene expression in heart tissue.},
  isbn = {0162145070000},
  keywords = {bayesian nonparametrics,conjugate dirichlet process mixture,correlated hypothesis test,dna microarray,expression,gene,model,model-based clustering,nosource}
}

@article{daiValueHeuristicJudgments2008,
  title = {The {{Value Heuristic}} in {{Judgments}} of {{Relative Frequency}}:},
  shorttitle = {The {{Value Heuristic}} in {{Judgments}} of {{Relative Frequency}}},
  author = {Dai, Xianchi and Wertenbroch, Klaus and Brendl, C. Miguel},
  year = {2008},
  month = jan,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {1467-9280},
  urldate = {2020-07-08},
  langid = {english},
  keywords = {nosource}
}

@article{daley2008introduction,
  title = {An Introduction to the Theory of Point Processes. {{Vol}}. {{II}}. {{General}} Theory and Structure. {{Probability}} and Its {{Applications}}},
  author = {Daley, Daryl J and {Vere-Jones}, David},
  year = {2008},
  publisher = {New York). Springer, New York},
  keywords = {duplicate-citation-key,nosource}
}

@article{daley2008introduction,
  title = {An Introduction to the Theory of Point Processes - Probability and Its Applications},
  author = {Daley, D. J. and {Vere-Jones}, D.},
  year = {2008},
  pages = {457--536},
  publisher = {New York). Springer, New York},
  doi = {10.1007/978-0-387-49835-5_7},
  abstract = {This last chapter provides an introduction to spatial point processes, meaning for the most part results for point processes in R 2 and R 3 where the order properties of the real line, which governed the development in the preceding chapter, are no longer available. The material we present falls into two main components. In the first four sections we review mainly descriptive properties, distinguishing between distance and directional properties of spatial point patterns, starting from finite models, moving on to the moment properties of line processes, and then revisiting space-time models, where time reappears so that many of the modelling concepts in Chapter 14 are again available, but spatial patterns also play an important role. The three final sections of the chapter provide an introduction to modelling centred around the concept of the Papangelou intensity; we provide some background and motivation from the statistical and physical settings, then attempt an introduction to the more mathematical theory.},
  isbn = {978-0-387-49835-5},
  keywords = {duplicate-citation-key,Mathematics and Statistics,nosource}
}

@article{dalgarnoShinyssdtoolsWebApplication2021,
  title = {Shinyssdtools: {{A}} Web Application for Fitting {{Species Sensitivity Distributions}} ({{SSDs}})},
  shorttitle = {Shinyssdtools},
  author = {Dalgarno, Seb},
  year = {2021},
  month = jan,
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {57},
  pages = {2848},
  issn = {2475-9066},
  doi = {10.21105/joss.02848},
  urldate = {2021-03-16},
  abstract = {The species sensitivity distribution (SSD) is the most widely used method for getting water quality benchmarks to characterize effects of chemical contaminants for water quality or ecological risk assessment (Fox et al., 2020). This typically involves estimating the concentration of a chemical that affects 5\% of the species considered (Posthuma et al., 2001). The ssdtools R package (Thorley \& Schwarz, 2018) has recently advanced SSD methods by providing model averaging using information-theoretic criteria and the construction of confidence intervals using bootstrapping (Fox et al., 2020).},
  langid = {english}
}

@article{damien,
  title = {Gibbs Sampling for {{Bayesian}} Non-Conjugate and Hierarchical Models by Using Auxiliary Variables},
  author = {Damien, Paul and Wakefield, Jon and Walker, Stephen},
  year = {1999},
  journal = {Journal of the Royal Statistical Society. Series B, Statistical Methodology},
  pages = {331--344},
  publisher = {JSTOR},
  keywords = {nosource}
}

@article{dangeloBayesianNonparametricAnalysis2023,
  title = {Bayesian Nonparametric Analysis for the Detection of Spikes in Noisy Calcium Imaging Data},
  author = {D'Angelo, Laura and Canale, Antonio and Yu, Zhaoxia and Guindani, Michele},
  year = {2023},
  journal = {Biometrics},
  volume = {79},
  number = {2},
  pages = {1370--1382},
  issn = {1541-0420},
  doi = {10.1111/biom.13626},
  urldate = {2024-03-04},
  abstract = {Recent advancements in miniaturized fluorescence microscopy have made it possible to investigate neuronal responses to external stimuli in awake behaving animals through the analysis of intracellular calcium signals. An ongoing challenge is deconvolving the temporal signals to extract the spike trains from the noisy calcium signals' time series. In this article, we propose a nested Bayesian finite mixture specification that allows the estimation of spiking activity and, simultaneously, reconstructing the distributions of the calcium transient spikes' amplitudes under different experimental conditions. The proposed model leverages two nested layers of random discrete mixture priors to borrow information between experiments and discover similarities in the distributional patterns of neuronal responses to different stimuli. Furthermore, the spikes' intensity values are also clustered within and between experimental conditions to determine the existence of common (recurring) response amplitudes. Simulation studies and the analysis of a dataset from the Allen Brain Observatory show the effectiveness of the method in clustering and detecting neuronal activities.},
  copyright = {{\copyright} 2022 The Authors. Biometrics published by Wiley Periodicals LLC on behalf of International Biometric Society.},
  langid = {english},
  keywords = {Dirichlet process,mixture of finite mixtures,model-based clustering,nested Dirichlet process,spike and slab},
  file = {/home/gkonkamking/pCloudDrive/papers/D'Angelo et al_2023_Bayesian nonparametric analysis for the detection of spikes in noisy calcium.pdf}
}

@article{daniels1999nonconjugate,
  title = {Nonconjugate Bayesian Estimation of Covariance Matrices and Its Use in Hierarchical Models},
  author = {Daniels, Michael J and Kass, Robert E},
  year = {1999},
  journal = {Journal of the American Statistical Association},
  volume = {94},
  number = {448},
  eprint = {2669939},
  eprinttype = {jstor},
  pages = {1254--1263},
  publisher = {Taylor \& Francis Group},
  issn = {01621459},
  doi = {10.2307/2669939},
  abstract = {The problem of estimating a covariance matrix in small samples has been considered by several authors following early work by Stein. This problem can be especially important in hierarchical models where the standard errors of fixed and random effects depend on estimation of the covariance matrix of the distribution of the random effects. We propose a set of hierarchical priors (HPs) for the covariance matrix that produce posterior shrinkage toward a specified structure-here we examine shrinkage toward diagonality. We then address the computational difficulties raised by incorporating these priors, and nonconjugate priors in general, into hierarchical models. We apply a combination of approximation, Gibbs sampling (possibly with a Metropolis step), and importance reweighting to fit the models, and compare this hybrid approach to alternative Markov Chain Monte Carlo methods. Our investigation involves three alternative HPs. The first works with the spectral decomposition of the covariance matrix and produces both shrinkage of the eigenvalues toward each other and shrinkage of the rotation matrix toward the identity. The second produces shrinkage of the correlations toward 0, and the third uses a conjugate Wishart distribution to shrink toward diagonality. A simulation study shows that the first two HPs can be very effective in reducing small-sample risk, whereas the conjugate Wishart version sometimes performs very poorly. We evaluate the computational algorithm in the context of a normal nonlinear random-effects model and illustrate the methodology with a logistic random-effects model. CR - Copyright \&\#169; 1999 American Statistical Association},
  isbn = {01621459},
  keywords = {givens angles,hierarchical prior,nosource,random effects,shrinkage,variance matrix}
}

@article{dassiosExactSimulationPoissonDirichlet2023,
  title = {Exact {{Simulation}} of {{Poisson-Dirichlet Distribution}} and {{Generalised Gamma Process}}},
  author = {Dassios, Angelos and Zhang, Junyi},
  year = {2023},
  month = jun,
  journal = {Methodology and Computing in Applied Probability},
  volume = {25},
  number = {2},
  pages = {64},
  issn = {1573-7713},
  doi = {10.1007/s11009-023-10040-3},
  urldate = {2024-02-22},
  abstract = {Let \$\$J\_1{$>$}J\_2{$>\backslash$}dots \$\$be the ranked jumps of a gamma process \$\${\textbackslash}tau \_\{{\textbackslash}alpha \}\$\$on the time interval \$\$[0,{\textbackslash}alpha ]\$\$, such that \$\${\textbackslash}tau \_\{{\textbackslash}alpha \}={\textbackslash}sum \_\{k=1\}{\textasciicircum}\{{\textbackslash}infty \}J\_k\$\$. In this paper, we design an algorithm that samples from the random vector \$\$(J\_1, {\textbackslash}dots , J\_N, {\textbackslash}sum \_\{k=N+1\}{\textasciicircum}\{{\textbackslash}infty \}J\_k)\$\$. Our algorithm provides an analog to the well-established inverse L{\'e}vy measure (ILM) algorithm by replacing the numerical inversion of exponential integral with an acceptance-rejection step. This research is motivated by the construction of Dirichlet process prior in Bayesian nonparametric statistics. The prior assigns weight to each atom according to a GEM distribution, and the simulation algorithm enables us to sample from the N largest random weights of the prior. Then we extend the simulation algorithm to a generalised gamma process. The simulation problem of inhomogeneous processes will also be considered. Numerical implementations are provided to illustrate the effectiveness of our algorithms.},
  langid = {english},
  keywords = {60J25,62F15,62G05,Exact simulation,Gamma process,Generalised gamma process,L{\'e}vy process,Poisson-Dirichlet distribution},
  file = {/home/gkonkamking/pCloudDrive/papers/Dassios_Zhang_2023_Exact Simulation of Poisson-Dirichlet Distribution and Generalised Gamma Process.pdf}
}

@article{Datta2013,
  title = {Asymptotic Properties of Bayes Risk for the Horseshoe Prior},
  author = {Datta, Jyotishka and Ghosh, Jayanta K.},
  year = {2013},
  journal = {Bayesian Analysis},
  volume = {8},
  number = {1},
  pages = {111--132},
  issn = {19360975},
  doi = {10.1214/13-BA805},
  keywords = {Asymptotic optimality,Bayes oracle,Horseshoe decision rule,Multiple testing,nosource}
}

@book{daudinStatistiqueInferentielleIdees1999,
  title = {{Statistique inf{\'e}rentielle : id{\'e}es, d{\'e}marches, exemples}},
  author = {Daudin, Jean-Jacques and Robin, St{\'e}phane and Vuillet, Colette},
  year = {1999},
  publisher = {Presses universitaires de Rennes},
  abstract = {Pr{\'e}sente les concepts et la d{\'e}marche de la statistique inf{\'e}rentielle qui a pour objectif de transformer des affirmations {\'e}tablies sur un petit nombre d'objets (l'{\'e}chantillon) vers une population plus g{\'e}n{\'e}rale voire infinie. Pour les {\'e}tudiants des {\'e}coles d'ing{\'e}nieurs ou de commerce, en sciences sociales et en sciences de la vie.},
  isbn = {978-2-86847-404-9},
  langid = {french}
}

@article{Davis2011,
  title = {Don't Judge Species on Their Origins.},
  author = {Davis, Mark A and Chew, Matthew K and Hobbs, Richard J and Lugo, Ariel E and Ewel, John J and Vermeij, Geerat J and Brown, James H and Rosenzweig, Michael L and Gardener, Mark R and Carroll, Scott P and Thompson, Ken and Pickett, Steward T A and Stromberg, Juliet C and Del Tredici, Peter and Suding, Katharine N and Ehrenfeld, Joan G and Grime, J Philip and Mascaro, Joseph and Briggs, John C},
  year = {2011},
  journal = {Nature},
  volume = {474},
  number = {9},
  pages = {153--154},
  issn = {0028-0836},
  doi = {10.1038/474153a},
  abstract = {By this point, partly fuelled by Elton's book, proponents of biodiversity preservation and ecological restoration commonly used military metaphors and exaggerated claims of impending harm to help convey the message that introduced species are the enemies of man and nature. [...] the introduction of non-native species has almost always increased the number of species in a region5.},
  isbn = {0028-0836},
  pmid = {21654782},
  keywords = {nosource}
}

@article{dawson2017,
  ids = {dawsonIntroductoryLecturesStochastic2017,dawsonIntroductoryLecturesStochastic2017a},
  title = {Introductory Lectures on Stochastic Population Systems},
  author = {Dawson, Donald A},
  year = {2017},
  journal = {arXiv preprint arXiv:1705.03781},
  eprint = {1705.03781},
  archiveprefix = {arXiv},
  keywords = {60J70,Mathematics - Probability},
  file = {/home/gkonkamking/Zotero/storage/Y5ECUUNZ/Dawson - 2017 - Introductory Lectures on Stochastic Population Sys.pdf}
}

@inproceedings{de1937prevision,
  title = {La Pr{\'e}vision : Ses Lois Logiques, Ses Sources Subjectives},
  booktitle = {Annales de l'{{I}}. {{H}}. {{P}}.},
  author = {De Finetti, Bruno},
  year = {1937},
  volume = {7},
  number = {1},
  pages = {1--68},
  doi = {citeulike-article-id:9976130},
  abstract = {[HTML]},
  isbn = {0-88275-296-0},
  organization = {Presses universitaires de France},
  keywords = {nosource}
}

@incollection{de2001observed,
  title = {Observed Regularities in Species Sensitivity Distributions for Aquatic Species},
  booktitle = {Species Sensitivity Distributions in Ecotoxicology},
  author = {{de Zwart}, Dick},
  year = {2001},
  publisher = {CRC Press},
  keywords = {nosource}
}

@article{de2004anova,
  title = {An {{ANOVA}} Model for Dependent Random Measures},
  author = {De Iorio, Maria and M{\"u}ller, Peter and Rosner, Gary L and Maceachern, Steven N.},
  year = {2004},
  journal = {Journal of the American Statistical Association},
  volume = {99},
  number = {465},
  pages = {205--215},
  publisher = {ASA},
  issn = {0162-1459},
  doi = {10.1198/016214504000000205},
  abstract = {We consider dependent nonparametric models for related random probability distributions. For example, the random distributions might be indexed by a categorical covariate indicating the treatment levels in a clinical trial and might represent random effects distributions under the respective treatment combinations. We propose a model that describes dependence across random distributions in an analysis of variance (ANOVA)-type fashion. We define a probability model in such a way that marginally each random measure follows a Dirichlet process (DP) and use the dependent Dirichlet process to define the desired dependence across the related random measures. The resulting probability model can alternatively be described as a mixture of ANOVA models with a DP prior on the unknown mixing measure. The main features of the proposed approach are ease of interpretation and computational simplicity. Because the model follows the standard ANOVAstructure, interpretation and inference parallels conventions for ANOVA models. This includes the notion of main effects, interactions, contrasts, and the like. Of course, the analogies are limited to structure and interpretation. The actual objects of the inference are random distributions instead of the unknown normal means in standard ANOVA models. Besides interpretation and model structure, another important feature of the proposed approach is ease of posterior simulation. Because the model can be rewritten as a DP mixture of ANOVAmodels, it inherits all computational advantages of standard DP mixture models. This includes availability of efficient Gibbs sampling schemes for posterior simulation and ease of implementation of even high-dimensional applications. Complexity of implementing posterior simulation is?at least conceptually?dimension independent.{\textbackslash}nWe consider dependent nonparametric models for related random probability distributions. For example, the random distributions might be indexed by a categorical covariate indicating the treatment levels in a clinical trial and might represent random effects distributions under the respective treatment combinations. We propose a model that describes dependence across random distributions in an analysis of variance (ANOVA)-type fashion. We define a probability model in such a way that marginally each random measure follows a Dirichlet process (DP) and use the dependent Dirichlet process to define the desired dependence across the related random measures. The resulting probability model can alternatively be described as a mixture of ANOVA models with a DP prior on the unknown mixing measure. The main features of the proposed approach are ease of interpretation and computational simplicity. Because the model follows the standard ANOVAstructure, interpretation and inference parallels conventions for ANOVA models. This includes the notion of main effects, interactions, contrasts, and the like. Of course, the analogies are limited to structure and interpretation. The actual objects of the inference are random distributions instead of the unknown normal means in standard ANOVA models. Besides interpretation and model structure, another important feature of the proposed approach is ease of posterior simulation. Because the model can be rewritten as a DP mixture of ANOVAmodels, it inherits all computational advantages of standard DP mixture models. This includes availability of efficient Gibbs sampling schemes for posterior simulation and ease of implementation of even high-dimensional applications. Complexity of implementing posterior simulation is?at least conceptually?dimension independent.},
  isbn = {0162-1459},
  keywords = {dependent dirichlet process,hierarchical models,nonparametric bayes,nosource}
}

@article{de2009bayesian,
  title = {Bayesian Nonparametric Nonproportional Hazards Survival Modeling},
  author = {De Iorio, Maria and Johnson, Wesley O and M{\"u}ller, Peter and Rosner, Gary L},
  year = {2009},
  journal = {Biometrics},
  volume = {65},
  number = {3},
  pages = {762--771},
  publisher = {Wiley Online Library},
  keywords = {nosource}
}

@article{de2010adaptive,
  title = {Adaptive Nonparametric {{Bayesian}} Inference Using Location-Scale Mixture Priors},
  author = {De Jonge, R. and Van Zanten, J. H.},
  year = {2010},
  journal = {Annals of Statistics},
  volume = {38},
  number = {6},
  pages = {3300--3320},
  publisher = {Institute of Mathematical Statistics},
  issn = {00905364},
  doi = {10.1214/10-AOS811},
  abstract = {We study location-scale mixture priors for nonparametric statistical problems, including multivariate regression, density estimation and classification. We show that a rate-adaptive procedure can be obtained if the prior is properly constructed. In particular, we show that adaptation is achieved if a kernel mixture prior on a regression function is constructed using a Gaussian kernel, an inverse gamma bandwidth, and Gaussian mixing weights. {\copyright} Institute of Mathematical Statistics, 2010.},
  keywords = {Adaptation,Bayesian inference,Kernel mixture priors,Nonparametric regression,nosource,Posterior distribution,Rate of convergence}
}

@article{de2012adaptive,
  title = {Adaptive Estimation of Multivariate Functions Using Conditionally {{Gaussian}} Tensor-Product Spline Priors},
  author = {{de Jonge}, R. and {van Zanten}, J. H.},
  year = {2012},
  journal = {Electronic Journal of Statistics},
  volume = {6},
  pages = {1984--2001},
  publisher = {Institute of Mathematical Statistics},
  issn = {19357524},
  doi = {10.1214/12-EJS735},
  keywords = {Adaptive estimation,Nonparametric Bayes procedure,nosource,Posterior contraction rate,Tensor-product splines}
}

@article{de2012asymptotic,
  title = {An Asymptotic Analysis of a Class of Discrete Nonparametric Priors},
  author = {De Blasi, Pierpaolo and Lijoi, Antonio and Pr{\"u}nster, Igor},
  year = {2013},
  journal = {Statistica Sinica},
  volume = {23},
  pages = {1299--1321},
  issn = {10170405},
  doi = {10.5705/ss.2012.047},
  keywords = {nosource}
}

@article{de2013unique,
  title = {Unique in the Crowd: {{The}} Privacy Bounds of Human Mobility},
  author = {De Montjoye, Yves-Alexandre and Hidalgo, C{\'e}sar A and Verleysen, Michel and Blondel, Vincent D},
  year = {2013},
  journal = {Scientific reports},
  volume = {3},
  number = {1},
  pages = {1--5},
  publisher = {Nature Publishing Group},
  doi = {10.1038/srep01376},
  keywords = {nosource}
}

@article{deathMDM,
  title = {The Multinomial Diversity Model: {{Linking Shannon}} Diversity to Multiple Predictors},
  author = {De'Ath, Glenn},
  year = {2012},
  journal = {Ecology},
  volume = {93},
  number = {10},
  pages = {2286--2296},
  issn = {00129658},
  doi = {10.1890/11-2155.1},
  abstract = {The multinomial diversity model, MDM, is a new method for relating Shannon diversity to complex environmental, spatial, and temporal predictors. It is based on a parameterized formulation of Shannon entropy and diversity, and a novel link between entropy and the log-likelihood of the multinomial model. The MDM relates diversity to the predictors by minimizing the entropy of the estimated species values. Model effects can be expressed as changes in entropy. Entropy can be partitioned within and between sites, species, and models, and changes in entropy can be attributed to model predictors. All entropies translate into diversity for meaningful ecological interpretation. This greatly enhances our capacity to model complex data sets, and yet also provide simple interpretations. By formulating diversity as a statistical model and working in terms of entropy, diversity is simplified both conceptually and analytically, and diversity analyses are extended beyond traditional simple hierarchies of alpha, beta, gamma, and measures of turnover. The MDM inherits the properties of generalized linear models, and thus proven methods can be used for model selection and graphical and numerical interpretation. A weighted version of the Shannon diversity model is proposed in order to extend the MDM to non-Shannon diversities. Two example analyses, based on simulated and field data, illustrate the theoretical concepts and the analytical methods.},
  isbn = {0012-9658},
  pmid = {23185889},
  keywords = {Abundance,Beta diversity,Deviance,Diversity,Diversity index,Entropy,Generalized linear model,Multinomial model,nosource,Parameterized diversity,Shannon}
}

@article{debenest2009sensitivity,
  title = {Sensitivity of Freshwater Periphytic Diatoms to Agricultural Herbicides},
  author = {Debenest, T. and Pinelli, E. and Coste, M. and Silvestre, J. and Mazzella, N. and Madigou, C. and Delmas, F.},
  year = {2009},
  journal = {Aquatic Toxicology},
  volume = {93},
  number = {1},
  pages = {11--17},
  publisher = {Elsevier},
  issn = {0166445X},
  doi = {10.1016/j.aquatox.2009.02.014},
  abstract = {The biomonitoring of pesticide pollution in streams and rivers using algae such as diatoms remains difficult. The responses of diatom communities to toxic stress in stream water are disturbed by the variations of environmental parameters. In this study, periphytic algae collected in situ were exposed under controlled conditions to two major herbicides used in French agriculture (isoproturon and s-metolachlor). Three exposure regimes were tested: 5 and 30 ??g L-1 for 6 days and 30 ??g L-1 for 3 days followed by a recovery period of 3 days. The algal biomasses were assessed from pigment concentrations (chlorophyll a and c) and from live cell density. The highest concentration (30 ??g L-1) of isoproturon inhibited the biomass increase statistically significantly. In periphyton exposed to 5 and 30 ??g L-1 of s-metolachlor, chlorophyll c concentration and live cell density were also statistically significantly lower than in the control. Periphyton left to recover after reduced exposure duration (3 days) showed higher growth rates after treatment with s-metolachlor than with isoproturon. Taxonomic identifications showed that species like Melosira varians, Nitzschia dissipata and Cocconeis placentula were not affected by the herbicide exposure. Other species like Eolimna minima and Navicula reichardtiana were more sensitive. Studying diatoms according to their trophic mode showed that facultative heterotroph species were statistically significantly favoured by isoproturon exposure at the highest concentration. Results obtained with s-metolachlor exposure showed a disturbance of cell multiplication rather than that of photosynthesis. These results suggest that photosynthesis inhibitors like isoproturon favour species able to survive when the autotroph mode is inhibited. ?? 2009 Elsevier B.V. All rights reserved.},
  isbn = {0166-445X},
  pmid = {19342109},
  keywords = {Algae,Commercial formulation,Growth inhibition,Mode of action,nosource,Trophic mode}
}

@article{DeBlasi2010class,
  title = {A Class of Neutral to the Right Priors Induced by Superposition of Beta Processes},
  author = {De Blasi, Pierpaolo and Favaro, Stefano and Muliere, Pietro},
  year = {2010},
  journal = {Journal of Statistical Planning and Inference},
  volume = {140},
  number = {6},
  pages = {1563--1575},
  publisher = {Elsevier},
  issn = {03783758},
  doi = {10.1016/j.jspi.2009.12.018},
  abstract = {A random distribution function on the positive real line which belongs to the class of neutral to the right priors is defined. It corresponds to the superposition of independent beta processes at the cumulative hazard level. The definition is constructive and starts with a discrete time process with random probability masses obtained from suitably defined products of independent beta random variables. The continuous time version is derived as the corresponding infinitesimal weak limit and is described in terms of completely random measures. It takes the interpretation of the survival distribution resulting from independent competing failure times. We discuss prior specification and illustrate posterior inference on a real data example. ?? 2009 Elsevier B.V. All rights reserved.},
  keywords = {Bayesian nonparametrics,Beta process,Beta-Stacy process,Completely random measures,Neutral to the right priors,nosource,Survival analysis}
}

@article{deblasi2015gibbs,
  title = {Are Gibbs-Type Priors the Most Natural Generalization of the Dirichlet Process?},
  author = {De Blasi, Pierpaolo and Favaro, Stefano and Lijoi, Antonio and Mena, Rams{\'e}s H. and Pr{\"u}nster, Igor and Ruggiero, Matteo},
  year = {2015},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {37},
  number = {2},
  eprint = {1503.00163v1},
  pages = {212--229},
  publisher = {IEEE},
  issn = {01628828},
  doi = {10.1109/TPAMI.2013.217},
  abstract = {Discrete random probability measures and the exchangeable random partitions they induce are key tools for addressing a variety of estimation and prediction problems in Bayesian inference. Here we focus on the family of Gibbs-type priors, a recent elegant generalization of the Dirichlet and the Pitman-Yor process priors. These priors share properties that are appealing both from a theoretical and an applied point of view: (i) they admit an intuitive predictive characterization justifying their use in terms of a precise assumption on the learning mechanism; (ii) they stand out in terms of mathematical tractability; (iii) they include several interesting special cases besides the Dirichlet and the Pitman-Yor processes. The goal of our paper is to provide a systematic and unified treatment of Gibbs-type priors and highlight their implications for Bayesian nonparametric inference. We deal with their distributional properties, the resulting estimators, frequentist asymptotic validation and the construction of time-dependent versions. Applications, mainly concerning mixture models and species sampling, serve to convey the main ideas. The intuition inherent to this class of priors and the neat results they lead to make one wonder whether it actually represents the most natural generalization of the Dirichlet process.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1503.00163v1},
  isbn = {0162-8828 VO - 37},
  pmid = {26353237},
  keywords = {Nonparametric statistics,Stochastic processes},
  file = {/home/gkonkamking/Zotero/storage/JWE4FEKB/De Blasi et al. - 2015 - Are gibbs-type priors the most natural generalizat.pdf}
}

@inproceedings{Decker2014,
  title = {Bitcoin Transaction Malleability and {{MtGox}}},
  booktitle = {Computer Security - {{ESORICS}} 2014: 19th European Symposium on Research in Computer Security},
  author = {Decker, Christian and Wattenhofer, Roger},
  editor = {Kutylowski, Miroslav and Vaidya, Jaideep},
  year = {2014},
  pages = {313--326},
  publisher = {Cham: Springer International Publishing},
  address = {Wroclaw, Poland},
  keywords = {nosource}
}

@incollection{dedecker2007weak,
  title = {Weak Dependence with Examples and Applications},
  booktitle = {Lecture Notes in Statistics},
  author = {Dedecker, J{\'e}r{\^o}me and Doukhan, Paul and Lang, Gabriel and Leon, Jos{\'e} Rafael and Louhichi, Sana and Prieur, Cl{\'e}mentine},
  year = {2007},
  volume = {190},
  publisher = {Springer},
  issn = {01439782},
  isbn = {978-0-387-69951-6},
  keywords = {nosource}
}

@article{Defeuilley2009,
  title = {Le Gaz Naturel En {{Europe Entre}} Lib{\'e}ralisation Des March{\'e}s et G{\'e}opolitique},
  author = {Defeuilley, Christophe},
  year = {2009},
  journal = {Flux},
  number = {75},
  pages = {99--111},
  issn = {11542721},
  keywords = {nosource}
}

@article{defruytCloningersPsychobiologicalModel2000,
  title = {Cloninger's {{Psychobiological Model}} of {{Temperament}} and {{Character}} and the {{Five-Factor Model}} of {{Personality}}},
  author = {De Fruyt, F and Van De Wiele, L and Van Heeringen, C},
  year = {2000},
  month = sep,
  journal = {Personality and Individual Differences},
  volume = {29},
  number = {3},
  pages = {441--452},
  issn = {0191-8869},
  doi = {10.1016/S0191-8869(99)00204-4},
  urldate = {2020-05-11},
  abstract = {The relationships between Cloninger's Temperament and Character dimensions [Cloninger, C. R. (1987). A systematic method for clinical description and classification of personality variants. Archives of General Psychiatry, 44 573--588; Cloninger, C. R., Svrakic, D. M., \& Przybeck, T. R. (1993). A psychobiological model of temperament and character. Archives of General Psychiatry, 50, 975--990] and the Five-Factor Model (FFM) of personality are investigated in a randomised sample of 130 patients admitted to the Emergency Psychiatric Unit of a large university hospital. Cloninger's psychobiological model identifies four dimensions of temperament (Novelty seeking, Harm avoidance, Reward dependence and Persistence) and three dimensions of character (Self-directedness, Cooperativeness and Self-transcendence). The FFM proposes the domains of Extraversion, Agreeableness, Conscientiousness, Neuroticism and Openness as the basic dimensions underlying individual differences. Five-factor scores are obtained with the NEO-PI-R [Costa, P. T., Jr., \& McCrae, R. R. (1992). NEO-PI-R. Professional manual. Odessa, FL: Psychological Assessment Resources]; Cloninger's personality dimensions are assessed with the Temperament and Character Inventory (Cloninger et al., 1993). The present study primarily focuses on the direct equivalence of Cloninger's scales with the NEO-PI-R domains and facets. Considerable overlap with the FFM dimensions is demonstrated and the results show that each TCI factor is substantially covered by the FFM.},
  langid = {english},
  keywords = {Assessment,Character,Individual differences,nosource,Personality,Temperament}
}

@article{DeHoop2016,
  title = {Time-Varying Effects of Aromatic Oil Constituents on the Survival of Aquatic Species: {{Deviations}} between Model Estimates and Observations},
  author = {De Hoop, Lisette and Viaene, Karel P.J. and Schipper, Aafke M. and Huijbregts, Mark A.J. and De Laender, Frederik and Hendriks, A. Jan},
  year = {2016},
  journal = {Environmental Toxicology and Chemistry},
  volume = {9999},
  number = {9999},
  pages = {1--9},
  issn = {07307268},
  doi = {10.1002/etc.3508},
  keywords = {nosource,toxicodynamic model,toxicokinetic}
}

@article{DeLaender2008,
  title = {Do We Have to Incorporate Ecological Interactions in the Sensitivity Assessment of Ecosystems? {{An}} Examination of a Theoretical Assumption Underlying Species Sensitivity Distribution Models.},
  author = {De Laender, Frederik and De Schamphelaere, Karel a C and a Vanrolleghem, Peter and Janssen, Colin R},
  year = {2008},
  month = apr,
  journal = {Environment international},
  volume = {34},
  number = {3},
  eprint = {17977598},
  eprinttype = {pubmed},
  pages = {390--6},
  issn = {0160-4120},
  doi = {10.1016/j.envint.2007.09.006},
  abstract = {Species sensitivity distributions (SSDs) are statistical distributions which extrapolate single-species toxicity test results to ecosystem effects. This SSD approach assumes that ecological interactions between populations, such as grazing and competition, do not influence the sensitivity of ecosystems. The validity of this assumption in a simple freshwater pelagic ecosystem was tested using ecosystem modelling. For each of a 1000 hypothetical toxicants, a lognormal SSD was fitted to chronic single-species EC10s of the species present. As such, these distributions did not account for ecological interactions and were therefore termed 'conventional SSDs' (cSSDs). Next, sensitivity distributions that did take into account ecological interactions were constructed (eco-SSD) for the same 1000 toxicants, using an ecosystem model. For 254 of the 1000 hypothetical toxicants, mean and/or variance of the cSSD were significantly higher than mean and/or variance of the eco-SSD, as such rejecting the general validity of the tested assumption. A classification tree approach indicated that especially toxicants which directly affect phytoplankton (i.e. herbicides) may have a higher mean for cSSD than for eco-SSD. Conversely, means of eco-SSD and cSSD tend to be equal for toxicants directly affecting zooplankton and fish, e.g. insecticides. For the 254 hypothetical toxicants for which the tested assumption was false, a predicted no effect concentration (PNEC) calculated as the lowest single-species EC10 divided by an application factor of 10 was on average a factor 10 lower than the corresponding ecosystem-NOEC calculated by the ecosystem model.},
  isbn = {0160-4120},
  pmid = {17977598},
  keywords = {Animals,Chemical,Chemical: toxicity,Computer Simulation,duplicate-citation-key,Ecology,Ecosystem,Herbicides,Herbicides: toxicity,Modelling,nosource,Phytoplankton,Phytoplankton: drug effects,Risk,SSD,Water Pollutants}
}

@article{DeLaender2008a,
  title = {Is Ecosystem Structure the Target of Concern in Ecological Effect Assessments?},
  author = {De Laender, Frederik and De Schamphelaere, Karel A C and Vanrolleghem, Peter A. and Janssen, Colin R.},
  year = {2008},
  month = may,
  journal = {Water Research},
  volume = {42},
  number = {10-11},
  eprint = {18258280},
  eprinttype = {pubmed},
  pages = {2395--2402},
  issn = {00431354},
  doi = {10.1016/j.watres.2008.01.006},
  abstract = {The species sensitivity distribution, a technique currently used to derive water-quality standards of chemicals, is associated with a set of inadequately tested assumptions. One of these assumptions is that ecosystem structure is as or more sensitive than ecosystem function, i.e., that structure is the target of concern. In this paper, we tested this assumption for a simple freshwater ecosystem exposed to different toxicants. Using an ecosystem model, we calculated no observed effect concentrations (NOECs) for ecosystem structure (ecosystem structure-NOECs) and function (ecosystem function-NOECs) for each of 1000 hypothetical toxicants. For 979 of these toxicants, the ecosystem structure-NOEC was lower than or equal to the ecosystem function-NOEC, indicating that the tested assumption can be considered valid. For 239 of these 979 toxicants, both NOECs were equal. For half of the 1000 toxicants, the structure of lower trophic levels (i.e., phytoplankton) appears to be more sensitive than the structure of higher trophic levels (i.e., fish). As such, ecosystem structure-NOECs are primarily determined by the sensitivity of the structure of lower trophic levels. In contrast, ecosystem functions associated with higher trophic levels (e.g., total ingestion by fish) are more sensitive than functions associated with lower trophic levels (e.g., total photosynthesis by phytoplankton) for 749 toxicants. ?? 2008 Elsevier Ltd. All rights reserved.},
  isbn = {0043-1354},
  pmid = {18258280},
  keywords = {Ecological interactions,Ecosystem modelling,nosource,Water-quality standard setting}
}

@article{DeLaender2010,
  title = {Ecological Significance of Hazardous Concentrations in a Planktonic Food Web.},
  author = {De Laender, Frederik and Soetaert, Karline and De Schamphelaere, Karel a C and Middelburg, Jack J and Janssen, Colin R},
  year = {2010},
  month = mar,
  journal = {Ecotoxicology and environmental safety},
  volume = {73},
  number = {3},
  eprint = {20045193},
  eprinttype = {pubmed},
  pages = {247--53},
  publisher = {Elsevier},
  issn = {1090-2414},
  doi = {10.1016/j.ecoenv.2009.12.008},
  abstract = {Species sensitivity distributions (SSDs) are statistical distributions that are used to estimate the potentially affected fraction (PAF) of species at a given toxicant concentration, the hazardous concentration for that fraction of species (HC(PAF)). Here, we use an aquatic food web model that includes 14 phytoplankton and 6 zooplankton species to estimate the number of species experiencing a biomass reduction when the food web is exposed to the HC(PAF) and this for 1000 hypothetical toxicants and for PAF=5-30\%. When choosing a 20\% decrease as a cut-off to categorize a species' biomass as affected, 0-1 and 2-5 out of the 20 species were affected at the HC(5) and HC(30), respectively. From this, it can be concluded that the PAF is a relatively good estimator of the number of affected species. However, when phytoplankton species experiencing {$>$}or=20\% biomass increase were also classified as affected, the number of affected species predicted by the food web model varied strongly among toxicants for PAF {$>$}5, with 2-16 out of 20 species affected at the HC(30). Phytoplankton species with extreme (both high and low) values for uptake rates and light limitation constants experienced smaller effects on their biomass than phytoplankton species with more average parameter values. We conclude that, next to measures of toxicity, ecological characteristics of species may help understanding ecological effects occurring in ecosystems also.},
  isbn = {0147-6513},
  pmid = {20045193},
  keywords = {Animals,Biodiversity,Biological,Biomass,Chemical,Chemical: analysis,Chemical: metabolism,Chemical: toxicity,duplicate-citation-key,Ecological effect assessments,Environmental Monitoring,Environmental Monitoring: statistics \& numerical d,Food Chain,Food web model,Models,nosource,Phytoplankton,Phytoplankton: chemistry,Phytoplankton: metabolism,Potentially affected fraction,Predictive Value of Tests,SSD,Water Pollutants,Xenobiotics,Xenobiotics: analysis,Xenobiotics: metabolism,Xenobiotics: toxicity,Zooplankton,Zooplankton: chemistry,Zooplankton: metabolism}
}

@article{DeLange2010,
  title = {Ecological Vulnerability in Risk Assessment - {{A}} Review and Perspectives},
  author = {De Lange, H. J. and Sala, S. and Vighi, M. and Faber, J. H.},
  year = {2010},
  journal = {Science of the Total Environment},
  volume = {408},
  number = {18},
  pages = {3871--3879},
  publisher = {Elsevier B.V.},
  issn = {00489697},
  doi = {10.1016/j.scitotenv.2009.11.009},
  abstract = {This paper reviews the application of ecological vulnerability analysis in risk assessment and describes new developments in methodology. For generic non-site-specific assessments (e.g. for the requirements of most European directives on dangerous chemicals) risk is characterised just on the basis of the ratio between an effect indicator and an exposure indicator. However, when the actual risk for a specific ecosystem is desired, the concept of ecological vulnerability may be more appropriate. This calls for a change in thinking, from sensitivity at the organism level to vulnerability at higher organization levels, and thus forms the link from laboratory toxicology to field effects at population, community or ecosystem level. To do so, biological and ecological characteristics of the ecosystems under concern are needed to estimate the ecological vulnerability.In this review we describe different vulnerability analysis methods developed for populations (of a single species), communities (consisting of different populations of species) and ecosystems (community and habitat combined). We also give some examples of methods developed for socio-ecological systems. Aspects that all methods share are the use of expert judgment, the input of stakeholders, ranking and mapping of the results, and the qualitative nature of the results.A new general framework is presented to guide future ecological vulnerability analysis. This framework can be used as part of ecological risk assessment, but also in risk management. We conclude that the further quantification of ecological vulnerability is a valuable contribution to vulnerability assessment. ?? 2009 Elsevier B.V.},
  isbn = {0048-9697},
  pmid = {20004002},
  keywords = {Ecological vulnerability assessment,Hazard,Hierarchical scale,nosource,Organization level,Resilience,Risk}
}

@article{Delignette-Muller2011a,
  title = {A New Perspective on the {{Dunnett}} Procedure: {{Filling}} the Gap between {{NOEC}}/{{LOEC}} and {{ECx}} Concepts},
  author = {{Delignette-Muller}, Marie-Laure Laure and Forfait, Carole and Billoir, Elise and Charles, Sandrine},
  year = {2011},
  month = oct,
  journal = {Environmental Toxicology and Chemistry},
  volume = {30},
  number = {12},
  pages = {2888--2891},
  publisher = {John Wiley \& Sons, Inc.},
  issn = {07307268},
  doi = {10.1002/etc.686},
  abstract = {The no-observed-effect concentration (NOEC) is known to be based on a wrong usage of hypothesis tests, and the use of confidence intervals is preferred. The purpose of the present study is to provide an easy and proper way to interpret ecotoxicological tests based on simultaneous confidence intervals associated with the commonly used Dunnett procedure, and to show how these intervals may allow one to infer ECx values (effective concentrations).},
  isbn = {1552-8618},
  pmid = {21932292},
  keywords = {Ecotoxicological bioassays,Multiple comparisons,nosource,Simultaneous confidence intervals,Toxicity endpoints}
}

@article{Delignette-Muller2015,
  title = {Fitdistrplus : {{An}} r Package for Fitting Distributions},
  author = {{Delignette-muller}, Marie Laure and Dutang, Christophe},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {64},
  number = {4},
  pages = {1--34},
  issn = {1548-7660},
  doi = {10.18637/jss.v064.i04},
  abstract = {The package fitdistrplus provides functions for fitting univariate distributions to different types of data (continuous censored or non-censored data and discrete data) and allowing different estimation methods (maximum likelihood, moment matching, quantile matching and maximum goodness-of-fit estimation). Outputs of fitdist and fitdistcens functions are S3 objects, for which kind generic methods are provided, including summary, plot and quantile. This package also provides various functions to compare the fit of several distributions to a same data set and can handle bootstrap of parameter estimates. Detailed examples are given in food risk assessment, ecotoxicology and insurance contexts.},
  isbn = {9781420065213},
  keywords = {bootstrap,censored data,distributions,maximum goodness-of-fit,maximum likelihood,moment matching,nosource,probability distribution fitting,quantile matching,r}
}

@article{delignette-mullerRobustFitToxicokinetic2017,
  title = {Robust {{Fit}} of {{Toxicokinetic}}--{{Toxicodynamic Models Using Prior Knowledge Contained}} in the {{Design}} of {{Survival Toxicity Tests}}},
  author = {{Delignette-Muller}, Marie Laure and Ruiz, Philippe and Veber, Philippe},
  year = {2017},
  month = apr,
  journal = {Environmental Science \& Technology},
  volume = {51},
  number = {7},
  pages = {4038--4045},
  publisher = {American Chemical Society},
  issn = {0013-936X},
  doi = {10.1021/acs.est.6b05326},
  urldate = {2021-05-25},
  abstract = {Toxicokinetics--toxicodynamic (TKTD) models have emerged as a powerful means to describe survival as a function of time and concentration in ecotoxicology. They are especially powerful to extrapolate survival observed under constant exposure conditions to survival predicted under realistic fluctuating exposure conditions. But despite their obvious benefits, these models have not yet been adopted as a standard to analyze data of survival toxicity tests. Instead simple dose--response models are still often used although they only exploit data observed at the end of the experiment. We believe a reason precluding a wider adoption of TKTD models is that available software still requires strong expertise in model fitting. In this work, we propose a fully automated fitting procedure that extracts prior knowledge on parameters of the model from the design of the toxicity test (tested concentrations and observation times). We evaluated our procedure on three experimental and 300 simulated data sets and showed that it provides robust fits of the model, both in the frequentist and the Bayesian framework, with a better robustness of the Bayesian approach for the sparsest data sets.},
  file = {/home/gkonkamking/Zotero/storage/Y46MC3Q5/Delignette-Muller et al. - 2017 - Robust Fit of Toxicokinetic–Toxicodynamic Models U.pdf}
}

@article{delignette2014statistical,
  title = {Statistical Handling of Reproduction Data for Exposure-Response Modeling},
  author = {{Delignette-Muller}, Marie Laure and Lopes, Christelle and Veber, Philippe and Charles, Sandrine},
  year = {2014},
  journal = {Environmental science \& technology},
  volume = {48},
  number = {13},
  pages = {7544--7551},
  publisher = {American Chemical Society},
  keywords = {nosource}
}

@article{delignette2017robust,
  title = {Robust Fit of Toxicokinetic-Toxicodynamic Models Using Prior Knowledge Contained in the Design of Survival Toxicity Tests.},
  author = {{Delignette-Muller}, Marie Laure and Ruiz, Philippe and Veber, Philippe},
  year = {2017},
  journal = {Environmental Science \& Technology},
  publisher = {ACS Publications},
  keywords = {nosource}
}

@article{delussu2023limits,
  title = {The Limits of Human Mobility Traces to Predict the Spread of {{COVID-19}}: {{A}} Transfer Entropy Approach},
  author = {Delussu, Federico and Tizzoni, Michele and Gauvin, Laetitia},
  year = {2023},
  journal = {PNAS nexus},
  volume = {2},
  number = {10},
  pages = {pgad302},
  publisher = {Oxford University Press US},
  doi = {10.1093/pnasnexus/pgad302}
}

@article{derexExperimentalEvidenceInfluence2013,
  title = {Experimental Evidence for the Influence of Group Size on Cultural Complexity},
  author = {Derex, Maxime and Beugin, Marie-Pauline and Godelle, Bernard and Raymond, Michel},
  year = {2013},
  month = nov,
  journal = {Nature},
  volume = {503},
  number = {7476},
  pages = {389--391},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature12774},
  urldate = {2020-04-16},
  abstract = {A dual-task computer game played by groups of different sizes is used to show that cultural evolution (the maintenance or improvement of cultural knowledge) strongly depends on population size; in larger groups of players, higher cultural complexity and cultural trait diversity are maintained, and improvements to existing cultural traits are more frequent.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {nosource}
}

@article{DeRossi2010,
  title = {Maximum Likelihood Estimation of the {{Cox-Ingersoll-Ross}} Model Using Particle Filters},
  author = {{de Rossi}, Giuliano},
  year = {2010},
  journal = {Computational Economics},
  volume = {36},
  number = {1},
  pages = {1--16},
  issn = {09277099},
  doi = {10.1007/s10614-010-9208-0},
  abstract = {This paper shows how to build in a computationally efficient way a maximum simulated likelihood procedure to estimate the Cox-Ingersoll-Ross model from multivariate time series. The advantage of this estimator is that it takes into account the exact likelihood function while avoiding the huge computational burden associated with MCMC methods and without the ad hoc assumption that certain bond yields are measured without error. The proposed methodology is implemented and tested on simulated data. For realistic parameter values the estimator seems to have good small sample properties, compared to the popular quasi maximum likelihood approach, even using moderate simulation sizes. The effect of simulation errors does not seem to undermine the estimation procedure. {\copyright} Springer Science+Business Media, LLC. 2010.},
  keywords = {Importance sampling,nosource,Sequential Monte Carlo method,Term structure of interest rates}
}

@article{DeSchamphelaere2010,
  title = {The Chronic Toxicity of Molybdate to Freshwater Organisms. {{I}}. {{Generating}} Reliable Effects Data.},
  author = {De Schamphelaere, K a C and Stubblefield, W and Rodriguez, P and Vleminckx, K and Janssen, C R},
  year = {2010},
  month = oct,
  journal = {The Science of the total environment},
  volume = {408},
  number = {22},
  eprint = {20813395},
  eprinttype = {pubmed},
  pages = {5362--71},
  publisher = {Elsevier B.V.},
  issn = {1879-1026},
  doi = {10.1016/j.scitotenv.2010.07.041},
  abstract = {The European Union regulation on Registration, Evaluation, Authorization and Restriction of Chemical substances (REACH) (EC, 2006) requires the characterization of the chronic toxicity of many chemicals in the aquatic environment, including molybdate (MoO(4)(2-)). Our literature review on the ecotoxicity of molybdate revealed that a limited amount of reliable chronic no observed effect concentrations (NOECs) for the derivation of a predicted no-effect concentration (PNEC) existed. This paper presents the results of additional ecotoxicity experiments that were conducted in order to fulfill the requirements for the derivation of a PNEC by means of the scientifically most robust species sensitivity distribution (SSD) approach (also called the statistical extrapolation approach). Ten test species were chronically exposed to molybdate (added as sodium molybdate dihydrate, Na(2)MoO(4)˙2H(2)O) according to internationally accepted standard testing guidelines or equivalent. The 10\% effective concentrations (EC10, expressed as measured dissolved molybdenum) for the most sensitive endpoint per species were 62.8-105.6 (mg Mo)/L for Daphnia magna (21day-reproduction), 78.2 (mg Mo)/L for Ceriodaphnia dubia (7day-reproduction), 61.2-366.2 (mg Mo)/L for the green alga Pseudokirchneriella subcapitata (72h-growth rate), 193.6 (mg Mo)/L for the rotifer Brachionus calyciflorus (48h-population growth rate), 121.4 (mg Mo)/L for the midge Chironomus riparius (14day-growth), 211.3 (mg Mo)/L for the snail Lymnaea stagnalis (28day-growth rate), 115.9 (mg Mo)/L for the frog Xenopus laevis (4day-larval development), 241.5 (mg Mo)/L for the higher plant Lemna minor (7day-growth rate), 39.3 (mg Mo)/L for the fathead minnow Pimephales promelas (34day-dry weight/biomass), and 43.2 (mg Mo)/L for the rainbow trout Oncorhynchus mykiss (78day-biomass). These effect concentrations are in line with the few reliable data currently available in the open literature. The data presented in this study can serve as a basis for the derivation of a PNEC(aquatic) that can be used for national and international regulatory purposes and for setting water quality criteria. Using all reliable data that are currently available, a HC(5,50\%) (median hazardous concentration affecting 5\% of the species) of 38.2 (mg Mo)/L was derived with the statistical extrapolation approach.},
  pmid = {20813395},
  keywords = {Animals,Chemical,Chemical: standards,Chemical: toxicity,Chironomidae,Chironomidae: drug effects,Chlorophyta,Chlorophyta: drug effects,Daphnia,Daphnia: drug effects,Dose-Response Relationship,Drug,duplicate-citation-key,Molybdenum,Molybdenum: standards,Molybdenum: toxicity,No-Observed-Adverse-Effect Level,nosource,Rotifera,Rotifera: drug effects,Snails,Snails: drug effects,Toxicity Tests,Water Pollutants}
}

@article{DeSchamphelaere2010,
  title = {The Chronic Toxicity of Molybdate to Marine Organisms. {{I}}. {{Generating}} Reliable Effects Data},
  author = {Heijerick, D. G. and Regoli, L. and Stubblefield, W.},
  year = {2012},
  month = oct,
  journal = {Science of the Total Environment},
  volume = {430},
  number = {22},
  eprint = {20813395},
  eprinttype = {pubmed},
  pages = {260--269},
  publisher = {Elsevier B.V.},
  issn = {00489697},
  doi = {10.1016/j.scitotenv.2012.03.045},
  abstract = {A scientific research program was initiated by the International Molybdenum Association (IMOA) which addressed identified gaps in the environmental toxicity data for the molybdate ion (MoO 4 2-). These gaps were previously identified during the preparation of EU-REACH-dossiers for different molybdenum compounds (European Union regulation on Registration, Evaluation, Authorization and Restriction of Chemical substances; EC, 2006). Evaluation of the open literature identified few reliable marine ecotoxicological data that could be used for deriving a Predicted No-Effect Concentration (PNEC) for the marine environment. Rather than calculating a PNEC marine using the assessment factor methodology on a combined freshwater/marine dataset, IMOA decided to generate sufficient reliable marine chronic data to permit derivation of a PNEC by means of the more scientifically robust species sensitivity distribution (SSD) approach (also called the statistical extrapolation approach). Nine test species were chronically exposed to molybdate (added as sodium molybdate dihydrate, Na 2MoO 4??2H 2O) according to published standard testing guidelines that are acceptable for a broad range of regulatory purposes. The selected test organisms were representative for typical marine trophic levels: micro-algae/diatom (Phaeodactylum tricornutum, Dunaliella tertiolecta), macro-alga (Ceramium tenuicorne), mysids (Americamysis bahia), copepod (Acartia tonsa), fish (Cyprinodon variegatus), echinoderms (Dendraster exentricus, Strongylocentrotus purpuratus) and molluscs (Mytilus edulis, Crassostrea gigas). Available NOEC/EC 10 levels ranged between 4.4mgMo/L (blue mussel M. edulis) and 1174mgMo/L (oyster C. gigas).Using all available reliable marine chronic effects data that are currently available, a HC 5,50\% (median hazardous concentration affecting 5\% of the species) of 5.74(mgMo)/L was derived with the statistical extrapolation approach, a value that can be used for national and international regulatory purposes. ?? 2012 Elsevier B.V.},
  isbn = {0048-9697},
  pmid = {20813395},
  keywords = {Aquatic toxicity,duplicate-citation-key,nosource,Sodium molybdate,Species sensitivity distribution}
}

@article{deshwarPhyloWGSReconstructingSubclonal2015,
  title = {{{PhyloWGS}}: {{Reconstructing}} Subclonal Composition and Evolution from Whole-Genome Sequencing of Tumors},
  shorttitle = {{{PhyloWGS}}},
  author = {Deshwar, Amit G. and Vembu, Shankar and Yung, Christina K. and Jang, Gun Ho and Stein, Lincoln and Morris, Quaid},
  year = {2015},
  month = feb,
  journal = {Genome Biology},
  volume = {16},
  number = {1},
  pages = {35},
  issn = {1465-6906},
  doi = {10.1186/s13059-015-0602-8},
  urldate = {2021-05-02},
  abstract = {Tumors often contain multiple subpopulations of cancerous cells defined by distinct somatic mutations. We describe a new method, PhyloWGS, which can be applied to whole-genome sequencing data from one or more tumor samples to reconstruct complete genotypes of these subpopulations based on variant allele frequencies (VAFs) of point mutations and population frequencies of structural variations. We introduce a principled phylogenic correction for VAFs in loci affected by copy number alterations and we show that this correction greatly improves subclonal reconstruction compared to existing methods. PhyloWGS is free, open-source software, available at https://github.com/morrislab/phylowgs.},
  keywords = {Copy Number Variation,Dirichlet Process,Markov Chain Monte Carlo,Population Frequency,Read Depth}
}

@article{Dette2011a,
  title = {Optimal Experimental Design Strategies for Detecting Hormesis.},
  author = {Dette, Holger and Pepelyshev, Andrey and Wong, Weng Kee},
  year = {2011},
  month = dec,
  journal = {Risk analysis : an official publication of the Society for Risk Analysis},
  volume = {31},
  number = {12},
  eprint = {21545627},
  eprinttype = {pubmed},
  pages = {1949--60},
  issn = {1539-6924},
  doi = {10.1111/j.1539-6924.2011.01625.x},
  abstract = {Hormesis is a widely observed phenomenon in many branches of life sciences, ranging from toxicology studies to agronomy, with obvious public health and risk assessment implications. We address optimal experimental design strategies for determining the presence of hormesis in a controlled environment using the recently proposed Hunt-Bowman model. We propose alternative models that have an implicit hormetic threshold, discuss their advantages over current models, and construct and study properties of optimal designs for (i) estimating model parameters, (ii) estimating the threshold dose, and (iii) testing for the presence of hormesis. We also determine maximin optimal designs that maximize the minimum of the design efficiencies when we have multiple design criteria or there is model uncertainty where we have a few plausible models of interest. We apply these optimal design strategies to a teratology study and show that the proposed designs outperform the implemented design by a wide margin for many situations.},
  pmid = {21545627},
  keywords = {Hormesis,Models,nosource,Research Design,Theoretical}
}

@article{DeValpine2013,
  title = {General Models for Resource Use or Other Compositional Count Data Using the {{Dirichlet-}} Multinomial Distribution},
  author = {De Valpine, Perry and Harmon, Alexandra N},
  year = {2013},
  journal = {Ecology},
  volume = {94},
  number = {12},
  eprint = {23597116\{\%\}5Cnhttp://www.jstor.org/stable/},
  eprinttype = {jstor},
  pages = {2678--2687},
  issn = {0012-9658},
  doi = {10.1890/12-0416.1},
  abstract = {Many ecological studies investigate how organisms use resources, such as habitats or foods, in relation to availability or other variables. Related statistical problems include analysis of proportions of species or genotypes in a community or population. These require statistical modeling of compositional count data: data on relative proportions of each category collected as counts. Common methods for analyzing compositional count data lack one or more important considerations. Some methods lack explicit accommodation of count data, dealing instead with proportions. Others do not handle between-sample heterogeneity for overdispersed data. Yet others do not allow general types of relationships between explanatory variables and resource use. All three components have been combined in a Bayesian framework, but for frequentist hypothesis tests and AIC model selection, maximum-likelihood estimation is needed. Here we propose the Dirichlet-multinomial distribution to accommodate overdispersed compositional count data. This approach can be used flexibly in combination with explanatory models, but the only correlations among compositional proportions that it can accommodate are the negative correlations due to the fact that proportions must sum to 1. Many existing models can be generalized to use the Dirichlet-multinomial distribution for residual variation, and the flexibility of the approach allows new hypotheses that have often not been considered in resource preference analysis, including that availability has no relation to use. We also highlight a new design for resource use studies, with multiple individual-use data sets from each of multiple sites, with different explanatory data for each site. We illustrate the approach with three examples. For two previously published habitat use data sets, we support the original conclusions and show that use is not unrelated to availability. For a data set of pollen collected by multiple bees from each of two sites, pollen use differs between the sites. Using bootstrap goodness-of-fit tests, we illustrate that the Dirichlet-multinomial is acceptable for two of the examples but unsuitable for one of the habitat use examples.},
  isbn = {0012-9658},
  pmid = {24597215},
  keywords = {compositional data,compound multinomial,dirichlet-multinomial,habitat preference,habitat selection,nosource,pollen preference,resource selection}
}

@article{devalpineProgrammingModelsWriting2017,
  title = {Programming with Models: Writing Statistical Algorithms for General Model Structures with {{NIMBLE}}},
  author = {{de Valpine}, P. and Turek, D. and Paciorek, C.J. and {Anderson-Bergman}, C. and Temple Lang, D. and Bodik, R.},
  year = {2017},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  pages = {403--417},
  doi = {10.1080/10618600.2016.1172487},
  keywords = {nosource}
}

@book{Devroye1986a,
  title = {Non-Uniform Random Variate Generation},
  author = {Devroye, Luc},
  year = {1986},
  publisher = {Springer-Verlag},
  address = {New York},
  isbn = {0-387-96305-7},
  keywords = {nosource}
}

@article{DEZWARTDICKandPOSTHUMA2005,
  title = {Complex Mixture Toxicity for Single and Multiple Species: Proposed Methodologies},
  author = {{de Zwart}, D and Posthuma, L},
  year = {2009},
  journal = {Environmental Toxicology and {\dots}},
  volume = {24},
  number = {10},
  pages = {2665--2676},
  abstract = {Methods for the assessment of ecological risks associated with exposure to defined mixtures of toxicants are reviewed and formalized for single-species toxicity. Depending on the modes of action of toxicants in a mixture, these methods apply either the model for concentration additivity (CA) or the model for response additivity (RA). For complex mixtures, the present paper advocates the use of a new, two-step, mixed-model approach as a logical extension of model selection: Mixture toxicity for individual modes of action is evaluated with the CA model, and the toxicities of different modes of action are combined using the RA model. Using comparable mixture toxicity strategies in combination with the concept of species-sensitivity distributions, we develop a method to address and predict the risk for direct effects on the composition of species assemblages and biodiversity. The data needed for modeling can be obtained from existing databases, and lack of data can, in part, be addressed by the use of toxicity patterns in those databases. Both single- and multiple-species methods of mixture risk prediction are useful for risk management, because they allow ranking of polluted sites and affected species as well as identification of the most hazardous contaminants, at least in a comparative way. Validation of the proposed methods is feasible but currently limited because of a lack of appropriate data.},
  keywords = {duplicate-citation-key,Mixture toxicity Species assemblages Ecological ri,nosource}
}

@article{DEZWARTDICKandPOSTHUMA2005,
  title = {Complex Mixture Toxicity for Single and Multiple Species: Proposed Methodologies.},
  author = {{de Zwart}, Dick and Posthuma, Leo},
  year = {2005},
  journal = {Environmental Toxicology and Chemistry},
  volume = {24},
  number = {10},
  pages = {2665--2676},
  publisher = {Wiley Online Library},
  institution = {{National Institute for Public Health and the Environment, Laboratory for Ecological Risk Assessment, P.O. Box 1, NL-3720 BA Bilthoven, The Netherlands. d.de.zwart@rivm.nl}},
  issn = {0730-7268},
  doi = {10.1897/04-639r.1},
  abstract = {Methods for the assessment of ecological risks associated with exposure to defined mixtures of toxicants are reviewed and formalized for single-species toxicity. Depending on the modes of action of toxicants in a mixture, these methods apply either the model for concentration additivity (CA) or the model for response additivity (RA). For complex mixtures, the present paper advocates the use of a new, two-step, mixed-model approach as a logical extension of model selection: Mixture toxicity for individual modes of action is evaluated with the CA model, and the toxicities of different modes of action are combined using the RA model. Using comparable mixture toxicity strategies in combination with the concept of species-sensitivity distributions, we develop a method to address and predict the risk for direct effects on the composition of species assemblages and biodiversity. The data needed for modeling can be obtained from existing databases, and lack of data can, in part, be addressed by the use of toxicity patterns in those databases. Both single- and multiple-species methods of mixture risk prediction are useful for risk management, because they allow ranking of polluted sites and affected species as well as identification of the most hazardous contaminants, at least in a comparative way. Validation of the proposed methods is feasible but currently limited because of a lack of appropriate data.},
  isbn = {0730-7268},
  pmid = {16268170},
  keywords = {duplicate-citation-key,mixture toxicity,Mixture toxicity Species assemblages Ecological ri,nosource}
}

@article{diaconis1986consistency,
  title = {On the Consistency of Bayes Estimates},
  author = {Diaconis, P. and Freedman, D.},
  year = {1986},
  journal = {The Annals of Statistics},
  volume = {14},
  number = {1},
  pages = {63--67},
  publisher = {JSTOR},
  issn = {0090-5364},
  doi = {10.1214/aos/1176349842},
  abstract = {We discuss frequency properties of Bayes rules, paying special attention to consistency. Some new and fairly natural counterexamples are given, involving nonparametric estimates of location. Even the Dirichlet prior can lead to inconsistent estimates if used too aggressively. Finally, we discuss reasons for Bayesians to be interested in frequency properties of Bayes rules. As a part of the discussion we give a subjective equivalent to consistency and compute the derivative of the map taking priors to posteriors.},
  isbn = {DI000728 00905364 DI983929 98P0070O},
  keywords = {nosource}
}

@article{dickeyMultipleHypergeometricFunctions1983,
  title = {Multiple {{Hypergeometric Functions}}: {{Probabilistic Interpretations}} and {{Statistical Uses}}},
  shorttitle = {Multiple {{Hypergeometric Functions}}},
  author = {Dickey, James M.},
  year = {1983},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {78},
  number = {383},
  pages = {628--637},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1983.10478022},
  urldate = {2021-10-18},
  abstract = {This article reviews and interprets recent mathematics of special functions, with emphasis on integral representations of multiple hypergeometric functions. B.C. Carlson's centrally important parameterized functions R and {$R$}, initially defined as Dirichlet averages, are expressed as probability-generating functions of mixed multinomial distributions. Various nested families generalizing the Dirichlet distributions are developed for Bayesian inference in multinomial sampling and contingency tables. In the case of many-way tables, this motivates a new generalization of the function {$R$}. These distributions are also useful for the modeling of populations of personal probabilities evolving under the process of inference from statistical data. A remarkable new integral identity is adapted from Carlson to represent the moments of quadratic forms under multivariate normal and, more generally, elliptically contoured distributions. This permits the computation of such moments by simple quadrature.},
  keywords = {Bayesian inference,Carlson's R and {$R$},Contingency tables,Generalized Dirichlet distributions,Generalized mean value,Moments of quadratic forms,Multinomial sampling,Multiple hypergeometric functions,Multivariate distributions,Populations of personal probabilities,Special functions},
  file = {/home/gkonkamking/Zotero/storage/3C966R47/Dickey - 1983 - Multiple Hypergeometric Functions Probabilistic I.pdf;/home/gkonkamking/Zotero/storage/SGHYQKLQ/01621459.1983.html}
}

@article{Diebold2012,
  title = {Better to Give than to Receive : {{Predictive}} Directional Measurement of Volatility Spillovers},
  author = {Diebold, Francis X and Yilmaz, Kamil},
  year = {2012},
  journal = {International Journal of Forecasting},
  volume = {28},
  number = {1},
  pages = {57--66},
  publisher = {Elsevier B.V.},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2011.02.006},
  keywords = {nosource}
}

@article{diggleSpatialSpatioTemporalLogGaussian2013,
  title = {Spatial and {{Spatio-Temporal Log-Gaussian Cox Processes}}: {{Extending}} the {{Geostatistical Paradigm}}},
  shorttitle = {Spatial and {{Spatio-Temporal Log-Gaussian Cox Processes}}},
  author = {Diggle, Peter J. and Moraga, Paula and Rowlingson, Barry and Taylor, Benjamin M.},
  year = {2013},
  journal = {Statistical Science},
  volume = {28},
  number = {4},
  eprint = {43288435},
  eprinttype = {jstor},
  pages = {542--563},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  doi = {10.1214/13-STS441},
  urldate = {2023-10-29},
  abstract = {In this paper we first describe the class of log-Gaussian Cox processes (LGCPs) as models for spatial and spatio-temporal point process data. We discuss inference, with a particular focus on the computational challenges of likelihood-based inference. We then demonstrate the usefulness of the LGCP by describing four applications: estimating the intensity surface of a spatial point process; investigating spatial segregation in a multi-type process; constructing spatially continuous maps of disease risk from spatially discrete data; and real-time health surveillance. We argue that problems of this kind fit naturally into the realm of geostatistics, which traditionally is defined as the study of spatially continuous processes using spatially discrete observations at a finite number of locations. We suggest that a more useful definition of geostatistics is by the class of scientific problems that it addresses, rather than by particular models or data formats.},
  file = {/home/gkonkamking/pCloudDrive/papers/Diggle et al_2013_Spatial and Spatio-Temporal Log-Gaussian Cox Processes.pdf}
}

@misc{DigitalServicesAct,
  title = {The {{Digital Services Act}} ({{DSA}})},
  urldate = {2023-10-25},
  howpublished = {https://www.eu-digital-services-act.com/}
}

@article{ding2018determination,
  title = {Determination and Validation of Soil Thresholds for Cadmium Based on Food Quality Standard and Health Risk Assessment},
  author = {Ding, Changfeng and Ma, Yibing and Li, Xiaogang and Zhang, Taolin and Wang, Xingxiang},
  year = {2018},
  journal = {Science of The Total Environment},
  volume = {619},
  pages = {700--706},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{Ditlevsen2014,
  title = {Estimation in the Partially Observed Stochastic {{Morris-Lecar}} Neuronal Model with Particle Filter and Stochastic Approximation Methods},
  author = {{Ditlevsen} and {Susanne} and {Samson} and {Adeline}},
  year = {2014},
  journal = {Annals of Applied Statistics},
  volume = {8},
  number = {2},
  eprint = {1207.1865v2},
  pages = {674--702},
  issn = {19417330},
  doi = {10.1214/14-AOAS729},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1207.1865v2},
  keywords = {Conductance-based neuron models,Diffusions,Membrane potential,Motoneurons,nosource,Pseudo likelihood,Sequential monte carlo,Stochastic approximation expectation maximization}
}

@techreport{Dixon2007,
  title = {The Use of Probability Bounds Analysis for Characterising and Propagating Uncertainty in Species Sensitivity Distributions},
  author = {Dixon, {\relax WJ}},
  year = {2007},
  abstract = {This report describes the application of Probability Bounds Analysis (PBA) to the characterisation and propagation of uncertainty in Species Sensitivity Distribution (SSD) models. While a number of previous approaches have sought to analyse some aspects of uncertainty in SSDs, a major limitation of the methods used to date is that they have not incorporated uncertainty estimates propagated from the original toxicity data. By focusing on the use of scalar summaries of effect, such as the No Observed Effect Concentration (NOEC), which by definition are uninformative about uncertainty, previous methods have either described uncertainty as a lack of fit of an assumed model or applied extrapolation techniques that are based on untested assumptions. In this report it is suggested that percentile type summaries, such as Effect Concentrations (EC) or Lethal Concentrations (LC) are better suited to SSD modelling because their associated confidence intervals can be used to describe and propagate uncertainty estimated directly from the original toxicity data. Through the application of PBA it is shown that these confidence intervals can be used to derive bounds on an SSD and subsequent risk estimates.},
  keywords = {duplicate-citation-key,nosource}
}

@article{dogucuCurrentStateUndergraduate2022,
  title = {The {{Current State}} of {{Undergraduate Bayesian Education}} and {{Recommendations}} for the {{Future}}},
  author = {Dogucu, Mine and Hu, Jingchen},
  year = {2022},
  month = jun,
  journal = {The American Statistician},
  volume = {0},
  number = {0},
  pages = {1--9},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2022.2089232},
  urldate = {2022-07-22},
  abstract = {As a result of the increased emphasis on mis- and over-use of p-values in scientific research and the rise in popularity of Bayesian statistics, Bayesian education is becoming more important at the undergraduate level. With the advances in computing tools, Bayesian statistics is also becoming more accessible for undergraduates. This study focuses on analyzing Bayesian courses for undergraduates. We explored whether an undergraduate Bayesian course is offered in our sample of 152 high-ranking research universities and liberal arts colleges. For each identified Bayesian course, we examined how it fits into the institution's undergraduate curricula, such as majors and prerequisites. Through a series of course syllabi analyses, we explored the topics covered and their popularity in these courses, and the adopted teaching and learning tools, such as software. This article presents our findings on the current practices of teaching full Bayesian courses at the undergraduate level. Based on our findings, we provide recommendations for programs that may consider offering Bayesian courses to their students.},
  keywords = {Bayesian Statistics,Computing,Curriculum,MCMC,Simulation-based learning},
  file = {/home/gkonkamking/pCloudDrive/papers/Dogucu_Hu_2022_The Current State of Undergraduate Bayesian Education and Recommendations for.pdf}
}

@article{doi:10.1021/es972418b,
  title = {Fallacies in Ecological Risk Assessment Practices},
  author = {Power, M and McCarty, L S},
  year = {1997},
  journal = {Environmental Science and Technology},
  volume = {31},
  number = {8},
  pages = {370A-375A},
  issn = {0013936X},
  doi = {10.1021/es972418b},
  abstract = {Risk},
  isbn = {0013-936X},
  keywords = {nosource}
}

@article{doi:10.1080/00031305.2018.1527253,
  title = {Abandon Statistical Significance},
  author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
  year = {2019},
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  eprint = {https://doi.org/10.1080/00031305.2018.1527253},
  pages = {235--245},
  publisher = {Taylor \& Francis},
  doi = {10.1080/00031305.2018.1527253},
  keywords = {nosource}
}

@article{doi:10.1080/01621459.1938.10502329,
  title = {Some Difficulties of Interpretation Encountered in the Application of the Chi-Square Test},
  author = {Berkson, Joseph},
  year = {1938},
  journal = {Journal of the American Statistical Association},
  volume = {33},
  number = {203},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1938.10502329},
  pages = {526--536},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.1938.10502329},
  keywords = {nosource}
}

@article{doi:10.1080/01621459.2016.1222291,
  title = {The Iterated Auxiliary Particle Filter},
  author = {Guarniero, Pieralberto and Johansen, Adam M. and Lee, Anthony},
  year = {2017},
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {520},
  eprint = {https://doi.org/10.1080/01621459.2016.1222291},
  pages = {1636--1647},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.2016.1222291},
  abstract = {ABSTRACTWe present an offline, iterated particle filter to facilitate statistical inference in general state space hidden Markov models. Given a model and a sequence of observations, the associated marginal likelihood L is central to likelihood-based inference for unknown statistical parameters. We define a class of ``twisted'' models: each member is specified by a sequence of positive functions {$\psi$} and has an associated {$\psi$}-auxiliary particle filter that provides unbiased estimates of L. We identify a sequence {$\psi$}* that is optimal in the sense that the {$\psi$}*-auxiliary particle filter's estimate of L has zero variance. In practical applications, {$\psi$}* is unknown so the {$\psi$}*-auxiliary particle filter cannot straightforwardly be implemented. We use an iterative scheme to approximate {$\psi$}* and demonstrate empirically that the resulting iterated auxiliary particle filter significantly outperforms the bootstrap particle filter in challenging settings. Applications include parameter estimation using a particle Markov chain Monte Carlo algorithm.},
  file = {/home/gkonkamking/pCloudDrive/papers/Guarniero et al_2017_The iterated auxiliary particle filter.pdf}
}

@article{doi:10.1080/01973533.2015.1012991,
  title = {Editorial},
  author = {Trafimow, David and {Michael Marks}},
  year = {2015},
  journal = {Basic and Applied Social Psychology},
  volume = {37},
  number = {1},
  eprint = {https://doi.org/10.1080/01973533.2015.1012991},
  pages = {1--2},
  publisher = {Routledge},
  doi = {10.1080/01973533.2015.1012991},
  keywords = {nosource}
}

@article{doi:10.1080/07350015.2015.1092977,
  title = {A Class of Non-Gaussian State Space Models with Exact Likelihood Inference},
  author = {Creal, Drew D},
  year = {2017},
  journal = {Journal of Business \& Economic Statistics},
  volume = {35},
  number = {4},
  pages = {585--597},
  publisher = {Taylor \& Francis},
  doi = {10.1080/07350015.2015.1092977},
  file = {/home/gkonkamking/Zotero/storage/GUJY5XY4/Creal - 2017 - A class of non-gaussian state space models with ex.pdf}
}

@article{doi:10.1080/10807030500430559,
  title = {Predictive Value of Species Sensitivity Distributions for Effects of Herbicides in Freshwater Ecosystems},
  author = {{Van den Brink}, Paul J. and Blake, Naomi and Brock, Theo C. M. and Maltby, Lorraine},
  year = {2006},
  journal = {Human and Ecological Risk Assessment: An International Journal},
  volume = {12},
  number = {778384761},
  pages = {645--674},
  issn = {1080-7039},
  doi = {10.1080/10807030500430559},
  abstract = {In this article we present a review of the laboratory and field toxicity of herbicides to aquatic ecosystems. Single-species acute toxicity data and ( micro) mesocosm data were collated for nine herbicides. These data were used to investigate the importance of test species selection in constructing species sensitivity distributions (SSDs), and in estimating hazardous concentrations (i.e., HC5) protective for freshwater aquatic ecosystems. A lognormal model was fitted to toxicity data ( acute EC50s and chronic NOECs) and the resulting distribution used to estimate lower (95\% confidence), median (50\% confidence), and upper (5\% confidence), HC5 values. The taxonomic composition of the species assemblage used to construct the SSD does have a significant influence on the assessment of hazard and only sensitive primary producers should be included for the risk assessment of herbicides. No systematic difference in sensitivity between standard and non-standard test species was observed. Hazardous concentrations estimated using laboratory-derived acute and chronic toxicity data for sensitive freshwater primary producers were compared to the response of herbicide-stressed freshwater ecosystems using a similar exposure regime. The lower limit of the acute HC5 and the median value of the chronic HC5 were protective of adverse effects in aquatic micro/mesocosms even under a long-term exposure regime. The median HC5 estimate based on acute data was protective of adverse ecological effects in freshwater ecosystems when a pulsed or short-term exposure regime was used in the microcosm and mesocosm experiments. There was also concordance between the predictions from the effect model PERPEST and the concentrations at which clear effects started to emerge in laboratory and field studies. However, compared to the SSD concept, the PERPEST model is able to provide more information on ecological risks when a common toxicological mode of action is evaluated as it considers both recovery and indirect effects.},
  isbn = {1080-7039},
  keywords = {environmental risk assessment,ERA,herbicides,nosource,prediction,SSD}
}

@article{doi:10.1093/biomet/asx081,
  title = {Convergence of Regression-Adjusted Approximate {{Bayesian}} Computation},
  author = {Li, Wentao and Fearnhead, Paul},
  year = {2018},
  journal = {Biometrika},
  volume = {105},
  number = {2},
  pages = {asx081},
  issn = {14643510},
  doi = {10.1093/biomet/asx081},
  keywords = {Approximate Bayesian computation,Importance sampling,Local-linear regression,nosource,Partial information}
}

@article{doi:10.1111/anzs.12087,
  title = {Diagnostic Tools for Approximate {{Bayesian}} Computation Using the Coverage Property},
  author = {Prangle, D and Blum, M G B and Popovic, G and Sisson, S A},
  journal = {Australian \& New Zealand Journal of Statistics},
  volume = {56},
  number = {4},
  pages = {309--329},
  doi = {10.1111/anzs.12087},
  abstract = {Summary Approximate Bayesian computation (ABC) is an approach to sampling from an approximate posterior distribution in the presence of a computationally intractable likelihood function. A common implementation is based on simulating model, parameter and dataset triples from the prior, and then accepting as samples from the approximate posterior, those model and parameter pairs for which the corresponding dataset, or a summary of that dataset, is `close' to the observed data. Closeness is typically determined though a distance measure and a kernel scale parameter. Appropriate choice of that parameter is important in producing a good quality approximation. This paper proposes diagnostic tools for the choice of the kernel scale parameter based on assessing the coverage property, which asserts that credible intervals have the correct coverage levels in appropriately designed simulation settings. We provide theoretical results on coverage for both model and parameter inference, and adapt these into diagnostics for the ABC context. We re-analyse a study on human demographic history to determine whether the adopted posterior approximation was appropriate. Code implementing the proposed methodology is freely available in the R package abctools.},
  keywords = {likelihood-free inference,model inference,nosource,parameter inference}
}

@article{Domene2008,
  title = {Ecological Risk Assessment of Organic Waste Amendments Using the Species Sensitivity Distribution from a Soil Organisms Test Battery.},
  author = {Domene, Xavier and Ram{\'i}rez, Wilson and Mattana, Stefania and Alca{\~n}iz, Josep Maria and Andr{\'e}s, Pilar},
  year = {2008},
  month = sep,
  journal = {Environmental pollution (Barking, Essex : 1987)},
  volume = {155},
  number = {2},
  eprint = {18295946},
  eprinttype = {pubmed},
  pages = {227--36},
  issn = {1873-6424},
  doi = {10.1016/j.envpol.2007.12.001},
  abstract = {Safe amendment rates (the predicted no-effect concentration or PNEC) of seven organic wastes were estimated from the species sensitivity distribution of a battery of soil biota tests and compared with different realistic amendment scenarios (different predicted environmental concentrations or PEC). None of the wastes was expected to exert noxious effects on soil biota if applied according either to the usual maximum amendment rates in Europe or phosphorus demands of crops (below 2 tonnes DM ha(-1)). However, some of the wastes might be problematic if applied according to nitrogen demands of crops (above 2 tonnes DM ha(-1)). Ammonium content and organic matter stability of the studied wastes are the most influential determinants of the maximum amendment rates derived in this study, but not pollutant burden. This finding indicates the need to stabilize wastes prior to their reuse in soils in order to avoid short-term impacts on soil communities.},
  pmid = {18295946},
  keywords = {Animals,duplicate-citation-key,Ecological risk assessment,Ecosystem,Europe,Invertebrates,Invertebrates: metabolism,nosource,Organic wastes,Plants,Plants: growth \& development,Risk Assessment,Risk Assessment: methods,Safe amendment rates,Sewage,Soil Microbiology,Soil Pollutants,Soil Pollutants: toxicity,Species sensitivity distribution,Toxicity Tests,Waste Management}
}

@incollection{Donati-Martin2015,
  title = {In Memoriam Marc Yor - S{\'e}minaire de Probabilit{\'e}s {{XLVII}}},
  booktitle = {Lecture Notes in Mathematics},
  author = {Devroye, Luc and Letac, G{\'e}rard},
  year = {2015},
  volume = {2137},
  pages = {657},
  issn = {0075-8434},
  doi = {10.1007/978-3-319-18585-9},
  chapter = {Copulas wi},
  isbn = {978-3-319-18585-9},
  keywords = {nosource}
}

@article{donnelly1986partition,
  title = {Partition Structures, {{Polya}} Urns, the {{Ewens}} Sampling Formula, and the Ages of Alleles},
  author = {Donnelly, Peter},
  year = {1986},
  journal = {Theoretical Population Biology},
  volume = {30},
  number = {2},
  pages = {271--288},
  publisher = {Elsevier},
  issn = {10960325},
  doi = {10.1016/0040-5809(86)90037-7},
  abstract = {It has recently been shown that the Ewens sampling formula may be generated by a Polya-like urn model. A genealogical proof of this result equates the labelling of balls in the urn to the partition by age of alleles in the sample. This urn construction is shown to be equivalent to the construction of Kingman (Proc. Roy. Soc. London Ser. A 361 (1978), 1-20) using a Poisson-Dirichlet "paintbox" and as a consequence, the partition by ages is seen to be equivalent to the size biased permutation of the Poisson-Dirichlet distribution. This approach unifies and extends many results on ages of alleles, the Polya urn, and the Poisson-Dirichlet distribution. Furthermore the Ewens sampling formula is characterized as being the only partition structure which may be generated by an urn-like mechanism. ?? 1986.},
  isbn = {0040-5809 (Print) 0040-5809 (Linking)},
  pmid = {3787504},
  keywords = {nosource}
}

@article{donnelly1993asymptotic,
  title = {On the Asymptotic Distribution of Large Prime Factors},
  author = {Donnelly, P. and Grimmett, G.},
  year = {1993},
  journal = {Journal of the London Mathematical Society},
  volume = {s2-47},
  number = {3},
  pages = {395--404},
  publisher = {Oxford University Press},
  issn = {0024-6107},
  doi = {10.1112/jlms/s2-47.3.395},
  abstract = {A random integer N, drawn uniformly from the set (1,2,...,n), has a prime factorization of the form N = \{alpha\}1 \{alpha\}2...\{alpha\}M where \{alpha\}1 [\&ge;] \{alpha\}2 [\&ge;] ... [\&ge;] \{alpha\}M. We establish the asymptotic distribution, as n [-{$>$}] \{infty\}, of the vector A(n) = (log \{alpha\}1,/log N: i: [\&ge;] 1) in a transparent manner. By randomly re-ordering the components of A(n), in a size-biased manner, we obtain a new vector B(n) whose asymptotic distribution is the GEM distribution with parameter 1; this is a distribution on the infinite-dimensional simplex of vectors (x1,x2,...) having non-negative components with unit sum. Using a standard continuity argument, this entails the weak convergence of A(n) to the corresponding Poisson-Dirichlet distribution on this simplex; this result was obtained by Billingsley [3].},
  keywords = {nosource}
}

@article{donnetNonparametricBayesianEstimation2020,
  title = {Nonparametric {{Bayesian Estimation}} for {{Multivariate Hawkes Processes}}},
  author = {Donnet, Sophie and Rivoirard, Vincent and Rousseau, Judith},
  year = {2020},
  journal = {The Annals of Statistics},
  volume = {48},
  number = {5},
  eprint = {27028719},
  eprinttype = {jstor},
  pages = {2698--2727},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  urldate = {2024-11-12},
  abstract = {This paper studies nonparametric estimation of parameters of multivariate Hawkes processes. We consider the Bayesian setting and derive posterior concentration rates. First, rates are derived for {$\mathbb{L}_1$}-metrics for stochastic intensities of the Hawkes process. We then deduce rates for the {$\mathbb{L}_1$}-norm of interactions functions of the process. Our results are exemplified by using priors based on piecewise constant functions, with regular or random partitions and priors based on mixtures of Betas distributions. We also present a simulation study to illustrate our results and to study empirically the inference on functional connectivity graphs of neurons},
  file = {/home/gkonkamking/pCloudDrive/papers/Donnet et al. - 2020 - Nonparametric Bayesian Estimation for Multivariate Hawkes Processes.pdf}
}

@article{donofrioJacobiDiffusionProcess2018,
  title = {The {{Jacobi}} Diffusion Process as a Neuronal Model},
  author = {D'Onofrio, Giuseppe and Tamborrino, Massimiliano and Lansky, Petr},
  year = {2018},
  month = oct,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {28},
  number = {10},
  pages = {103119},
  publisher = {AIP Publishing LLC AIP Publishing},
  issn = {1054-1500},
  doi = {10.1063/1.5051494},
  urldate = {2022-06-30},
  abstract = {The Jacobi process is a stochastic diffusion characterized by a linear drift and a special form of multiplicative noise which keeps the process confined between two boundaries. One example of such a process can be obtained as the diffusion limit of the Stein's model of membrane depolarization which includes both excitatory and inhibitory reversal potentials. The reversal potentials create the two boundaries between which the process is confined. Solving the first-passage-time problem for the Jacobi process, we found closed-form expressions for mean, variance, and third moment that are easy to implement numerically. The first two moments are used here to determine the role played by the parameters of the neuronal model; namely, the effect of multiplicative noise on the output of the Jacobi neuronal model with input-dependent parameters is examined in detail and compared with the properties of the generic Jacobi diffusion. It appears that the dependence of the model parameters on the rate of inhibition turns out to be of primary importance to observe a change in the slope of the response curves. This dependence also affects the variability of the output as reflected by the coefficient of variation. It often takes values larger than one, and it is not always a monotonic function in dependency on the rate of excitation.},
  copyright = {{\copyright} 2018 Author(s).},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/D’Onofrio et al_2018_The Jacobi diffusion process as a neuronal model.pdf}
}

@article{Donoho:1998p1331,
  title = {Minimax Estimation via Wavelet Shrinkage},
  author = {Donoho, David L and Johnstone, Iain M},
  year = {1998},
  journal = {The Annals of Statistics},
  volume = {26},
  number = {3},
  eprint = {120061},
  eprinttype = {jstor},
  pages = {879--921},
  abstract = {We attempt to recover an unknown function from noisy, sampled data. Using orthonormal bases of compactly supported wavelets, we develop a nonlinear method which works in the wavelet domain by simple nonlinear shrinkage of the empirical wavelet coefficients. The shrinkage can be tuned to be nearly minimax over any member of a wide range of Triebel- and Besov-type smoothness constraints and asymptotically minimax over Besov bodies with p {\^a}‰¤ q. Linear estimates cannot achieve even the minimax rates over Triebel and Besov classes with p {$<$} 2, so the method can significantly outperform every linear method (e.g., kernel, smoothing spline, sieve) in a minimax sense. Variants of our method based on simple threshold nonlinear estimators are nearly minimax. Our method possesses the interpretation of spatial adaptivity; it reconstructs using a kernel which may vary in shape and bandwidth from point to point, depending on the data. Least favorable distributions for certain of the Triebel and Besov scales generate objects with sparse wavelet transforms. Many real objects have similarly sparse transforms, which suggests that these minimax results are relevant for practical problems. Sequels to this paper, which was first drafted in November 1990, discuss practical implementation, spatial adaptation properties, universal near minimaxity and applications to inverse problems.},
  isbn = {00905364},
  keywords = {nosource}
}

@article{doob1949application,
  title = {Application of the Theory of Martingales},
  author = {Doob, Joseph L},
  year = {1949},
  journal = {Le calcul des probabilites et ses applications},
  pages = {23--27},
  publisher = {Colloques Internationaux du Centre National de la Recherche Scientifique Paris},
  keywords = {duplicate-citation-key,nosource}
}

@article{doob1949application,
  title = {Applications of the Theory of Martingales},
  author = {Doob, J L},
  year = {1948},
  journal = {Colloques Internationaux du C.N.R.S},
  pages = {22--28},
  publisher = {Colloques Internationaux du Centre National de la Recherche Scientifique Paris},
  keywords = {duplicate-citation-key,nosource}
}

@article{douc2011consistency,
  title = {Consistency of the Maximum Likelihood Estimator for General Hidden {{Markov}} Models},
  author = {Douc, Randal and Moulines, Eric and Olsson, Jimmy and Van Handel, Ramon},
  year = {2011},
  journal = {the Annals of Statistics},
  volume = {39},
  number = {1},
  pages = {474--513},
  publisher = {Institute of Mathematical Statistics},
  file = {/home/gkonkamking/Zotero/storage/2JQEINR7/Douc et al. - 2011 - Consistency of the maximum likelihood estimator fo.pdf}
}

@article{doucetEfficientBlockSampling2006,
  title = {Efficient {{Block Sampling Strategies}} for {{Sequential Monte Carlo Methods}}},
  author = {Doucet, Arnaud and Briers, Mark and S{\'e}n{\'e}cal, St{\'e}phane},
  year = {2006},
  month = sep,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {15},
  number = {3},
  pages = {693--711},
  publisher = {ASA Website},
  issn = {1061-8600},
  doi = {10.1198/106186006X142744},
  urldate = {2024-12-18},
  abstract = {Sequential Monte Carlo (SMC) methods are a powerful set of simulation-based techniques for sampling sequentially from a sequence of complex probability distributions. These methods rely on a combination of importance sampling and resampling techniques. In a Markov chain Monte Carlo (MCMC) framework, block sampling strategies often perform much better than algorithms based on one-at-a-time sampling strategies if ``good'' proposal distributions to update blocks of variables can be designed. In an SMC framework, standard algorithms sequentially sample the variables one at a time whereas, like MCMC, the efficiency of algorithms could be improved significantly by using block sampling strategies. Unfortunately, a direct implementation of such strategies is impossible as it requires the knowledge of integrals which do not admit closed-form expressions. This article introduces a new methodology which by-passes this problem and is a natural extension of standard SMC methods. Applications to several sequential Bayesian inference problems demonstrate these methods.},
  keywords = {Block sequential Monte Carlo,Importance sampling,Markov chain Monte Carlo,Optimal filtering,Particle filtering,State-space models},
  file = {/home/gkonkamking/pCloudDrive/papers/Doucet et al. - 2006 - Efficient Block Sampling Strategies for Sequential Monte Carlo Methods.pdf}
}

@book{douglas1988nonlinear,
  title = {Nonlinear Regression Analysis and Its Applications},
  author = {Bates, Douglas M. and Watts, {\relax DG}},
  year = {1988},
  publisher = {John Wiley \& Sons},
  address = {New York},
  issn = {01621459},
  doi = {10.2307/2289810},
  abstract = {Wiley-Interscience Paperback SeriesThe Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists."The authors have put together an extraordinary presentation of concepts and methods concerning the use and analysis of nonlinear regression models . . . highly recommend[ed] . . . for anyone needing to use and/or understand issues concerning the analysis of nonlinear regression models."---Technometrics"[This book] provides a good balance of relevant theory and application with many examples . . . [and it] provides the most balanced approach to theory and application appropriate for a first course in nonlinear regression modeling for graduate statistics students."---Mathematical Reviews"[This book] joins a distinguished list of publications with a reputation for balancing technical rigor with readability, and theory with application. [It] upholds tradition . . . [and is] a worthwhile reference for the marketing researcher with a serious interest in linear models."---Journal of Marketing ResearchThis book offers a balanced presentation of the theoretical, practical, and computational aspects of nonlinear regression and provides background material on linear regression, including the geometrical development for linear and nonlinear least squares. The authors employ real data sets throughout, and their extensive use of geometric constructs and continuing examples makes the progression of ideas appear very natural. The book also includes pseudocode for computing algorithms.},
  isbn = {0-471-81643-4},
  pmid = {121},
  keywords = {nosource}
}

@article{Dowse2013,
  title = {Risk Assessment Using the Species Sensitivity Distribution Method: {{Data}} Quality versus Data Quantity},
  author = {Dowse, Renee and Tang, Doudou and Palmer, Carolyn G. and Kefford, Ben J.},
  year = {2013},
  month = jun,
  journal = {Environmental Toxicology and Chemistry},
  volume = {32},
  number = {6},
  eprint = {23440771},
  eprinttype = {pubmed},
  pages = {1360--1369},
  issn = {07307268},
  doi = {10.1002/etc.2190},
  abstract = {Species sensitivity distributions (SSDs) are cumulative distributions of measures of species sensitivity to a stressor or toxicant, and are used to estimate concentrations that will protect p\% of a community (PCp ). There is conflict between the desire to use high-quality sensitivity data in SSDs, and to construct them with a large number of species forming a representative sample. Trade-offs between data quality and quantity were investigated using the effects of increasing salinity on the macroinvertebrate community from the Hunter River catchment, in eastern Australia. Five SSDs were constructed, representing five points along a continuum of data quality versus data quantity and representativeness. This continuum was achieved by the various inclusion/exclusion of censored data, nonmodeled data, and extrapolation from related species. Protective concentrations were estimated using the Burr type III distribution, Kaplan-Meier survival function, and two Bayesian statistical models. The dominant taxonomic group was the prime determinant of protective concentrations, with an increase in PC95 values resulting from a decrease in the proportion of Ephemeropteran species included in the SSD. In addition, decreases in data quantity in a SSD decreased community representativeness. The authors suggest, at least for salinity, that the inclusion of right censored data provides a more representative sample of species that reflects the natural biotic assemblage of an area to be protected, and will therefore improve risk assessment.},
  isbn = {1552-8618},
  pmid = {23440771},
  keywords = {Ecological risk assessment,Freshwater invertebrate,Hunter River,Major ions,nosource,Salinity,Water quality guideline}
}

@article{DPpackage,
  title = {{{DPpackage}}: {{Bayesian}} Semi- and Nonparametric Modeling in r},
  author = {Jara, A and Hanson, T E and Quintana, Fernando A and M{\"u}ller, P and Rosner, G L},
  year = {2011},
  journal = {Journal of Statistical Software},
  volume = {40},
  number = {5},
  pages = {1},
  publisher = {NIH Public Access},
  issn = {1548-7660},
  abstract = {Data analysis sometimes requires the relaxation of parametric assumptions in order to gain modeling flexibility and robustness against mis-specification of the probability model. In the Bayesian context, this is accomplished by placing a prior distribution on a function space, such as the space of all probability distributions or the space of all regression functions. Unfortunately, posterior distributions ranging over function spaces are highly complex and hence sampling methods play a key role. This paper provides an introduction to a simple, yet comprehensive, set of programs for the implementation of some Bayesian nonparametric and semiparametric models in R, DPpackage. Currently, DPpackage includes models for marginal and conditional density estimation, receiver operating characteristic curve analysis, interval-censored data, binary regression data, item response data, longitudinal and clustered data using generalized linear mixed models, and regression data using generalized additive models. The package also contains functions to compute pseudo-Bayes factors for model comparison and for eliciting the precision parameter of the Dirichlet process prior, and a general purpose Metropolis sampling algorithm. To maximize computational efficiency, the actual sampling for each model is carried out using compiled C, C++ or Fortran code.},
  isbn = {1548-7660},
  keywords = {nosource}
}

@article{duan2007generalized,
  title = {Generalized Spatial Dirichlet Process Models},
  author = {Duan, Jason A and Guindani, Michele and Gelfand, Alan E.},
  year = {2013},
  journal = {Biometrika},
  volume = {94},
  number = {4},
  pages = {809--825},
  publisher = {Biometrika Trust},
  doi = {10.1093/biomet/asmO71},
  keywords = {duplicate-citation-key,nosource}
}

@article{Duboudin2009a,
  title = {Effects of Data Manipulation and Statistical Methods on Species Sensitivity Distributions},
  author = {Duboudin, C and Ciffroy, P and Magaud, H},
  year = {2009},
  journal = {{\dots} toxicology and chemistry},
  volume = {23},
  number = {2},
  pages = {489--499},
  isbn = {1552-8618},
  keywords = {nosource,risk assessment,species sensitivity distribution,weighted bootstrapping}
}

@article{Ducrot2010,
  title = {Development of Partial Life-Cycle Experiments to Assess the Effects of Endocrine Disruptors on the Freshwater Gastropod {{Lymnaea}} Stagnalis: {{A}} Case-Study with Vinclozolin},
  author = {Ducrot, Virginie and {Teixeira-Alves}, Micka{\"e}l and Lopes, Christelle and {Delignette-Muller}, Marie Laure and Charles, Sandrine and Lagadic, Laurent},
  year = {2010},
  journal = {Ecotoxicology},
  volume = {19},
  number = {7},
  pages = {1312--1321},
  publisher = {Springer},
  issn = {09639292},
  doi = {10.1007/s10646-010-0518-8},
  abstract = {Long-term effects of endocrine disruptors (EDs) on aquatic invertebrates remain difficult to assess, mainly due to the lack of appropriate sensitive toxicity test methods and relevant data analysis procedures. This study aimed at identifying windows of sensitivity to EDs along the life-cycle of the freshwater snail Lymnaea stagnalis, a candidate species for the development of forthcoming test guidelines. Juveniles, sub-adults, young adults and adults were exposed for 21 days to the fungicide vinclozolin (VZ). Survival, growth, onset of reproduction, fertility and fecundity were monitored weekly. Data were analyzed using standard statistical analysis procedures and mixed-effect models. No deleterious effect on survival and growth occurred in snails exposed to VZ at environmentally relevant concentrations. A significant impairment of the male function occurred in young adults, leading to infertility at concentrations exceeding 0.025 {$\mu$}g/L. Furthermore, fecundity was impaired in adults exposed to concentrations exceeding 25 {$\mu$}g/L. Biological responses depended on VZ concentration, exposure duration and on their interaction, leading to complex response patterns. The use of a standard statistical approach to analyze those data led to underestimation of VZ effects on reproduction, whereas effects could reliably be analyzed by mixed-effect models. L. stagnalis may be among the most sensitive invertebrate species to VZ, a 21-day reproduction test allowing the detection of deleterious effects at environmentally relevant concentrations of the fungicide. These results thus reinforce the relevance of L. stagnalis as a good candidate species for the development of guidelines devoted to the risk assessment of EDs.},
  isbn = {0963-9292},
  pmid = {20623335},
  keywords = {Anti-androgens,Ecological risk assessment,Gastropods,Mixed-effect models,nosource,Partial life-cycle tests,Reproduction}
}

@article{Ducrot2010a,
  title = {From Individual to Population Level Effects of Toxicants in the Tubicifid {\textbackslash}textit\{\vphantom\}{{Branchiura}} Sowerbyi\vphantom\{\} Using Threshold Effect Models in a Bayesian Framework},
  author = {Ducrot, Virginie and Billoir, Elise and P{\'e}ry, Alexandre R R and Garric, Jeanne and Charles, Sandrine},
  year = {2010},
  journal = {Environmental Science \& Technology},
  volume = {44(9)},
  number = {9},
  pages = {3566--3571},
  keywords = {nosource}
}

@article{DUNBAR1992469,
  title = {Neocortex Size as a Constraint on Group Size in Primates},
  author = {Dunbar, R.I.M.},
  year = {1992},
  journal = {Journal of Human Evolution},
  volume = {22},
  number = {6},
  pages = {469--493},
  issn = {0047-2484},
  doi = {10.1016/0047-2484(92)90081-j},
  abstract = {Two general kinds of theory (one ecological and one social) have been advanced to explain the fact that primates have larger brains and greater congnitive abilities than other animals. Data on neocortex volume, group size and a number of behavioural ecology variables are used to test between the various theories. Group size is found to be a function of relative neocortical volume, but the ecological variables are not. This is interpreted as evidence in favour of the social intellect theory and against the ecological theories. It is suggested that the number of neocortical neurons limits the organism's information-processing capacity and that this then limits the number of relationships that an individual can monitor simultaneously. When a group's size exceeds this limit, it becomes unstable and begins to fragment. This then places an upper limit on the size of groups which any given species can maintain as cohesive social units through time. The data suggest that the information overload occurs in terms of the structure of relationships within tightly bonded grooming cliques rather than in terms of the total number of dyads within the group as a whole that an individual has to monitor. It thus appears that, among primates, large groups are created by welding together sets of smaller grooming cliques. One implication of these results is that, since the actual group size will be determined by the ecological characteristics of the habitat in any given case, species will only be able to invade habitats that require larger groups than their current limit if they evolve larger neocortices.},
  keywords = {behavioural ecology,body size,brain size,grooming,social intellect}
}

@article{Dunson2006,
  title = {Bayesian Dynamic Modeling of Latent Trait Distributions},
  author = {Dunson, David B.},
  year = {2006},
  journal = {Biostatistics},
  volume = {7},
  number = {4},
  pages = {551--568},
  issn = {14654644},
  doi = {10.1093/biostatistics/kxj025},
  abstract = {Studies of latent traits often collect data for multiple items measuring different aspects of the trait. For such data, it is common to consider models in which the different items are manifestations of a normal latent variable, which depends on covariates through a linear regression model. This article proposes a flexible Bayesian alternative in which the unknown latent variable density can change dynamically in location and shape across levels of a predictor. Scale mixtures of underlying normals are used in order to model flexibly the measurement errors and allow mixed categorical and continuous scales. A dynamic mixture of Dirichlet processes is used to characterize the latent response distributions. Posterior computation proceeds via a Markov chain Monte Carlo algorithm, with predictive densities used as a basis for inferences and evaluation of model fit. The methods are illustrated using data from a study of DNA damage in response to oxidative stress.},
  isbn = {1465-4644},
  pmid = {16488893},
  keywords = {Dynamic Dirichlet process,Factor analysis,Hierarchical model,Latent variables,Measurement error,nosource,Random effect,Surrogate data}
}

@article{dunson2007bayesian,
  title = {Bayesian Density Regression},
  author = {Wang, Lianming and Dunson, David B.},
  year = {2011},
  journal = {Biometrika},
  volume = {98},
  number = {2},
  pages = {537--551},
  publisher = {Wiley Online Library},
  issn = {00063444},
  doi = {10.1093/biomet/asr025},
  abstract = {Density regression models allow the conditional distribution of the response given predictors to change flexibly over the predictor space. Such models are much more flexible than nonparametric mean regression models with nonparametric residual distributions, and are well supported in many applications. A rich variety of Bayesian methods have been proposed for density regression, but it is not clear whether such priors have full support so that any true data-generating model can be accurately approximated. This article develops a new class of density regression models that incorporate stochastic-ordering constraints which are natural when a response tends to increase or decrease monotonely with a predictor. Theory is developed showing large support. Methods are developed for hypothesis testing, with posterior computation relying on a simple Gibbs sampler. Frequentist properties are illustrated in a simulation study, and an epidemiology application is considered.},
  pmid = {22822259},
  keywords = {Conditional density estimation,Dependent Dirichlet process,duplicate-citation-key,Hypothesis test,Isotonic regression,Nonparametric Bayes,nosource,Quantile regression,Stochastic ordering}
}

@article{dunson2007bayesian,
  title = {Bayesian Density Regression},
  author = {Dunson, D B and Pillai, N and Park, J H},
  year = {2007},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {69},
  number = {2},
  pages = {163--183},
  publisher = {Wiley Online Library},
  keywords = {duplicate-citation-key,nosource}
}

@article{dunson2008bayesian,
  title = {Bayesian Nonparametric Inference on Stochastic Ordering},
  author = {Dunson, David B. and Peddada, Shyamal D.},
  year = {2008},
  journal = {Biometrika},
  volume = {95},
  number = {4},
  pages = {859--874},
  publisher = {Biometrika Trust},
  issn = {00063444},
  doi = {10.1093/biomet/asn043},
  abstract = {We consider Bayesian inference about collections of unknown distributions subject to a partial stochastic ordering. To address problems in testing of equalities between groups and estimation of group-specific distributions, we propose classes of restricted dependent Dirichlet process priors. These priors have full support in the space of stochastically ordered distributions, and can be used for collections of unknown mixture distributions to obtain a flexible class of mixture models. Theoretical properties are discussed, efficient methods are developed for posterior computation using Markov chain Monte Carlo simulation and the methods are illustrated using data from a study of DNA damage and repair.},
  keywords = {Dependent Dirichlet process,Hypothesis testing,Mixture model,Nonparametric Bayes inference,nosource,Order restriction}
}

@article{dunson2008kernel,
  title = {Kernel Stick-Breaking Processes},
  author = {Dunson, David B. and Park, J H},
  year = {2008},
  journal = {Biometrika},
  volume = {95},
  number = {2},
  pages = {307},
  publisher = {Biometrika Trust},
  issn = {0006-3444},
  keywords = {nosource}
}

@article{dunson2009nonparametric,
  title = {Nonparametric Bayes Modeling of Multivariate Categorical Data},
  author = {Dunson, David B. and Xing, Chuanhua},
  year = {2009},
  journal = {Journal of the American Statistical Association},
  volume = {104},
  number = {487},
  pages = {1042--1051},
  issn = {0162-1459},
  doi = {10.1198/jasa.2009.tm08439},
  abstract = {Modeling of multivariate unordered categorical (nominal) data is a challenging problem, particularly in high dimensions and cases in which one wishes to avoid strong assumptions about the dependence structure. Commonly used approaches rely on the incorporation of latent Gaussian random variables or parametric latent class models. The goal of this article is to develop a nonparametric Bayes approach, which defines a prior with full support on the space of distributions for multiple unordered categorical variables. This support condition ensures that we are not restricting the dependence structure a priori. We show this can be accomplished through a Dirichlet process mixture of product multinomial distributions, which is also a convenient form for posterior computation. Methods for nonparametric testing of violations of independence are proposed, and the methods are applied to model positional dependence within transcription factor binding motifs.},
  pmid = {23606777},
  keywords = {bayes factor,dirichlet process,duplicate-citation-key,goodness-of-fit test,latent class,mixture model,motif data,nosource,product multinomial,unordered categorical}
}

@article{dunson2009nonparametric,
  title = {Nonparametric \{\vphantom\}{{B}}\vphantom\{\}ayes Modeling of Multivariate Categorical Data},
  author = {Dunson, David B and Xing, Chuanhua},
  year = {2009},
  journal = {Journal of the American Statistical Association},
  volume = {104},
  number = {487},
  keywords = {duplicate-citation-key,nosource}
}

@incollection{dunson2010nonparametric,
  title = {In {\textbackslash}citet\{hjort2010bayesian\}},
  author = {Walker, S G and Dunson, David B.},
  year = {2010},
  pages = {223--273},
  publisher = {Cambridge University Press},
  chapter = {Bayesian n},
  keywords = {nosource}
}

@article{durante2012locally,
  title = {Locally Adaptive {{Bayesian}} Covariance Regression},
  author = {Durante, Daniele and Scarpa, Bruno and Dunson, David B.},
  year = {2012},
  journal = {Time},
  eprint = {1210.2022v1},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1210.2022v1},
  keywords = {bayesian nonparametrics,locally varying smoothness,long-range dependence,multivariate time series,nested gaussian process,nosource,stochastic volatility}
}

@inproceedings{duval2022synthesis,
  title = {Synthesis of 4 Years of Evaluation of Natural Substances in the Control of Apple Scab with a View to Reduce Copper Doses},
  booktitle = {{{XXXI}} International Horticultural Congress ({{IHC2022}}): {{International}} Symposium on Sustainable Control of Pests and Diseases 1378},
  author = {{Duval-Chaboussou}, A and Leblois, A},
  year = {2022},
  pages = {97--104}
}

@article{dyerComparisonSpeciesSensitivity2008,
  title = {Comparison of {{Species Sensitivity Distributions Derived}} from {{Interspecies Correlation Models}} to {{Distributions}} Used to {{Derive Water Quality Criteria}}},
  author = {Dyer, Scott D. and Versteeg, Donald J. and Belanger, Scott E. and Chaney, Joel G. and Raimondo, Sandy and Barron, Mace G.},
  year = {2008},
  month = apr,
  journal = {Environmental Science \& Technology},
  volume = {42},
  number = {8},
  pages = {3076--3083},
  publisher = {American Chemical Society},
  issn = {0013-936X},
  doi = {10.1021/es702302e},
  urldate = {2024-10-28},
  abstract = {Species sensitivity distributions (SSD) require a large number of measured toxicity values to define a hazard level protective of multiple species. This investigation comprehensively evaluated the accuracy of SSDs generated from toxicity values predicted from interspecies correlation estimation (ICE) models. ICE models are log--log correlations of multiple chemical toxicity values for a pair of species that allow the toxicity of multiple species to be predicted from a single measured acute toxicity value for a surrogate species. ICE SSDs were generated using four surrogate species (fathead minnow, Pimephales promelas; rainbow trout, Oncorhynchus mykiss; sheepshead minnow, Cyprinodon varigatus; and water flea, Daphnia magna). ICE-based hazard concentrations (HC5s) from the 5th percentile of the log--logistic distribution of toxicity values were compared to HC5s determined from the acute toxicity of 55 chemicals from the United States Environmental Protection Agency Ambient Water Quality Criteria (AWQC). Measured fish and invertebrate acute toxicity data and HC5s from the AWQC data sets were compared to ICE-based HC5s. Surrogate species choice was found to be an important consideration in developing predictive HC5s. These results illustrated that fish predict fish better than invertebrates and D. magna predicted invertebrates better than most fish. For example, a mixed model of predicted fish and invertebrates from fathead minnow and D. magna as surrogate species provided predictive relationships with an average factor of 3.0 ({\textpm}6.7) over 7 orders of toxic magnitude and several chemical classes (HC5predicted/HC5measured). The application of ICE models is recommended as a valid approach for generating SSDs and hazard concentrations for chemicals with limited toxicity data.},
  file = {/home/gkonkamking/Zotero/storage/FM5BWSE9/Dyer et al. - 2008 - Comparison of Species Sensitivity Distributions Derived from Interspecies Correlation Models to Dist.pdf}
}

@article{ebersoleManyLabs32016,
  title = {Many {{Labs}} 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
  shorttitle = {Many {{Labs}} 3},
  author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and {Joy-Gaba}, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and {van Allen}, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
  year = {2016},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {68--82},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2015.10.012},
  urldate = {2020-04-14},
  abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences---conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
  langid = {english},
  keywords = {Cognitive psychology,Individual differences,Participant pool,Replication,Sampling effects,Situational effects,Social psychology},
  file = {/home/gkonkamking/Zotero/storage/2FAXGEHQ/Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf}
}

@incollection{ECHA2008,
  title = {Characterisation of Dose Concentration-Response for Environment},
  booktitle = {Guidance on Information Requirements and Chemical Safety Assessment},
  author = {{ECHA}},
  year = {2008},
  number = {May},
  publisher = {European Chemicals Agency},
  address = {Helsinki},
  chapter = {R.10},
  keywords = {nosource}
}

@techreport{ECHA2008tech,
  title = {Guidance on Information Requirements and Chemical Safety Assessment},
  author = {{ECHA}},
  year = {2008},
  number = {May},
  address = {Helsinki},
  institution = {European Chemicals Agency},
  chapter = {R.10 Chara},
  keywords = {nosource}
}

@techreport{ECHAEuropeanChemicalAgency2012,
  title = {Uncertainty Analysis},
  booktitle = {Guidance on Information Requirements and Chemical Safety Assessment},
  author = {Analysis, Uncertainty},
  year = {1988},
  month = nov,
  number = {Moffat},
  pages = {2--7},
  chapter = {R.19},
  keywords = {nosource}
}

@techreport{ecotox_usepa,
  title = {{{ECOTOX}} Database},
  address = {Duluth, Minnesota},
  institution = {US Agency Environmental Protection},
  keywords = {nosource}
}

@incollection{efron1993introduction,
  title = {An Introduction to the Bootstrap},
  booktitle = {Monographs on Statistics and Applied Probability},
  author = {Efron, B., \& Tibshirani, R. J.},
  year = {1994},
  volume = {57},
  publisher = {CRC press},
  isbn = {978-0-412-04231-7},
  keywords = {nosource}
}

@article{efron2001empirical,
  title = {Empirical {{Bayes}} Analysis of a Microarray Experiment},
  author = {Efron, Bradley and Tibshirani, Robert and Storey, John D and Tusher, Virginia},
  year = {2001},
  journal = {Journal of the American statistical association},
  volume = {96},
  number = {456},
  pages = {1151--1160},
  publisher = {Taylor \& Francis},
  doi = {10.1198/016214501753382129}
}

@article{efsapanelonplantprotectionproductsandtheirresiduespprScientificOpinionState2018,
  title = {Scientific {{Opinion}} on the State of the Art of {{Toxicokinetic}}/{{Toxicodynamic}} ({{TKTD}}) Effect Models for Regulatory Risk Assessment of Pesticides for Aquatic Organisms},
  author = {{EFSA Panel on Plant Protection Products and their Residues (PPR)} and Ockleford, Colin and Adriaanse, Paulien and Berny, Philippe and Brock, Theodorus and Duquesne, Sabine and Grilli, Sandro and {Hernandez-Jerez}, Antonio F and Bennekou, Susanne Hougaard and Klein, Michael and Kuhl, Thomas and Laskowski, Ryszard and Machera, Kyriaki and Pelkonen, Olavi and Pieper, Silvia and Smith, Robert H and Stemmer, Michael and Sundh, Ingvar and Tiktak, Aaldrik and Topping, Christopher J. and Wolterink, Gerrit and Cedergreen, Nina and Charles, Sandrine and Focks, Andreas and Reed, Melissa and Arena, Maria and Ippolito, Alessio and Byers, Harry and Teodorovic, Ivana},
  year = {2018},
  journal = {EFSA Journal},
  volume = {16},
  number = {8},
  pages = {e05377},
  issn = {1831-4732},
  doi = {10.2903/j.efsa.2018.5377},
  urldate = {2021-11-17},
  abstract = {Following a request from EFSA, the Panel on Plant Protection Products and their Residues (PPR) developed an opinion on the state of the art of Toxicokinetic/Toxicodynamic (TKTD) models and their use in prospective environmental risk assessment (ERA) for pesticides and aquatic organisms. TKTD models are species- and compound-specific and can be used to predict (sub)lethal effects of pesticides under untested (time-variable) exposure conditions. Three different types of TKTD models are described, viz., (i) the `General Unified Threshold models of Survival' (GUTS), (ii) those based on the Dynamic Energy Budget theory (DEBtox models), and (iii) models for primary producers. All these TKTD models follow the principle that the processes influencing internal exposure of an organism, (TK), are separated from the processes that lead to damage and effects/mortality (TD). GUTS models can be used to predict survival rate under untested exposure conditions. DEBtox models explore the effects on growth and reproduction of toxicants over time, even over the entire life cycle. TKTD model for primary producers and pesticides have been developed for algae, Lemna and Myriophyllum. For all TKTD model calibration, both toxicity data on standard test species and/or additional species can be used. For validation, substance and species-specific data sets from independent refined-exposure experiments are required. Based on the current state of the art (e.g. lack of documented and evaluated examples), the DEBtox modelling approach is currently limited to research applications. However, its great potential for future use in prospective ERA for pesticides is recognised. The GUTS model and the Lemna model are considered ready to be used in risk assessment.},
  langid = {english},
  keywords = {aquatic organisms,model calibration,model evaluation,model validation,prospective risk assessment,time-variable exposure,Toxicokinetic/Toxicodynamic models},
  file = {/home/gkonkamking/Zotero/storage/IJQEVAC8/EFSA Panel on Plant Protection Products and their Residues (PPR) et al. - 2018 - Scientific Opinion on the state of the art of Toxi.pdf}
}

@article{egozcueCompositionalDataSample2019,
  title = {Compositional Data: The Sample Space and Its Structure},
  shorttitle = {Compositional Data},
  author = {Egozcue, Juan Jos{\'e} and {Pawlowsky-Glahn}, Vera},
  year = {2019},
  month = sep,
  journal = {TEST},
  volume = {28},
  number = {3},
  pages = {599--638},
  issn = {1863-8260},
  doi = {10.1007/s11749-019-00670-6},
  urldate = {2024-06-24},
  abstract = {The log-ratio approach to compositional data (CoDa) analysis has now entered a mature phase. The principles and statistical tools introduced by J. Aitchison in the eighties have proven successful in solving a number of applied problems. The algebraic--geometric structure of the sample space, tailored to those principles, was developed at the beginning of the millennium. Two main ideas completed the J. Aitchison's seminal work: the conception of compositions as equivalence classes of proportional vectors, and their representation in the simplex endowed with an interpretable Euclidean structure. These achievements allowed the representation of compositions in meaningful coordinates (preferably Cartesian), as well as orthogonal projections compatible with the Aitchison distance introduced two decades before. These ideas and concepts are reviewed up to the normal distribution on the simplex and the associated central limit theorem. Exploratory tools, specifically designed for CoDa, are also reviewed. To illustrate the adequacy and interpretability of the sample space structure, a new inequality index, based on the Aitchison norm, is proposed. Most concepts are illustrated with an example of mean household gross income per capita in Spain.},
  langid = {english},
  keywords = {62-02,62-07,Aitchison geometry,Biplot,Dendrogram,Equivalence class,Euclidean space,Household income,Isometric log-ratio coordinates,Logistic-normal,Normal distribution on the simplex,Principal balances,Principal components,Simplex},
  file = {/home/gkonkamking/pCloudDrive/papers/Egozcue_Pawlowsky-Glahn_2019_Compositional data.pdf}
}

@article{enard2014genome,
  title = {Genome-Wide Signals of Positive Selection in Human Evolution},
  author = {Enard, David and Messer, Philipp W and Petrov, Dmitri A},
  year = {2014},
  journal = {Genome research},
  volume = {24},
  number = {6},
  pages = {885--895},
  publisher = {Cold Spring Harbor Lab},
  issn = {15495469},
  doi = {10.1101/gr.164822.113},
  abstract = {The role of positive selection in human evolution remains controversial. On the one hand, scans for positive selection have identified hundreds of candidate loci, and the genome-wide patterns of polymorphism show signatures consistent with frequent positive selection. On the other hand, recent studies have argued that many of the candidate loci are false positives and that most genome-wide signatures of adaptation are in fact due to reduction of neutral diversity by linked deleterious mutations, known as background selection. Here we analyze human polymorphism data from the 1000 Genomes Project and detect signatures of positive selection once we correct for the effects of background selection. We show that levels of neutral polymorphism are lower near amino acid substitutions, with the strongest reduction observed specifically near functionally consequential amino acid substitutions. Furthermore, amino acid substitutions are associ-ated with signatures of recent adaptation that should not be generated by background selection, such as unusually long and frequent haplotypes and specific distortions in the site frequency spectrum. We use forward simulations to argue that the observed signatures require a high rate of strongly adaptive substitutions near amino acid changes. We further demonstrate that the observed signatures of positive selection correlate better with the presence of regulatory sequences, as predicted by the ENCODE Project Consortium, than with the positions of amino acid substitutions. Our results suggest that adaptation was frequent in human evolution and provide support for the hypothesis of King and Wilson that adaptive divergence is primarily driven by regulatory changes. [Supplemental material is available for this article.] The rate and patterns of positive selection are of fundamental interest for the study of human evolution. Population genomic studies should, in principle, allow us to quantify positive selec-tion from its expected signatures in sequence polymorphism and divergence data. Surprisingly, despite the sequencing of thou-sands of human genomes (The 1000 Genomes Project Consor-tium 2012) and the availability of whole-genome sequences of closely related species, the extent to which adaptation has left identifiable signatures in the patterns of polymorphism in the human genome remains highly controversial (Akey 2009; Hernandez et al. 2011). On the one hand, recent studies have identified a large number of loci showing signatures of recent selective sweeps (Voight et al. 2006; Sabeti et al. 2007; Williamson et al. 2007; Pickrell et al. 2009; Grossman et al. 2013), and McDonald-Kreitman (MK) analyses inferred that ;10\%--20\% of amino acid changes have been adaptive in human evolution (Boyko et al. 2008; Messer and Petrov 2013). Consistently, regions of high functional density, high rate of amino acid substitutions, and low recombination all show reduced levels of neutral diversity (Cai et al. 2009; Lohmueller et al. 2011), as expected under recurrent selective sweeps in functional regions. On the other hand, there are reasons to question the notion that adaptation left clear signatures in the human genome. First, different scans for positive selection have identified largely nonoverlapping sets of candidates (Akey 2009), which could be due to a high rate of false positives. Second, MK analyses can be confounded by a number of factors, such as perturbations left by demographic events and by the presence of slightly deleterious mutations (Eyre-Walker and Keightley 2009; Messer and Petrov 2013), and some MK analyses have failed to find evidence for adaptation in the human lineage (Eyre-Walker and Keightley 2009). Finally, it has been shown that background selection (BGS) (Charlesworth et al. 1993), a process in which deleterious muta-tions remove linked neutral variation from the population, re-duces levels of polymorphism in regions of higher functional density and low recombination, providing an alternative expla-nation for the observation of these correlations in the human genome. One signature of positive selection---lower levels of neu-tral variation near functional substitutions (Andolfatto 2007; Macpherson et al. 2007; Cai et al. 2009)---is not generally expected under BGS and should therefore provide the clearest ge-nomic evidence for the action of positive selection. While this sig-nature was found in the human genome by Cai et al. (2009), it could not be detected by two recent studies using the newest large-scale data sets of human diversity (Hernandez et al. 2011; Lohmueller et al. 2011). In particular, Hernandez et al. (2011) searched for lower levels of neutral diversity near functional substitutions by contrasting levels of neutral diversity near nonsynonymous com-pared with synonymous substitutions. They did not find this sig-nature in the human genome and, moreover, found that diversity might in fact be marginally higher near nonsynonymous sub-stitutions. Simulations showed that this puts sharp limits on the amount of adaptation by classic selective sweeps in recent human evolution (Hernandez et al. 2011).},
  isbn = {1549-5469 (Electronic){\textbackslash}n1088-9051 (Linking)},
  pmid = {24619126},
  keywords = {nosource}
}

@article{ENC:5944260,
  title = {Convention on Biological Diversity},
  author = {{CDB}},
  year = {2011},
  journal = {Environmental Conservation},
  volume = {20},
  number = {04},
  pages = {364},
  issn = {0470016175},
  doi = {10.1002/9780470015902.a0020474},
  isbn = {9780470015902},
  pmid = {21051603},
  keywords = {nosource}
}

@book{engen1978stochastic,
  title = {Stochastic Abundance Models, with Emphasis on Biological Communities and Species Diversity},
  author = {Engen, Steinar},
  year = {1978},
  publisher = {{Chapman and Hall Ltd.}},
  keywords = {nosource}
}

@article{EnterMatrixFactorization2018,
  title = {Enter the {{Matrix}}: {{Factorization Uncovers Knowledge}} from {{Omics}}},
  shorttitle = {Enter the {{Matrix}}},
  year = {2018},
  month = oct,
  journal = {Trends in Genetics},
  volume = {34},
  number = {10},
  pages = {790--805},
  publisher = {Elsevier Current Trends},
  issn = {0168-9525},
  doi = {10.1016/j.tig.2018.07.003},
  urldate = {2021-05-04},
  abstract = {Omics data contain signals from the molecular, physical, and kinetic inter- and intracellular interactions that control biological systems. Matrix fac{\dots}},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/CVVTZ95A/2018 - Enter the Matrix Factorization Uncovers Knowledge.pdf}
}

@article{epifani2003exponential,
  title = {Functionals and Means of Neutral-to-the-Right {{Exponential}} Priors},
  author = {Lijoi, Antonio},
  year = {2010},
  journal = {Biometrika},
  volume = {90},
  number = {4},
  pages = {791--808},
  publisher = {Biometrika Trust},
  keywords = {duplicate-citation-key,nosource}
}

@article{epifani2003exponential,
  title = {Exponential Functionals and Means of Neutral-to-the-Right Priors},
  author = {Epifani, Ilenia and Lijoi, Antonio and Pr{\"u}nster, Igor},
  year = {2003},
  journal = {Biometrika},
  volume = {90},
  number = {4},
  pages = {791--808},
  publisher = {Biometrika Trust},
  keywords = {duplicate-citation-key,nosource}
}

@article{epifani2009moment,
  title = {Moment-Based Approximations for the Law of Functionals of {{Dirichlet}} Processes},
  author = {Epifani, Ilenia and Guglielmi, Alessandra and Melilli, Eugenio},
  year = {2009},
  journal = {Applied Mathematical Sciences},
  volume = {3},
  number = {20},
  pages = {979--1004},
  keywords = {nosource}
}

@article{epleyCreatingSocialConnection2008,
  title = {Creating {{Social Connection Through Inferential Reproduction}}: {{Loneliness}} and {{Perceived Agency}} in {{Gadgets}}, {{Gods}}, and {{Greyhounds}}},
  shorttitle = {Creating {{Social Connection Through Inferential Reproduction}}},
  author = {Epley, Nicholas and Akalis, Scott and Waytz, Adam and Cacioppo, John T.},
  year = {2008},
  month = feb,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {1467-9280},
  urldate = {2020-08-18},
  abstract = {People are motivated to maintain social connection with others, and those who lack social connection with other humans may try to compensate by creating a sense...},
  copyright = {{\copyright} 2008 Association for Psychological Science},
  langid = {english},
  keywords = {nosource}
}

@article{ericsonExpectationsEndowmentsEvidence2011,
  title = {Expectations as {{Endowments}}: {{Evidence}} on {{Reference-Dependent Preferences}} from {{Exchange}} and {{Valuation Experiments}}},
  shorttitle = {Expectations as {{Endowments}}},
  author = {Ericson, Keith Marzilli and Fuster, Andreas},
  year = {2011},
  month = nov,
  journal = {The Quarterly Journal of Economics},
  volume = {126},
  number = {4},
  pages = {1879--1907},
  publisher = {Oxford Academic},
  issn = {0033-5533},
  doi = {10.1093/qje/qjr034},
  urldate = {2020-04-16},
  abstract = {Abstract.  While evidence suggests that people evaluate outcomes with respect to reference points, little is known about what determines them. We conduct two ex},
  langid = {english},
  keywords = {nosource}
}

@article{Erman2001,
  title = {Approximate Identifiability, Moments and Censored Data},
  author = {Erman, Simeon M B},
  year = {2001},
  volume = {43},
  number = {September 1999},
  pages = {221--230},
  issn = {13691473},
  keywords = {censored distribution,clinical trial,competing risks,estimator,identifiability,kaplan,meier,moments,nosource,survival distribution,weierstrass theorem}
}

@article{escobar1995bayesian,
  title = {Bayesian Density Estimation and Inference Using Mixtures},
  author = {Escobar, Michael D. and West, Mike},
  year = {1995},
  journal = {Journal of the American Statistical Association},
  volume = {90},
  number = {430},
  pages = {577--588},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1995.10476550},
  abstract = {We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation and are exemplified by special cases where data are mo.deled as a s{\~a}p.le f{\~r}m{\~ }i{\~x}ures of normal distributions. Eflicient simulation methods are used to approximate various prior, postenor, and pred1ct1ve d1stnbutions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general dass of normal mixture models. KEY},
  isbn = {0162-1459},
  pmid = {2291069},
  keywords = {Kernel estimation,Mixtures ofDirichlet processes,Multimodality,Normal mixtures,nosource,Posterior sampling,Smoothing parameter estimation.}
}

@article{estesHeadFootObject2008,
  title = {Head {{Up}}, {{Foot Down}}: {{Object Words Orient Attention}} to the {{Objects}}' {{Typical Location}}},
  shorttitle = {Head {{Up}}, {{Foot Down}}},
  author = {Estes, Zachary and Verges, Michelle and Barsalou, Lawrence W.},
  year = {2008},
  month = feb,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {1467-9280},
  urldate = {2020-07-08},
  abstract = {Many objects typically occur in particular locations, and object words encode these spatial associations. We tested whether such object words (e.g., head, foot)...},
  langid = {english},
  keywords = {nosource}
}

@article{Estoup2012,
  title = {Estimation of Demo-Genetic Model Probabilities with {{Approximate Bayesian Computation}} Using Linear Discriminant Analysis on Summary Statistics},
  author = {Estoup, A and Lombaert, E and Marin, J M and Guillemaud, T and Pudlo, P and Robert, C P and Cornuet, J M},
  year = {2012},
  journal = {Molecular Ecology Resources},
  volume = {12},
  number = {5},
  pages = {846--855},
  issn = {1755-0998},
  doi = {10.1111/j.1755-0998.2012.03153.x},
  abstract = {Comparison of demo-genetic models using Approximate Bayesian Computation (ABC) is an active research field. Although large numbers of populations and models (i.e. scenarios) can be analysed with ABC using molecular data obtained from various marker types, methodological and computational issues arise when these numbers become too large. Moreover, Robert et similar to al. (Proceedings of the National Academy of Sciences of the United States of America, 2011, 108, 15112) have shown that the conclusions drawn on ABC model comparison cannot be trusted per se and required additional simulation analyses. Monte Carlo inferential techniques to empirically evaluate confidence in scenario choice are very time-consuming, however, when the numbers of summary statistics (Ss) and scenarios are large. We here describe a methodological innovation to process efficient ABC scenario probability computation using linear discriminant analysis (LDA) on Ss before computing logistic regression. We used simulated pseudo-observed data sets (pods) to assess the main features of the method (precision and computation time) in comparison with traditional probability estimation using raw (i.e. not LDA transformed) Ss. We also illustrate the method on real microsatellite data sets produced to make inferences about the invasion routes of the coccinelid Harmonia axyridis. We found that scenario probabilities computed from LDA-transformed and raw Ss were strongly correlated. Type I and II errors were similar for both methods. The faster probability computation that we observed (speed gain around a factor of 100 for LDA-transformed Ss) substantially increases the ability of ABC practitioners to analyse large numbers of pods and hence provides a manageable way to empirically evaluate the power available to discriminate among a large set of complex scenarios.},
  isbn = {1755-098X},
  pmid = {22571382},
  keywords = {ABC,Approximate Bayesian Computation,coalescence,discriminant analysis,EVOLUTION,evolutionary scenario,FRAMEWORK,genetics,INVASION,LADYBIRD,LIKELIHOOD-FREE INFERENCE,model probability,molecular markers,nosource,ORIGIN,population,POPULATION HISTORY,SELECTION}
}

@article{Etheridge2007,
  title = {Diffusion Process Models in Population Genetics},
  author = {Etheridge, Alison},
  year = {2007},
  journal = {Notes},
  pages = {1--4},
  keywords = {nosource}
}

@incollection{Etheridge2011,
  title = {Some Mathematical Models from Population Genetics},
  booktitle = {Some Mathematical Models from Population Genetics},
  author = {Etheridge, Alison},
  year = {2011},
  edition = {Lecture No},
  volume = {2012},
  pages = {1--111},
  publisher = {Springer},
  isbn = {0075-8434{\textbackslash}r978-3-642-16631-0},
  keywords = {nosource}
}

@article{etheridgeCoalescentDualProcess2009,
  title = {A Coalescent Dual Process in a {{Moran}} Model with Genic Selection},
  author = {Etheridge, A. M. and Griffiths, R. C.},
  year = {2009},
  month = jun,
  journal = {Theoretical Population Biology},
  series = {Sam {{Karlin}}: {{Special Issue}}},
  volume = {75},
  number = {4},
  pages = {320--330},
  issn = {0040-5809},
  doi = {10.1016/j.tpb.2009.03.004},
  urldate = {2022-12-07},
  abstract = {A coalescent dual process for a multi-type Moran model with genic selection is derived using a generator approach. This leads to an expansion of the transition functions in the Moran model and the Wright--Fisher diffusion process limit in terms of the transition functions for the coalescent dual. A graphical representation of the Moran model (in the spirit of Harris) identifies the dual as a strong dual process following typed lines backwards in time. An application is made to the harmonic measure problem of finding the joint probability distribution of the time to the first loss of an allele from the population and the distribution of the surviving alleles at the time of loss. Our dual process mirrors the Ancestral Selection Graph of [Krone, S. M., Neuhauser, C., 1997. Ancestral processes with selection. Theoret. Popul. Biol. 51, 210--237; Neuhauser, C., Krone, S. M., 1997. The genealogy of samples in models with selection. Genetics 145, 519--534], which allows one to reconstruct the genealogy of a random sample from a population subject to genic selection. In our setting, we follow [Stephens, M., Donnelly, P., 2002. Ancestral inference in population genetics models with selection. Aust. N. Z. J. Stat. 45, 395--430] in assuming that the types of individuals in the sample are known. There are also close links to [Fearnhead, P., 2002. The common ancestor at a nonneutral locus. J. Appl. Probab. 39, 38--54]. However, our methods and applications are quite different. This work can also be thought of as extending a dual process construction in a Wright--Fisher diffusion in [Barbour, A.D., Ethier, S.N., Griffiths, R.C., 2000. A transition function expansion for a diffusion model with selection. Ann. Appl. Probab. 10, 123--162]. The application to the harmonic measure problem extends a construction provided in the setting of a neutral diffusion process model in [Ethier, S.N., Griffiths, R.C., 1991. Harmonic measure for random genetic drift. In: Pinsky, M.A. (Ed.), Diffusion Processes and Related Problems in Analysis, vol. 1. In: Progress in Probability Series, vol. 22, Birkh{\"a}user, Boston, pp. 73--81].},
  langid = {english},
  keywords = {Ancestral Selection Graph,Coalescent process,Genic selection,Harmonic measure,Moran model,Wright--Fisher diffusion},
  file = {/home/gkonkamking/pCloudDrive/papers/Etheridge_Griffiths_2009_A coalescent dual process in a Moran model with genic selection.pdf}
}

@article{Ethier1981,
  title = {The Infinitely-Many-Neutral-Alleles Diffusion Model},
  author = {Ethier, S. N. and Kurtz, Thomas G.},
  year = {1981},
  journal = {Advances in Applied Probability},
  volume = {13},
  number = {3},
  pages = {429--452},
  keywords = {Ethier,Kurtz,nosource}
}

@article{ethier1986markov,
  title = {Markov Processes: {{Characterization}} and Convergence. {{A}} John Wiley \& Sons},
  author = {Ethier, Stewart N and Kurtz, Thomas G},
  year = {1986},
  journal = {Inc., Publication},
  volume = {1},
  keywords = {nosource}
}

@article{ethierTransitionFunctionFlemingViot1993,
  title = {The {{Transition Function}} of a {{Fleming-Viot Process}}},
  author = {Ethier, S. N. and Griffiths, R. C.},
  year = {1993},
  journal = {The Annals of Probability},
  volume = {21},
  number = {3},
  eprint = {2244588},
  eprinttype = {jstor},
  pages = {1571--1590},
  publisher = {Institute of Mathematical Statistics},
  issn = {0091-1798},
  urldate = {2020-04-21},
  abstract = {Let S be a compact metric space, let {\texttheta} {$\geq$} 0, and let {$\nu$}0 be a Borel probability measure on S. An explicit formula is found for the transition function of the Fleming-Viot process with type space S and mutation operator (Af)(x) = (1/2){\texttheta}{$\int$}S(f({$\xi$}) - f(x)){$\nu$}0(d{$\xi$}).},
  keywords = {nosource}
}

@article{evansMethodsApproximatingIntegrals1995,
  title = {Methods for {{Approximating Integrals}} in {{Statistics}} with {{Special Emphasis}} on {{Bayesian Integration Problems}}},
  author = {Evans, Michael and Swartz, Tim},
  year = {1995},
  journal = {Statistical Science},
  volume = {10},
  number = {3},
  eprint = {2246014},
  eprinttype = {jstor},
  pages = {254--272},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  doi = {10.1214/ss/1177009938},
  urldate = {2023-08-01},
  abstract = {This paper is a survey of the major techniques and approaches available for the numerical approximation of integrals in statistics. We classify these into five broad categories; namely, asymptotic methods, importance sampling, adaptive importance sampling, multiple quadrature and Markov chain methods. Each method is discussed, giving an outline of the basic supporting theory and particular features of the technique. Conclusions are drawn concerning the relative merits of the methods based on the discussion and their application to three examples. The following broad recommendations are made. Asymptotic methods should only be considered in contexts where the integrand has a dominant peak with approximate ellipsoidal symmetry. Importance sampling (and preferably adaptive importance sampling) based on a multivariate Student should be used instead of asymptotics methods in such a context. Multiple quadrature and, in particular, subregion adaptive integration are the algorithms of choice for lower-dimensional integrals. Due to the difficulties in assessing convergence to stationarity and the error in estimates, Markov chain methods are recommended only when there is no adequate alternative. In certain very high dimensional problems, however, Markov chain methods are the only hope. The importance of the parameterization of the integral is noted for the success of all the methods, and several useful reparameterizations are presented.},
  file = {/home/gkonkamking/pCloudDrive/papers/Evans_Swartz_1995_Methods for Approximating Integrals in Statistics with Special Emphasis on.pdf}
}

@article{ewens1972sampling,
  title = {Sampling Theory of Selectively Neutral Alleles},
  author = {Ewens, W J},
  year = {1972},
  journal = {Theoretical Population Biology},
  volume = {3},
  number = {1},
  pages = {87-\&},
  publisher = {Elsevier},
  doi = {10.1016/0040-5809(72)90035-4},
  isbn = {0040-5809},
  keywords = {nosource}
}

@incollection{ewens1990population,
  title = {Population Genetics Theory - the Past and the Future},
  booktitle = {Mathematical and Statistical Developments of Evolutionary Theory},
  author = {Ewens, W J},
  year = {1990},
  number = {1979},
  pages = {177--227},
  publisher = {Springer},
  doi = {10.1007/978-94-009-0513-9_4},
  isbn = {978-94-010-6717-1},
  keywords = {nosource}
}

@misc{FacebookDataGood,
  title = {Facebook {{Data For Good HDX}}},
  urldate = {2023-10-25},
  abstract = {Facebook Data For Good, HDX},
  howpublished = {https://dataforgood.facebook.com/dfg/docs/high-resolution-population-density-maps-demographic-estimates-documentation},
  langid = {english}
}

@article{Fairbrother2015,
  title = {Risk Management Decisions for Pesticides and Threatened and Endangered Species: {{The}} Role of Uncertainty Analysis},
  author = {Fairbrother, Anne and Hartl, Brett and Hope, Bruce K. and Jenkins, Jeffrey J. and Li, Ya-Wei and Moore, Dwayne R.J.},
  year = {2015},
  journal = {Human and Ecological Risk Assessment: An International Journal},
  volume = {7039},
  number = {January 2016},
  pages = {00--00},
  issn = {1080-7039},
  doi = {10.1080/10807039.2015.1089400},
  abstract = {AbstractDespite data gaps and information shortfalls, government agencies in the U.S. are expected to produce timely and defensible decisions to regulate pesticide use under the Federal Insecticide, Fungicide, and Rodenticide Act (FIFRA) and in compliance with the Endangered Species Act (ESA). The decision to register a pesticide is predicated on a conclusion that no unreasonable effects will accrue to the environment, including threatened and endangered species. We recognize that the definition of acceptable risk is a policy judgment stemming from legislative language and judicial interpretation. However, a common risk assessment approach with similar technical underpinnings and a high degree of transparency used by all the agencies would be cost effective and more likely to achieve consensus among interested parties. Quantitative probabilistic risk assessment (PRA) methods can be used to develop risk estimates and to describe the level of confidence in these estimates. PRA methods can also differentiate...},
  keywords = {endangered species,ESA,FIFRA,nosource,pesticide,Risk assessment,uncertainty}
}

@article{fallGibbsSamplingMethods,
  title = {Gibbs Sampling Methods for {{Pitman-Yor}} Mixture Models},
  author = {Fall, Mame Diarra and Barat, {\'E}ric},
  pages = {20},
  abstract = {We introduce a new sampling strategy for the two-parameter Poisson-Dirichlet process mixture model, also known as Pitman-Yor process mixture model (PYM). Our sampler is therefore applicable to the wellknown Dirichlet process mixture model (DPM). Inference in DPM and PYM is usually performed via Markov Chain Monte Carlo (MCMC) methods, specifically Gibbs sampler. These sampling methods are usually divided in two classes: marginal and conditional algorithms. Each method has its merits and limitations. The aim of this paper is to propose a new sampler which combines the main advantages of each class. Our method relies on a result of [Pit96b] for updating Pitman-Yor processes. The infinite part of the unconditional process is sampled in two ways, leading to two variants of the proposed sampler. We also propose a threshold to improve mixing in the first variant of our algorithm. The two variants of our sampler are compared with a marginal method (algorithm 8 of [Nea00]) and two state of the art conditional algorithms which are formulated in the space of cluster labels namely the efficient slice sampler of [KGW11] and the truncated blocked Gibbs sampler of [IJ01]. We also investigate effects of removing the proposed threshold in the first variant of our algorithm and introducing the threshold in the efficient slice sampler of [KGW11]. Results on real and simulated data sets illustrate that our algorithms outperform the other conditionals in terms of mixing properties.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/Zotero/storage/3F7TMLMA/Fall and Barat - Gibbs sampling methods for Pitman-Yor mixture mode.pdf}
}

@inproceedings{fan2014cityspectrum,
  title = {Cityspectrum: A Non-Negative Tensor Factorization Approach},
  booktitle = {Proceedings of the 2014 {{ACM}} International Joint Conference on Pervasive and Ubiquitous Computing},
  author = {Fan, Zipei and Song, Xuan and Shibasaki, Ryosuke},
  year = {2014},
  pages = {213--223},
  keywords = {nosource}
}

@article{fanVARIABLESELECTIONLINEAR2012,
  title = {{{VARIABLE SELECTION IN LINEAR MIXED EFFECTS MODELS}}},
  author = {Fan, Yingying and Li, Runze},
  year = {2012},
  month = aug,
  journal = {Annals of statistics},
  volume = {40},
  number = {4},
  pages = {2043--2068},
  issn = {0090-5364},
  doi = {10.1214/12-AOS1028},
  urldate = {2024-06-26},
  abstract = {This paper is concerned with the selection and estimation of fixed and random effects in linear mixed effects models. We propose a class of nonconcave penalized profile likelihood methods for selecting and estimating important fixed effects. To overcome the difficulty of unknown covariance matrix of random effects, we propose to use a proxy matrix in the penalized profile likelihood. We establish conditions on the choice of the proxy matrix and show that the proposed procedure enjoys the model selection consistency where the number of fixed effects is allowed to grow exponentially with the sample size. We further propose a group variable selection strategy to simultaneously select and estimate important random effects, where the unknown covariance matrix of random effects is replaced with a proxy matrix. We prove that, with the proxy matrix appropriately chosen, the proposed procedure can identify all true random effects with asymptotic probability one, where the dimension of random effects vector is allowed to increase exponentially with the sample size. Monte Carlo simulation studies are conducted to examine the finite-sample performance of the proposed procedures. We further illustrate the proposed procedures via a real data example.},
  pmcid = {PMC4026175},
  pmid = {24850975},
  file = {/home/gkonkamking/pCloudDrive/papers/Fan_Li_2012_VARIABLE SELECTION IN LINEAR MIXED EFFECTS MODELS.pdf}
}

@article{farrahi2011discovering,
  title = {Discovering Routines from Large-Scale Human Locations Using Probabilistic Topic Models},
  author = {Farrahi, Katayoun and {Gatica-Perez}, Daniel},
  year = {2011},
  journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume = {2},
  number = {1},
  pages = {1--27},
  publisher = {ACM New York, NY, USA},
  keywords = {nosource}
}

@misc{faulknerSamplingAlgorithmsStatistical2022,
  title = {Sampling Algorithms in Statistical Physics: A Guide for Statistics and Machine Learning},
  shorttitle = {Sampling Algorithms in Statistical Physics},
  author = {Faulkner, Michael F. and Livingstone, Samuel},
  year = {2022},
  month = aug,
  number = {arXiv:2208.04751},
  eprint = {2208.04751},
  primaryclass = {cond-mat, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.04751},
  urldate = {2023-02-17},
  abstract = {We discuss several algorithms for sampling from unnormalized probability distributions in statistical physics, but using the language of statistics and machine learning. We provide a self-contained introduction to some key ideas and concepts of the field, before discussing three well-known problems: phase transitions in the Ising model, the melting transition on a two-dimensional plane and simulation of an all-atom model for liquid water. We review the classical Metropolis, Glauber and molecular dynamics sampling algorithms before discussing several more recent approaches, including cluster algorithms, novel variations of hybrid Monte Carlo and Langevin dynamics and piece-wise deterministic processes such as event chain Monte Carlo. We highlight cross-over with statistics and machine learning throughout and present some results on event chain Monte Carlo and sampling from the Ising model using tools from the statistics literature. We provide a simulation study on the Ising and XY models, with reproducible code freely available online, and following this we discuss several open areas for interaction between the disciplines that have not yet been explored and suggest avenues for doing so.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Statistical Mechanics,Statistics - Computation},
  file = {/home/gkonkamking/pCloudDrive/papers/Faulkner_Livingstone_2022_Sampling algorithms in statistical physics.pdf}
}

@article{fav12,
  title = {{{MCMC}} for Normalized Random Measure Mixture Models},
  author = {Favaro, Stefano and Teh, Yee Whye},
  year = {2013},
  journal = {Statistical Science},
  volume = {28},
  number = {3},
  pages = {335--359},
  publisher = {Institute of Mathematical Statistics},
  keywords = {nosource}
}

@article{Favaro2009,
  title = {Bayesian Non-Parametric Inference for Species Variety with a Two-Parameter {{Poisson-Dirichlet}} Process Prior},
  author = {Favaro, Stefano and Lijoi, Antonio and Mena, Rams{\'e}s H. and Pr{\"u}nster, Igor},
  year = {2009},
  journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
  volume = {71},
  number = {5},
  pages = {993--1008},
  issn = {13697412},
  doi = {10.1111/j.1467-9868.2009.00717.x},
  keywords = {Asymptotics,Bayesian non-parametrics,Expressed sequence tags analysis,nosource,Posterior probability of discovering a new species,Sample coverage,Species sampling,Two-parameter Poisson-Dirichlet process}
}

@article{favaro2009gibbs,
  title = {On a {{Gibbs}} Sampler Based Random Process in {{Bayesian}} Nonparametrics},
  author = {Favaro, Stefano and Ruggiero, Matteo and Walker, Stephen G},
  year = {2009},
  journal = {Electronic Journal of Statistics},
  volume = {3},
  pages = {1556--1566},
  publisher = {{The Institute of Mathematical Statistics and the Bernoulli Society}},
  keywords = {nosource}
}

@article{favaro2012conditional,
  title = {Conditional Formulae for {{Gibbs-type}} Exchangeable Random Partitions},
  author = {Favaro, Stefano and Lijoi, Antonio and Pr{\"u}nster, Igor},
  year = {2013},
  journal = {Annals of Applied Probability},
  volume = {23},
  number = {5},
  pages = {1721--1754},
  issn = {10505164},
  doi = {10.1214/12-AAP843},
  keywords = {{$\sigma$}-diversity,Bayesian nonparametrics,Exchangeable random partitions,Gibbs-type random partitions,nosource,Sampling formulae,Small blocks,Species sampling problems}
}

@article{favaro2012new,
  title = {A New Estimator of the Discovery Probability},
  author = {Favaro, Stefano and Lijoi, Antonio and Pr{\"u}nster, Igor},
  year = {2012},
  journal = {Biometrics},
  volume = {68},
  number = {4},
  pages = {1188--1196},
  publisher = {Wiley Online Library},
  issn = {0006341X},
  doi = {10.1111/j.1541-0420.2012.01793.x},
  abstract = {Species sampling problems have a long history in ecological and biological studies and a number of issues, including the evaluation of species richness, the design of sampling experiments, and the estimation of rare species variety, are to be addressed. Such inferential problems have recently emerged also in genomic applications, however, exhibiting some peculiar features that make them more challenging: specifically, one has to deal with very large populations (genomic libraries) containing a huge number of distinct species (genes) and only a small portion of the library has been sampled (sequenced). These aspects motivate the Bayesian nonparametric approach we undertake, since it allows to achieve the degree of flexibility typically needed in this framework. Based on an observed sample of size n, focus will be on prediction of a key aspect of the outcome from an additional sample of size m, namely, the so-called discovery probability. In particular, conditionally on an observed basic sample of size n, we derive a novel estimator of the probability of detecting, at the (n+m+1)th observation, species that have been observed with any given frequency in the enlarged sample of size n+m. Such an estimator admits a closed-form expression that can be exactly evaluated. The result we obtain allows us to quantify both the rate at which rare species are detected and the achieved sample coverage of abundant species, as m increases. Natural applications are represented by the estimation of the probability of discovering rare genes within genomic libraries and the results are illustrated by means of two expressed sequence tags datasets.},
  pmid = {23025286},
  keywords = {Bayesian nonparametrics,Gibbs-type priors,nosource,Rare species discovery,Species sampling models,Two-parameter Poisson-Dirichlet process}
}

@article{favaroClassSstablePoisson2015,
  title = {On a Class of {$\sigma$}-Stable {{Poisson}}--{{Kingman}} Models and an Effective Marginalized Sampler},
  author = {Favaro, Stefano and Lomeli, Maria and Teh, Yee Whye},
  year = {2015},
  journal = {Statistics and Computing},
  volume = {25},
  number = {1},
  pages = {67--78},
  publisher = {Springer},
  doi = {10.1007/s11222-014-9499-4},
  keywords = {nosource}
}

@article{favaroSliceSamplingSstable2013,
  title = {Slice Sampling {$\sigma$}-Stable {{Poisson-Kingman}} Mixture Models},
  author = {Favaro, Stefano and Walker, Stephen G},
  year = {2013},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {22},
  number = {4},
  pages = {830--847},
  publisher = {Taylor \& Francis},
  doi = {10.1080/10618600.2012.681211},
  keywords = {nosource}
}

@article{favaroStickbreakingRepresentationNormalized2012,
  title = {On the Stick-Breaking Representation of Normalized Inverse {{Gaussian}} Priors},
  author = {Favaro, S. and Lijoi, A. and Pr{\"u}nster, I.},
  year = {2012},
  month = sep,
  journal = {Biometrika},
  volume = {99},
  number = {3},
  pages = {663--674},
  issn = {0006-3444},
  doi = {10.1093/biomet/ass023},
  urldate = {2022-04-06},
  abstract = {Random probability measures are the main tool for Bayesian nonparametric inference, with their laws acting as prior distributions. Many well-known priors used in practice admit different, though equivalent, representations. In terms of computational convenience, stick-breaking representations stand out. In this paper we focus on the normalized inverse Gaussian process and provide a completely explicit stick-breaking representation for it. This result is of interest both from a theoretical viewpoint and for statistical practice.},
  file = {/home/gkonkamking/Zotero/storage/HI6KR9GZ/Favaro et al. - 2012 - On the stick-breaking representation of normalized.pdf}
}

@article{favaroStickbreakingRepresentationSstable2014,
  title = {On the Stick-Breaking Representation of {$\sigma$}-Stable {{Poisson-Kingman}} Models},
  author = {Favaro, Stefano and Lomeli, Maria and Nipoti, Bernardo and Teh, Yee Whye},
  year = {2014},
  journal = {Electronic Journal of Statistics},
  volume = {8},
  number = {1},
  pages = {1063--1085},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  doi = {10.1214/14-EJS921},
  file = {/home/gkonkamking/Zotero/storage/MLXG94N9/Favaro et al. - 2014 - On the stick-breaking representation of σ-stable P.pdf}
}

@article{faveroDualProcessCoupled2021,
  title = {A Dual Process for the Coupled {{Wright}}--{{Fisher}} Diffusion},
  author = {Favero, Martina and Hult, Henrik and Koski, Timo},
  year = {2021},
  month = jan,
  journal = {Journal of Mathematical Biology},
  volume = {82},
  number = {1},
  pages = {6},
  issn = {1432-1416},
  doi = {10.1007/s00285-021-01555-9},
  urldate = {2021-10-15},
  abstract = {The coupled Wright--Fisher diffusion is a multi-dimensional Wright--Fisher diffusion for multi-locus and multi-allelic genetic frequencies, expressed as the strong solution to a system of stochastic differential equations that are coupled in the drift, where the pairwise interaction among loci is modelled by an inter-locus selection. In this paper, an ancestral process, which is dual to the coupled Wright--Fisher diffusion, is derived. The dual process corresponds to the block counting process of coupled ancestral selection graphs, one for each locus. Jumps of the dual process arise from coalescence, mutation, single-branching, which occur at one locus at the time, and double-branching, which occur simultaneously at two loci. The coalescence and mutation rates have the typical structure of the transition rates of the Kingman coalescent process. The single-branching rate not only contains the one-locus selection parameters in a form that generalises the rates of an ancestral selection graph, but it also contains the two-locus selection parameters to include the effect of the pairwise interaction on the single loci. The double-branching rate reflects the particular structure of pairwise selection interactions of the coupled Wright--Fisher diffusion. Moreover, in the special case of two loci, two alleles, with selection and parent independent mutation, the stationary density for the coupled Wright--Fisher diffusion and the transition rates of the dual process are obtained in an explicit form.},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Favero et al_2021_A dual process for the coupled Wright–Fisher diffusion.pdf;/home/gkonkamking/Zotero/storage/S4BXXNKL/Favero et al. - 2021 - A dual process for the coupled Wright–Fisher diffu.pdf}
}

@article{Fay2010,
  title = {Interval Censored Data Analysis},
  author = {Fay, Michael P},
  year = {2010},
  journal = {Analysis},
  keywords = {nosource}
}

@inproceedings{FBDSW13,
  title = {Stochastic Collapsed Variational \{\vphantom\}{{Bayesian}}\vphantom\{\} Inference for Latent \{\vphantom\}{{Dirichlet}}\vphantom\{\} Allocation},
  booktitle = {Proceedings of the 19th {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Foulds, James and Boyles, Levi and DuBois, Christopher and Smyth, Padhraic and Welling, Max},
  year = {2013},
  series = {{{KDD}} '13},
  pages = {446--454},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2487575.2487697},
  abstract = {There has been an explosion in the amount of digital text information available in recent years, leading to challenges of scale for traditional inference algorithms for topic models. Recent advances in stochastic variational inference algorithms for latent Dirichlet allocation (LDA) have made it feasible to learn topic models on very large-scale corpora, but these methods do not currently take full advantage of the collapsed representation of the model. We propose a stochastic algorithm for collapsed variational Bayesian inference for LDA, which is simpler and more efficient than the state of the art method. In experiments on large-scale text corpora, the algorithm was found to converge faster and often to a better solution than previous methods. Human-subject experiments also demonstrated that the method can learn coherent topics in seconds on small corpora, facilitating the use of topic models in interactive document analysis software.},
  isbn = {978-1-4503-2174-7},
  keywords = {nosource,stochastic learning,topic models,variational inference}
}

@article{Fearnhead2010,
  title = {Constructing Summary Statistics for Approximate {{Bayesian}} Computation: Semi-Automatic Approximate {{Bayesian}} Computation},
  author = {Fearnhead, P and Prangle, D},
  year = {2012},
  journal = {Journal of the Royal Statistical Society - Series B: Statistical Methodology},
  volume = {74},
  number = {3},
  eprint = {1004.1112v1},
  pages = {419--474},
  issn = {1369-7412},
  doi = {10.1111/j.1467-9868.2011.01010.x},
  abstract = {. Many modern statistical applications involve inference for complex stochastic models, where it is easy to simulate from the models, but impossible to calculate likelihoods. Approximate Bayesian computation (ABC) is a method of inference for such models. It replaces calculation of the likelihood by a step which involves simulating artificial data for different parameter values, and comparing summary statistics of the simulated data with summary statistics of the observed data. Here we show how to construct appropriate summary statistics for ABC in a semi-automatic manner. We aim for summary statistics which will enable inference about certain parameters of interest to be as accurate as possible. Theoretical results show that optimal summary statistics are the posterior means of the parameters. Although these cannot be calculated analytically, we use an extra stage of simulation to estimate how the posterior means vary as a function of the data; and we then use these estimates of our summary statistics within ABC. Empirical results show that our approach is a robust method for choosing summary statistics that can result in substantially more accurate ABC analyses than the ad hoc choices of summary statistics that have been proposed in the literature. We also demonstrate advantages over two alternative methods of simulation-based inference.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1004.1112v1},
  keywords = {Indirect inference,Likelihood-free inference,Markov chain Monte Carlo methods,Simulation,Stochastic kinetic networks},
  file = {/home/gkonkamking/Zotero/storage/ZMKFY69Y/Fearnhead and Prangle - 2012 - Constructing summary statistics for approximate Ba.pdf}
}

@article{Fedorenkova2012,
  title = {Ranking Ecological Risks of Multiple Chemical Stressors on Amphibians},
  author = {Fedorenkova, Anastasia and Vonk, J. Arie and Lenders, H. J Rob and Creemers, Raymond C M and Breure, Anton M. and Hendriks, A. Jan},
  year = {2012},
  month = jun,
  journal = {Environmental Toxicology and Chemistry},
  volume = {31},
  number = {6},
  eprint = {22488839},
  eprinttype = {pubmed},
  pages = {1416--1421},
  issn = {07307268},
  doi = {10.1002/etc.1831},
  abstract = {Populations of amphibians have been declining worldwide since the late 1960s. Despite global concern, no studies have quantitatively assessed the major causes of this decline. In the present study, species sensitivity distributions (SSDs) were developed to analyze the sensitivity of anurans for ammonium, nitrate, heavy metals (cadmium, copper), pesticides (18 compounds), and acidification (pH) based on laboratory toxicity data. Ecological risk (ER) was calculated as the probability that a measured environmental concentration of a particular stressor in habitats where anurans were observed would exceed the toxic effect concentrations derived from the species sensitivity distributions. The assessment of ER was used to rank the stressors according to their potential risk to anurans based on a case study of Dutch freshwater bodies. The derived ERs revealed that threats to populations of anurans decreased in the sequence of pH, copper, diazinon, ammonium, and endosulfan. Other stressors studied were of minor importance. The method of deriving ER by combining field observation data and laboratory data provides insight into potential threats to species in their habitats and can be used to prioritize stressors, which is necessary to achieve effective management in amphibian conservation.},
  pmid = {22488839},
  keywords = {Acidification,Anurans,Ecological risk assessment,nosource,Pesticides,Species sensitivity distributions}
}

@article{ferguson1972representation,
  title = {A Representation of Independent Increment Processes without Gaussian Components},
  author = {Ferguson, Thomas and Klass, Michael},
  year = {1972},
  journal = {The Annals of Mathematical Statistics},
  volume = {43},
  number = {5},
  eprint = {2240085\{\%\}5Cnpapers2://publication/uuid/F44BD729-9D32-4DA7-8CC1-BDF04C712AD0},
  eprinttype = {jstor},
  pages = {1634--1643},
  publisher = {JSTOR},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177692395},
  keywords = {nosource}
}

@article{ferguson1973bayesian,
  title = {A \{\vphantom\}{{Bayesian}}\vphantom\{\} Analysis of Some Nonparametric Problems},
  author = {Ferguson, T S},
  year = {1973},
  journal = {Annals of Statistics},
  volume = {1},
  number = {2},
  pages = {209--230},
  publisher = {JSTOR},
  issn = {0090-5364},
  doi = {10.1214/aos/1176342360},
  isbn = {0090-5364},
  pmid = {89027},
  keywords = {nosource}
}

@article{Ferguson1974,
  title = {Prior Distributions on Spaces of Probability Measures},
  author = {Ferguson, Thomas},
  year = {1974},
  journal = {Ann. Statist.},
  volume = {2},
  number = {4},
  eprint = {2958401\{\%\}5Cnpapers2://publication/uuid/F38624E4-B3C8-4D68-BC4E-B1DE3E661B2F},
  eprinttype = {jstor},
  pages = {615--629},
  publisher = {Institute of Mathematical Statistics},
  issn = {2168-8966},
  doi = {10.2307/2958401},
  abstract = {Methods of generating prior distributions on spaces of probability measures for use in Bayesian nonparametric inference are reviewed with special emphasis on the Dirichlet processes, the tailfree processes, and processes neutral to the right. Some applications are given.},
  isbn = {0090-5364},
  keywords = {nosource}
}

@article{Ferrante1990,
  title = {On Necessary Conditions for the Existence of Finite-Dimensional Filters in Discrete Time},
  author = {Ferrante, Marco and Runggaldier, Wolfgang J.},
  year = {1990},
  journal = {Systems and Control Letters},
  volume = {14},
  number = {1},
  pages = {63--69},
  issn = {01676911},
  doi = {10.1016/0167-6911(90)90083-7},
  abstract = {Under some regularity assumptions we show that, if a finite-dimensional filter in discrete time exists, then the observation, prediction, and filtering distributions are all of exponential class. Our results, motivated by analogous results in the field of statistics, hold for arbitrary (finite) dimensions of the state and observation spaces as well as of the filter itself. {\copyright} 1990.},
  keywords = {exponential class of distributions,finite-dimensional filters,Nonlinear filtering,nosource}
}

@article{Ferrante1992,
  ids = {ferranteExistenceFinitedimensionalFilters1992},
  title = {On the Existence of Finite-Dimensional Filters in Discrete Time},
  author = {Ferrante, Marco},
  year = {1992},
  journal = {Stochastics and Stochastic Reports},
  volume = {40},
  number = {3-4},
  pages = {169--179},
  issn = {1045-1129},
  doi = {10.1080/17442509208833787},
  keywords = {exponential class of distributions,Finite dimensional filtering,Gaussian finite dimensional filtering,nosource}
}

@article{ferrante1998finite,
  title = {Finite Dimensional Filters for Nonlinear Stochastic Difference Equations with Multiplicative Noises},
  author = {Ferrante, Marco and Vidoni, Paolo},
  year = {1998},
  journal = {Stochastic processes and their applications},
  volume = {77},
  number = {1},
  pages = {69--81},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{fielding2021spatial,
  title = {Spatial and Temporal Variation in Proximity Networks of Commercial Dairy Cattle in {{Great Britain}}},
  author = {Fielding, Helen R and Silk, Matthew J and McKinley, Trevelyan J and Delahay, Richard J and {Wilson-Aggarwal}, Jared K and Gauvin, Laetitia and Ozella, Laura and Cattuto, Ciro and McDonald, Robbie A},
  year = {2021},
  journal = {Preventive Veterinary Medicine},
  volume = {194},
  pages = {105443},
  publisher = {Elsevier},
  doi = {10.1016/j.prevetmed.2021.105443},
  keywords = {nosource}
}

@article{filliben1975probability,
  title = {The Probability Plot Correlation Coefficient Test for Normality},
  author = {Filliben, James J.},
  year = {1975},
  journal = {Technometrics},
  volume = {17},
  number = {1},
  eprint = {1268008},
  eprinttype = {jstor},
  pages = {111},
  publisher = {Taylor \& Francis},
  issn = {00401706},
  doi = {10.2307/1268008},
  abstract = {This paper introduces the normal probability plot correlation coefficient as a test statsitic in complete sampels for the composite hypothesis of normality. The proposed test statistic is conceptually simple, is computationally convenient, and is readily extendible to testing non-normal distributional hypotheses. An empirical power study shows that the normal probabily plot correlation coefficient compares favorably with 7 other normal test statistics. Percent points are tabulated for n=3(1)50(5)100.},
  isbn = {0040-1706},
  keywords = {nosource}
}

@article{fischer2008selective,
  title = {Selective Exposure and Information Quantity: How Different Information Quantities Moderate Decision Makers' Preference for Consistent and Inconsistent Information.},
  author = {Fischer, Peter and {Schulz-Hardt}, Stefan and Frey, Dieter},
  year = {2008},
  journal = {Journal of personality and social psychology},
  volume = {94},
  number = {2},
  pages = {231},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{fischer2008self,
  title = {Self-Regulation and Selective Exposure: The Impact of Depleted Self-Regulation Resources on Confirmatory Information Processing.},
  author = {Fischer, Peter and Greitemeyer, Tobias and Frey, Dieter},
  year = {2008},
  journal = {Journal of personality and social psychology},
  volume = {94},
  number = {3},
  pages = {382},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{fisher1943relation,
  title = {The Relation between the Number of Species and the Number of Individuals in a Random Sample of an Animal Population},
  author = {a Fisher, R and Corbet, a Steven and Williams, C B},
  year = {1943},
  journal = {Journal of Animal Ecology},
  volume = {12},
  number = {1},
  eprint = {1411},
  eprinttype = {jstor},
  pages = {42--58},
  publisher = {JSTOR},
  issn = {00218790},
  doi = {10.2307/1411},
  abstract = {Part 1. It is shown that in a large collection of Lepidoptera captured in Malaya the frequency of the number of species represented by different numbers of individuals fitted somewhat closely to a hyperbola type of curve, so long as only the rarer species were considered. The data for the commoner species was not so strictly `randomized', but the whole series could be closely fitted by a series of the logarithmic type as described by Fisher in Part 3. Other data for random collections of insects in the field were also shown to fit fairly well to this series. Part 2. Extensive data on the capture of about 1500 Macrolepidoptera of about 240 species in a light-trap at Harpenden is analysed in relation to Fisher's mathematical theory and is shown to fit extremely closely to the calculations. The calculations are applied first to the frequency of occurrence of species represented by different numbers of individuals-and secondly to the number of species in samples of different sizes from the same population. The parameter alpha which it is suggested should be called the `index of diversity', is shown to have a regular seasonal change in the case of the Macrolepidoptera in the trap. In addition, samples from two traps which overlooked somewhat different vegetation are shown to have alpha values which are significantly different. It is shown that, provided the samples are not small, alpha is the increase in the number of species obtained by increasing the size of a sample by e (2.718). A diagram is given (Fig. 8) from which any one of the values, total number of species, total number of individuals and index of diversity (alpha), can be obtained approximately if the other two are known. The standard error of alpha is also indicated on the same diagram. Part 3. A theoretical distribution is developed which appears to be suitable for the frequencies with which different species occur in a random collection, in the common case in which many species are so rare that their chance of inclusion is small. The relationships of the new distribution with the negative binomial and the Poisson series are established. Numerical processes are exhibited for fitting the series to observations containing given numbers of species and individuals, and for estimating the parameter alpha representing the richness in species of the material sampled; secondly, for calculating the standard error of alpha, and thirdly, for testing whether the series exhibits a significant deviation from the limiting form used. Special tables are presented for facilitating these calculations.},
  isbn = {00218790},
  keywords = {nosource}
}

@article{folco2022data,
  title = {Data-Driven Micromobility Network Planning for Demand and Safety},
  author = {Folco, Pietro and Gauvin, Laetitia and Tizzoni, Michele and Szell, Michael},
  year = {2022},
  journal = {Environment and Planning B: Urban Analytics and City Science},
  pages = {23998083221135611},
  publisher = {SAGE Publications Sage UK: London, England},
  doi = {10.1177/23998083221135611}
}

@article{fongMartingalePosteriorDistributions2023,
  title = {Martingale Posterior Distributions},
  author = {Fong, Edwin and Holmes, Chris and Walker, Stephen G},
  year = {2023},
  month = nov,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {85},
  number = {5},
  pages = {1357--1391},
  issn = {1369-7412},
  doi = {10.1093/jrsssb/qkad005},
  urldate = {2025-01-21},
  abstract = {The prior distribution is the usual starting point for Bayesian uncertainty. In this paper, we present a different perspective that focuses on missing observations as the source of statistical uncertainty, with the parameter of interest being known precisely given the entire population. We argue that the foundation of Bayesian inference is to assign a distribution on missing observations conditional on what has been observed. In the i.i.d. setting with an observed sample of size n, the Bayesian would thus assign a predictive distribution on the missing Yn+1:{$\infty$} conditional on Y1:n, which then induces a distribution on the parameter. We utilize Doob's theorem, which relies on martingales, to show that choosing the Bayesian predictive distribution returns the conventional posterior as the distribution of the parameter. Taking this as our cue, we relax the predictive machine, avoiding the need for the predictive to be derived solely from the usual prior to posterior to predictive density formula. We introduce the martingale posterior distribution, which returns Bayesian uncertainty on any statistic via the direct specification of the joint predictive. To that end, we introduce new predictive methodologies for multivariate density estimation, regression and classification that build upon recent work on bivariate copulas.},
  file = {/home/gkonkamking/pCloudDrive/papers/Fong et al. - 2023 - Martingale posterior distributions 1.pdf;/home/gkonkamking/pCloudDrive/papers/Fong et al. - 2023 - Martingale posterior distributions.pdf}
}

@article{Forbes2002,
  title = {Species Sensitivity Distributions Revisited: {{A}} Critical Appraisal},
  author = {Forbes, Valery E. and Calow, Peter},
  year = {2002},
  journal = {Human and Ecological Risk Assessment},
  volume = {8},
  number = {3},
  pages = {473--492},
  issn = {1080-7039},
  doi = {10.1080/20028091057033},
  abstract = {We revisit the assumptions associated with the derivation and application of species sensitivity distributions (SSDs). Our questions are (1) Do SSDs clarify or obscure the setting of ecological effects thresholds for risk assessment? and (2) Do SSDs reduce or introduce uncertainty into risk assessment? Our conclusions are that if we could determine a community sensitivity distribution, this would provide a better estimate of an ecologically relevant effects threshold and therefore be an improvement for risk assessment. However, the distributions generated are typically based on haphazard collections of species and endpoints and by adjusting these to reflect more realistic trophic structures we show that effects thresholds can be shifted but in a direction and to an extent that is not predictable. Despite claims that the SSD approach uses all available data to assess effects, we demonstrate that in certain frequently used applications only a small fraction of the species going into the SSD determine the effects threshold. If the SSD approach is to lead to better risk assessments, improvements are needed in how the theory is put into practice. This requires careful definition of the risk assessment targets and of the species and endpoints selected for use in generating SSDs.},
  isbn = {1080-7039},
  keywords = {nosource}
}

@article{Forbes2008,
  title = {The Extrapolation Problem and How Population Modeling Can Help.},
  author = {Forbes, Valery E and Calow, Peter and Sibly, Richard M},
  year = {2008},
  month = oct,
  journal = {Environmental toxicology and chemistry / SETAC},
  volume = {27},
  number = {10},
  eprint = {19108041},
  eprinttype = {pubmed},
  pages = {1987--94},
  issn = {0730-7268},
  abstract = {We argue that population modeling can add value to ecological risk assessment by reducing uncertainty when extrapolating from ecotoxicological observations to relevant ecological effects. We review other methods of extrapolation, ranging from application factors to species sensitivity distributions to suborganismal (biomarker and "-omics") responses to quantitative structure-activity relationships and model ecosystems, drawing attention to the limitations of each. We suggest a simple classification of population models and critically examine each model in an extrapolation context. We conclude that population models have the potential for adding value to ecological risk assessment by incorporating better understanding of the links between individual responses and population size and structure and by incorporating greater levels of ecological complexity. A number of issues, however, need to be addressed before such models are likely to become more widely used. In a science context, these involve challenges in parameterization, questions about appropriate levels of complexity, issues concerning how specific or general the models need to be, and the extent to which interactions through competition and trophic relationships can be easily incorporated.},
  isbn = {0730-7268},
  pmid = {19108041},
  keywords = {Data Interpretation,duplicate-citation-key,Ecotoxicology,Ecotoxicology: statistics \& numerical data,Environmental Pollutants,Environmental Pollutants: toxicity,Environmental Pollution,Environmental Pollution: statistics \& numerical da,Models,nosource,Population,Quantitative Structure-Activity Relationship,Statistical}
}

@article{Forbes2011,
  title = {Adding Value to Ecological Risk Assessment with Population Modeling},
  author = {Forbes, Valery E. and Calow, Peter and Grimm, Volker and Hayashi, Takehiko I. and Jager, Tjalling and Katholm, Agnete and Palmqvist, Annemette and Pastorok, Rob and Salvito, Dan and Sibly, Richard M and Spromberg, Julann and Stark, John and a. Stillman, Richard},
  year = {2011},
  journal = {Human and Ecological Risk Assessment},
  volume = {17},
  number = {2},
  pages = {287--299},
  issn = {1080-7039},
  doi = {10.1080/10807039.2011.552391},
  abstract = {Current measures used to estimate the risks of toxic chemicals are not relevant to the goals of the environmental protection process, and thus ecological risk assessment (ERA) is not used as extensively as it should be as a basis for cost-effective management of environmental resources. Appropriate population models can provide a powerful basis for expressing ecological risks that better inform the environmental management process and thus that are more likely to be used by managers. Here we provide at least five reasons why population modeling should play an important role in bridging the gap between what we measure and what we want to protect. We then describe six actions needed for its implementation into management-relevant ERA.},
  isbn = {1080-7039},
  keywords = {nosource}
}

@article{Forfait-Dubuc2012,
  title = {Survival Data Analyses in Ecotoxicology: {{Critical}} Effect Concentrations, Methods and Models. {{What}} Should We Use?},
  author = {{Forfait-Dubuc}, Carole and Charles, Sandrine and Billoir, Elise and {Delignette-Muller}, Marie Laure},
  year = {2012},
  journal = {Ecotoxicology},
  volume = {21},
  number = {4},
  pages = {1072--1083},
  publisher = {Springer},
  issn = {09639292},
  doi = {10.1007/s10646-012-0860-0},
  abstract = {In ecotoxicology, critical effect concentrations are the most common indicators to quantitatively assess risks for species exposed to contaminants. Three types of critical effect concentrations are classically used: lowest/ no observed effect concentration (LOEC/NOEC), LC( x) (x\% lethal concentration) and NEC (no effect concentration). In this article, for each of these three types of critical effect concentration, we compared methods or models used for their estimation and proposed one as the most appropriate. We then compared these critical effect concentrations to each other. For that, we used nine survival data sets corresponding to D. magna exposition to nine different contaminants, for which the time-course of the response was monitored. Our results showed that: (i) LOEC/NOEC values at day 21 were method-dependent, and that the Cochran-Armitage test with a step-down procedure appeared to be the most protective for the environment; (ii) all tested concentration-response models we compared gave close values of LC50 at day 21, nevertheless the Weibull model had the lowest global mean deviance; (iii) a simple threshold NEC-model both concentration and time dependent more completely described whole data (i.e. all timepoints) and enabled a precise estimation of the NEC. We then compared the three critical effect concentrations and argued that the use of the NEC might be a good option for environmental risk assessment.},
  isbn = {0963-9292},
  pmid = {22302371},
  keywords = {Bayesian inference,Concentration-response curves,Hypothesis testing,NOEC/LCx/NEC,nosource,Risk assessment}
}

@article{forman2008pearson,
  title = {The Pearson Diffusions: {{A}} Class of Statistically Tractable Diffusion Processes},
  author = {Forman, Julie Lyng and Sorensen, Michael},
  year = {2008},
  journal = {Scandinavian Journal of Statistics},
  volume = {35},
  number = {3},
  pages = {438--465},
  doi = {10.1111/j.1467-9469.2007.00592.x},
  abstract = {Abstract. The Pearson diffusions form a flexible class of diffusions defined by having linear drift and quadratic squared diffusion coefficient. It is demonstrated that for this class explicit statistical inference is feasible. A complete model classification is presented for the ergodic Pearson diffusions. The class of stationary distributions equals the full Pearson system of distributions. Well-known instances are the Ornstein--Uhlenbeck processes and the square root (CIR) processes. Also diffusions with heavy-tailed and skew marginals are included. Explicit formulae for the conditional moments and the polynomial eigenfunctions are derived. Explicit optimal martingale estimating functions are found. The discussion covers GMM, quasi-likelihood, non-linear weighted least squares estimation and likelihood inference too. The analytical tractability is inherited by transformed Pearson diffusions, integrated Pearson diffusions, sums of Pearson diffusions and Pearson stochastic volatility models. For the non-Markov models, explicit optimal prediction-based estimating functions are found. The estimators are shown to be consistent and asymptotically normal.},
  keywords = {eigenfunction,ergodic diffusion,integrated diffusion,likelihood inference,martingale estimating function,mixing,nosource,optimal estimating function,Pearson system,prediction-based estimating function,quasi-likelihood,spectral methods,stochastic differential equation,stochastic volatility},
  file = {/home/gkonkamking/pCloudDrive/papers/Forman_Sorensen_2008_The pearson diffusions.pdf}
}

@article{forteMethodsToolsBayesian2018,
  title = {Methods and {{Tools}} for {{Bayesian Variable Selection}} and {{Model Averaging}} in {{Normal Linear Regression}}},
  author = {Forte, Anabel and {Garcia-Donato}, Gonzalo and Steel, Mark},
  year = {2018},
  journal = {International Statistical Review},
  volume = {86},
  number = {2},
  pages = {237--258},
  issn = {1751-5823},
  doi = {10.1111/insr.12249},
  urldate = {2022-09-07},
  abstract = {In this paper, we briefly review the main methodological aspects concerned with the application of the Bayesian approach to model choice and model averaging in the context of variable selection in regression models. This includes prior elicitation, summaries of the posterior distribution and computational strategies. We then examine and compare various publicly available R-packages, summarizing and explaining the differences between packages and giving recommendations for applied users. We find that all packages reviewed (can) lead to very similar results, but there are potentially important differences in flexibility and efficiency of the packages.},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Forte et al_2018_Methods and Tools for Bayesian Variable Selection and Model Averaging in Normal.pdf;/home/gkonkamking/Zotero/storage/AHSWTPFY/msbmainr_isr_r1bwithannex.pdf}
}

@article{Fox2008,
  title = {{{NECS}}, {{NOECS}} and the {{ECx}}},
  author = {Fox, David R.},
  year = {2008},
  journal = {Australasian Journal of Ecotoxicology},
  volume = {14},
  pages = {7--9},
  abstract = {Australian researchers have made a number of significant contributions to the science of ecological toxicity (ecotox) testing. A significant development was the generalisation of the method proposed by Aldenberg and Slob (1993) for setting confidence limits on a hazardous concentration},
  keywords = {nosource}
}

@article{Fox2008a,
  title = {An {{HDP-HMM}} for Systems with State Persistence},
  author = {Fox, Emily B. and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
  year = {2008},
  journal = {Proceedings of the 25th International Conference on Machine Learning (ICML)},
  number = {JANUARY 2008},
  pages = {312--319},
  doi = {10.1145/1390156.1390196},
  abstract = {The hierarchical Dirichlet process hidden Markov model (HDP-HMM) is a flexible, nonparametric model which allows state spaces of unknown size to be learned from data. We demonstrate some limitations of the original HDP-HMM formulation (Teh et al., 2006), and propose a sticky extension which allows more robust learning of smoothly varying dynamics. Using DP mixtures, this formulation also allows learning of more complex, multimodal emission distributions. We further develop a sampling algorithm that employs a truncated approximation of the DP to jointly resample the full state sequence, greatly improving mixing rates. Via extensive experiments with synthetic data and the NIST speaker diarization database, we demonstrate the advantages of our sticky extension, and the utility of the HDP-HMM in real-world applications.},
  isbn = {9781605582054},
  keywords = {Dirichlet Process,HDP,HMM,nosource,persistence,speaker diarization}
}

@article{Fox2010,
  title = {A {{Bayesian}} Approach for Determining the No Effect Concentration and Hazardous Concentration in Ecotoxicology.},
  author = {Fox, David R},
  year = {2010},
  month = feb,
  journal = {Ecotoxicology and environmental safety},
  volume = {73},
  number = {2},
  eprint = {19836077},
  eprinttype = {pubmed},
  pages = {123--31},
  publisher = {Elsevier},
  issn = {1090-2414},
  doi = {10.1016/j.ecoenv.2009.09.012},
  abstract = {This paper describes a Bayesian modeling approach to the estimation of the no effect concentration (NEC) and the hazardous concentration (HC(x)) as an alternative to conventional methods based on NOECs - the no observed effect concentration. The advantage of the proposed method is that it combines a plausible model for dose-response data with prior information or belief about the model's parameters to generate posterior distributions for the parameters - one of those being the NEC. The posterior distribution can be used to derive point and interval estimates for the NEC as well as providing uncertainty bounds when used in the development of a species sensitivity distribution (SSD). This latter feature is particularly attractive and overcomes a recognized deficiency of the NOEC-based approach. Examples using previously published data sets are provided which illustrate how the NEC/HC(x) estimation problem is re-cast and solved in this Bayesian framework.},
  isbn = {1090-2414 (Electronic){\textbackslash}n0147-6513 (Linking)},
  pmid = {19836077},
  keywords = {Bayes Theorem,Biological,Dose-Response Relationship,Drug,duplicate-citation-key,Ecosystem protection,Ecotoxicology,Ecotoxicology: methods,Environmental Monitoring,Environmental Monitoring: methods,Hazardous Substances,Hazardous Substances: analysis,Hazardous Substances: toxicity,Hypothesis testing,Models,NOECs,nosource,Risk Assessment,Species sensitivity distribution,Species Specificity,Statistical,Uncertainty}
}

@article{Fox2011,
  title = {Individual versus Population Effects in Concentration-Response Modeling},
  author = {Fox, David R. and Billoir, Elise},
  year = {2011},
  month = jul,
  journal = {Integrated Environmental Assessment and Management},
  volume = {7},
  number = {3},
  eprint = {21692173},
  eprinttype = {pubmed},
  pages = {501--502},
  issn = {15513793},
  doi = {10.1002/ieam.199},
  abstract = {... Article first published online: 20 JUN 2011. DOI: 10.1002 / ieam . 199 . Copyright {\copyright} 2011 SETAC. Issue. Integrated Environmental Assessment and Management. ... Integrated Environmental Assessment and Management, 7: 501--502. doi: 10.1002 / ieam . 199 . Author Information. 1 ... {\textbackslash}n},
  pmid = {21692173},
  keywords = {Dose-Response Relationship,Drug,Ecotoxicology,Ecotoxicology: methods,Humans,Models,nosource,Risk Assessment,Theoretical}
}

@article{fox2011sticky,
  title = {A Sticky {{HDP-HMM}} with Application to Speaker Diarization},
  author = {Fox, Emily B. and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
  year = {2011},
  journal = {The Annals of Applied Statistics},
  volume = {5},
  number = {2A},
  eprint = {0905.2592},
  pages = {1020--1056},
  publisher = {Institute of Mathematical Statistics},
  issn = {19326157},
  doi = {10.1214/10-AOAS395},
  abstract = {We consider the problem of speaker diarization, the problem of segmenting an audio recording of a meeting into temporal segments corresponding to individual speakers. The problem is rendered particularly difficult by the fact that we are not allowed to assume knowledge of the number of people participating in the meeting. To address this problem, we take a Bayesian nonparametric approach to speaker diarization that builds on the hierarchical Dirichlet process hidden Markov model (HDP-HMM) of Teh et al. [J. Amer. Statist. Assoc. 101 (2006) 1566--1581]. Although the basic HDP-HMM tends to over-segment the audio data---creating redundant states and rapidly switching among them---we describe an augmented HDP-HMM that provides effective control over the switching rate. We also show that this augmentation makes it possible to treat emission distributions nonparametrically. To scale the resulting architecture to realistic diarization problems, we develop a sampling algorithm that employs a truncated approximation of the Dirichlet process to jointly resample the full state sequence, greatly improving mixing rates. Working with a benchmark NIST data set, we show that our Bayesian nonparametric architecture yields state-of-the-art speaker diarization results.},
  archiveprefix = {arXiv},
  arxivid = {0905.2592},
  isbn = {1932-6157},
  keywords = {Bayesian nonparametrics,Hidden markov models,Hierarchical dirichlet processes,nosource,Speaker diarization}
}

@article{Fox2012,
  title = {What to Do with {{NOECS}}/{{NOELS}} Prohibition or Innovation?},
  author = {Fox, David R. and Billoir, Elise and Charles, Sandrine and {Delignette-Muller}, Marie Laure and Lopes, Christelle},
  year = {2012},
  month = jan,
  journal = {Integrated environmental assessment and management},
  volume = {8},
  number = {4},
  pages = {764--766},
  abstract = {... Article first published online: 14 SEP 2012. DOI: 10.1002 / ieam . 1350 . Copyright {\copyright} 2012 SETAC. Issue. Integrated Environmental Assessment and Management. ... Integr Environ Assess Manag, 8: 764--766. doi: 10.1002 / ieam . 1350 . Author Information. 1 ...},
  keywords = {nosource}
}

@article{Fox2013a,
  ids = {Fox2013},
  title = {Time-Dependent Species Sensitivity Distributions.},
  author = {Fox, David R and Billoir, Elise},
  year = {2013},
  month = feb,
  journal = {Environmental toxicology and chemistry / SETAC},
  volume = {32},
  number = {2},
  eprint = {23161611},
  eprinttype = {pubmed},
  pages = {378--83},
  issn = {1552-8618},
  doi = {10.1002/etc.2063},
  abstract = {Time is a central component of toxicity assessments. However, current ecotoxicological practice marginalizes time in concentration-response (C-R) modeling and species sensitivity distribution (SSD) analyses. For C-R models, time is invariably fixed, and toxicity measures are estimated from a function fitted to the data at that time. The estimated toxicity measures are used as inputs to the SSD modeling phase, which similarly avoids explicit recognition of the temporal component. The present study extends some commonly employed probability models for SSDs to derive theoretical results that characterize the time-dependent nature of hazardous concentration (HCx) values. The authors' results show that even from very simple assumptions, more complex patterns in the SSD time dependency can be revealed. Environ. Toxicol. Chem. 2013;32:378-383. {\copyright} 2012 SETAC.},
  isbn = {1552-8618},
  pmid = {23161611},
  keywords = {Concentration-response,concentration{\`a}response,Hazardous concentrations,Log-logistic distribution,Log-normal distribution,nosource,Temporal modeling}
}

@article{Fox2015,
  title = {Selection Bias Correction for Species Sensitivity Distribution Modelling},
  author = {Fox, David R.},
  year = {2015},
  journal = {Environmental Toxicology and Chemistry},
  volume = {3098},
  number = {8},
  pages = {1727--1734},
  doi = {10.1002/etc.},
  keywords = {kinetic,nosource}
}

@article{fox2015more,
  title = {More Noise Does Not Mean More Precision: {{A}} Review of {{Aldenberg}} and {{Rorije}} (2013)},
  author = {Fox, David R.},
  year = {2015},
  journal = {Alternatives to laboratory animals : ATLA},
  volume = {43},
  number = {4},
  pages = {241--249},
  issn = {02611929},
  keywords = {nosource}
}

@article{foxRecentDevelopmentsSpecies2021,
  title = {Recent {{Developments}} in {{Species Sensitivity Distribution Modeling}}},
  author = {Fox, D.r. and {van Dam}, R.a. and Fisher, R. and Batley, G.e. and Tillmanns, A.r. and Thorley, J. and Schwarz, C.j. and Spry, D.j. and McTavish, K.},
  year = {2021},
  journal = {Environmental Toxicology and Chemistry},
  volume = {40},
  number = {2},
  pages = {293--308},
  issn = {1552-8618},
  doi = {10.1002/etc.4925},
  urldate = {2023-11-24},
  abstract = {The species sensitivity distribution (SSD) is a statistical approach that is used to estimate either the concentration of a chemical that is hazardous to no more than x\% of all species (the HCx) or the proportion of species potentially affected by a given concentration of a chemical. Despite a significant body of published research and critical reviews over the past 20 yr aimed at improving the methodology, the fundamentals remain unchanged. Although there have been some recent suggestions for improvements to SSD methods in the literature, in general, few of these suggestions have been formally adopted. Furthermore, critics of the approach can rightly point to the fact that differences in technical implementation can lead to marked differences in results, thereby undermining confidence in SSD approaches. Despite the limitations, SSDs remain a practical tool and, until a demonstrably better inferential framework is available, developments and enhancements to conventional SSD practice will and should continue. We therefore believe the time has come for the scientific community to decide how it wants SSD methods to evolve. The present study summarizes the current status of, and elaborates on several recent developments for, SSD methods, specifically, model averaging, multimodality, and software development. We also consider future directions with respect to the use of SSDs, with the ultimate aim of helping to facilitate greater international collaboration and, potentially, greater harmonization of SSD methods. Environ Toxicol Chem 2021;40:293--308. {\copyright} 2020 SETAC},
  copyright = {{\copyright} 2020 SETAC},
  langid = {english},
  keywords = {Computer software,Hazardous concentration,Species sensitivity distrbution,Statistical inference},
  file = {/home/gkonkamking/pCloudDrive/papers/Fox et al_2021_Recent Developments in Species Sensitivity Distribution Modeling.pdf}
}

@article{Frampton2009,
  title = {Effects of Pesticides on Soil Invertebrates in Laboratory Studies: {{A}} Review and Analysis Using Species Sensitivity Distributions},
  author = {Frampton, Geoff K and Jaensch, Stephan and {Scott-Fordsmand}, Janeck J and Roembke, Joerg and {van den Brink}, Paul J},
  year = {2006},
  journal = {Environmental Toxicology and Chemistry},
  volume = {25},
  number = {9},
  pages = {2480--2489},
  publisher = {Wiley Online Library},
  issn = {0730-7268},
  doi = {10.1897/05-438R.1},
  abstract = {Species sensitivity distributions (SSD) and 5\% hazardous concentrations{\textbackslash}n(HC5) are distribution-based approaches for assessing environmental{\textbackslash}nrisks of pollutants. These methods have potential for application in{\textbackslash}npesticide risk assessments, but their applicability for assessing{\textbackslash}npesticide risks to soil invertebrate communities has not been evaluated.{\textbackslash}nUsing data obtained in a systematic review, the present study{\textbackslash}ninvestigates the relevance of SSD and HC5 for predicting pesticide risks{\textbackslash}nto soil invertebrates. Altogether, 1,950 laboratory toxicity data were{\textbackslash}nobtained, representing 250 pesticides and 67 invertebrate taxa. The{\textbackslash}nmajority (96\%) of pesticides have toxicity data for fewer than five{\textbackslash}nspecies. Based on a minimum of five species, the best available endpoint{\textbackslash}ndata (acute mortality median lethal concentration) enabled SSD and HC5{\textbackslash}nto be calculated for 11 pesticides (atrazine, carbendazim, chlorpyrifos,{\textbackslash}ncopper compounds, diazinon, dimethoate, gamma-hexachlorocyclohexane,{\textbackslash}nlambda-cyhalothrin, parathion, pentachlorophenol, and propoxur).{\textbackslash}nArthropods and oligochaetes exhibit pronounced differences in their{\textbackslash}nsensitivity to most of these pesticides. The standard test earthworm{\textbackslash}nspecies, Eisenia fetida sensu lato, is the species that is least{\textbackslash}nsensitive to insecticides based on acute mortality, whereas the standard{\textbackslash}nCollembola test species, Folsomia candida, is among the most sensitive{\textbackslash}nspecies for a broad range of toxic modes of action (biocide, fungicide,{\textbackslash}nherbicide, and insecticide). These findings suggest that soil arthropods{\textbackslash}nshould be tested routinely in regulatory risk assessments. In addition,{\textbackslash}nthe data indicate that the uncertainty factor for earthworm acute{\textbackslash}nmortality tests (i.e., 10) does not fully cover the range of earthworm{\textbackslash}nspecies sensitivities and that acute mortality tests would not provide{\textbackslash}nthe most sensitive risk estimate for earthworms in the majority (95\%){\textbackslash}nof cases.},
  keywords = {acute to chronic ratio,nosource}
}

@article{Frampton2009a,
  title = {Effects of Pesticides on Soil Invertebrates in Laboratory Studies: A Review and Analysis Using Species Sensitivity Distributions},
  author = {Frampton, {\relax GK}},
  year = {2009},
  journal = {Environmental {\dots}},
  volume = {25},
  number = {9},
  pages = {2480--2489},
  keywords = {acute to chronic ratio,duplicate-citation-key,nosource}
}

@article{francoisBayesianClusteringUsing2006,
  title = {Bayesian {{Clustering Using Hidden Markov Random Fields}} in {{Spatial Population Genetics}}},
  author = {Fran{\c c}ois, Olivier and Ancelet, Sophie and Guillot, Gilles},
  year = {2006},
  month = oct,
  journal = {Genetics},
  volume = {174},
  number = {2},
  pages = {805--816},
  issn = {0016-6731, 1943-2631},
  doi = {10.1534/genetics.106.059923},
  urldate = {2020-09-02},
  abstract = {We introduce a new Bayesian clustering algorithm for studying population structure using individually geo-referenced multilocus data sets. The algorithm is based on the concept of hidden Markov random field, which models the spatial dependencies at the cluster membership level. We argue that (i) a Markov chain Monte Carlo procedure can implement the algorithm efficiently, (ii) it can detect significant geographical discontinuities in allele frequencies and regulate the number of clusters, (iii) it can check whether the clusters obtained without the use of spatial priors are robust to the hypothesis of discontinuous geographical variation in allele frequencies, and (iv) it can reduce the number of loci required to obtain accurate assignments. We illustrate and discuss the implementation issues with the Scandinavian brown bear and the human CEPH diversity panel data set.},
  langid = {english},
  keywords = {nosource}
}

@article{frazier2015consistency,
  title = {On Consistency of Approximate Bayesian Computation},
  author = {Frazier, David T and Martin, Gael M and Robert, Christian P},
  year = {2015},
  journal = {arXiv preprint arXiv:1508.05178},
  eprint = {1508.05178},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@article{frazier2016asymptotic,
  title = {Asymptotic Properties of Approximate {{Bayesian}} Computation},
  author = {Frazier, David T and Martin, Gael M and Robert, Christian P and Rousseau, Judith},
  year = {2016},
  journal = {arXiv preprint arXiv:1607.06903},
  eprint = {1607.06903},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@article{frazier2018approximate,
  title = {Approximate Bayesian Forecasting},
  author = {Frazier, David T and Maneesoonthorn, Worapree and Martin, Gael M and McCabe, Brendan P M},
  year = {2018},
  journal = {International Journal of Forecasting},
  volume = {forthcomin},
  keywords = {nosource}
}

@misc{FrequentlyAskedQuestions,
  title = {Frequently {{Asked Questions}} - {{Humanitarian Data Exchange}}},
  urldate = {2023-10-25},
  howpublished = {https://data.humdata.org/faq},
  langid = {australian}
}

@inproceedings{frias2012relationship,
  title = {On the Relationship between Socio-Economic Factors and Cell Phone Usage},
  booktitle = {Proceedings of the Fifth International Conference on Information and Communication Technologies and Development},
  author = {{Frias-Martinez}, Vanessa and Virseda, Jesus},
  year = {2012},
  pages = {76--84},
  keywords = {nosource}
}

@article{friedlineTransformingWealthUsing2015,
  title = {Transforming Wealth: {{Using}} the Inverse Hyperbolic Sine ({{IHS}}) and Splines to Predict Youth's Math Achievement},
  shorttitle = {Transforming Wealth},
  author = {Friedline, Terri and Masa, Rainier D. and Chowa, Gina A. N.},
  year = {2015},
  month = jan,
  journal = {Social Science Research},
  volume = {49},
  pages = {264--287},
  issn = {0049-089X},
  doi = {10.1016/j.ssresearch.2014.08.018},
  urldate = {2020-05-12},
  abstract = {The natural log and categorical transformations commonly applied to wealth for meeting the statistical assumptions of research may not always be appropriate for adjusting for skewness given wealth's unique properties. Finding and applying appropriate transformations is becoming increasingly important as researchers consider wealth as a predictor of well-being. We present an alternative transformation---the inverse hyperbolic sine (IHS)---for simultaneously dealing with skewness and accounting for wealth's unique properties. Using the relationship between household wealth and youth's math achievement as an example, we apply the IHS transformation to wealth data from US and Ghanaian households. We also explore non-linearity and accumulation thresholds by combining IHS transformed wealth with splines. IHS transformed wealth relates to youth's math achievement similarly when compared to categorical and natural log transformations, indicating that it is a viable alternative to other transformations commonly used in research. Non-linear relationships and accumulation thresholds emerge that predict youth's math achievement when splines are incorporated. In US households, accumulating debt relates to decreases in math achievement whereas accumulating assets relates to increases in math achievement. In Ghanaian households, accumulating assets between the 25th and 50th percentiles relates to increases in youth's math achievement.},
  langid = {english},
  keywords = {Inverse hyperbolic sine (IHS) transformation,Math achievement,Panel study of income dynamics,Wealth,YouthSave Ghana Experiment}
}

@book{friedman2001elements,
  title = {The Elements of Statistical Learning},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2001},
  volume = {1},
  publisher = {Springer series in statistics New York, NY, USA:},
  keywords = {nosource}
}

@article{friedman2010regularization,
  title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2010},
  journal = {Journal of statistical software},
  volume = {33},
  number = {1},
  pages = {1},
  publisher = {NIH Public Access},
  keywords = {nosource}
}

@article{fruhwirth-schnatterGeneralizedMixturesFinite2021,
  title = {Generalized {{Mixtures}} of {{Finite Mixtures}} and {{Telescoping Sampling}}},
  author = {{Fr{\"u}hwirth-Schnatter}, Sylvia and {Malsiner-Walli}, Gertraud and Gr{\"u}n, Bettina},
  year = {2021},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {16},
  number = {4},
  pages = {1279--1307},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/21-BA1294},
  urldate = {2023-03-06},
  abstract = {Within a Bayesian framework, a comprehensive investigation of mixtures of finite mixtures (MFMs), i.e., finite mixtures with a prior on the number of components, is performed. This model class has applications in model-based clustering as well as for semi-parametric density estimation and requires suitable prior specifications and inference methods to exploit its full potential. We contribute by considering a generalized class of MFMs where the hyperparameter {$\gamma$}K of a symmetric Dirichlet prior on the weight distribution depends on the number of components. We show that this model class may be regarded as a Bayesian non-parametric mixture outside the class of Gibbs-type priors. We emphasize the distinction between the number of components K of a mixture and the number of clusters K+, i.e., the number of filled components given the data. In the MFM model, K+ is a random variable and its prior depends on the prior on K and on the hyperparameter {$\gamma$}K. We employ a flexible prior distribution for the number of components K and derive the corresponding prior on the number of clusters K+ for generalized MFMs. For posterior inference we propose the novel telescoping sampler which allows Bayesian inference for mixtures with arbitrary component distributions without resorting to reversible jump Markov chain Monte Carlo (MCMC) methods. The telescoping sampler explicitly samples the number of components, but otherwise requires only the usual MCMC steps of a finite mixture model. The ease of its application using different component distributions is demonstrated on several data sets.},
  keywords = {62H30,65C40,Bayesian mixtures,Dirichlet process mixtures,Gibbs-type priors,Pitman-Yor process mixtures,reversible jump MCMC,sparse finite mixtures},
  file = {/home/gkonkamking/pCloudDrive/papers/Frühwirth-Schnatter et al_2021_Generalized Mixtures of Finite Mixtures and Telescoping Sampling.pdf}
}

@article{fruhwirth-schnatterHereInfinitySparse2019,
  title = {From Here to Infinity: Sparse Finite versus {{Dirichlet}} Process Mixtures in Model-Based Clustering},
  shorttitle = {From Here to Infinity},
  author = {{Fr{\"u}hwirth-Schnatter}, Sylvia and {Malsiner-Walli}, Gertraud},
  year = {2019},
  month = mar,
  journal = {Advances in Data Analysis and Classification},
  volume = {13},
  number = {1},
  pages = {33--64},
  issn = {1862-5355},
  doi = {10.1007/s11634-018-0329-y},
  urldate = {2022-07-29},
  abstract = {In model-based clustering mixture models are used to group data points into clusters. A useful concept introduced for Gaussian mixtures by Malsiner Walli et al. (Stat Comput 26:303--324, 2016) are sparse finite mixtures, where the prior distribution on the weight distribution of a mixture with K components is chosen in such a way that a priori the number of clusters in the data is random and is allowed to be smaller than K with high probability. The number of clusters is then inferred a posteriori from the data. The present paper makes the following contributions in the context of sparse finite mixture modelling. First, it is illustrated that the concept of sparse finite mixture is very generic and easily extended to cluster various types of non-Gaussian data, in particular discrete data and continuous multivariate data arising from non-Gaussian clusters. Second, sparse finite mixtures are compared to Dirichlet process mixtures with respect to their ability to identify the number of clusters. For both model classes, a random hyper prior is considered for the parameters determining the weight distribution. By suitable matching of these priors, it is shown that the choice of this hyper prior is far more influential on the cluster solution than whether a sparse finite mixture or a Dirichlet process mixture is taken into consideration.},
  langid = {english},
  keywords = {62C10,62F15,62P99,Count data,Dirichlet prior,Latent class analysis,Marginal likelihoods,Mixture distributions,Skew distributions},
  file = {/home/gkonkamking/pCloudDrive/papers/Frühwirth-Schnatter_Malsiner-Walli_2019_From here to infinity.pdf}
}

@article{Fulwiler2014,
  title = {The Rhetoric of "{{Job Market}}" and the Reality of the Academic Labor System},
  author = {Bousquet, Marc},
  year = {2011},
  journal = {College English},
  volume = {66},
  number = {2},
  pages = {207--228},
  keywords = {nosource}
}

@article{gabry2019visualization,
  title = {Visualization in Bayesian Workflow},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  year = {2019},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {182},
  number = {2},
  pages = {389--402},
  publisher = {Wiley Online Library},
  keywords = {nosource}
}

@inproceedings{gael2009infinite,
  title = {The Infinite Factorial Hidden {{Markov}} Model},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Gael, Jurgen V and Teh, Yee W and Ghahramani, Zoubin},
  year = {2009},
  pages = {1697--1704},
  keywords = {nosource}
}

@article{gailTwoApproachesEstimating1999,
  title = {Two {{Approaches}} for {{Estimating Disease Prevalence}} from {{Population-Based Registries}} of {{Incidence}} and {{Total Mortality}}},
  author = {Gail, Mitchell H. and Kessler, Larry and Midthune, Douglas and Scoppa, Steven},
  year = {1999},
  journal = {Biometrics},
  volume = {55},
  number = {4},
  pages = {1137--1144},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.1999.01137.x},
  urldate = {2021-05-07},
  abstract = {Summary. Two approaches are described for estimating the prevalence of a disease that may have developed in a previous restricted age interval among persons of a given age at a particular calendar time. The prevalence for all those who ever developed disease is treated as a special case. The counting method (CM) obtains estimates of prevalence by dividing the estimated number of diseased persons by the total population size, taking loss to follow-up into account. The transition rate method (TRM) uses estimates of transition rates and competing risk calculations to estimate prevalence. Variance calculations are described for CM and TRM as well as for a variant of CM, called counting method times 10 (CM10), that is designed to yield more precise estimates than CM. We compare these three estimators in terms of precision and in terms of the underlying assumptions required to justify the methods. CM makes fewer assumptions but is typically less precise than TRM or CM10. For common diseases such as breast cancer, CM may be preferred because its precision is excellent even though not as high as for TRM or CM10. For less common diseases, such as brain cancer, however, TRM or CM10 and other methods that make stabilizing assumptions may be preferred to CM.},
  langid = {english},
  keywords = {Bias of prevalence estimate,Cancer registry,Chronic disease prevalence,Lexis diagram,Precision of prevalence estimate,Prevalence estimation},
  file = {/home/gkonkamking/Zotero/storage/724QDGMQ/Gail et al. - 1999 - Two Approaches for Estimating Disease Prevalence f.pdf}
}

@article{galinskyPowerPerspectivesNot2016,
  ids = {galinskyPowerPerspectivesNot2016a},
  title = {Power and {{Perspectives Not Taken}}:},
  shorttitle = {Power and {{Perspectives Not Taken}}},
  author = {Galinsky, Adam D. and Magee, Joe C. and Inesi, M. Ena and Gruenfeld, Deborah H.},
  year = {2016},
  month = may,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {10.1111\_j.1467-9280.2006.01824.x},
  urldate = {2020-05-11},
  abstract = {Four experiments and a correlational study explored the relationship between power and perspective taking. In Experiment 1, participants primed with high power ...},
  langid = {english},
  keywords = {nosource}
}

@article{galtier2001maximum,
  title = {Maximum-Likelihood Phylogenetic Analysis under a Covarion-like Model},
  author = {Galtier, Nicolas},
  year = {2001},
  journal = {Molecular Biology and Evolution},
  volume = {18},
  number = {5},
  pages = {866--873},
  publisher = {Oxford University Press},
  keywords = {nosource}
}

@article{galtier2004markov,
  title = {Markov-Modulated {{Markov}} Chains and the Covarion Process of Molecular Evolution},
  author = {Galtier, Nicolas and {Jean-Marie}, Alain},
  year = {2004},
  journal = {Journal of Computational Biology},
  volume = {11},
  number = {4},
  pages = {727--733},
  publisher = {Mary Ann Liebert, Inc. 2 Madison Avenue Larchmont, NY 10538 USA},
  keywords = {nosource}
}

@article{garcia2021exact,
  title = {Exact Simulation of Coupled {{Wright}}--{{Fisher}} Diffusions},
  author = {{Garc{\'i}a-Pareja}, Celia and Hult, Henrik and Koski, Timo},
  year = {2021},
  journal = {Advances in Applied Probability},
  volume = {53},
  number = {4},
  pages = {923--950},
  publisher = {Cambridge University Press},
  doi = {10.1017/apr.2021.9},
  file = {/home/gkonkamking/pCloudDrive/papers/García-Pareja et al_2021_Exact simulation of coupled Wright–Fisher diffusions.pdf}
}

@article{Garner2015,
  title = {Species Sensitivity Distributions for Engineered Nanomaterials},
  author = {Garner, Kendra L. and Suh, Sangwon and Lenihan, Hunter S. and Keller, Arturo A.},
  year = {2015},
  journal = {Environmental Science and Technology},
  volume = {49},
  number = {9},
  pages = {5753--5759},
  issn = {15205851},
  doi = {10.1021/acs.est.5b00081},
  abstract = {Engineered nanomaterials (ENMs) are a relatively new strain of materials for which little is understood about their impacts. A species sensitivity distribution (SSDs) is a cumulative probability distribution of a chemical's toxicity measurements obtained from single-species bioassays of various species that can be used to estimate the ecotoxicological impacts of a chemical. The recent increase in the availability of acute toxicity data for ENMs enabled the construction of 10 ENM-specific SSDs, with which we analyzed (1) the range of toxic concentrations, (2) whether ENMs cause greater hazard to an ecosystem than the ionic or bulk form, and (3) the key parameters that affect variability in toxicity. The resulting estimates for hazardous concentrations at which 5\% of species will be harmed ranged from {$<$}1 ug/L for PVP-coated n-Ag to {$>$}3.5 mg/L for CNTs. The results indicated that size, formulation, and the presence of a coating can alter toxicity, and thereby corresponding SSDs. Few statistical differences were observed between SSDs of an ENM and its ionic counterpart. However, we did find a significant correlation between the solubility of ENMs and corresponding SSD. Uncertainty in SSD values can be reduced through greater consideration of ENM characteristics and physiochemical transformations in the environment.},
  pmid = {25875138},
  keywords = {nosource}
}

@article{Garnier-laplace2006,
  title = {First Derivation of Predicted-No-Effect Values for Freshwater and Terrestrial Ecosystems Exposed to Radioactive Substances.},
  author = {{Garnier-laplace}, Jacqueline and {Della-Vedova}, Claire and Gilbin, Rodolphe and Copplestone, David and Hingston, Joanne and Ciffroy, Philippe},
  year = {2006},
  month = oct,
  journal = {Environmental science \& technology},
  volume = {40},
  number = {20},
  eprint = {17120586},
  eprinttype = {pubmed},
  pages = {6498--505},
  issn = {0013-936X},
  abstract = {The FASSET Radiation Effects Database (FRED) constitutes a unique structured resource of the biological effects of ionizing radiation on non-human species mainly from temperate ecosystems, encompassing 26,000 primary data entries. Quality-assessed data were extracted from FRED and dose-effect relationships were constructed to provide estimates of ED50 and EDR10. These estimates are Doses (or Dose Rates) related to the percent change in the average level of the endpoint for a particular effect (50\% or 10\% for acute or chronic exposure regimes, respectively). Acute and chronic Species Sensitivity Distributions (SSDs) were built on the basis of these data sets, and the Assessment Factor Method (AFM) was applied when data were too scarce. The Hazardous Dose corresponding to 5\% of species acutely affected at the 50\% effect level varied from 1 to 5.5 Gy according to the ecosystem. For chronic gamma external irradiation exposure, no-effect values varied from 10 microGy/h for freshwaters through application of the AFM to 67 microGy/h for terrestrial ecosystems, corresponding to the 5th percentile of the non-weighted SSD (vs 229 microGy/h when trophic weights are applied). These values are higher by ca. x50 to x100 than the upper bound of natural background, and lower than dose rates triggering effects at individual levels on contaminated sites.},
  isbn = {0013-936X},
  pmid = {17120586},
  keywords = {Amphibians,Amphibians: growth \& development,Animals,Daphnia,Daphnia: growth \& development,Daphnia: radiation effects,Dose-Response Relationship,duplicate-citation-key,Ecosystem,Eukaryota,Eukaryota: growth \& development,Eukaryota: radiation effects,Fishes,Fishes: growth \& development,Fresh Water,Fresh Water: analysis,Ionizing,nosource,Radiation,Radiation Monitoring,Radiation Monitoring: methods,Radioactive,Radioactive Waste,Radioactive Waste: analysis,Radioactive: analysis,Soil Pollutants,Water Pollutants}
}

@article{Garnier-Laplace2010,
  title = {A Multi-Criteria Weight of Evidence Approach for Deriving Ecological Benchmarks for Radioactive Substances.},
  author = {{Garnier-Laplace}, J and {Della-Vedova}, C and Andersson, P and Copplestone, D and Cailes, C and a Beresford, N and Howard, B J and Howe, P and Whitehouse, P},
  year = {2010},
  month = jun,
  journal = {Journal of radiological protection : official journal of the Society for Radiological Protection},
  volume = {30},
  number = {2},
  eprint = {20530866},
  eprinttype = {pubmed},
  pages = {215--33},
  issn = {1361-6498},
  doi = {10.1088/0952-4746/30/2/S02},
  abstract = {Dose rate benchmarks are required in the tiered approaches used to screen out benign exposure scenarios in radiological ecological risk assessment. Such screening benchmarks, namely the predicted no-effect dose rates (PNEDR), have been derived by applying, as far as possible, the European guidance developed for chemicals. To derive the ecosystem level (or generic) PNEDR, radiotoxicity EDR(10) data (dose rates giving a 10\% effect in comparison with the control) were used to fit a species sensitivity distribution (SSD) and estimate the HDR(5) (the hazardous dose rate affecting 5\% of species with a 10\% effect). Then, a multi-criteria approach was developed to justify using an assessment factor (AF) to apply to the HDR(5) for estimating a PNEDR value. Several different statistical data treatments were considered which all gave reasonably similar results. The suggested generic screening value of 10 microGy h(-1) (incremental dose rate) was derived using the lowest available EDR(10) value per species, an unweighted SSD, and an AF of 2 applied to the estimated HDR(5). Consideration was also given to deriving screening benchmark values for organism groups but this was not thought to be currently appropriate due to few relevant data being currently available.},
  isbn = {09524746 (ISSN)},
  pmid = {20530866},
  keywords = {Animals,Benchmarking,duplicate-citation-key,Ecosystem,Environmental Exposure,Environmental Exposure: prevention \& control,Environmental Exposure: standards,nosource,Radiation Dosage,Radiation Injuries,Radiation Injuries: prevention \& control,Radiation Injuries: veterinary,Radiation Monitoring,Radiation Monitoring: standards,Radioisotopes,Radioisotopes: analysis}
}

@article{Garske2011,
  title = {Travel Patterns in China},
  author = {Garske, Tini and Yu, Hongjie and Peng, Zhibin and Ye, Min and Zhou, Hang and Cheng, Xiaowen and Wu, Jiabing and Ferguson, Neil},
  year = {2011},
  month = jan,
  journal = {PLoS ONE},
  volume = {6},
  number = {2},
  pages = {e16364},
  issn = {19326203},
  doi = {10.1371/journal.pone.0016364},
  abstract = {The spread of infectious disease epidemics is mediated by human travel. Yet human mobility patterns vary substantially between countries and regions. Quantifying the frequency of travel and length of journeys in well-defined population is therefore critical for predicting the likely speed and pattern of spread of emerging infectious diseases, such as a new influenza pandemic. Here we present the results of a large population survey undertaken in 2007 in two areas of China: Shenzhen city in Guangdong province, and Huangshan city in Anhui province. In each area, 10,000 randomly selected individuals were interviewed, and data on regular and occasional journeys collected. Travel behaviour was examined as a function of age, sex, economic status and home location. Women and children were generally found to travel shorter distances than men. Travel patterns in the economically developed Shenzhen region are shown to resemble those in developed and economically advanced middle income countries with a significant fraction of the population commuting over distances in excess of 50 km. Conversely, in the less developed rural region of Anhui, travel was much more local, with very few journeys over 30 km. Travel patterns in both populations were well-fitted by a gravity model with a lognormal kernel function. The results provide the first quantitative information on human travel patterns in modern China, and suggest that a pandemic emerging in a less developed area of rural China might spread geographically sufficiently slowly for containment to be feasible, while spatial spread in the more economically developed areas might be expected to be much more rapid, making containment more difficult.},
  isbn = {1932-6203 (Electronic){\textbackslash}r1932-6203 (Linking)},
  pmid = {21311745},
  keywords = {80 and over,Adolescent,Adult,Aged,Child,China,China: epidemiology,Communicable Diseases,Communicable Diseases: epidemiology,Communicable Diseases: transmission,Family Characteristics,Female,Humans,Infant,Male,Middle Aged,Newborn,nosource,Occupations,Occupations: statistics \& numerical data,Preschool,Rural Population,Rural Population: statistics \& numerical data,Schools,Schools: statistics \& numerical data,Transportation,Transportation: statistics \& numerical data,Travel,Travel: statistics \& numerical data,Workplace,Workplace: statistics \& numerical data,Young Adult}
}

@article{gascuelBIONJImprovedVersion1997,
  title = {{{BIONJ}}: An Improved Version of the {{NJ}} Algorithm Based on a Simple Model of Sequence Data},
  shorttitle = {{{BIONJ}}},
  author = {Gascuel, O.},
  year = {1997},
  month = jul,
  journal = {Molecular Biology and Evolution},
  volume = {14},
  number = {7},
  pages = {685--695},
  issn = {0737-4038, 1537-1719},
  doi = {10.1093/oxfordjournals.molbev.a025808},
  urldate = {2021-02-22},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/5WBU37KP/Gascuel - 1997 - BIONJ an improved version of the NJ algorithm bas.pdf}
}

@article{gauvin2006valeurs,
  title = {Valeurs Moyennes de La Caract{\'e}ristique d'{{Euler-Poincar{\'e}}} et Des Nombres de {{Betti}} Du Modele de {{Potts}}},
  author = {Gauvin, Laetitia},
  year = {2006},
  keywords = {⛔ No DOI found,nosource}
}

@article{gauvin2009phase,
  title = {Phase Diagram of a {{Schelling}} Segregation Model},
  author = {Gauvin, Laetitia and Vannimenus, Jean and Nadal, J-P},
  year = {2009},
  journal = {The European Physical Journal B},
  volume = {70},
  number = {2},
  pages = {293--304},
  publisher = {Springer-Verlag},
  doi = {10.1140/epjb/e2009-00234-0},
  keywords = {nosource}
}

@phdthesis{gauvin2010modelisation,
  title = {Mod{\'e}lisation de Syst{\`e}mes Socio-{\'E}conomiques {\`a} l'aide Des Outils de Physique Statistique},
  author = {Gauvin, Laetitia},
  year = {2010},
  school = {Universit{\'e} Pierre et Marie Curie-Paris VI},
  keywords = {nosource}
}

@article{gauvin2010schelling,
  title = {Schelling Segregation in an Open City: {{A}} Kinetically Constrained {{Blume-Emery-Griffiths}} Spin-1 System},
  author = {Gauvin, Laetitia and Nadal, Jean-Pierre and Vannimenus, Jean},
  year = {2010},
  journal = {Physical Review E},
  volume = {81},
  number = {6},
  pages = {066120},
  publisher = {APS},
  doi = {10.1103/PhysRevE.81.066120},
  keywords = {nosource}
}

@article{gauvin2013activity,
  title = {Activity Clocks: Spreading Dynamics on Temporal Networks of Human Contact},
  author = {Gauvin, Laetitia and Panisson, Andr{\'e} and Cattuto, Ciro and Barrat, Alain},
  year = {2013},
  journal = {Scientific reports},
  volume = {3},
  number = {1},
  pages = {1--6},
  publisher = {Nature Publishing Group},
  doi = {10.1038/srep03099},
  keywords = {nosource}
}

@article{gauvin2013modeling,
  title = {Modeling Urban Housing Market Dynamics: Can the Socio-Spatial Segregation Preserve Some Social Diversity?},
  author = {Gauvin, Laetitia and Vignes, Annick and Nadal, Jean-Pierre},
  year = {2013},
  journal = {Journal of Economic Dynamics and Control},
  volume = {37},
  number = {7},
  pages = {1300--1321},
  publisher = {Elsevier},
  doi = {10.1016/j.jedc.2013.03.001},
  keywords = {nosource}
}

@inproceedings{gauvin2013schelling,
  title = {Schelling-Type Urban Segregation Models with Switching and Preferential Dynamics},
  booktitle = {Proc. {{Of}} the European Conference on Complex Systems, {{ECCS2013}}},
  author = {Gauvin, Laetitia and Hazan, Aur{\'e}lien and {Randon-Furling}, Julien},
  year = {2013},
  keywords = {⛔ No DOI found,nosource}
}

@article{Gauvin2014,
  ids = {gauvin2014detecting},
  title = {Detecting the Community Structure and Activity Patterns of Temporal Networks: {{A}} Non-Negative Tensor Factorization Approach},
  author = {Gauvin, Laetitia and Panisson, Andr{\'e} and Cattuto, Ciro},
  year = {2014},
  journal = {PLoS ONE},
  volume = {9},
  number = {1},
  eprint = {1308.0723v3},
  publisher = {Public Library of Science},
  issn = {19326203},
  doi = {10.1371/journal.pone.0086028},
  abstract = {The increasing availability of temporal network data is calling for more research on extracting and characterizing mesoscopic structures in temporal networks and on relating such structure to specific functions or properties of the system. An outstanding challenge is the extension of the results achieved for static networks to time-varying networks, where the topological structure of the system and the temporal activity patterns of its components are intertwined. Here we investigate the use of a latent factor decomposition technique, non-negative tensor factorization, to extract the community-activity structure of temporal networks. The method is intrinsically temporal and allows to simultaneously identify communities and to track their activity over time. We represent the time-varying adjacency matrix of a temporal network as a three-way tensor and approximate this tensor as a sum of terms that can be interpreted as communities of nodes with an associated activity time series. We summarize known computational techniques for tensor decomposition and discuss some quality metrics that can be used to tune the complexity of the factorized representation. We subsequently apply tensor factorization to a temporal network for which a ground truth is available for both the community structure and the temporal activity patterns. The data we use describe the social interactions of students in a school, the associations between students and school classes, and the spatio-temporal trajectories of students over time. We show that non-negative tensor factorization is capable of recovering the class structure with high accuracy. In particular, the extracted tensor components can be validated either as known school classes, or in terms of correlated activity patterns, i.e., of spatial and temporal coincidences that are determined by the known school activity schedule.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1308.0723v3},
  pmid = {24497935},
  keywords = {nosource},
  file = {/home/gkonkamking/pCloudDrive/papers/Gauvin et al_2014_Detecting the community structure and activity patterns of temporal networks.pdf}
}

@article{gauvin2015revealing,
  title = {Revealing Latent Factors of Temporal Networks for Mesoscale Intervention in Epidemic Spread},
  author = {Gauvin, Laetitia and Panisson, Andr{\'e} and Barrat, Alain and Cattuto, Ciro},
  year = {2015},
  journal = {arXiv preprint arXiv:1501.02758},
  eprint = {1501.02758},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,nosource}
}

@article{gauvin2018randomized,
  title = {Randomized Reference Models for Temporal Networks},
  author = {Gauvin, Laetitia and G{\'e}nois, Mathieu and Karsai, M{\'a}rton and Kivel{\"a}, Mikko and Takaguchi, Taro and Valdano, Eugenio and Vestergaard, Christian L},
  year = {2018},
  journal = {Accepted in SIAM},
  keywords = {⛔ No DOI found,nosource}
}

@article{gauvin2020gender,
  title = {Gender Gaps in Urban Mobility},
  author = {Gauvin, Laetitia and Tizzoni, Michele and Piaggesi, Simone and Young, Andrew and Adler, Natalia and Verhulst, Stefaan and Ferres, Leo and Cattuto, Ciro},
  year = {2020},
  journal = {Humanities and Social Sciences Communications},
  volume = {7},
  number = {1},
  pages = {1--13},
  publisher = {Palgrave},
  doi = {10.1057/s41599-020-0500-x},
  file = {/home/gkonkamking/Zotero/storage/AHW5KS4V/Gauvin et al. - 2020 - Gender gaps in urban mobility.pdf}
}

@article{gauvin2021socio,
  title = {Socio-Economic Determinants of Mobility Responses during the First Wave of {{COVID-19}} in {{Italy}}: From Provinces to Neighbourhoods},
  author = {Gauvin, Laetitia and Bajardi, Paolo and Pepe, Emanuele and Lake, Brennan and Privitera, Filippo and Tizzoni, Michele},
  year = {2021},
  journal = {Journal of The Royal Society Interface},
  volume = {18},
  number = {181},
  pages = {20210092},
  publisher = {The Royal Society},
  doi = {10.1098/rsif.2021.0092},
  keywords = {nosource}
}

@article{gauvinGenderGapsUrban2020,
  title = {Gender Gaps in Urban Mobility},
  author = {Gauvin, Laetitia and Tizzoni, Michele and Piaggesi, Simone and Young, Andrew and Adler, Natalia and Verhulst, Stefaan and Ferres, Leo and Cattuto, Ciro},
  year = {2020},
  month = jun,
  journal = {Humanities and Social Sciences Communications},
  volume = {7},
  number = {1},
  pages = {1--13},
  publisher = {Palgrave},
  issn = {2662-9992},
  doi = {10.1057/s41599-020-0500-x},
  urldate = {2023-10-23},
  abstract = {Mobile phone data have been extensively used to study urban mobility. However, studies based on gender-disaggregated large-scale data are still lacking, limiting our understanding of gendered aspects of urban mobility and our ability to design policies for gender equality. Here we study urban mobility from a gendered perspective, combining commercial and open datasets for the city of Santiago, Chile. We analyze call detail records for a large cohort of anonymized mobile phone users and reveal a gender gap in mobility: women visit fewer unique locations than men, and distribute their time less equally among such locations. Mapping this mobility gap over administrative divisions, we observe that a wider gap is associated with lower income and lack of public and private transportation options. Our results uncover a complex interplay between gendered mobility patterns, socio-economic factors and urban affordances, calling for further research and providing insights for policymakers and urban planners.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Science,Sociology,technology and society},
  file = {/home/gkonkamking/pCloudDrive/papers/Gauvin et al_2020_Gender gaps in urban mobility.pdf}
}

@inproceedings{gauvinModelingUrbanHousing2011,
  title = {Modeling Urban Housing Market Dynamics: Can the Socio-Spatial Segregation Preserve Some Social Diversity?},
  booktitle = {International {{Conference}} on {{Complex Systems}}},
  author = {Gauvin, Laetitia and Vignes, Annick and Nadal, Jean-Pierre},
  year = {2011},
  keywords = {nosource}
}

@inproceedings{gauvinSchellingsSegregationModel2010,
  title = {Schelling's Segregation Model for an Open City : Emergence of Physical Frontiers from a Socio-Spatial Dynamics},
  booktitle = {European {{Conference}} on {{Complex Systems}} 2010},
  author = {Gauvin, Laetitia and Nadal, Jean-Pierre and Vannimenus, Jean},
  year = {2010},
  keywords = {nosource}
}

@article{GBZWP16,
  title = {Simultaneous Identification of Specifically Interacting Paralogs and Interprotein Contacts by Direct Coupling Analysis},
  author = {Gueudr{\'e}, Thomas and Baldassi, Carlo and Zamparo, Marco and Weigt, Martin and Pagnani, Andrea},
  year = {2016},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {43},
  pages = {12186--12191},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1607570113},
  abstract = {Understanding protein-protein interactions is central to our understanding of almost all complex biological processes. Computational tools exploiting rapidly growing genomic databases to characterize protein-protein interactions are urgently needed. Such methods should connect multiple scales from evolutionary conserved interactions between families of homologous proteins, over the identification of specifically interacting proteins in the case of multiple paralogs inside a species, down to the prediction of residues being in physical contact across interaction interfaces. Statistical inference methods detecting residue-residue coevolution have recently triggered considerable progress in using sequence data for quaternary protein structure prediction; they require, however, large joint alignments of homologous protein pairs known to interact. The generation of such alignments is a complex computational task on its own; application of coevolutionary modeling has, in turn, been restricted to proteins without paralogs, or to bacterial systems with the corresponding coding genes being colocalized in operons. Here we show that the direct coupling analysis of residue coevolution can be extended to connect the different scales, and simultaneously to match interacting paralogs, to identify interprotein residue-residue contacts and to discriminate interacting from noninteracting families in a multiprotein system. Our results extend the potential applications of coevolutionary analysis far beyond cases treatable so far.},
  keywords = {nosource}
}

@article{gelfand1996model,
  title = {Model Determination Using Sampling-Based Methods},
  author = {Gelfand, Alan E.},
  year = {1996},
  journal = {Markov chain Monte Carlo in practice},
  pages = {145--161},
  publisher = {{London: Chapman and Hall}},
  keywords = {nosource}
}

@article{gelfand1998model,
  title = {Model Choice: A Minimum Posterior Predictive Loss Approach},
  author = {Gelfand, Alan E and Ghosh, Sujit K},
  year = {1998},
  journal = {Biometrika},
  volume = {85},
  number = {1},
  pages = {1--11},
  publisher = {Oxford University Press},
  doi = {10.1093/biomet/85.1.1},
  file = {/home/gkonkamking/Zotero/storage/IAC9AKXZ/Gelfand and Ghosh - 1998 - Model choice a minimum posterior predictive loss .pdf}
}

@article{gelfand2005bayesian,
  title = {Bayesian Nonparametric Spatial Modeling with \{\vphantom\}{{D}}\vphantom\{\}irichlet Process Mixing},
  author = {Gelfand, A E and Kottas, A and MacEachern, S N},
  year = {2005},
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {471},
  pages = {1021--1035},
  publisher = {ASA},
  keywords = {duplicate-citation-key,nosource}
}

@article{gelfand2005bayesian,
  title = {Bayesian Nonparametric Spatial Modeling with Dirichlet Process Mixing},
  author = {Gelfand, Alan E. and Kottas, Athanasios and Maceachern, Steven N.},
  year = {2005},
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {471},
  pages = {1021--1035},
  publisher = {ASA},
  issn = {0162-1459},
  doi = {10.1198/016214504000002078},
  abstract = {Customary modeling for continuous point-referenced data assumes a Gaussian process that is often taken to be stationary. When such models are fitted within a Bayesian framework, the unknown parameters of the process are assumed to be random, so a random Gaussian process results. Here we propose a novel spatial Dirichlet process mixture model to produce a random spatial process that is neither Gaussian nor stationary. We first develop a spatial Dirichlet process model for spatial data and discuss its properties. Because of familiar limitations associated with direct use of Dirichlet process models, we introduce mixing by convolving this process with a pure error process. We then examine properties of models created through such Dirichlet process mixing. In the Bayesian framework, we implement posterior inference using Gibbs sampling. Spatial prediction raises interesting questions, but these can be handled. Finally, we illustrate the approach using simulated data, as well as a dataset involving precipitation measurements over the Languedoc-Roussillon region in southern France.},
  keywords = {dependent dirichlet process,dirichlet process mixture models,duplicate-citation-key,gaussian process,markov chain monte carlo,nonsta-,nosource,point-referenced spatial data,random distribution,tionarity}
}

@article{Gelman1992,
  title = {Inference from Iterative Simulation Using Multiple Sequences},
  author = {Gelman, Andrew G and Rubin, Donald B.},
  year = {1992},
  journal = {Statistical Science},
  volume = {7},
  number = {4},
  pages = {457--511},
  issn = {0883-4237},
  doi = {10.1214/ss/1177011136},
  abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients. CR - Copyright \&\#169; 1992 Institute of Mathematical Statistics},
  isbn = {08834237},
  keywords = {1992,4,457-472,7,andrew gelman and donald,author,b,erence from iterative simulation,institute of mathematical statistics,no,nosource,nov,pp,published by,rubin,s,source,statistical science,using multiple sequences,vol}
}

@article{gelman2001models,
  title = {Models, Assumptions and Model Checking in Ecological Regressions},
  author = {Gelman, Andrew and Park, David K and Ansolabehere, Stephen and Price, Phillip N and Minnite, Lorraine C},
  year = {2001},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {164},
  number = {1},
  pages = {101--118},
  publisher = {Wiley Online Library},
  keywords = {nosource}
}

@article{Gelman2006,
  title = {Prior Distributions for Variance Parameters in Hierarchical Models ({{Comment}} on {{Article}} by {{Browne}} and {{Draper}})},
  author = {Gelman, Andrew G},
  year = {2006},
  journal = {Bayesian Analysis},
  volume = {1},
  number = {3},
  pages = {515--534},
  issn = {19360975},
  doi = {10.1214/06-BA117A},
  abstract = {Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-t family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of "noninformative" prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-t family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-t family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.},
  isbn = {9781139460934},
  keywords = {Bayesian inference,Conditional conjugacy,Folded-noncentral-t distribution,Half-t distribution,Hierarchical model,Multilevel model,Noninformative prior distribution,nosource,Weakly informative prior distribution}
}

@article{Gelman2006a,
  title = {Multilevel(Hierarchical) Modeling: What It Can and Cannot Do},
  author = {Gelman, Andrew G},
  year = {2006},
  journal = {Technometrics},
  volume = {48},
  number = {3},
  pages = {432--435},
  issn = {0040-1706},
  doi = {10.1198/004017005000000661},
  abstract = {Multilevel (hierarchical) modeling is a generalization of linear and generalized linear modeling in which regression coefficients are themselves given a model, whose parameters are also estimated from data. We illustrate the strengths and limitations of multilevel modeling through an example of the prediction of home radon levels in U.S. counties. The multilevel model is highly effective for predictions at both levels.},
  isbn = {0040-1706{\textbackslash}r1537-2723},
  keywords = {contextual effects,hierarchical model,multilevel regression,nosource}
}

@book{Gelman2007,
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  author = {Gelman, Andrew G and Hill, J},
  year = {2007},
  publisher = {Cambridge University Press, Cambridge},
  issn = {0012-9658},
  doi = {10.2277/0521867061},
  abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout. Author resource page: http://www.stat.columbia.edu{\~/}gelman/arm/},
  isbn = {0-521-86706-1},
  pmid = {14341096},
  keywords = {nosource}
}

@article{gelman2007average,
  title = {Average Predictive Comparisons for Models with Nonlinearity, Interactions, and Variance Components},
  author = {Gelman, Andrew and Pardoe, Iain},
  year = {2007},
  journal = {Sociological Methodology},
  volume = {37},
  number = {1},
  pages = {23--51},
  publisher = {Wiley Online Library},
  keywords = {nosource}
}

@article{Gelman2013,
  title = {Understanding Predictive Information Criteria for {{Bayesian}} Models},
  author = {Gelman, Andrew G and Hwang, Jessica and Vehtari, Aki},
  year = {2013},
  journal = {Statistics and Computing},
  number = {July},
  eprint = {1307.5928v1},
  pages = {1--20},
  issn = {09603174},
  doi = {10.1007/s11222-013-9416-2},
  abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian{\textbackslash}nperspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these{\textbackslash}nmeasures, and we compare them in three simple examples, one theoretical and two applied. The{\textbackslash}ncontribution of this paper is to put all these information criteria into a Bayesian predictive{\textbackslash}ncontext and to better understand, through small examples, how these methods can apply in{\textbackslash}npractice.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1307.5928v1},
  isbn = {0960-3174},
  keywords = {AIC,Bayes,Cross-validation,DIC,nosource,Prediction,WAIC}
}

@book{gelman2013bayesian,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew G and Carlin, John B. and Stern, Hal S. and Rubin, Donald B.},
  year = {2014},
  edition = {Third},
  publisher = {CRC press},
  address = {Boca Raton, FL},
  isbn = {978-1-4398-9820-8},
  keywords = {nosource}
}

@article{gelman2013philosophy,
  title = {Philosophy and the Practice of {{Bayesian}} Statistics},
  author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
  year = {2013},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {66},
  number = {1},
  pages = {8--38},
  publisher = {Wiley Online Library},
  keywords = {nosource}
}

@article{gelman2014beyond,
  title = {Beyond Power Calculations: {{Assessing}} Type {{S}} (Sign) and Type {{M}} (Magnitude) Errors},
  author = {Gelman, Andrew and Carlin, John},
  year = {2014},
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {641--651},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  keywords = {nosource}
}

@book{gelman2020regression,
  title = {Regression and Other Stories},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year = {2020},
  publisher = {Cambridge University Press},
  keywords = {nosource}
}

@article{gelmanNotOnlyDefended2013,
  title = {``{{Not Only Defended But Also Applied}}'': {{The Perceived Absurdity}} of {{Bayesian Inference}}},
  shorttitle = {``{{Not Only Defended But Also Applied}}''},
  author = {Gelman, Andrew and Robert, Christian P.},
  year = {2013},
  month = feb,
  journal = {The American Statistician},
  volume = {67},
  number = {1},
  pages = {1--5},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2013.760987},
  urldate = {2021-06-22},
  abstract = {The missionary zeal of many Bayesians of old has been matched, in the other direction, by an attitude among some theoreticians that Bayesian methods were absurd---not merely misguided but obviously wrong in principle. We consider several examples, beginning with Feller's classic text on probability theory and continuing with more recent cases such as the perceived Bayesian nature of the so-called doomsday argument. We analyze in this note the intellectual background behind various misconceptions about Bayesian statistics, without aiming at a complete historical coverage of the reasons for this dismissal.},
  file = {/home/gkonkamking/Zotero/storage/QRVLIFEV/Gelman and Robert - 2013 - “Not Only Defended But Also Applied” The Perceive.pdf}
}

@article{gelmanPriorCanOften2017,
  title = {The {{Prior Can Often Only Be Understood}} in the {{Context}} of the {{Likelihood}}},
  author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
  year = {2017},
  month = oct,
  journal = {Entropy},
  volume = {19},
  number = {10},
  pages = {555},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e19100555},
  urldate = {2022-10-05},
  abstract = {A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys' priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Bayesian inference,default priors,prior distribution},
  file = {/home/gkonkamking/pCloudDrive/papers/Gelman et al_2017_The Prior Can Often Only Be Understood in the Context of the Likelihood.pdf}
}

@article{gelmanStatisticalCrisisScience2014,
  title = {The Statistical Crisis in Science: Data-Dependent Analysis--a \&quot;Garden of Forking Paths\&quot;--Explains Why Many Statistically Significant Comparisons Don't Hold Up},
  shorttitle = {The Statistical Crisis in Science},
  author = {Gelman, Andrew and Loken, Eric},
  year = {2014},
  month = nov,
  journal = {American Scientist},
  volume = {102},
  number = {6},
  pages = {460--466},
  publisher = {Sigma Xi, The Scientific Research Society},
  issn = {00030996},
  urldate = {2020-03-18},
  langid = {english},
  keywords = {nosource}
}

@article{genon2003non,
  title = {A Non-Linear Explicit Filter},
  author = {{Genon-Catalot}, Valentine},
  year = {2003},
  journal = {Statistics \& probability letters},
  volume = {61},
  number = {2},
  pages = {145--154},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{genon2004random,
  title = {Random Scale Perturbation of an {{AR}} (1) Process and Its Properties as a Nonlinear Explicit Filter},
  author = {{Genon-Catalot}, Valentine and Kessler, Mathieu},
  year = {2004},
  journal = {Bernoulli},
  volume = {10},
  number = {4},
  pages = {701--720},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  keywords = {nosource}
}

@article{Gentleman1994,
  title = {Maximum Likelihood for Interval Censored Data : And Computation {{Consistency}}},
  author = {Geyer, Charles J},
  year = {2011},
  journal = {Biometrika},
  volume = {81},
  number = {3},
  pages = {618--623},
  issn = {0006-3444},
  doi = {10.1093/biomet/81.3.618},
  keywords = {nosource}
}

@article{genz2003computation,
  title = {Computation of the Normalization Constant for Exponentially Weighted {{Dirichlet}} Distribution Integrals},
  author = {Genz, Alan and Joyce, Paul},
  year = {2003},
  journal = {Comput Sci Stat},
  volume = {35},
  pages = {557--563},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/pCloudDrive/papers/Genz_Joyce_2003_Computation of the normalization constant for exponentially weighted Dirichlet.pdf}
}

@article{george2014survival,
  title = {Survival Analysis and Regression Models},
  author = {George, Brandon and Seals, Samantha and Aban, Inmaculada},
  year = {2014},
  journal = {Journal of Nuclear Cardiology},
  volume = {21},
  number = {4},
  pages = {686--694},
  publisher = {Springer},
  keywords = {nosource}
}

@article{georgeApproachesBayesianVariable1997,
  title = {Approaches for {{Bayesian}} Variable Selection},
  author = {George, Edward I. and McCulloch, Robert E.},
  year = {1997},
  journal = {Statistica Sinica},
  volume = {7},
  number = {2},
  eprint = {24306083},
  eprinttype = {jstor},
  pages = {339--373},
  publisher = {Institute of Statistical Science, Academia Sinica},
  issn = {1017-0405},
  urldate = {2022-02-03},
  abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George and McCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperparameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for posterior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probability. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
  file = {/home/gkonkamking/Zotero/storage/YQ4C6BAU/George and McCulloch - 1997 - APPROACHES FOR BAYESIAN VARIABLE SELECTION.pdf}
}

@article{georgeVariableSelectionGibbs1993,
  title = {Variable {{Selection}} via {{Gibbs Sampling}}},
  author = {George, Edward I. and McCulloch, Robert E.},
  year = {1993},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {423},
  pages = {881--889},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1993.10476353},
  urldate = {2021-09-21},
  abstract = {A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability---the promising ones---can then be identified by their more frequent appearance in the Gibbs sample.},
  keywords = {Data augmentation,Hierarchical Bayes,Latent variables,Mixture,Multiple regression},
  file = {/home/gkonkamking/Zotero/storage/QYZABWAV/George and McCulloch - 1993 - Variable Selection via Gibbs Sampling.pdf}
}

@misc{gerberGlobalStochasticOptimization2022,
  title = {A {{Global Stochastic Optimization Particle Filter Algorithm}}},
  author = {Gerber, Mathieu and Douc, Randal},
  year = {2022},
  month = jul,
  number = {arXiv:2007.04803},
  eprint = {2007.04803},
  primaryclass = {math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.04803},
  urldate = {2022-07-07},
  abstract = {We introduce a new online algorithm for expected log-likelihood maximization in situations where the objective function is multi-modal and/or has saddle points, that we term G-PFSO. The key element underpinning G-PFSO is a probability distribution which (a) is shown to concentrate on the target parameter value as the sample size increases and (b) can be efficiently estimated by means of a standard particle filter algorithm. This distribution depends on a learning rate, where the faster the learning rate the quicker it concentrates on the desired element of the search space, but the less likely G-PFSO is to escape from a local optimum of the objective function. In order to achieve a fast convergence rate with a slow learning rate, G-PFSO exploits the acceleration property of averaging, well-known in the stochastic gradient literature. Considering several challenging estimation problems, the numerical experiments show that, with high probability, G-PFSO successfully finds the highest mode of the objective function and converges to its global maximizer at the optimal rate. While the focus of this work is expected log-likelihood maximization, the proposed methodology and its theory apply more generally for optimizing a function defined through an expectation.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/gkonkamking/pCloudDrive/papers/Gerber_Douc_2022_A Global Stochastic Optimization Particle Filter Algorithm.pdf}
}

@article{geyerAnnealingMarkovChain2012,
  title = {Annealing {{Markov Chain Monte Carlo}} with {{Applications}} to {{Ancestral Inference}}},
  author = {Geyer, Charles J. and Thompson, Elizabeth A.},
  year = {2012},
  month = feb,
  journal = {Journal of the American Statistical Association},
  publisher = {Taylor \& Francis Group},
  urldate = {2020-10-05},
  abstract = {Markov chain Monte Carlo (MCMC; the Metropolis-Hastings algorithm) has been used for many statistical problems, including Bayesian inference, likelihood inference, and tests of significance. Though...},
  copyright = {Copyright Taylor and Francis Group, LLC},
  langid = {english},
  file = {/home/gkonkamking/Downloads/geyer1995.pdf}
}

@inproceedings{ghahramani1996factorial,
  title = {Factorial Hidden Markov Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ghahramani, Zoubin and Jordan, Michael I},
  year = {1996},
  pages = {472--478},
  keywords = {nosource}
}

@inproceedings{ghahramani2006infinite,
  title = {Infinite Latent Feature Models and the {{Indian}} Buffet Process},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ghahramani, Zoubin and Griffiths, Thomas L},
  year = {2006},
  pages = {475--482},
  issn = {1049-5258},
  abstract = {We define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. We identify a simple generative process that results in the same distribution over equivalence classes, which we call the Indian buffet process. We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algorithm for inference in this model and applying the algorithm to an image dataset.},
  isbn = {978-0-262-23253-1},
  keywords = {nosource}
}

@article{ghosal:01,
  title = {Convergence Rates for Density Estimation with {{Bernstein}} Polynomials},
  author = {Ghosal, Subhashis},
  year = {2001},
  journal = {Annals of Statistics},
  volume = {29},
  number = {5},
  pages = {1264--1280},
  keywords = {and phrases,bayesian bootstrap,bernstein polynomial,entropy,likelihood estimate,maximum,mixture of beta,nosource,posterior distribution,rate of convergence,sieve,strong}
}

@article{Ghosal:2000p242,
  ids = {ghosalConvergenceRatesPosterior2000},
  title = {Convergence Rates of Posterior Distributions},
  author = {Ghosal, Subhashis and Ghosh, Jayanta K and Vaart, AW Van Der},
  year = {2000},
  journal = {Annals of Statistics},
  volume = {28},
  number = {2},
  pages = {500--531},
  publisher = {Institute of Mathematical Statistics},
  abstract = {Ann. Statist. 2000, Vol. 28, No. 2, 500-531 CONVERGENCE RATES OF POSTERIOR DISTRIBUTIONS By Subhashis Ghosal, Jayanta K. Ghosh AND {\cyrchar\CYRL}{\cyrchar\CYRA}{\cyrchar\CYRV} W. Free University Amsterdam, Indian Statistical Institute and Free},
  file = {/home/gkonkamking/Zotero/storage/59UHP52H/Ghosal et al. - 2000 - Convergence Rates of Posterior Distributions.pdf}
}

@article{Ghosal:2008p6659,
  title = {Nonparametric {{Bayesian}} Model Selection and Averaging},
  author = {Ghosal, Subhashis and Lember, J{\"u}ri and Van Der Vaart, Aad},
  year = {2008},
  journal = {Electronic Journal of Statistics},
  volume = {2},
  number = {July 2007},
  eprint = {0802.0069v1},
  pages = {63--89},
  publisher = {Institute of Mathematical Statistics},
  issn = {1935-7524},
  doi = {10.1214/07-EJS090},
  abstract = {We consider nonparametric Bayesian estimation of a probability density p based on a random sample of size n from this density using a hierarchical prior. The prior consists, for instance, of prior weights on the regularity of the unknown density combined with priors that are appropriate given that the density has this regularity. More generally, the hierarchy consists of prior weights on an abstract model index and a prior on a density model for each model index. We present a general theorem on the rate of contraction of the resulting posterior distribution as nto infty, which gives conditions under which the rate of contraction is the one attached to the model that best approximates the true density of the observations. This shows that, for instance, the posterior distribution can adapt to the smoothness of the underlying density. We also study the posterior distribution of the model index, and find that under the same conditions the posterior distribution gives negligible weight to models that are bigger than the optimal one, and thus selects the optimal model or smaller models that also approximate the true density well. We apply these result to log spline density models, where we show that the prior weights on the regularity index interact with the priors on the models, making the exact rates depend in a complicated way on the priors, but also that the rate is fairly robust to specification of the prior weights.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:0802.0069v1},
  keywords = {nosource}
}

@article{ghosal1999posterior,
  title = {Posterior Consistency of {{Dirichlet}} Mixtures in Density Estimation},
  author = {Ghosal, Subhashis and Ghosh, Jayanta K and Ramamoorthi, R V},
  year = {1999},
  journal = {Annals of Statistics},
  volume = {27},
  number = {1},
  pages = {143--158},
  publisher = {Institute of Mathematical Statistics},
  keywords = {duplicate-citation-key,nosource}
}

@article{ghosal2001entropies,
  ids = {ghosalEntropiesRatesConvergence2001},
  title = {Entropies and Rates of Convergence for Maximum Likelihood and Bayes Estimation for Mixtures of Normal Densities},
  author = {Ghosal, Subhashis and Vaart, A W Van Der},
  year = {2001},
  journal = {Annals of Statistics},
  volume = {29},
  number = {5},
  eprint = {2699987},
  eprinttype = {jstor},
  pages = {1233--1263},
  publisher = {Institute of Mathematical Statistics},
  keywords = {62G07,62G20,bracketing,Dirichlet mixture,Entropy,maximum likelihood,mixture of normals,posterior distribution,rate of convergence,sieve},
  file = {/home/gkonkamking/Zotero/storage/EX9V9IEK/Ghosal and Vaart - 2001 - Entropies and rates of convergence for maximum lik.pdf}
}

@article{ghosal2003bayesian,
  title = {On Bayesian Adaptation},
  author = {Ghosal, Subhashis and Lember, J{\"u}ri and Van Der Vaart, Aad},
  year = {2003},
  journal = {Acta Applicandae Mathematicae},
  volume = {79},
  number = {1-2},
  pages = {165--175},
  publisher = {Springer},
  issn = {01678019},
  doi = {10.1023/A:1025856016236},
  keywords = {Adaptation,Model selection,nosource,Posterior distribution,Rate of convergence,Sieves,Splines}
}

@article{ghosal2007convergence,
  title = {Convergence Rates of Posterior Distributions for Noniid Observations},
  author = {Ghosal, Subhashis and {van der Vaart}, Aad W},
  year = {2007},
  journal = {The Annals of Statistics},
  volume = {35},
  number = {1},
  pages = {192--223},
  publisher = {Institute of Mathematical Statistics},
  keywords = {nosource}
}

@article{ghosal2007posterior,
  title = {Posterior Convergence Rates of {{Dirichlet}} Mixtures at Smooth Densities},
  author = {Ghosal, Subhashis and {van der Vaart}, A W},
  year = {2007},
  journal = {The Annals of Statistics},
  volume = {35},
  number = {2},
  pages = {697--723},
  publisher = {Institute of Mathematical Statistics},
  keywords = {nosource}
}

@book{ghosalFundamentalsNonparametricBayesian2017,
  title = {Fundamentals of Nonparametric {{Bayesian}} Inference},
  author = {Ghosal, Subhashis and {Van der Vaart}, Aad},
  year = {2017},
  volume = {44},
  publisher = {Cambridge University Press},
  keywords = {nosource}
}

@book{ghosh2003bayesian,
  title = {Bayesian Nonparametrics},
  author = {Ghosh, Jayanta K and Ramamoorthi, R V},
  year = {2003},
  publisher = {Springer},
  keywords = {nosource}
}

@article{ghoshBayesianVariableSelection2015,
  title = {Bayesian {{Variable Selection Under Collinearity}}},
  author = {Ghosh, Joyee and Ghattas, Andrew E.},
  year = {2015},
  month = jul,
  journal = {The American Statistician},
  volume = {69},
  number = {3},
  pages = {165--173},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2015.1031827},
  urldate = {2022-09-07},
  abstract = {In this article, we highlight some interesting facts about Bayesian variable selection methods for linear regression models in settings where the design matrix exhibits strong collinearity. We first demonstrate via real data analysis and simulation studies that summaries of the posterior distribution based on marginal and joint distributions may give conflicting results for assessing the importance of strongly correlated covariates. The natural question is which one should be used in practice. The simulation studies suggest that posterior inclusion probabilities and Bayes factors that evaluate the importance of correlated covariates jointly are more appropriate, and some priors may be more adversely affected in such a setting. To obtain a better understanding behind the phenomenon, we study some toy examples with Zellner's g-prior. The results show that strong collinearity may lead to a multimodal posterior distribution over models, in which joint summaries are more appropriate than marginal summaries. Thus, we recommend a routine examination of the correlation matrix and calculation of the joint inclusion probabilities for correlated covariates, in addition to marginal inclusion probabilities, for assessing the importance of covariates in Bayesian variable selection.},
  keywords = {Bayesian model averaging,Linear regression,Marginal inclusion probability,Median probability model,Multimodality,Zellner's g-prior.}
}

@article{gianicoloGenderSpecificExcess2021,
  title = {Gender Specific Excess Mortality in {{Italy}} during the {{COVID-19}} Pandemic Accounting for Age},
  author = {Gianicolo, Emilio A. L. and Russo, Antonello and B{\"u}chler, Britta and Taylor, Katherine and Stang, Andreas and Blettner, Maria},
  year = {2021},
  month = feb,
  journal = {European Journal of Epidemiology},
  volume = {36},
  number = {2},
  pages = {213--218},
  issn = {1573-7284},
  doi = {10.1007/s10654-021-00717-9},
  urldate = {2021-05-07},
  abstract = {Since the beginning of the COVID-19 pandemic, data have been accumulated to examine excess mortality in the first half of 2020. Mortality in the preceding year or years is used to calculate the expected number of deaths, which is then compared with the actual number of deaths in 2020. We calculated weekly age- and sex-specific mortality rates for 93.1\% of the Italian municipalities for the years 2015--2019 and for the first 26~weeks in 2020. We assumed the mortality experience during 2015--2019 as the reference period to calculate standardised mortality ratios. Furthermore, in order to compare the mortality experience of males and females, we calculated sex- and age- specific weekly direct standardised mortality rates and differences between the observed and expected number of deaths. We observed considerable changes in the demographics in the Italian population between the years 2015 and 2020, particularly among people 60~years and older and among males.The population is aging and the proportion of elderly males has increased, which was not reflected adequately in previous estimates of excess mortality. Standardized excess mortality results show that in Italy between the 8th and 26th weeks in 2020, there were 33,035 excess deaths, which is only 643 fewer deaths than the official COVID-19 death toll for this time period. A comparative increase in the mortality rates was observed in March among both sexes, but particularly for males. Comparisons with recently published data show considerably higher excess deaths, but these data were either not covering the complete country or did not account for age and sex. Neglecting the demographic changes in a region, even over a short time span, can result in biased estimates.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/7N76BSD9/Gianicolo et al. - 2021 - Gender specific excess mortality in Italy during t.pdf}
}

@article{Giardina2007,
  title = {Duality and Exact Correlations for a Model of Heat Conduction},
  author = {Giardin{\`a}, Cristian and Kurchan, Jorge and Redig, Frank},
  year = {2007},
  journal = {Journal of Mathematical Physics},
  volume = {48},
  number = {3},
  issn = {00222488},
  doi = {10.1063/1.2711373},
  abstract = {We study a model of heat conduction with stochastic diffusion of energy. We obtain a dual particle process which describes the evolution of all the correlation functions. An exact expression for the covariance of the energy exhibits long-range correlations in the presence of a current. We discuss the formal connection of this model with the simple symmetric exclusion process.},
  arxiv = {0612198 [cond-mat]},
  arxivid = {cond-mat/0612198},
  keywords = {nosource}
}

@phdthesis{gibbs1997bayesian,
  title = {Bayesian {{Gaussian}} Processes for Regression and Classification},
  booktitle = {Unpublished Doctoral Dissertation, University of Cambridge},
  author = {Gibbs, Mn},
  year = {1997},
  abstract = {Bayesian inference offers us a powerful tool with which to tackle the problem of data modelling. However the performance of Bayesian methods is crucially dependent on being able to find good models for our data. The principal focus of this thesis is the development of models based on Gaussian process priors. Such models, which can be thought of as the infinite extension of several existing finite models have the flexibility to model complex phenomena while being mathematically simple. In thesis, I present a review of the theory of Gaussian processes and their covariance functions and demonstrate how they fit into the Bayesian framework. The efficient implementation of a Gaussian process is discussed with particular reference to approximate methods for matrix inversion based on the work of Skilling (1993). Several regression problems are examined. Non-stationary covariance functions are developed for the regression of neuron spike data and the use of Gaussian processes to model the potential energy surfaces of weakly bound molecules is discussed. Classification methods based on Gaussian processes are implemented using variational methods. Existing bounds (Jaakkola and Jordan 1996) for the sigmoid function are used to tackle binary problems and multi-dimensional bounds on the softmax function are presented for the multiple class case. The performance of the variational classifier is compared with that of other methods using the CRABS and PIMA datasets (Ripley 1996) and the problem of predicting the cracking of welds based on their chemical composition is also investigated. The theoretical calculation of the density of states of crystal structures is discussed in detail. Three possible approaches to the problem are described based on free energy minimization, Gaussian processes and the theory of random matrices. Results from these approaches are compared with the state-of-the-art techniques (Pickard 1997).},
  school = {Citeseer},
  keywords = {nosource}
}

@article{Giddings2013,
  title = {The Relative Sensitivity of Macrophyte and Algal Species to Herbicides and Fungicides: {{An}} Analysis Using Species Sensitivity Distributions},
  author = {Giddings, Jeffrey M. and Arts, G. H P and Hommen, Udo},
  year = {2013},
  month = apr,
  journal = {Integrated Environmental Assessment and Management},
  volume = {9},
  number = {2},
  eprint = {23229339},
  eprinttype = {pubmed},
  pages = {308--318},
  issn = {15513793},
  doi = {10.1002/ieam.1387},
  abstract = {Lemna spp. are the standard test species representing aquatic macrophytes in the current risk assessment schemes for herbicides and plant growth regulators in the European Union and North America. At a Society of Environmental Toxicology and Chemistry (SETAC) 2008 workshop on Aquatic Macrophyte Risk Assessment for Pesticides (AMRAP), a Species Sensitivity Distribution (SSD) working group was formed to address uncertainties about the sensitivity of Lemna spp. relative to other aquatic macrophyte species. For 11 herbicides and 3 fungicides for which relevant and reliable data were found for at least 6 macrophyte species, SSDs were fitted using lognormal regression. The positions of L. gibba (the most commonly tested Lemna species) and Myriophyllum spicatum (for which standardized test methods are under development) in each SSD were determined where data were available. The sensitivity of standard algal test species required for pesticide registration in the United States under the Federal Insecticide, Fungicide, and Rodenticide Act (FIFRA) relative to the macrophytes in each SSD was also examined (algae were not included in the SSD). L. gibba was among the most sensitive macrophyte species for approximately 50\% of the chemicals examined. M. spicatum was among the most sensitive macrophytes for approximately 25\% of the chemicals. In most cases, the lowest FIFRA algal species endpoint was lower than the most sensitive macrophyte endpoint. Although no single species consistently represented the most sensitive aquatic plant species, for 12 of 14 chemicals L. gibba and the FIFRA algae included an endpoint near or below the 5th percentile of the macrophyte SSD. For the other compounds, M. spicatum was the most sensitive species of all aquatic plants considered.},
  pmid = {23229339},
  keywords = {Algae,Lemna,Macrophytes,Myriophyllum,nosource,Species sensitivity distribution}
}

@article{giddingsDerivationCombinedSpecies2019,
  title = {Derivation of Combined Species Sensitivity Distributions for Acute Toxicity of Pyrethroids to Aquatic Animals},
  author = {Giddings, Jeffrey M. and Wirtz, Jeffrey and Campana, David and Dobbs, Michael},
  year = {2019},
  month = mar,
  journal = {Ecotoxicology},
  volume = {28},
  number = {2},
  pages = {242--250},
  issn = {1573-3017},
  doi = {10.1007/s10646-019-02018-0},
  urldate = {2021-09-22},
  abstract = {The aquatic toxicity profiles of synthetic pyrethroid insecticides are remarkably similar, and results for a large number of species can be combined across compounds in Species Sensitivity Distributions (SSDs). Normalizing acute toxicity values (median lethal concentrations, LC50s) for each species and each pyrethroid to the LC50 of the same pyrethroid to the freshwater amphipod Hyalella azteca (the most sensitive species to all pyrethroids tested) enabled expression of LC50s as Hyalella equivalents that can be pooled across pyrethroids. The resulting normalized LC50s (geometric means for each species across pyrethroids) were analyzed using SSDs. Based on tests with measured exposure concentrations, the fifth percentiles (Hazard Concentrations, HC5s) of the SSDs were 4.8 Hyalella equivalents for arthropods (36 species) and 256 Hyalella equivalents for fish (24 species). HC5 values are useful as effects metrics for screening-level risk assessments, and the full SSDs can be integrated with estimated exposure distributions for higher-level risk characterization. The combined pyrethroid SSDs provide a more taxonomically representative and statistically robust basis for risk characterization than data for the most sensitive single species or SSDs based on data for a single pyrethroid alone, and are especially useful for pyrethroids that have been tested with smaller numbers of species.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/4L7I4W8N/Giddings et al. - 2019 - Derivation of combined species sensitivity distrib.pdf}
}

@article{giessnerHighHierarchyHow2007,
  title = {High in the Hierarchy: {{How}} Vertical Location and Judgments of Leaders' Power Are Interrelated},
  shorttitle = {High in the Hierarchy},
  author = {Giessner, Steffen R. and Schubert, Thomas W.},
  year = {2007},
  month = sep,
  journal = {Organizational Behavior and Human Decision Processes},
  volume = {104},
  number = {1},
  pages = {30--44},
  issn = {0749-5978},
  doi = {10.1016/j.obhdp.2006.10.001},
  urldate = {2020-06-19},
  abstract = {Leadership implies power. We argue, from a social embodiment perspective, that thinking about power involves mental simulation of vertical location. Three studies tested whether judgments of leaders' power and information on a vertical location are interrelated. In Studies 1a--1c, participants judged a leader's power after being presented with, among other information, an organization chart containing either a long or a short vertical line. A longer vertical line increased judged power. Study 2 showed that this effect persists when longer (vs. shorter) vertical lines are presented in an independent priming task and not in an organization chart, and that horizontal lines do not have the same effect. Finally, Studies 3a and 3b showed the reverse causal effect: information about a leader's power influenced participants' vertical positioning of a leader's box in an organization chart and of a leader picture into a team picture. Implications for leadership communication are discussed.},
  langid = {english},
  keywords = {Embodiment,Leadership,Metaphor,nosource,Person perception,Power}
}

@article{gilks1994language,
  title = {A Language and Program for Complex {{Bayesian}} Modelling},
  author = {Gilks, Wally R and Thomas, Andrew and Spiegelhalter, David J},
  year = {1994},
  journal = {The Statistician},
  pages = {169--177},
  publisher = {JSTOR},
  isbn = {0039-0526},
  keywords = {bayesian computation,bayesian inference,duplicate-citation-key,gibbs sampling,graphical model,nosource,statistical software}
}

@book{gilks1995markov,
  title = {Markov Chain Monte Carlo in Practice},
  author = {Gilks, Walter R and Richardson, Sylvia and Spiegelhalter, David},
  year = {1995},
  publisher = {CRC press},
  keywords = {nosource}
}

@article{gill1979bayesian,
  title = {Bayesian Estimation of {{Shannon}}'s Index of Diversity},
  author = {Gill, {\relax CA} and Joanes, {\relax DN}},
  year = {1979},
  journal = {Biometrika},
  volume = {66},
  number = {1},
  pages = {81--85},
  publisher = {Biometrika Trust},
  issn = {0006-3444},
  abstract = {This paper is concerned with methods of estimating Shannon's information measure, and with the sampling properties of those estimates. A Bayesian approach is used to suggest two possible classes of estimators, and their properties are compared with those of the usual Shannon measure of diversity. Particular attention is given to comparisons involving the MacArthur model of species abundances.},
  keywords = {Abundance,Diversity,Ecology,nosource}
}

@article{gillisPyCloneVIScalableInference2020,
  title = {{{PyClone-VI}}: Scalable Inference of Clonal Population Structures Using Whole Genome Data},
  shorttitle = {{{PyClone-VI}}},
  author = {Gillis, Sierra and Roth, Andrew},
  year = {2020},
  month = dec,
  journal = {BMC Bioinformatics},
  volume = {21},
  number = {1},
  pages = {571},
  issn = {1471-2105},
  doi = {10.1186/s12859-020-03919-2},
  urldate = {2021-05-04},
  abstract = {At diagnosis tumours are typically composed of a mixture of genomically distinct malignant cell populations. Bulk sequencing of tumour samples coupled with computational deconvolution can be used to identify these populations and study cancer evolution. Existing computational methods for populations deconvolution are slow and/or potentially inaccurate when applied to large datasets generated by whole genome sequencing data.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/RZBT2MIX/Gillis and Roth - 2020 - PyClone-VI scalable inference of clonal populatio.pdf}
}

@article{Giolo2004,
  title = {Turnbull's Nonparametric Estimator for Interval-Censored Data'},
  author = {Giolo, Sr},
  year = {2004},
  journal = {Department of Statistics, Federal University of Paran{\'a}},
  pages = {1--10},
  abstract = {In most applications, the data may be interval-censored. By interval-censored data, we mean that a random variable of interest is known only to lie in an interval, instead of being observed exactly. In such cases, the only information we have for each individual is that their event time falls in an interval, but the exact time is unknown. A nonparametric estimate of the survival function can also be found in such intervalcensored situations. The survival function is perhaps the most important function in medical and health studies. In this work we describe and illustrate the iterative procedure proposed by Turnbull (1976) to estimate such function. This procedure has been implemented in the software R and the code used is provided in this work.},
  keywords = {nosource}
}

@article{giudici2018determines,
  title = {What Determines Bitcoin Exchange Prices? {{A}} Network {{VAR}} Approach},
  author = {Giudici, Paolo and {Abu-Hashish}, Iman},
  year = {2018},
  journal = {Finance Research Letters},
  publisher = {Elsevier},
  keywords = {nosource}
}

@book{Glaskin2012,
  title = {Cycling Science: {{How}} Rider and Machine Work Together},
  author = {Glaskin, Max},
  year = {2012},
  publisher = {University of Chicago Press},
  isbn = {ISBN-13: 978-0-226-92187-7 (e-book)},
  keywords = {nosource}
}

@article{gligoric2022biased,
  title = {Biased Bytes: {{On}} the Validity of Estimating Food Consumption from Digital Traces},
  author = {Gligoric, Kristina and Dordevic, Irena and West, Robert},
  year = {2022},
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {6},
  number = {CSCW2},
  pages = {1--27},
  publisher = {ACM New York, NY, USA},
  keywords = {❓ Multiple DOI}
}

@article{gnedin2006exchangeable,
  title = {Exchangeable \{\vphantom\}{{G}}\vphantom\{\}ibbs Partitions and \{\vphantom\}{{S}}\vphantom\{\}tirling Triangles},
  author = {Gnedin, Alexander and Pitman, Jim},
  year = {2006},
  journal = {Journal of Mathematical Sciences},
  volume = {138},
  number = {3},
  pages = {5674--5685},
  publisher = {Springer},
  keywords = {nosource}
}

@article{Gneiting2007,
  title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  year = {2007},
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1198/016214506000001437},
  abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distribution F if he or she issues the probabilistic forecast F, rather than G?=F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster tomake careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed.We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
  isbn = {0162-1459},
  keywords = {bayes factor,bregman divergence,brier score,coherent,continuous ranked probability score,cross-validation,distribution,entropy,kernel score,loss function,minimum contrast estimation,negative definite function,prediction interval,predictive,quantile forecast,scoring rule,skill score,strictly proper,utility function},
  file = {/home/gkonkamking/Zotero/storage/IRIPTI8G/Gneiting and Raftery - 2007 - Strictly proper scoring rules, prediction, and est.pdf}
}

@article{Goldman1994,
  title = {A Codon-Based Model of Nucleotide Substitution for Protein-Coding {{DNA}} Sequences.},
  author = {Goldman, Nick and Yang, Z},
  year = {1994},
  journal = {Molecular biology and evolution},
  volume = {11},
  number = {5},
  pages = {725--736},
  issn = {0737-4038},
  doi = {10.1186/s13059-014-0542-8},
  abstract = {A codon-based model for the evolution of protein-coding DNA sequences is presented for use in phylogenetic estimation. A Markov process is used to describe substitutions between codons. Transition/transversion rate bias and codon usage bias are allowed in the model, and selective restraints at the protein level are accommodated using physicochemical distances between the amino acids coded for by the codons. Analyses of two data sets suggest that the new codon-based model can provide a better fit to data than can nucleotide-based models and can produce more reliable estimates of certain biologically important measures such as the transition/transversion rate ratio and the synonymous/nonsynonymous substitution rate ratio.},
  isbn = {0737-4038 (Print){\textbackslash}n0737-4038 (Linking)},
  pmid = {7968486},
  keywords = {address for corrmpondence and,dr,models,nick goldman,nonsynonymous substitutions,nosource,nucleotide,onymous substitutions,phylogenetic estimation,protein-coding sequences,reprints,substitution,syn-}
}

@article{Gonzalez2008a,
  title = {Understanding Individual Human Mobility Patterns},
  author = {Gonz{\'a}lez, Marta Cecilia and Hidalgo, C{\'e}sar Augusto and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2008},
  month = jun,
  journal = {Nature},
  volume = {453},
  number = {7196},
  eprint = {18528393},
  eprinttype = {pubmed},
  pages = {779--782},
  issn = {1476-4687},
  doi = {10.1038/nature07850},
  abstract = {Despite their importance for urban planning [1], traffic forecasting [2], and the spread of biological [3, 4, 5] and mobile viruses [6], our understanding of the basic laws govern- ing human motion remains limited thanks to the lack of tools to monitor the time resolved location of individuals. Here we study the trajectory of 100, 000 anonymized mobile phone users whose position is tracked for a six month period. We find that in contrast with the random trajectories predicted by the prevailing L{\textasciiacute} evy flight and random walk models [7], human trajectories show a high degree of temporal and spatial regularity, each individual being characterized by a time independent characteristic length scale and a significant prob- ability to return to a few highly frequented locations. After correcting for differences in travel distances and the inherent anisotropy of each trajectory, the individual travel patterns collapse into a single spatial probability distribution, indicating that despite the diversity of their travel history, humans follow simple reproducible patterns. This inherent similarity in travel patterns could impact all phenomena driven by human mobility, from epidemic prevention to emergency response, urban planning and agent based modeling.},
  arxiv = {0806.1256v1},
  arxivid = {0806.1256v1},
  isbn = {0028-0836},
  pmid = {18528393},
  keywords = {*Locomotion,Biological Physics,Biology,Cellular Phone,Cellular Phone: statistics \& numerical data,Cellular Phone/*statistics \& numerical data,Computers and Society,Disaster Planning,Emergencies,Forecasting,Geographic Information Systems,history,Human,human mobility,Humans,Locomotion,Massachusetts,mobility mining,model,Models,Physics,Physics and Society,Probability,Rab,Research,Research Support,Science,Statistical,Statistical Mechanics,statistics /\&/ numerical data,Travel,Travel: statistics \& numerical data,Travel/*statistics \& numerical data,Universities,Viruses,wireless cellular networks},
  file = {/home/gkonkamking/Zotero/storage/ZQGPLAHY/González et al. - 2008 - Understanding individual human mobility patterns.pdf}
}

@article{goplerudPartiallyFactorizedVariational2024,
  title = {Partially Factorized Variational Inference for High-Dimensional Mixed Models},
  author = {Goplerud, M and Papaspiliopoulos, O and Zanella, G},
  year = {2024},
  month = nov,
  journal = {Biometrika},
  pages = {asae067},
  issn = {1464-3510},
  doi = {10.1093/biomet/asae067},
  urldate = {2024-12-10},
  abstract = {While generalized linear mixed models are a fundamental tool in applied statistics, many specifications, such as those involving categorical factors with many levels or interaction terms, can be computationally challenging to estimate due to the need to compute or approximate high-dimensional integrals. Variational inference is a popular way to perform such computations, especially in the Bayesian context. However, naive use of such methods can provide unreliable uncertainty quantification. We show that this is indeed the case for mixed models, proving that standard mean-field variational inference dramatically underestimates posterior uncertainty in high-dimensions. We then show how appropriately relaxing the mean-field assumption leads to methods whose uncertainty quantification does not deteriorate in high-dimensions, and whose total computational cost scales linearly with the number of parameters and observations. Our theoretical and numerical results focus on mixed models with Gaussian or binomial likelihoods, and rely on connections to random graph theory to obtain sharp high-dimensional asymptotic analysis. We also provide generic results, which are of independent interest, relating the accuracy of variational inference to the convergence rate of the corresponding coordinate ascent algorithm that is used to find it. Our proposed methodology is implemented in the R package vglmer. Numerical results with simulated and real data examples illustrate the favourable computation cost versus accuracy trade-off of our approach compared to various alternatives.},
  file = {/home/gkonkamking/pCloudDrive/papers/Goplerud et al. - 2024 - Partially factorized variational inference for high-dimensional mixed models.pdf}
}

@article{gorbachHierarchicalBayesianMixture2020,
  title = {A {{Hierarchical Bayesian Mixture Model Approach}} for {{Analysis}} of {{Resting-State Functional Brain Connectivity}}: {{An Alternative}} to {{Thresholding}}},
  shorttitle = {A {{Hierarchical Bayesian Mixture Model Approach}} for {{Analysis}} of {{Resting-State Functional Brain Connectivity}}},
  author = {Gorbach, Tetiana and Lundquist, Anders and de Luna, Xavier and Nyberg, Lars and Salami, Alireza},
  year = {2020},
  month = jun,
  journal = {Brain Connectivity},
  volume = {10},
  number = {5},
  pages = {202},
  doi = {10.1089/brain.2020.0740},
  urldate = {2024-10-28},
  abstract = {This article proposes a Bayesian hierarchical mixture model to analyze functional brain connectivity where mixture components represent ``positively connected'' and ``non-connected'' brain regions. Such an approach provides a data-informed separation of ...},
  langid = {english},
  pmid = {32308015},
  file = {/home/gkonkamking/Zotero/storage/F5H92U7F/Gorbach et al. - 2020 - A Hierarchical Bayesian Mixture Model Approach for Analysis of Resting-State Functional Brain Connec.pdf}
}

@article{Gory2017,
  title = {Bayesian Inference of Selection in the Wright-Fisher Diffusion Model},
  author = {Gory, Jeffrey J. and Herbei, Radu and Kubatko, Laura S.},
  year = {2017},
  eprint = {1711.02691},
  pages = {1--33},
  abstract = {The increasing availability of population-level allele frequency data across one or more related populations necessitates the development of methods that can efficiently estimate population genetics parameters, such as the strength of selection acting on the population(s), from such data. Existing methods for this problem in the setting of the Wright-Fisher diffusion model are primarily likelihood-based, and rely on numerical approximation for likelihood computation and on bootstrapping for assessment of variability in the resulting estimates, requiring extensive computation. Recent work (Jenkins and Spano, 2015) has provided a method for obtaining exact samples from general Wright-Fisher diffusion processes, enabling the development of methods for Bayesian estimation in this setting. We develop and implement a Bayesian method for estimating the strength of selection based on the Wright-Fisher diffusion for data sampled at a single time point. The method utilizes the work of Jenkins and Spano (2015) to develop a Markov chain Monte Carlo algorithm to draw samples from the joint posterior distribution of the selection coefficient and the allele frequencies. We demonstrate that when assumptions about the initial allele frequencies are accurate the method performs well for both simulated data and for an empirical data set on hypoxia in flies (Zhou et al. 2011), where we find evidence for strong positive selection in a region of chromosome 2L previously identified by Ronen et al. (2013). We discuss possible extensions of our method to the more general settings commonly encountered in practice, highlighting the advantages of Bayesian approaches to inference in this setting.},
  archiveprefix = {arXiv},
  arxivid = {1711.02691},
  file = {/home/gkonkamking/Zotero/storage/9N8RAKVB/Gory et al. - 2017 - Bayesian inference of selection in the wright-fish.pdf}
}

@article{Gottschalk2013,
  title = {A Probabilistic Method for Species Sensitivity Distributions Taking into Account the Inherent Uncertainty and Variability of Effects to Estimate Environmental Risk},
  author = {Gottschalk, Fadri and Nowack, Bernd},
  year = {2013},
  month = jan,
  journal = {Integrated Environmental Assessment and Management},
  volume = {9},
  number = {1},
  eprint = {22745057},
  eprinttype = {pubmed},
  pages = {79--86},
  issn = {15513793},
  doi = {10.1002/ieam.1334},
  abstract = {This article presents a method of probabilistically computing species sensitivity distributions (SSD) that is well-suited to cope with distinct data scarcity and variability. First, a probability distribution that reflects the uncertainty and variability of sensitivity is modeled for each species considered. These single species sensitivity distributions are then combined to create an SSD for a particular ecosystem. A probabilistic estimation of the risk is carried out by combining the probability of critical environmental concentrations with the probability of organisms being impacted negatively by these concentrations. To evaluate the performance of the method, we developed SSD and risk calculations for the aquatic environment exposed to triclosan. The case studies showed that the probabilistic results reflect the empirical information well, and the method provides a valuable alternative or supplement to more traditional methods for calculating SSDs based on averaging raw data and/or on using theoretical distributional forms. A comparison and evaluation with single SSD values (5th-percentile [HC5]) revealed the robustness of the proposed method.},
  isbn = {1551-3793},
  pmid = {22745057},
  keywords = {nosource,Predicted environmental concentrations,Probabilistic and/or stochastic modeling,Risk assessment,Risk potential profiles,Species sensitivity distribution}
}

@article{gourierouxAutoregressiveGammaProcesses2006,
  title = {Autoregressive Gamma Processes},
  author = {Gourieroux, Christian and Jasiak, Joann},
  year = {2006},
  journal = {Journal of Forecasting},
  volume = {25},
  number = {2},
  pages = {129--152},
  issn = {1099-131X},
  doi = {10.1002/for.978},
  urldate = {2023-10-29},
  abstract = {We introduce a class of autoregressive gamma processes with conditional distributions from the family of noncentred gamma (up to a scale factor). The paper provides the stationarity and ergodicity conditions for ARG processes of any autoregressive order p, including long memory, and closed-form expressions of conditional moments. The nonlinear state space representation of an ARG process is used to derive the filtering, smoothing and forecasting algorithms. The paper also presents estimation and inference methods, illustrated by an application to interquote durations data on an infrequently traded stock listed on the Toronto Stock Exchange (TSX). Copyright {\copyright} 2006 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2005 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {autoregressive gamma,CIR,high frequency,intertrade durations},
  file = {/home/gkonkamking/pCloudDrive/papers/Gourieroux_Jasiak_2006_Autoregressive gamma processes.pdf}
}

@article{gowerPropertiesIndividualDifferences2022,
  title = {Properties of Individual Differences Scaling and Its Interpretation},
  author = {Gower, John C. and Le Roux, Ni{\"e}l J. and {Gardner-Lubbe}, Sugnet},
  year = {2022},
  month = aug,
  journal = {Statistical Papers},
  volume = {63},
  number = {4},
  pages = {1221--1245},
  issn = {1613-9798},
  doi = {10.1007/s00362-021-01275-8},
  urldate = {2024-04-09},
  abstract = {Indscal models consider symmetric matrices \$\${\textbackslash}varvec\{B\}\_\{k\}={\textbackslash}varvec\{X\}{\textbackslash}varvec\{W\}\_\{k\}{\textbackslash}varvec\{X\}'\$\$for \$\$k = 1, {\textbackslash}ldots , K\$\$, where \$\${\textbackslash}varvec\{X\}: n {\textbackslash}times R\$\$is a compromise matrix termed the group-average and \$\${\textbackslash}varvec\{W\}\_\{k\}\$\$is a diagonal matrix of weights given by the kth individual to the R, specified in advance, columns of \$\${\textbackslash}varvec\{X\}\$\$; non-negative weights are preferred and usually \$\$R {$<$} n\$\$. We propose a new two-phase alternating least squares (ALS) algorithm, which emphasizes the two main components (group average and weighting parameters) of the Indscal model and specifically helps with the interpretation of the model. Furthermore, it has thrown new light on the properties of the converged solution, that would be satisfied by any algorithm that minimizes the basic Indscal criterion: \$\$min{\textbackslash}sum \_\{k=1\}{\textasciicircum}\{K\}{\textbackslash}Vert {\textbackslash}varvec\{B\}\_\{k\}-{\textbackslash}varvec\{X\}{\textbackslash}varvec\{W\}\_\{k\}{\textbackslash}varvec\{X\}'{\textbackslash}Vert {\textasciicircum}\{2\}\$\$where the minimization is over \$\${\textbackslash}varvec\{X\}\$\$and the \$\${\textbackslash}varvec\{W\}\_\{k\}\$\$. The new algorithm has also proved to be a useful tool in unravelling the algebraic understanding of the role played by parameter constraints and their interpretation in variants of the Indscal model. The proposed analysis focusses on Indscal but the approach may be of more widespread interest, especially in the field of multidimensional data analysis. A major issue is that simultaneous least-squares estimates of the parameters may be found without imposing constraints. However, group average and individual weighting parameters may not be estimated uniquely, without imposing some subjective constraint that could encourage misleading interpretations. We encourage the use of linear constraints \$\${\textbackslash}sum \_\{k=1\}{\textasciicircum}\{K\}{\textbackslash}varvec\{1'W\}\_\{k\}= {\textbackslash}varvec\{1'\}\$\$, as it enables a comparison of the weights obtained (i) within group k and (ii) between the same item drawn from two or more groups. However, it is easy to exchange one system of constraints to another in a post- or pre-analysis. The new two-phase ALS algorithm (i) computes for fixed \$\${\textbackslash}varvec\{X\}: n {\textbackslash}times R\$\$the weights \$\${\textbackslash}varvec\{W\}\_\{k\}\$\$subject to \$\${\textbackslash}sum \_\{k=1\}{\textasciicircum}\{K\}{\textbackslash}varvec\{1'W\}\_\{k\}= {\textbackslash}varvec\{1'\}\$\$, and then (ii) keeping \$\${\textbackslash}varvec\{W\}\_\{k\}\$\$fixed, it updates \$\${\textbackslash}varvec\{X\}\$\$. At convergence, the estimates of \$\${\textbackslash}varvec\{X\}: n {\textbackslash}times R\$\$and the \$\${\textbackslash}varvec\{W\}\_\{k\}\$\$will apply to all algorithms that minimize the Indscal criterion. Furthermore, we show that only at convergence an analysis-of-variance property holds on the demarcation region between over- and under-fitting. When the analysis-of-variance is valid, its validity extends over the whole matrix domain, over trace operations, and to individual matrix elements. The optimization process is unusual in that optima and local optima occur on the edges of what seem to be closely related to Heywood cases in Factor analysis.},
  langid = {english},
  keywords = {62H30,65F35,91C15,91C20,Alternating least squares,Analysis of variance,Indscal,Multidimensional data analysis,Pmindscal},
  file = {/home/gkonkamking/pCloudDrive/papers/Gower et al_2022_Properties of individual differences scaling and its interpretation.pdf}
}

@book{gradshteyn1965table,
  title = {Table of Integrals, Series, and Products},
  author = {Gradshteyn, Izrail Solomonovich and Ryzhik, Iosif Moiseevich and Jeffrey, Alan and Zwillinger, Daniel and Technica, Scripta},
  year = {1965},
  volume = {6},
  publisher = {Academic press New York},
  keywords = {duplicate-citation-key,nosource}
}

@book{gradshteyn1965table,
  title = {Table of Integrals, Series and Products},
  author = {Gradshteyn I, Ryzhik I},
  year = {2007},
  volume = {6},
  eprint = {1011.1669v3},
  publisher = {Academic press New York},
  issn = {1467-9280},
  doi = {10.1007/s13398-014-0173-7.2},
  abstract = {In Arabic, as in many languages, the future is ``ahead'' and the past is ``behind.'' Yet in the research reported here, we showed that Arabic speakers tend to conceptualize the future as behind and the past as ahead of them, despite using spoken metaphors that suggest the opposite. We propose a new account of how space-time mappings become activated in individuals' minds and entrenched in their cultures, the temporal-focus hypothesis: People should conceptualize either the future or the past as in front of them to the extent that their culture (or subculture) is future oriented or past oriented. Results support the temporal-focus hypothesis, demonstrating that the space-time mappings in people's minds are conditioned by their cultural attitudes toward time, that they depend on attentional focus, and that they can vary independently of the space-time mappings enshrined in language. Keywords},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {978-0-87421-656-1},
  pmid = {25052830},
  keywords = {conceptual metaphor,cross-cultural differences,duplicate-citation-key,mental models,nosource,open data,space,time}
}

@article{grahamLiberalsConservativesRely2009,
  title = {Liberals and Conservatives Rely on Different Sets of Moral Foundations},
  author = {Graham, Jesse and Haidt, Jonathan and Nosek, Brian A.},
  year = {2009},
  journal = {Journal of Personality and Social Psychology},
  volume = {96},
  number = {5},
  pages = {1029--1046},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1315(Electronic),0022-3514(Print)},
  doi = {10.1037/a0015141},
  abstract = {How and why do moral judgments vary across the political spectrum? To test moral foundations theory (J. Haidt \& J. Graham, 2007; J. Haidt \& C. Joseph, 2004), the authors developed several ways to measure people's use of 5 sets of moral intuitions: Harm/care, Fairness/reciprocity, Ingroup/loyalty, Authority/respect, and Purity/sanctity. Across 4 studies using multiple methods, liberals consistently showed greater endorsement and use of the Harm/care and Fairness/reciprocity foundations compared to the other 3 foundations, whereas conservatives endorsed and used the 5 foundations more equally. This difference was observed in abstract assessments of the moral relevance of foundation-related concerns such as violence or loyalty (Study 1), moral judgments of statements and scenarios (Study 2), ``sacredness'' reactions to taboo trade-offs (Study 3), and use of foundation-related words in the moral texts of religious sermons (Study 4). These findings help to illuminate the nature and intractability of moral disagreements in the American ``culture war.'' (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Ideology,Morality,nosource,Political Conservatism,Political Liberalism,Social Processes}
}

@article{grauAttitudesReferenceReplaceability2014,
  title = {Attitudes {{Towards Reference}} and {{Replaceability}}},
  author = {Grau, Christopher and Pury, Cynthia L. S.},
  year = {2014},
  month = jun,
  journal = {Review of Philosophy and Psychology},
  volume = {5},
  number = {2},
  pages = {155--168},
  issn = {1878-5166},
  doi = {10.1007/s13164-013-0162-3},
  urldate = {2020-05-11},
  abstract = {Robert Kraut has proposed an analogy between valuing a loved one as irreplaceable and the sort of ``rigid'' attachment that (according to Saul Kripke's account) occurs with the reference of proper names. We wanted to see if individuals with Kripkean intuitions were indeed more likely to value loved ones (and other persons and things) as irreplaceable. In this empirical study, 162 participants completed an online questionnaire asking them to consider how appropriate it would be to feel the same way about a perfect replica of a loved one, as well as other questions about replaceability. Participants who previously had endorsed Kripkean reference (n\,=\,96) rated loved ones as less replaceable on two different measures than participants who had previously endorsed Descriptivist reference (n\,=\,66, t(160)\,{$\geq$}\,2.27, p\,{$\leq$}\,0.02, {$\eta$}2\,{$\geq$}\,0.03). Additional results suggest that this difference extends to other targets as well and is at least partially dependent on sentimental attachment.},
  langid = {english},
  keywords = {nosource}
}

@article{grayMoralTypecastingDivergent2009,
  title = {Moral Typecasting: {{Divergent}} Perceptions of Moral Agents and Moral Patients},
  shorttitle = {Moral Typecasting},
  author = {Gray, Kurt and Wegner, Daniel M.},
  year = {2009},
  journal = {Journal of Personality and Social Psychology},
  volume = {96},
  number = {3},
  pages = {505--520},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1315(Electronic),0022-3514(Print)},
  doi = {10.1037/a0013748},
  abstract = {Moral agency is the capacity to do right or wrong, whereas moral patiency is the capacity to be a target of right or wrong. Through 7 studies, the authors explored moral typecasting---an inverse relation between perceptions of moral agency and moral patiency. Across a range of targets and situations, good- and evil-doers (moral agents) were perceived to be less vulnerable to having good and evil done to them. The recipients of good and evil (moral patients), in turn, were perceived as less capable of performing good or evil actions. Moral typecasting stems from the dyadic nature of morality and explains curious effects such as people's willingness to inflict greater pain on those who do good than those who do nothing. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Decision Making,Judgment,Morality,nosource,Perception,Reasoning}
}

@article{Green2001,
  title = {Modelling Heterogeneity with and without the Dirichlet Process},
  author = {Green, Peter J and Richardson, Sylvia},
  year = {2001},
  journal = {Scandinavian Journal of Statistics},
  volume = {28},
  number = {2},
  pages = {355--375},
  issn = {0303-6898},
  doi = {10.1111/1467-9469.00242},
  abstract = {We investigate the relationships between Dirichlet process (DP) based models and allocation models for a variable number of components, based on exchangeable distributions. It is shown that the DP partition distribution is a limiting case of a Dirichlet-multinomial allocation model. Comparisons of posterior performance of DP and allocation models are made in the Bayesian paradigm and illustrated in the context of univariate mixture models. It is shown in particular that the unbalancedness of the allocation distribution, present in the prior DP model, persists a posteriori. Exploiting the model connections, a new MCMC sampler for general DP based models is introduced, which uses split/merge moves in a reversible jump framework. Performance of this new sampler relative to that of some traditional samplers for DP processes is then explored.},
  isbn = {1467-9469},
  keywords = {allocation,bayesian non-parametrics,entropy,geneity,hetero-,markov chain monte carlo,merge moves,nite mixture distributions,normal mixtures,nosource,partition,reversible jump algorithms,semi-parametric density estimation,sensitivity analysis,split}
}

@article{Green2015,
  title = {The Challenge: {{Statistical}} Challenges in Ecotoxicology},
  author = {Green, John W.},
  year = {2015},
  journal = {Environmental Toxicology and Chemistry},
  volume = {34},
  number = {11},
  pages = {2437},
  issn = {15528618},
  doi = {10.1002/etc.3105},
  pmid = {26496133},
  keywords = {nosource}
}

@article{greeneCognitiveLoadSelectively2008,
  title = {Cognitive Load Selectively Interferes with Utilitarian Moral Judgment},
  author = {Greene, Joshua D. and Morelli, Sylvia A. and Lowenberg, Kelly and Nystrom, Leigh E. and Cohen, Jonathan D.},
  year = {2008},
  month = jun,
  journal = {Cognition},
  volume = {107},
  number = {3},
  pages = {1144--1154},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.11.004},
  urldate = {2020-04-16},
  abstract = {Traditional theories of moral development emphasize the role of controlled cognition in mature moral judgment, while a more recent trend emphasizes intuitive and emotional processes. Here we test a dual-process theory synthesizing these perspectives. More specifically, our theory associates utilitarian moral judgment (approving of harmful actions that maximize good consequences) with controlled cognitive processes and associates non-utilitarian moral judgment with automatic emotional responses. Consistent with this theory, we find that a cognitive load manipulation selectively interferes with utilitarian judgment. This interference effect provides direct evidence for the influence of controlled cognitive processes in moral judgment, and utilitarian moral judgment more specifically.},
  langid = {english},
  keywords = {Cognitive control,Cognitive load,Moral judgment,Morality,nosource,Utilitarian}
}

@article{greenlandStatisticalTestsValues2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  year = {2016},
  month = apr,
  journal = {European Journal of Epidemiology},
  volume = {31},
  number = {4},
  pages = {337--350},
  issn = {1573-7284},
  doi = {10.1007/s10654-016-0149-3},
  urldate = {2020-12-06},
  abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so---and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
  langid = {english}
}

@article{griffin2006order,
  title = {Order-Based Dependent {{Dirichlet}} Processes},
  author = {Griffin, J E and Steel, M F J},
  year = {2006},
  journal = {Journal of the American statistical Association},
  volume = {101},
  number = {473},
  pages = {179--194},
  publisher = {ASA},
  issn = {0162-1459},
  doi = {10.1198/016214505000000727},
  abstract = {In this article we propose a new framework for Bayesian nonparametric modeling with continuous covariates. In particular. we allow the nonparametric distribution to depend on covariates through ordering the random variables building the weights in the stick-breaking representation. We focus mostly on the class of random distributions that induces a Dirichlet process at each covariate value. We derive the correlation between distributions at different covariate values and use a point process to implement a practically useful type of ordering, Two main constructions with analytically known correlation structures are proposed. Practical and efficient computational methods are introduced. We apply our framework, through mixtures of these processes, to regression modeling, the modeling of stochastic volatility in time series data, and spatial geostatistical modeling.},
  keywords = {bayesian nonparametrics,markov chain monte carlo,modelling,nonparametric regression,nosource,spatial,volatility modelling}
}

@article{griffin2008bayesian,
  title = {Bayesian Nonparametric Modelling with the {{Dirichlet}} Process Regression Smoother},
  author = {Griffin, J E and Steel, M F J},
  year = {2008},
  journal = {Statistica Sinica},
  volume = {20},
  pages = {1507--1527},
  publisher = {Citeseer},
  issn = {10170405},
  abstract = {In this paper we discuss implementing Bayesian fully nonparametric re-gression by defining a process prior on distributions that depend on covariates. We consider the problem of centring our process over a class of regression models, and propose fully nonparametric regression models with flexible location structures. We also introduce an extension of a dependent finite mixture model proposed by Chung and Dunson (2011) to a dependent infinite mixture model and propose a specific prior, the Dirichlet Process Regression Smoother, which allows us to control the smoothness of the process. Computational methods are developed for the models described. Results are presented for simulated and for real data examples.},
  keywords = {and phrases,model centring,Nonlinear regression,nonparametric regression,nosource,stick-breaking prior}
}

@article{Griffin2011posterior,
  title = {Posterior Simulation of Normalized Random Measure Mixtures},
  author = {Griffin, J.E. and Walker, S.G.},
  year = {2011},
  journal = {J. Comput. Graph. Stat.},
  volume = {8600},
  number = {December 2013},
  pages = {37--41},
  publisher = {amstat.tandfonline.com},
  issn = {1061-8600},
  doi = {10.1198/jcgs.2010.08176},
  abstract = {This article describes posterior simulation methods for mixture models whose mixing distribution has a Normalized Random Measure prior. The methods use slice sampling ideas and introduce no truncation error. The approach can be easily applied to both homogeneous and nonhomogeneous Normalized Random Measures and allows the updating of the parameters of the random measure. The methods are illustrated on data examples using both Dirichlet and Normalized Generalized Gamma process priors. In particular, the methods are shown to be computationally competitive with previously developed samplers for Dirichlet process mixture models. Matlab code to implement these methods is available as supplemental material.},
  keywords = {nosource,QA276 Mathematical statistics}
}

@article{griffin2011stick,
  title = {Stick-Breaking Autoregressive Processes},
  author = {Griffin, Jim E and Steel, Mark F J},
  year = {2011},
  journal = {Journal of econometrics},
  volume = {162},
  number = {2},
  pages = {383--396},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{griffin2013comparing,
  title = {Comparing Distributions by Using Dependent Normalized Random-Measure Mixtures},
  author = {Griffin, J. E. and Kolossiatis, M. and Steel, M. F J},
  year = {2013},
  journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
  volume = {75},
  number = {3},
  pages = {499--529},
  publisher = {Wiley Online Library},
  issn = {13697412},
  doi = {10.1111/rssb.12002},
  abstract = {A methodology for the simultaneous Bayesian nonparametric modelling of several distributions{\textbackslash}r{\textbackslash}nis developed. Our approach uses normalized random measures with independent{\textbackslash}r{\textbackslash}nincrements and builds dependence through the superposition of shared processes. The properties{\textbackslash}r{\textbackslash}nof the prior are described and the modelling possibilities of this framework are explored in{\textbackslash}r{\textbackslash}nsome detail. Efficient slice sampling methods are developed for inference. Various posterior{\textbackslash}r{\textbackslash}nsummaries are introduced which allow better understanding of the differences between distributions.{\textbackslash}r{\textbackslash}nThe methods are illustrated on simulated data and examples from survival analysis{\textbackslash}r{\textbackslash}nand stochastic frontier analysis.},
  isbn = {1369-7412},
  keywords = {Bayesian non-parametrics,Dependent distributions,Dirichlet process,Normalized generalized gamma process,nosource,Slice sampling,Utility function}
}

@article{Griffin2017,
  title = {Hierarchical Shrinkage Priors for Regression Models},
  author = {Griffin, Jim and Brown, Phil},
  year = {2017},
  journal = {Bayesian Analysis},
  volume = {12},
  number = {1},
  pages = {135--159},
  issn = {19316690},
  doi = {10.1214/15-BA990},
  abstract = {In some linear models, such as those with interactions, it is natural to include the relationship between the regression coefficients in the analysis. In this paper, we consider how robust hierarchical continuous prior distributions can be used to express dependence between the size but not the sign of the regression coefficients. For example, to include ideas of heredity in the analysis of linear models with interactions.We develop a simple method for controlling the shrinkage of regression effects to zero at different levels of the hierarchy by considering the behaviour of the continuous prior at zero. Applications to linear models with interactions and generalized additive models are used as illustrations.},
  keywords = {Bayesian regularization,Generalized additive models,Generalized beta mixture prior,Interactions,Normal-gamma prior,Normal-gamma-gamma prior,nosource,Strong and weak heredity,Structured priors}
}

@article{griffiths2009diffusion,
  title = {Diffusion Processes and Coalescent Trees. {{Chapter}} 15 of Probability and Mathematical Genetics: {{Papers}} in Honour of Sir John Kingman},
  author = {Griffiths, R and Spano, D},
  year = {2010},
  journal = {London Mathematical Society Lecture Notes Series. Cambridge University Press},
  volume = {10},
  keywords = {nosource}
}

@article{griffithsAsymptoticLinedescentDistributions1984,
  title = {Asymptotic Line-of-Descent Distributions},
  author = {Griffiths, R. C.},
  year = {1984},
  month = dec,
  journal = {Journal of Mathematical Biology},
  volume = {21},
  number = {1},
  pages = {67--75},
  issn = {0303-6812, 1432-1416},
  doi = {10.1007/BF00275223},
  urldate = {2023-01-26},
  abstract = {Asymptotic distributions are derived for the number of non-mutant ancestors, at time t in the past, of a sample of n from a neutral infinite alleles model. Either the n u m b e r of non-mutant ancestors Ln (t) has a normal distribution or n-Ln(t) has a Poisson distribution as n-{\textasciitilde}, t-{\textasciitilde}O.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/E2FH5SG4/Griffiths - 1984 - Asymptotic line-of-descent distributions.pdf}
}

@article{griffithsLinesDescentDiffusion1980,
  title = {Lines of Descent in the Diffusion Approximation of Neutral {{Wright-Fisher}} Models},
  author = {Griffiths, R.C.},
  year = {1980},
  month = feb,
  journal = {Theoretical Population Biology},
  volume = {17},
  number = {1},
  pages = {37--50},
  issn = {00405809},
  doi = {10.1016/0040-5809(80)90013-1},
  urldate = {2023-01-26},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Griffiths_1980_Lines of descent in the diffusion approximation of neutral Wright-Fisher models.pdf}
}

@article{griffithsLinesDescentDiffusion1980a,
  title = {Lines of Descent in the Diffusion Approximation of Neutral {{Wright-Fisher}} Models},
  author = {Griffiths, R.C.},
  year = {1980},
  month = feb,
  journal = {Theoretical Population Biology},
  volume = {17},
  number = {1},
  pages = {37--50},
  issn = {00405809},
  doi = {10.1016/0040-5809(80)90013-1},
  urldate = {2020-05-21},
  langid = {english},
  keywords = {nosource}
}

@article{griffithsTransitionDensityExpansion1979,
  title = {A Transition Density Expansion for a Multi-Allele Diffusion Model},
  author = {Griffiths, R. C.},
  year = {1979},
  month = jun,
  journal = {Advances in Applied Probability},
  volume = {11},
  number = {2},
  pages = {310--325},
  publisher = {Cambridge University Press},
  issn = {0001-8678, 1475-6064},
  doi = {10.2307/1426842},
  urldate = {2020-04-21},
  abstract = {An expansion in orthogonal polynomials is found for the transition density in a neutral multi-allele diffusion model where the mutation rates of allele types A i  {$\rightarrow$} A j  are assumed to be u j ({$\geq$} O). The density is found when the mutation rate is positive for all allele types, and when some or all have zero mutation. The asymptotic conditional density is found for a mixture of positive and zero mutation rates. The infinite alleles limit with equal mutation is studied. Eigenfunctions of the process are derived and the frequency spectrum found. An important result is that the first eigenfunction depends only on the homozygosity. A density for the time to fixation with zero mutation is found for the K allele, and infinite alleles model.},
  langid = {english},
  keywords = {ALLELE FREQUENCY,DIFFUSION,DIRICHLET DISTRIBUTION,nosource,ORTHOGONAL POLYNOMIALS,TRANSITION DENSITY}
}

@article{griffithsWrightFisherDiffusion2018,
  title = {Wright--{{Fisher}} Diffusion Bridges},
  author = {Griffiths, Robert C. and Jenkins, Paul A. and Span{\`o}, Dario},
  year = {2018},
  month = jul,
  journal = {Theoretical Population Biology},
  series = {Paul {{Joyce}}},
  volume = {122},
  pages = {67--77},
  issn = {0040-5809},
  doi = {10.1016/j.tpb.2017.09.005},
  urldate = {2022-07-26},
  abstract = {The trajectory of the frequency of an allele which begins at x at time 0 and is known to have frequency z at time T can be modelled by the bridge process of the Wright--Fisher diffusion. Bridges when x=z=0 are particularly interesting because they model the trajectory of the frequency of an allele which appears at a time, then is lost by random drift or mutation after a time T. The coalescent genealogy back in time of a population in a neutral Wright--Fisher diffusion process is well understood. In this paper we obtain a new interpretation of the coalescent genealogy of the population in a bridge from a time t{$\in$}(0,T). In a bridge with allele frequencies of 0 at times 0 and T the coalescence structure is that the population coalesces in two directions from t to 0 and t to T such that there is just one lineage of the allele under consideration at times 0 and T. The genealogy in Wright--Fisher diffusion bridges with selection is more complex than in the neutral model, but still with the property of the population branching and coalescing in two directions from time t{$\in$}(0,T). The density of the frequency of an allele at time t is expressed in a way that shows coalescence in the two directions. A new algorithm for exact simulation of a neutral Wright--Fisher bridge is derived. This follows from knowing the density of the frequency in a bridge and exact simulation from the Wright--Fisher diffusion. The genealogy of the neutral Wright--Fisher bridge is also modelled by branching P{\'o}lya urns, extending a representation in a Wright--Fisher diffusion. This is a new very interesting representation that relates Wright--Fisher bridges to classical urn models in a Bayesian setting.},
  langid = {english},
  keywords = {Coalescent processes in Wright--Fisher diffusion bridges,Wright--Fisher diffusion bridges},
  file = {/home/gkonkamking/pCloudDrive/papers/Griffiths et al_2018_Wright–Fisher diffusion bridges.pdf;/home/gkonkamking/Zotero/storage/BLDDXU87/Griffiths et al. - 2018 - Wright–Fisher diffusion bridges.pdf}
}

@article{Grimmer2001,
  title = {How to Obtain Confidence Intervals without Simulation : {{The}} Delta Method How the Delta Method Works ( This Is Pretty Tech- Nical and Not Necessary )},
  author = {Grimmer, Justin and Kern, Holger},
  year = {2001},
  volume = {2},
  pages = {1--4},
  keywords = {nosource}
}

@article{Grist2006,
  title = {Bayesian and Time-Independent Species Sensitivity Distributions for Risk Assessment of Chemicals.},
  author = {Grist, Eric P M and O'Hagan, Anthony and Crane, Mark and Sorokin, Neal and Sims, Ian and Whitehouse, Paul},
  year = {2006},
  month = jan,
  journal = {Environmental science \& technology},
  volume = {40},
  number = {1},
  eprint = {16433377},
  eprinttype = {pubmed},
  pages = {395--401},
  issn = {0013-936X},
  abstract = {Species sensitivity distributions (SSDs) are increasingly used to analyze toxicity data but have been criticized for a lack of consistency in data inputs, lack of relevance to the real environment, and a lack of transparency in implementation. This paper shows how the Bayesian approach addresses concerns arising from frequentist SSD estimation. Bayesian methodologies are used to estimate SSDs and compare results obtained with time-dependent (LC50) and time-independent (predicted no observed effect concentration) endpoints for the insecticide chlorpyrifos. Uncertainty in the estimation of each SSD is obtained either in the form of a pointwise percentile confidence interval computed by bootstrap regression or an associated credible interval. We demonstrate that uncertainty in SSD estimation can be reduced by applying a Bayesian approach that incorporates expert knowledge and that use of Bayesian methodology permits estimation of an SSD that is more robust to variations in data. The results suggest that even with sparse data sets theoretical criticisms of the SSD approach can be overcome.},
  isbn = {0013-936X},
  pmid = {16433377},
  keywords = {Animals,Chlorpyrifos,Chlorpyrifos: toxicity,Data Interpretation,duplicate-citation-key,Fishes,Insecticides,Insecticides: toxicity,No-Observed-Adverse-Effect Level,nosource,Regression Analysis,Risk Assessment,Sensitivity and Specificity,Species Specificity,Statistical,Water Pollutants,Water Pollutants: toxicity}
}

@article{Grist2009,
  title = {Better Bootstrap Estimation of Hazardous Concentration Thresholds for Aquatic Assemblages},
  author = {Grist, {\relax EPM} and Leung, {\relax KMY}},
  year = {2009},
  journal = {Environmental {\dots}},
  volume = {21},
  number = {7},
  pages = {1515--1524},
  keywords = {bootstrap regression,duplicate-citation-key,nosource,risk assessment,species sensitivity distribution}
}

@article{Grist2009,
  title = {Better Bootstrap Estimation of Hazardous Concentration Thresholds for Aquatic Assemblages},
  author = {Grist, EPM Eric P M and Leung, Kenneth M Y KMY Kenneth M Y and Wheeler, James R and Crane, Mark},
  year = {2002},
  journal = {Environmental {\dots}},
  volume = {21},
  number = {7},
  pages = {1515--1524},
  issn = {07307268 (ISSN)},
  doi = {10.1002/etc.5620210725},
  abstract = {The introduction of species sensitivity distribution (SSD) approaches to ecological risk assessment offers the potential for a more transparent scientific basis for the derivation of predicted no-effect concentrations. However, conventional SSD methodologies have relied on standard distributions (e.g., log logistic, log normal) that are not necessarily based on sound ecological or statistical grounds. More recently, bootstrap resampling techniques that do not rely on distributional assumptions have been applied to the problem. Here we describe how a more advanced bootstrap methodology may be applied to derive better point estimates and confidence intervals for SSD estimates of safe environmental concentrations. Motivated by the fact that the true SSD may not fit any standard model category, we go on to consider a hybrid bootstrap regression approach. This can yield a substantially different estimate for the SSD when compared with both the basic bootstrap and the more frequently used parametric curve approaches. With increasing use of SSDs in ecological risk assessment, it is now imperative that the scientific community develops agreement over appropriate methods for their derivation.},
  isbn = {1552-8618},
  pmid = {12109754},
  keywords = {bootstrap regression,Bootstrap regression,duplicate-citation-key,nosource,risk assessment,Risk assessment,species sensitivity distribution,Species sensitivity distribution}
}

@techreport{gsmaMobileEconomyLatin2021,
  title = {The Mobile Economy, {{Latin America}} 2021. {{Technical Report}}},
  author = {GSMA},
  year = {2021},
  keywords = {nosource}
}

@article{GSS14,
  title = {Computing Energy Eigenvalues of Anharmonic Oscillators Using the Double Exponential Sinc Collocation Method},
  author = {Gaudreau, Philippe and Slevinsky, Richard and Safouhi, Hassan},
  year = {2014},
  abstract = {A quantum anharmonic oscillator is defined by the Hamiltonian H, where the potential is given by V. Using the Sinc collocation method combined with the double exponential transformation, we develop a method to efficiently compute highly accurate approximations of energy eigenvalues for anharmonic oscillators. Convergence properties of the proposed method are presented. Using the principle of minimal sensitivity, we introduce an alternate expression for the mesh size for the Sinc collocation method which improves considerably the accuracy in computing eigenvalues for potentials with multiple wells. We apply our method to a number of potentials including potentials with multiple wells. The numerical results section clearly illustrates the high efficiency and accuracy of the proposed method. All our codes are written using the programming language Julia and are available upon request.},
  keywords = {nosource}
}

@article{guha2008bayesian,
  title = {Bayesian Hidden {{Markov}} Modeling of Array {{CGH}} Data},
  author = {Guha, Subharup and Li, Yi and Neuberg, Donna},
  year = {2008},
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {482},
  pages = {485--497},
  publisher = {Taylor \& Francis},
  keywords = {nosource}
}

@article{guhaPosteriorContractionParameters2021,
  title = {On Posterior Contraction of Parameters and Interpretability in {{Bayesian}} Mixture Modeling},
  author = {Guha, Aritra and Ho, Nhat and Nguyen, XuanLong},
  year = {2021},
  month = nov,
  journal = {Bernoulli},
  volume = {27},
  number = {4},
  pages = {2159--2188},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  doi = {10.3150/20-BEJ1275},
  urldate = {2021-10-21},
  abstract = {We study posterior contraction behaviors for parameters of interest in the context of Bayesian mixture modeling, where the number of mixing components is unknown while the model itself may or may not be correctly specified. Two representative types of prior specification will be considered: one requires explicitly a prior distribution on the number of mixture components, while the other places a nonparametric prior on the space of mixing distributions. The former is shown to yield an optimal rate of posterior contraction on the model parameters under minimal conditions, while the latter can be utilized to consistently recover the unknown number of mixture components, with the help of a fast probabilistic post-processing procedure. We then turn the study of these Bayesian procedures to the realistic settings of model misspecification. It will be shown that the modeling choice of kernel density functions plays perhaps the most impactful roles in determining the posterior contraction rates in the misspecified situations. Drawing on concrete posterior contraction rates established in this paper we wish to highlight some aspects about the interesting tradeoffs between model expressiveness and interpretability that a statistical modeler must negotiate in the rich world of mixture modeling.},
  keywords = {Bayesian inference,Bayesian nonparametrics,misspecified models,Mixture models,post-processing algorithm,Wasserstein distance},
  file = {/home/gkonkamking/pCloudDrive/papers/Guha et al_2021_On posterior contraction of parameters and interpretability in Bayesian mixture.pdf}
}

@article{guindani2014bayesian,
  title = {A {{Bayesian}} Semiparametric Approach for the Differential Analysis of Sequence Counts Data},
  author = {Guindani, Michele and Sep{\'u}lveda, Nuno and Paulino, Carlos Daniel and M{\"u}ller, Peter},
  year = {2014},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {63},
  number = {3},
  pages = {385--404},
  publisher = {Wiley Online Library},
  keywords = {nosource}
}

@article{gundemEvolutionaryHistoryLethal2015,
  title = {The Evolutionary History of Lethal Metastatic Prostate Cancer},
  author = {Gundem, Gunes and Van Loo, Peter and Kremeyer, Barbara and Alexandrov, Ludmil B. and Tubio, Jose M. C. and Papaemmanuil, Elli and Brewer, Daniel S. and Kallio, Heini M. L. and H{\"o}gn{\"a}s, Gunilla and Annala, Matti and Kivinummi, Kati and Goody, Victoria and Latimer, Calli and O'Meara, Sarah and Dawson, Kevin J. and Isaacs, William and {Emmert-Buck}, Michael R. and Nykter, Matti and Foster, Christopher and {Kote-Jarai}, Zsofia and Easton, Douglas and Whitaker, Hayley C. and Neal, David E. and Cooper, Colin S. and Eeles, Rosalind A. and Visakorpi, Tapio and Campbell, Peter J. and McDermott, Ultan and Wedge, David C. and Bova, G. Steven},
  year = {2015},
  month = apr,
  journal = {Nature},
  volume = {520},
  number = {7547},
  pages = {353--357},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14347},
  urldate = {2021-05-02},
  abstract = {The subclonal composition of human prostate tumours and their metastases has been mapped by whole-genome sequencing, thus establishing the evolutionary trees behind the development and spread of these cancers; an important observation was that metastases could be re-seeded multiple times, and spread from one tumour to another was frequently seen.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english}
}

@article{Gunther1981Finite,
  title = {Finite Dimensional Filter Systems in Discrete Time},
  author = {Guenther, Sawitzki},
  year = {1981},
  journal = {Stochastics},
  volume = {5},
  number = {1-2},
  pages = {107--114},
  issn = {0090-9491},
  doi = {10.1080/17442508108833176},
  abstract = {Under regularity conditions, a finite dimensional filter system exists for a partially observable process if and only if the conditional distributions involved each form an exponential family of distributions. The filter equation can be derived directly from the exponential representations of these families.},
  keywords = {nosource}
}

@article{Gutierrez2016,
  title = {A Time Dependent {{Bayesian}} Nonparametric Model for Air Quality Analysis},
  author = {Guti{\'e}rrez, Luis and Mena, Rams{\'e}s H and Ruggiero, Matteo},
  year = {2016},
  journal = {Computational Statistics \& Data Analysis},
  volume = {95},
  pages = {161--175},
  publisher = {Elsevier B.V.},
  issn = {0167-9473},
  doi = {http://dx.doi.org/10.1016/j.csda.2015.10.002},
  abstract = {Abstract Air quality monitoring is based on pollutants concentration levels, typically recorded in metropolitan areas. These exhibit spatial and temporal dependence as well as seasonality trends, and their analysis demands flexible and robust statistical models. Here we propose to model the measurements of particulate matter, composed by atmospheric carcinogenic agents, by means of a Bayesian nonparametric dynamic model which accommodates the dependence structures present in the data and allows for fast and efficient posterior computation. Lead by the need to infer the probability of threshold crossing at arbitrary time points, crucial in contingency decision making, we apply the model to the time-varying density estimation for a \{PM\} 2.5 dataset collected in Santiago, Chile, and analyze various other quantities of interest derived from the estimate.},
  keywords = {Density estimation,Dependent process,Dirichlet process,nosource,Particulate matter,Stick-breaking construction}
}

@article{hackl2018mobility,
  title = {Mobility Equity in a Globalized World: {{Reducing}} Inequalities in the Sustainable Development Agenda},
  author = {Hackl, Andreas},
  year = {2018},
  journal = {World development},
  volume = {112},
  pages = {150--162},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{Halpern1998,
  title = {Evolutionary Distances for Protein-Coding Sequences: Modeling Site-Specific Residue Frequencies.},
  author = {Halpern, a L and Bruno, W J},
  year = {1998},
  journal = {Molecular biology and evolution},
  volume = {15},
  number = {7},
  pages = {910--917},
  issn = {0737-4038},
  abstract = {Estimation of evolutionary distances from coding sequences must take into account protein-level selection to avoid relative underestimation of longer evolutionary distances. Current modeling of selection via site-to-site rate heterogeneity generally neglects another aspect of selection, namely position-specific amino acid frequencies. These frequencies determine the maximum dissimilarity expected for highly diverged but functionally and structurally conserved sequences, and hence are crucial for estimating long distances. We introduce a codon-level model of coding sequence evolution in which position-specific amino acid frequencies are free parameters. In our implementation, these are estimated from an alignment using methods described previously. We use simulations to demonstrate the importance and feasibility of modeling such behavior; our model produces linear distance estimates over a wide range of distances, while several alternative models underestimate long distances relative to short distances. Site-to-site differences in rates, as well as synonymous/nonsynonymous and first/second/third-codon-position differences, arise as a natural consequence of the site-to-site differences in amino acid frequencies.},
  isbn = {0737-4038 (Print){\textbackslash}n0737-4038 (Linking)},
  pmid = {9656490},
  keywords = {aaron l,address for correspondence and,and microbiology,de-,evolutionary distances,halpern,health sciences,lection,maximum likelihood,nosource,partment of molecular genetics,reprints,saturation,se-,site-specific frequencies,variable-rate models}
}

@article{Halse2003,
  title = {Salinisation and Prospects for Biodiversity in Rivers and Wetlands of South-West {{Western Australia}}},
  author = {Halse, S. A. and Ruprecht, J. K. and Pinder, A. M.},
  year = {2003},
  journal = {Australian Journal of Botany},
  volume = {51},
  number = {6},
  pages = {673--688},
  issn = {00671924},
  doi = {10.1071/BT02113},
  abstract = {Saline water was common in south- west Western Australian aquatic systems prior to land- clearing because most streams and wetlands were ephemeral and evapo- concentrated as they dried, and there were high concentrations of stored salt in groundwater and soil profiles. Nevertheless, a 1998 review of salinity trends in rivers of south- west Western Australia showed that 20- fold increases in salinity concentrations had occurred since clearing in the medium- rainfall zone 300 - 700 mm). More recent data confirm these trends and show that elevated salinities have already caused substantial changes to the biological communities of aquatic ecosystems. Further substantial changes will occur, despite the flora and fauna of the south- west being comparatively well adapted to the presence of salinity in the landscape. Up to one- third of wetland and river invertebrate species, large numbers of plants and a substantial proportion of the waterbird fauna will disappear from the wheatbelt, a region that has high biodiversity value and endemism. Increased salinities are not the only threat associated with salinisation: increased water volumes, longer periods of inundation and more widespread acidity are also likely to be detrimental to the biota.},
  isbn = {0067-1924},
  keywords = {nosource}
}

@article{hamilton1990analysis,
  title = {Analysis of Time Series Subject to Changes in Regime},
  author = {Hamilton, James D},
  year = {1990},
  journal = {Journal of econometrics},
  volume = {45},
  number = {1-2},
  pages = {39--70},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{hanson2002modeling,
  title = {Modeling Regression Error with a Mixture of Polya Trees},
  author = {Hanson, Timothy E and Johnson, Wesley O},
  year = {2002},
  journal = {Journal of American Statistical Association},
  number = {97},
  pages = {1020--1033},
  keywords = {nosource}
}

@article{hanson2006inference,
  title = {Inference for Mixtures of Finite Polya Tree Models},
  author = {Hanson, Timothy E},
  year = {2006},
  journal = {J Am Stat Assoc},
  volume = {101},
  number = {476},
  pages = {1548--1565},
  issn = {0162-1459},
  doi = {10.2307/27639772},
  abstract = {Mixtures of Polya tree models provide a flexible alternative when a parametric model may only hold approximately. I provide computational strategies for obtaining full semiparametric inference for mixtures of finite Polya tree models given a standard parameterization, including models that would be troublesome to fit using Dirichlet process mixtures. Recommendations are put forth on choosing the level of a finite Polya tree, and model comparison is discussed. Several examples demonstrate the utility of finite Polya tree modeling, including data fit to generalized linear mixed models and several survival models.},
  isbn = {01621459},
  keywords = {nosource}
}

@article{Harbers2006,
  title = {Estimating the Impact of High-Production-Volume Chemicals on Remote Ecosystems by Toxic Pressure Calculation},
  author = {Harbers, Jasper V. and Huijbregts, Mark A.J. and Posthuma, Leo and Van De Meent, Dik},
  year = {2006},
  journal = {Environmental Science and Technology},
  volume = {40},
  number = {5},
  pages = {1573--1580},
  issn = {0013936X},
  doi = {10.1021/es051633m},
  abstract = {Although many chemicals are in use, the environmental impacts of only a few have been established, usually on per-chemical basis. Uncertainty remains about the overall impact of chemicals. This paper estimates combined toxic pressure on coastal North Sea ecosystems from 343 high-production-volume chemicals used within the catchment of rivers Rhine, Meuse, and Scheldt. Multimedia fate modeling and species sensitivity distribution-based effects estimation are applied. Calculations start from production volumes and emission rates and use physicochemical substance properties and aquatic ecotoxicity data. Parameter uncertainty is addressed by Monte Carlo simulations. Results suggest that the procedure is technically feasible. Combined toxic pressure of all 343 chemicals in coastal North Seawater is 0.025 (2.5\% of the species are exposed to concentration levels above EC50 values), with a wide confidence interval of nearly 0-1. This uncertainty appears to be largely due to uncertainties in interspecies variances of aquatic toxicities and, to a lesser extent, to uncertainties in emissions and degradation rates. Due to these uncertainties, the results support gross ranking of chemicals in categories: negligible and possibly relevant contributions only. With 95\% confidence, 283 of the 343 chemicals (83\%) contribute negligibly (less than 0.1\%) to overall toxic pressure, and only 60 (17\%) need further consideration.},
  keywords = {nosource}
}

@article{harshman1970foundations,
  title = {Foundations of the {{PARAFAC}} Procedure: {{Models}} and Conditions for an" Explanatory" Multi-Modal Factor Analysis},
  author = {Harshman, Richard A},
  year = {1970},
  publisher = {University of California at Los Angeles Los Angeles, CA},
  keywords = {nosource}
}

@article{hartigan1985dip,
  title = {The Dip Test of Unimodality},
  author = {a. Hartigan, J. and Hartigan, P. M.},
  year = {1985},
  journal = {The Annals of Statistics},
  volume = {13},
  number = {1},
  pages = {70--84},
  publisher = {JSTOR},
  issn = {0090-5364},
  doi = {10.1214/aos/1176346577},
  abstract = {Evidence exists for a nonverbal capacity for the apprehension of number, in humans [1] (including infants [2, 3]) and in other primates [4-6]. Here, we show that perceived numerosity is susceptible to adaptation, like primary visual properties of a scene, such as color, contrast, size, and speed. Apparent numerosity was decreased by adaptation to large numbers of dots and increased by adaptation to small numbers, the effect depending entirely on the numerosity of the adaptor, not on contrast, size, orientation, or pixel density, and occurring with very low adaptor contrasts. We suggest that the visual system has the capacity to estimate numerosity and that it is an independent primary visual property, not reducible to others like spatial frequency or density of texture [7].},
  isbn = {00905364},
  keywords = {nosource}
}

@article{hastieSamplingDirichletProcess2015,
  title = {Sampling from {{Dirichlet}} Process Mixture Models with Unknown Concentration Parameter: Mixing Issues in Large Data Implementations},
  shorttitle = {Sampling from {{Dirichlet}} Process Mixture Models with Unknown Concentration Parameter},
  author = {Hastie, David I. and Liverani, Silvia and Richardson, Sylvia},
  year = {2015},
  month = sep,
  journal = {Statistics and Computing},
  volume = {25},
  number = {5},
  pages = {1023--1037},
  issn = {1573-1375},
  doi = {10.1007/s11222-014-9471-3},
  urldate = {2022-11-29},
  abstract = {We consider the question of Markov chain Monte Carlo sampling from a general stick-breaking Dirichlet process mixture model, with concentration parameter \$\${\textbackslash}alpha \$\$. This paper introduces a Gibbs sampling algorithm that combines the slice sampling approach of Walker (Communications in Statistics - Simulation and Computation 36:45--54, 2007) and the retrospective sampling approach of Papaspiliopoulos and Roberts (Biometrika 95(1):169--186, 2008). Our general algorithm is implemented as efficient open source C++ software, available as an R package, and is based on a blocking strategy similar to that suggested by Papaspiliopoulos (A note on posterior sampling from Dirichlet mixture models, 2008) and implemented by Yau et al. (Journal of the Royal Statistical Society, Series B (Statistical Methodology) 73:37--57, 2011). We discuss the difficulties of achieving good mixing in MCMC samplers of this nature in large data sets and investigate sensitivity to initialisation. We additionally consider the challenges when an additional layer of hierarchy is added such that joint inference is to be made on \$\${\textbackslash}alpha \$\$. We introduce a new label-switching move and compute the marginal partition posterior to help to surmount these difficulties. Our work is illustrated using a profile regression (Molitor et al. Biostatistics 11(3):484--498, 2010) application, where we demonstrate good mixing behaviour for both synthetic and real examples.},
  langid = {english},
  keywords = {Bayesian clustering,Dirichlet process,Mixture model,Profile regression},
  file = {/home/gkonkamking/pCloudDrive/papers/Hastie et al_2015_Sampling from Dirichlet process mixture models with unknown concentration.pdf}
}

@article{hauserCooperatingFuture2014,
  title = {Cooperating with the Future},
  author = {Hauser, Oliver P. and Rand, David G. and Peysakhovich, Alexander and Nowak, Martin A.},
  year = {2014},
  month = jul,
  journal = {Nature},
  volume = {511},
  number = {7508},
  pages = {220--223},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature13530},
  urldate = {2020-04-16},
  abstract = {An intergenerational cooperation game has been developed to study decision-making regarding resource use: when decisions about resource extraction were made individually the resource was rapidly depleted by a minority of defectors; the resource was sustainably maintained across generations, however, when decisions were made democratically by voting.},
  copyright = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {nosource}
}

@article{hauserDissociationMoralJudgments2007,
  title = {A {{Dissociation Between Moral Judgments}} and {{Justifications}}},
  author = {Hauser, Marc and Cushman, Fiery and Young, Liane and Jin, R. Kang-Xing and Mikhail, John},
  year = {2007},
  journal = {Mind \& Language},
  volume = {22},
  number = {1},
  pages = {1--21},
  issn = {1468-0017},
  doi = {10.1111/j.1468-0017.2006.00297.x},
  urldate = {2020-06-09},
  abstract = {Abstract: To what extent do moral judgments depend on conscious reasoning from explicitly understood principles? We address this question by investigating one particular moral principle, the principle of the double effect. Using web-based technology, we collected a large data set on individuals' responses to a series of moral dilemmas, asking when harm to innocent others is permissible. Each moral dilemma presented a choice between action and inaction, both resulting in lives saved and lives lost. Results showed that: (1) patterns of moral judgments were consistent with the principle of double effect and showed little variation across differences in gender, age, educational level, ethnicity, religion or national affiliation (within the limited range of our sample population) and (2) a majority of subjects failed to provide justifications that could account for their judgments. These results indicate that the principle of the double effect may be operative in our moral judgments but not open to conscious introspection. We discuss these results in light of current psychological theories of moral cognition, emphasizing the need to consider the unconscious appraisal system that mentally represents the causal and intentional properties of human action.},
  copyright = {2007 Blackwell Publishing Ltd},
  langid = {english},
  keywords = {nosource}
}

@article{hautphenne2021birth,
  title = {Birth-and-Death Processes in {{Python}}: {{The BirDePy}} Package},
  author = {Hautphenne, Sophie and Patch, Brendan},
  year = {2021},
  journal = {arXiv preprint arXiv:2110.05067},
  eprint = {2110.05067},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/pCloudDrive/papers/Hautphenne_Patch_2021_Birth-and-death processes in Python.pdf}
}

@article{havrda1967quantification,
  title = {Quantification Method of Classification Processes. {{Concept}} of Structural Alpha-Entropy},
  author = {Havrda, J and Charv{\'a}t, F},
  year = {1967},
  journal = {Kybernetika},
  volume = {3},
  number = {1},
  pages = {30--35},
  publisher = {{Institute of Information Theory and Automation AS CR}},
  issn = {1558-254X},
  keywords = {nosource}
}

@article{Hayashi2010,
  title = {A Bayesian Method for Deriving Species-Sensitivity Distributions: {{Selecting}} the Best-Fit Tolerance Distributions of Taxonomic Groups},
  author = {Hayashi, Takehiko I. and Kashiwagi, Nobuhisa},
  year = {2010},
  month = apr,
  journal = {Human and Ecological Risk Assessment: An International Journal},
  volume = {16},
  number = {2},
  pages = {251--263},
  issn = {1080-7039},
  doi = {10.1080/10807031003670279},
  isbn = {1080703100367},
  keywords = {bayesian statistics,bution,deviance information criterion,duplicate-citation-key,ecological risk assessment,model selection,nosource,species-sensitivity distri}
}

@article{Hayashi2011,
  title = {A {{Bayesian}} Approach to Probabilistic Ecological Risk Assessment: Risk Comparison of Nine Toxic Substances in {{Tokyo}} Surface Waters.},
  author = {Hayashi, Takehiko I and Kashiwagi, Nobuhisa},
  year = {2011},
  month = mar,
  journal = {Environmental science and pollution research international},
  volume = {18},
  number = {3},
  eprint = {20686862},
  eprinttype = {pubmed},
  pages = {365--75},
  issn = {1614-7499},
  doi = {10.1007/s11356-010-0380-5},
  abstract = {Quantitative risk comparison of toxic substances is necessary to decide which substances should be prioritized to achieve effective risk management. This study compared the ecological risk among nine major toxic substances (ammonia, bisphenol-A, chloroform, copper, hexavalent chromium, lead, manganese, nickel, and zinc) in Tokyo surface waters by adopting an integrated risk analysis procedure using Bayesian statistics.},
  isbn = {1135601003},
  pmid = {20686862},
  keywords = {Ammonia,Ammonia: analysis,Ammonia: toxicity,Bayes Theorem,Bayesian statistics,Chemical,Chemical: analysis,Chemical: statistics \& numerical,Chemical: toxicity,Chloroform,Chloroform: analysis,Chloroform: toxicity,duplicate-citation-key,Ecological risk assessment,Environmental Monitoring,Environmental Monitoring: methods,Fresh Water,Fresh Water: chemistry,Hazardous Substances,Hazardous Substances: analysis,Hazardous Substances: toxicity,Heavy,Heavy: analysis,Heavy: toxicity,Metals,Monte Carlo Method,nosource,Phenols,Phenols: analysis,Phenols: toxicity,Probabilistic risk analysis,Probability,Quantitative risk comparison,Risk Assessment,Risk Assessment: methods,Species sensitivity distribution,Species Specificity,Tokyo,Uncertainty analysis,Water Pollutants,Water Pollution}
}

@article{He2014,
  title = {Ecological Risk Assessment and Priority Setting for Typical Toxic Pollutants in the Water from {{Beijing-Tianjin-Bohai}} Area Using {{Bayesian}} Matbugs Calculator ({{BMC}})},
  author = {He, Wei and Qin, Ning and Kong, Xiangzhen and Liu, Wenxiu and Wu, Wenjing and He, Qishuang and Yang, Chen and Jiang, Yujiao and Wang, Qingmei and Yang, Bin and Xu, Fuliu},
  year = {2014},
  journal = {Ecological Indicators},
  volume = {45},
  pages = {209--218},
  publisher = {Elsevier Ltd},
  issn = {1470160X},
  doi = {10.1016/j.ecolind.2014.04.008},
  abstract = {A novel platform, named the Bayesian matbugs calculator (BMC), was developed to select the best SSD model, assess ecological risk at high-, mid- and low-levels of the 95\% credible interval (CI), and to set the priority of toxic substances. The BMC platform was applied to the ecological risk assessment and priority setting of 32 toxic substances, including polycyclic aromatic hydrocarbons (PAHs) and organochlorine pesticides (OCPs), in the water from the Beijing-Tianjin-Bohai (BTB) area of northern China. The results showed that most of the studied PAH and OCP compounds have a high-level ecological risk with potential affected fraction (PAF) {$>$} 10-3 except for benzo(a)anthracene, pyrene, chrysene and ??-hexachlorocyclohexane (??-HCH). The Yongdinghe River, Yongdingxinhe River, and Guanting Reservoir had the highest multiple substance combined PAF (msPAF) at mid-level, whereas the Qingshuihe River had the lowest msPAF, ranging from 2.91 ?? 10 -7 to 1.15 ?? 10-1 at various levels. On the basis of ecological risk at the high level of 95\% CI, the priorities for PAHs and OCPs were anthracene, chrysene, benzo(a)pyrene, ??-HCH, p,p???- dichlorodiphenyldichloroethane (p,p???-DDD), heptachlor epoxide, endosulfan sulfate, methoxychlor, and endosulfan II. The BMC platform can be concluded to be a friendly, accessible, efficient tool to select the best SSD model, calculate relevant indicators, assess ecological risks with uncertainty, and to set the priority of toxic substances. ?? 2014 Elsevier Ltd.},
  keywords = {Bayesian matbugs calculator,Beijing-Tianjin- Bohai area,Ecological risk indicator,nosource,Priority setting,Species sensitivity distribution model,Toxic substances}
}

@article{Heaps2017,
  title = {Generalising Rate Heterogeneity across Sites in Statistical Phylogenetics},
  author = {Heaps, Sarah E. and Nye, Tom M. W. and Boys, Richard J. and Williams, Tom A. and Cherlin, Svetlana and Embley, T. Martin},
  year = {2017},
  eprint = {1702.05972},
  pages = {1--29},
  abstract = {In phylogenetics, alignments of molecular sequence data for a collection of species are used to learn about their phylogeny - an evolutionary tree which places these species as leaves and ancestors as internal nodes. Sequence evolution on each branch of the tree is generally modelled using a continuous time Markov process, characterised by an instantaneous rate matrix. Early models assumed the same rate matrix governed substitutions at all sites of the alignment, ignoring the variation in evolutionary constraints. Substantial improvements in phylogenetic inference and model fit were achieved by augmenting these models with a set of multiplicative random effects that allowed different sites to evolve at different rates which scaled the baseline rate matrix. Motivated by this pioneering work, we consider an extension which allows quadratic, rather than linear, site-specific transformations of the baseline rate matrix. We derive properties of the resulting process and show that when combined with a particular class of non-stationary models, we obtain one that allows sequence composition to vary across both sites of the alignment and taxa. Formulating the model in a Bayesian framework, a Markov chain Monte Carlo algorithm for posterior inference is described. We consider two applications to alignments concerning the tree of life, fitting stationary and non-stationary models. In each case we compare inferences obtained under our site-specific quadratic transformation, with those under linear and site-homogeneous models.},
  archiveprefix = {arXiv},
  arxivid = {1702.05972},
  keywords = {nosource}
}

@article{heathFinettisTheoremExchangeable1976,
  title = {De {{Finetti}}'s {{Theorem}} on {{Exchangeable Variables}}},
  author = {Heath, David and Sudderth, William},
  year = {1976},
  month = nov,
  journal = {The American Statistician},
  volume = {30},
  number = {4},
  pages = {188--189},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.1976.10479175},
  urldate = {2022-02-09},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Heath_Sudderth_1976_De Finetti's Theorem on Exchangeable Variables.pdf}
}

@article{Hector2007,
  title = {Biodiversity and Ecosystem Multifunctionality.},
  author = {Hector, Andy and Bagchi, Robert},
  year = {2007},
  journal = {Nature},
  volume = {448},
  number = {7150},
  eprint = {17625564},
  eprinttype = {pubmed},
  pages = {188--90},
  issn = {1476-4687},
  doi = {10.1038/nature05947},
  abstract = {Biodiversity loss can affect ecosystem functions and services. Individual ecosystem functions generally show a positive asymptotic relationship with increasing biodiversity, suggesting that some species are redundant. However, ecosystems are managed and conserved for multiple functions, which may require greater biodiversity. Here we present an analysis of published data from grassland biodiversity experiments, and show that ecosystem multifunctionality does require greater numbers of species. We analysed each ecosystem function alone to identify species with desirable effects. We then calculated the number of species with positive effects for all possible combinations of functions. Our results show appreciable differences in the sets of species influencing different ecosystem functions, with average proportional overlap of about 0.2 to 0.5. Consequently, as more ecosystem processes were included in our analysis, more species were found to affect overall functioning. Specifically, for all of the analysed experiments, there was a positive saturating relationship between the number of ecosystem processes considered and the number of species influencing overall functioning. We conclude that because different species often influence different functions, studies focusing on individual processes in isolation will underestimate levels of biodiversity required to maintain multifunctional ecosystems.},
  isbn = {0028-0836},
  pmid = {17625564},
  keywords = {Biodiversity,Biomass,Conservation of Natural Resources,Ecology,Ecosystem,Europe,nosource,Plants,Plants: classification,Regression Analysis}
}

@book{Helsel2005,
  title = {Nondetects and Data Analysis. {{Statistics}} for Censored Environmental Data},
  author = {Helsel, Dennis R},
  year = {2005},
  publisher = {Wiley-Interscience},
  keywords = {duplicate-citation-key,nosource}
}

@article{Helsel2006,
  title = {Size Distribution and Anthropogenic Sources Apportionment of Airborne Trace Metals in {{Kanazawa}}, {{Japan}}},
  author = {Wang, Xilong and Sato, Tsutomu and Xing, Baoshan},
  year = {2006},
  month = dec,
  journal = {Chemosphere},
  volume = {65},
  number = {11},
  eprint = {16737727},
  eprinttype = {pubmed},
  pages = {2434--2439},
  issn = {00456535},
  doi = {10.1016/j.chemosphere.2006.04.051},
  abstract = {Aerosol samples were collected from Kanazawa, Japan to examine the size distribution of 12 elements and to identify the major sources of anthropogenic elements. Key emission sources were identified and, concentrations contributed from individual sources were estimated as well. Concentrations of elements V, Ca, Cd, Fe, Ba, Mg, Mn, Pb, Sr, Zn, Co and Cu in aerosols were determined with ICP-MS. The results showed that Ca, Mg, Sr, Mn, Co and Fe were mainly associated with coarse particles ({$>$}2.1 ??m), primarily from natural sources. In contrast, the elements Zn, Ba, Cd, V, Pb and Cu dominated in fine aerosol particles ({$<$}2.1 ??m), implying that the anthropogenic origin is the dominant source. Results of the factor analysis on elements with high EFCrust values ({$>$}10) showed that emissions from waste combustion in incinerators, oil combustion (involving waste oil burning and oil combustion in both incinerators and electricity generation plants), as well as coal combustion in electricity generation plants were major contributors of anthropogenic metals in the ambient atmosphere in Kanazawa. Quantitatively estimated sum of mean concentrations of anthropogenic elements from the key sources were in good agreement with the observed values. Results of this study elucidate the need for making pollution control strategy in this area. ?? 2006 Elsevier Ltd.},
  isbn = {0045-6535 (Print){\textbackslash}r0045-6535 (Linking)},
  pmid = {16762391},
  keywords = {Airborne particles,Enrichment factor method,Factor analysis,ICP-MS,nosource,Source identification}
}

@book{helsel2011statistics,
  title = {Statistics for Censored Environmental Data Using {{Minitab}} and {{R}}},
  author = {Helsel, Dennis R},
  year = {2011},
  volume = {77},
  publisher = {John Wiley \& Sons},
  keywords = {nosource}
}

@article{hePlugplayInferenceDisease2010,
  title = {Plug-and-Play Inference for Disease Dynamics: Measles in Large and Small Populations as a Case Study},
  shorttitle = {Plug-and-Play Inference for Disease Dynamics},
  author = {He, Daihai and Ionides, Edward L. and King, Aaron A.},
  year = {2010},
  month = feb,
  journal = {Journal of the Royal Society Interface},
  volume = {7},
  number = {43},
  pages = {271--283},
  issn = {1742-5689},
  doi = {10.1098/rsif.2009.0151},
  urldate = {2021-03-24},
  abstract = {Statistical inference for mechanistic models of partially observed dynamic systems is an active area of research. Most existing inference methods place substantial restrictions upon the form of models that can be fitted and hence upon the nature of the scientific hypotheses that can be entertained and the data that can be used to evaluate them. In contrast, the so-called plug-and-play methods require only simulations from a model and are thus free of such restrictions. We show the utility of the plug-and-play approach in the context of an investigation of measles transmission dynamics. Our novel methodology enables us to ask and answer questions that previous analyses have been unable to address. Specifically, we demonstrate that plug-and-play methods permit the development of a modelling and inference framework applicable to data from both large and small populations. We thereby obtain novel insights into the nature of heterogeneity in mixing and comment on the importance of including extra-demographic stochasticity as a means of dealing with environmental stochasticity and model misspecification. Our approach is readily applicable to many other epidemiological and ecological systems.},
  pmcid = {PMC2842609},
  pmid = {19535416},
  file = {/home/gkonkamking/Zotero/storage/TNARK96A/He et al. - 2010 - Plug-and-play inference for disease dynamics meas.pdf}
}

@article{HICKEY2008,
  ids = {hickey2008making},
  title = {Making Species Salinity Sensitivity Distributions Reflective of Naturally Occurring Communities: Using Rapid Testing and {{Bayesian}} Statistics},
  author = {Hickey, {\relax GL} and Kefford, {\relax BJ}},
  year = {2009},
  journal = {Environmental {\dots}},
  volume = {27},
  number = {11},
  pages = {2403--2411},
  isbn = {0730-7268},
  pmid = {18522453},
  keywords = {nosource,rapid testing,species sensitivity distribution}
}

@article{Hickey2009,
  title = {On the Application of Loss Functions in Determining Assessment Factors for Ecological Risk},
  author = {Hickey, Graeme L. and Craig, Peter S. and Hart, Andy},
  year = {2009},
  journal = {Ecotoxicology and Environmental Safety},
  volume = {72},
  number = {2},
  pages = {293--300},
  issn = {01476513},
  doi = {10.1016/j.ecoenv.2008.06.004},
  abstract = {Assessment factors have been proposed as a means to extrapolate from data on the concentrations hazardous to a small sample of species to the concentration hazardous to p\% of the species in a given community (HCp). Aldenberg and Jaworska [2000. Uncertainty of the hazardous concentration and fraction affected for normal species sensitivity distributions. Ecotoxicol. Environ. Saf. 46, 1-18] proposed estimators that prescribed universal assessment factors which made use of distributional assumptions associated with species sensitivity distributions. In this paper we maintain those assumptions but introduce loss functions which punish over- and under-estimation. Furthermore, the final loss function is parameterised such that conservatism can be asymmetrically and non-linearly controlled which enables one to better represent the reality of risk assessment scenarios. We describe the loss functions and derive Bayes rules for each. We demonstrate the method by producing a table of universal factors that are independent of the substance being assessed and which can be combined with the toxicity data in order to estimate the HC5. Finally, through an example we illustrate the potential strength of the newly proposed estimators which rationally accounts for the costs of under- and over-estimation to choose an estimator; as opposed to arbitrarily choosing a one-sided lower confidence limit. ?? 2008 Elsevier Inc. All rights reserved.},
  pmid = {18691758},
  keywords = {Assessment factor,Bayesian,Generalised absolute loss,Hazardous concentration,HCp,LINEX,nosource,Probabilistic,Species sensitivity distribution,Uncertainty}
}

@article{Hickey2010,
  title = {Ecotoxicological Risk Assessment: Developments in {{PNEC}} Estimation},
  author = {Hickey, Graeme Lee},
  year = {2010},
  number = {February},
  abstract = {Ecotoxicological risk assessment must be undertaken before a chemical can be deemed safe for application. The assessment is based on three components: hazard assessment, exposure assessment and risk characterisation. The latter is a combination of the former two. One standard approach is based on the deterministic comparison of exposure concentration estimates to the concentration of the toxicant below which adverse effects are unlikely to occur to the potentially exposed ecological assemblage. This concentration is known as the `predicted no effect concentration' (PNEC). At the level of hazard assessment we are concerned with, there is a requirement that procedures be straightforward and efficient, as well as being transparent. The PNEC is in general currently determined using either a fixed assessment factor applied to a summary statistic of observed laboratory derived toxicity data, or as a percentile of a distribution over the ecological community sensitivity. Often it is the situation that a hazard assessment will be based on substantially small samples of data. In this thesis we evaluate proposals for determining a PNEC according to regulatory guidance and scientific literature. In particular, we explore these methods under the context of alternative probabilistic models. We also focus on the determination of conservative probabilistic estimators, which may be appropriate for this level of risk assessment. Additionally, we also discuss the detection of species non-exchangeability, a concept which is recognised by scientists and risk assessors, yet typically discounted in practice. A proposal on incorporating knowledge of a non-exchangeable species for probabilistic estimators is discussed and evaluated. The final topic of research examines a generalised deterministic estimator proposed in a recent European Food Safety Authority report. In particular, we analyse the robustness and analytical properties of some cases of this estimator which (at least) maintains the expected level of protection currently attributed. Proposals made within this thesis, many of which extend upon what is currently scientifically accepted, satisfy the requirements of being tractably straightforward to apply and are scientifically defensible. This will appeal to end users and increase the chances of gaining regulatory acceptance. All developments are fully illustrated with real-life examples.},
  keywords = {nosource}
}

@article{Hickey2012b,
  ids = {Hickey2012e},
  title = {Competing Statistical Methods for the Fitting of Normal Species Sensitivity Distributions: Recommendations for Practitioners.},
  author = {Hickey, Graeme L and Craig, Peter S},
  year = {2012},
  month = jul,
  journal = {Risk analysis : an official publication of the Society for Risk Analysis},
  volume = {32},
  number = {7},
  eprint = {22050459},
  eprinttype = {pubmed},
  pages = {1232--43},
  issn = {1539-6924},
  doi = {10.1111/j.1539-6924.2011.01728.x},
  abstract = {A species sensitivity distribution (SSD) models data on toxicity of a specific toxicant to species in a defined assemblage. SSDs are typically assumed to be parametric, despite noteworthy criticism, with a standard proposal being the log-normal distribution. Recently, and confusingly, there have emerged different statistical methods in the ecotoxicological risk assessment literature, independent of the distributional assumption, for fitting SSDs to toxicity data with the overall aim of estimating the concentration of the toxicant that is hazardous to \% of the biological assemblage (usually with small). We analyze two such estimators derived from simple linear regression applied to the ordered log-transformed toxicity data values and probit transformed rank-based plotting positions. These are compared to the more intuitive and statistically defensible confidence limit-based estimator. We conclude based on a large-scale simulation study that the latter estimator should be used in typical assessments where a pointwise value of the hazardous concentration is required.},
  pmid = {22050459},
  keywords = {ecotoxicological risk assessment,Ecotoxicological risk assessment,hazardous concentration,Hazardous concentration,nosource,species sensitivity,Species sensitivity distribution}
}

@article{Hickey2012c,
  ids = {Hickey2012},
  title = {On the Quantification of Intertest Variability in Ecotoxicity Data with Application to Species Sensitivity Distributions},
  author = {Hickey, {\relax GL} and Craig, {\relax PS}},
  year = {2012},
  month = aug,
  journal = {Environmental {\dots}},
  volume = {31},
  number = {8},
  pages = {1903--10},
  issn = {1552-8618},
  doi = {10.1002/etc.1891},
  abstract = {Ecotoxicological hazard assessment relies on species effect data to estimate quantities such as the predicted no-effect concentration. While there is a concerted effort to quantify uncertainty in risk assessments, the uncertainty due to intertest variability in species effect measurements is an overlooked component. The European Union Registration, Evaluation, Authorisation, and Restriction of Chemicals (REACH) guidance document suggests that multiple toxicity records for a given chemical-species combination should be aggregated by the geometric mean. Ignoring this issue or applying unjustified so-called harmonization methods weakens the defensibility of uncertainty quantification and interpretation about properties of ecological models, for example, the predicted no-effect concentration. In the present study, the authors propose a simple and broadly theoretically justifiable model to quantify intertest variability and analyze it using Bayesian methods. The value of data in ecotoxicity databases is maximized by using (interval-)censored data. An exploratory analysis is provided to support the model. The authors conclude, based on a large ecotoxicity database of acute effects to aquatic species, that the standard deviation of intertest variability is approximately a factor (or fold-difference) of 3. The consequences for decision makers of (not) adjusting for intertest variability are demonstrated. Environ. Toxicol. Chem. 2012; 31: 1903-1910. {\copyright} 2012 SETAC.},
  pmid = {22619109},
  keywords = {Bayesian statistics,intertest variability,Intertest variability,nosource,REACH,Species sensitivity distribution,Toxicity data},
  file = {/home/gkonkamking/pCloudDrive/papers/Hickey_Craig_2012_On the quantification of intertest variability in ecotoxicity data with2.pdf}
}

@article{hill1973diversity,
  title = {Diversity and Evenness: A Unifying Notation and Its Consequences},
  author = {Hill, Mo},
  year = {1973},
  journal = {Ecology},
  volume = {54},
  number = {2},
  pages = {427--432},
  publisher = {Eco Soc America},
  issn = {0012-9658},
  doi = {10.2307/1934352},
  abstract = {Three commonly used measures of diversity, Simpson's index, Shannon's entropy, and the total number of species, are related to Renyi's definition of a generalized entropy. A unified concept of diversity is presented, according to which there is a continuum of possible diversity measures. In a sense which becomes apparent, these measures provide estimates of the effective number of species present, and differ only in their tendency to include or to ignore the relatively rarer species. The notion of the diversity of a community as opposed to that of a sample is examined, and is related to the asymptotic form of the species---abundance curve. A new and plausible definition of evenness is derived. See full-text article at JSTOR},
  isbn = {00129658},
  pmid = {169},
  keywords = {nosource}
}

@article{hill1979posterior,
  title = {Posterior Moments of the Number of Species in a Finite Population and the Posterior Probability of Finding a New Species},
  author = {Hill, Bruce M},
  year = {1979},
  journal = {Journal of the American Statistical Association},
  volume = {74},
  number = {367},
  pages = {668--673},
  publisher = {Taylor \& Francis Group},
  issn = {01621459},
  doi = {10.2307/2286989},
  keywords = {nosource}
}

@misc{hitchcockCauseNorm2009,
  title = {Cause and {{Norm}}},
  author = {Hitchcock, Christopher and Knobe, Joshua},
  year = {2009},
  month = nov,
  journal = {The Journal of Philosophy},
  volume = {106},
  number = {11},
  pages = {587--612},
  doi = {10.5840/jphil20091061128},
  urldate = {2020-04-16},
  howpublished = {https://www.pdcnet.org/pdc/bvdb.nsf/purchase?openform\&fp=jphil\&id=jphil\_2009\_0106\_0011\_0587\_0612},
  keywords = {nosource}
}

@article{hjort1990nonparametric,
  title = {Nonparametric {{Bayes}} Estimators Based on Beta Processes in Models for Life History Data},
  author = {Hjort, N},
  year = {1990},
  journal = {The Annals of Statistics},
  volume = {18},
  number = {3},
  eprint = {2242052\{\%\}5Cnpapers2://publication/uuid/8BA29DE7-09C9-4525-8DA7-DB5C7C59A9BB},
  eprinttype = {jstor},
  pages = {1259--1294},
  publisher = {JSTOR},
  issn = {0090-5364},
  doi = {10.1214/aos/1176347749},
  abstract = {in models for life history data and choose to concentrate on A instead of B. Thus a natural task to undertake is the estimation of A in},
  keywords = {nosource}
}

@book{hjort2010bayesian,
  title = {Bayesian Nonparametrics},
  author = {Hjort, Nils Lid and Holmes, Chris and M{\"u}ller, Peter and Walker, Stephen G},
  year = {2010},
  volume = {28},
  publisher = {Cambridge University Press},
  keywords = {nosource}
}

@inproceedings{HLP14,
  title = {Parallel Algebraic Modeling for Stochastic Optimization},
  booktitle = {{{HPTCDL}}'14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages},
  author = {Huchette, Joey and Lubin, Miles and Petra, Cosmin},
  year = {2014},
  pages = {29--35},
  publisher = {ACM},
  address = {New York},
  doi = {10.1109/HPTCDL.2014.6},
  abstract = {We present scalable algebraic modeling software, StochJuMP, for stochastic optimization as applied to power grid economic dispatch. It enables the user to express the problem in a high-level algebraic format with minimal boilerplate. StochJuMP allows efficient parallel model instantiation across nodes and efficient data localization. Computational results are presented showing that the model construction is efficient, requiring less than one percent of solve time. StochJuMP is configured with the parallel interior-point solver PIPS-IPM but is sufficiently generic to allow straight forward adaptation to other solvers.},
  keywords = {nosource}
}

@article{hoConvergenceRatesParameter2016,
  title = {Convergence Rates of Parameter Estimation for Some Weakly Identifiable Finite Mixtures},
  author = {Ho, Nhat and Nguyen, XuanLong},
  year = {2016},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {44},
  number = {6},
  issn = {0090-5364},
  doi = {10.1214/16-AOS1444},
  urldate = {2022-06-13},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/Y8TBYZYR/Ho and Nguyen - 2016 - Convergence rates of parameter estimation for some.pdf}
}

@article{hoetingBayesianModelAveraging1999,
  title = {Bayesian {{Model Averaging}}: {{A Tutorial}}},
  shorttitle = {Bayesian {{Model Averaging}}},
  author = {Hoeting, Jennifer A. and Madigan, David and Raftery, Adrian E. and Volinsky, Chris T.},
  year = {1999},
  journal = {Statistical Science},
  volume = {14},
  number = {4},
  eprint = {2676803},
  eprinttype = {jstor},
  pages = {382--401},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  urldate = {2020-10-05},
  abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-of-sample predictive performance. We also provide a catalogue of currently available BMA software.},
  file = {/home/gkonkamking/Downloads/euclid.ss.1009212519.pdf}
}

@article{Hoeven1997,
  title = {How to Measure No Effect. {{Part III}}: {{Statistical}} Aspects of {{NOEC}}, {{ECx}} and {{NEC}} Estimates},
  author = {Van Der Hoeven, Nelly},
  year = {1997},
  journal = {Environmetrics},
  volume = {8},
  number = {3},
  pages = {255--261},
  issn = {11804009},
  doi = {10.1002/(SICI)1099-095X(199705)8:3<255::AID-ENV246>3.0.CO;2-P},
  abstract = {One of the principal aims of ecotoxicity tests is to determine the concentration level below which the test chemical will have no or at most a negligible effect on the test parameter. Nowadays, the NOEC (no observed effect concentration) is normally used as an estimate for this concentration. The NOEC has, however, several major drawbacks as a summary statistic: (i) it is based on wrong usage of hypothesis testing - the acceptance of a null hypothesis (no difference); (ii) the estimate depends on the accuracy of the experimental test - when the sample error is small, i.e. the test is performed accurately, the NOEC will be lower; (iii) the estimate depends on the sample size - the larger the sample size, the lower the NOEC will be; (iv) the NOEC is a test concentration; (v) the NOEC depends on the chosen significance level. An alternative is proposed, the ECx, i.e. the concentration causing an effect of x per cent: 1. To estimate the ECx a concentration-response model is needed. The Value of x in the ECx estimation should be chosen so that the ECx estimate is not too model dependent. 2. In NOEC estimation a certain deviation from the control is accepted. For an ECx estimation to give comparable protection with an NOEC, the x must be chosen somewhat smaller than the maximum non-significant deviation in the case where the test is performed according to normal guideline procedures and the variance in the control is the mean variance for such tests. Another alternative is parametric NEC (no effect concentration) estimation. The NEC is the threshold concentration below which the test chemical will not induce an effect. (a) All NEC estimates will be model dependent. Therefore, they are not appropriate in standard ecotoxicity tests where data will seldom be sufficient for model verification. (C) 1997 by John Wiley \& Sons, Ltd},
  isbn = {1099-095X},
  pmid = {629},
  keywords = {chronic toxicity,ecotoxicity tests,nosource,test guidelines}
}

@article{Hoeven2004,
  title = {Current Issues in Statistics and Models for Ecotoxicological Risk Assessment},
  author = {{der Hoeven}, N Van},
  year = {2004},
  journal = {Acta biotheoretica},
  pages = {201--217},
  isbn = {0001-5342},
  pmid = {15456984},
  keywords = {bioaccumulation,bioavailability,combination toxicology,duplicate-citation-key,ecx,ECx,equilibrium partition,food-web models,hazardous concentration,noec,NOEC,nosource,sensitivity distribution,species,Species Sensitivity Distribution,toxic units,Toxic Units}
}

@book{hoff2009first,
  title = {A First Course in {{Bayesian}} Statistical Methods},
  author = {Hoff, Peter D},
  year = {2009},
  volume = {580},
  publisher = {Springer},
  keywords = {nosource}
}

@article{Hoffman-Gelman:2011,
  ids = {good1953population},
  title = {Association Analysis of Insulin-like Growth Factor-1 Axis Parameters with Survival and Functional Status in Nonagenarians of the {{Leiden Longevity Study}}},
  author = {{van der Spoel}, Evie and Rozing, Maarten P. and {Houwing-Duistermaat}, Jeanine J. and Eline Slagboom, P. and Beekman, Marian and {de Craen}, Anton J M and Westendorp, Rudi G J and {van Heemst}, Diana},
  year = {2015},
  journal = {Aging},
  volume = {7},
  number = {11},
  eprint = {1011.1669v3},
  pages = {956--963},
  publisher = {Biometrika Trust},
  issn = {19454589},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {Reduced insulin/insulin-like growth factor 1 (IGF-1) signaling has been associated with longevity in various model organisms. However, the role of insulin/IGF-1 signaling in human survival remains controversial. The aim of this study was to test whether circulating IGF-1 axis parameters associate with old age survival and functional status in nonagenarians from the Leiden Longevity Study. This study examined 858 Dutch nonagenarian (males{$\geq$}89 years; females{$\geq$}91 years) siblings from 409 families, without selection on health or demographic characteristics. Nonagenarians were divided over sex-specific strata according to their levels of IGF-1, IGF binding protein 3 and IGF-1/IGFBP3 molar ratio. We found that lower IGF-1/IGFBP3 ratios were associated with improved survival: nonagenarians in the quartile of the lowest ratio had a lower estimated hazard ratio (95\% confidence interval) of 0.73 (0.59 -- 0.91) compared to the quartile with the highest ratio (ptrend=0.002). Functional status was assessed by (Instrumental) Activities of Daily Living ((I)ADL) scales. Compared to those in the quartile with the highest IGF-1/IGFBP3 ratio, nonagenarians in the lowest quartile had higher scores for ADL (ptrend=0.001) and IADL (ptrend=0.003). These findings suggest that IGF-1 axis parameters are associated with increased old age survival and better functional status in nonagenarians from the Leiden Longevity Study.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {9788578110796},
  pmid = {25246403},
  keywords = {duplicate-citation-key,Familial longevity,Functional status,Human,icle,IGF-1 axis,nosource,Survival}
}

@article{Hoffman-Gelman:2011,
  title = {The {{No-}}\{\vphantom\}{{U}}\vphantom\{\}-{{Turn}} Sampler: {{Adaptively}} Setting Path Lengths in \{\vphantom\}{{H}}\vphantom\{\}amiltonian \{\vphantom\}{{M}}\vphantom\{\}onte \{\vphantom\}{{C}}\vphantom\{\}arlo},
  author = {Hoffman, Matthew D. and Gelman, Andrew G},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {April},
  eprint = {1111.4246},
  pages = {1593--1623},
  issn = {15337928},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{{\textbackslash}epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{{\textbackslash}epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
  archiveprefix = {arXiv},
  arxivid = {1111.4246},
  keywords = {adaptive monte carlo,bayesian inference,dual averaging,duplicate-citation-key,hamiltonian monte carlo,markov chain monte carlo,monte carlo,nosource}
}

@article{hoffmann2013adaptive,
  title = {On Adaptive Posterior Concentration Rates},
  author = {Hoffmann, Marc and Rousseau, Judith and {Schmidt-Hieber}, Johannes},
  year = {2013},
  journal = {arXiv preprint arXiv:1305.5270},
  volume = {43},
  number = {5},
  eprint = {1305.5270},
  pages = {2259--2295},
  issn = {0090-5364},
  doi = {10.1214/15-AOS1341},
  abstract = {We investigate the problem of deriving posterior concentration rates under different loss functions in nonparametric Bayes. We first provide a lower bound on posterior coverages of shrinking neighbourhoods. This lower bound relates the metric or loss under which the shrinking neighbourhood is considered, and an intrinsic (pre)-metric linked to frequentist separation rates. The result sheds some light on proof strategies to derive posterior concentration rates. In the context of the Gaussian white noise model, we construct feasible priors based on a spike and slab procedure reminiscent of wavelet thresholding that achieve adaptive rates of contraction under \$L{\textasciicircum}2\$ or \$L{\textasciicircum}\$ metrics when the underlying parameter belongs to a collection of H{\textbackslash}"older balls and that moreover achieve our lower bound. We also discuss some consequences on the asymptotic behaviour of posterior credible balls. Our results are appended with an upper bound for the contraction rate under an arbitrary loss in a generic regular experiment. The upper bound is attained for certain sieve priors and enables in particular to extend our adaptation results to the model of density estimation.},
  archiveprefix = {arXiv},
  arxivid = {1305.5270},
  keywords = {62g07,62g20 62g08,bayesian nonparametrics,mathematical subject classification,minimax adaptive estimation,nosource,poste-,rates of convergence,rior concentration rates,sup-norm}
}

@article{hoffSimulationMatrixBingham2009,
  title = {Simulation of the {{Matrix Bingham}}--von {{Mises}}--{{Fisher Distribution}}, {{With Applications}} to {{Multivariate}} and {{Relational Data}}},
  author = {Hoff, Peter D.},
  year = {2009},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {18},
  number = {2},
  pages = {438--456},
  publisher = {ASA Website},
  issn = {1061-8600},
  doi = {10.1198/jcgs.2009.07177},
  urldate = {2024-11-29},
  abstract = {Orthonormal matrices play an important role in reduced-rank matrix approximations and the analysis of matrix-valued data. A matrix Bingham--von Mises--Fisher distribution is a probability distribution on the set of orthonormal matrices that includes linear and quadratic terms in the log-density, and arises as a posterior distribution in latent factor models for multivariate and relational data. This article describes rejection and Gibbs sampling algorithms for sampling from this family of distributions, and illustrates their use in the analysis of a protein--protein interaction network. Supplemental materials, including code and data to generate all of the numerical results in this article, are available online.},
  keywords = {Bayesian inference,Eigenvalue decomposition,Markov chain Monte Carlo,Random matrix,Social network,Stiefel manifold},
  file = {/home/gkonkamking/pCloudDrive/papers/Hoff - 2009 - Simulation of the Matrix Bingham–von Mises–Fisher Distribution, With Applications to Multivariate an.pdf}
}

@misc{hoIdentifiabilityOptimalRates2015,
  title = {Identifiability and Optimal Rates of Convergence for Parameters of Multiple Types in Finite Mixtures},
  author = {Ho, Nhat and Nguyen, XuanLong},
  year = {2015},
  month = jan,
  number = {arXiv:1501.02497},
  eprint = {1501.02497},
  primaryclass = {math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1501.02497},
  urldate = {2022-06-13},
  abstract = {This paper studies identifiability and convergence behaviors for parameters of multiple types in finite mixtures, and the effects of model fitting with extra mixing components. First, we present a general theory for strong identifiability, which extends from the previous work of Nguyen [2013] and Chen [1995] to address a broad range of mixture models and to handle matrix-variate parameters. These models are shown to share the same Wasserstein distance based optimal rates of convergence for the space of mixing distributions --- \$n{\textasciicircum}\{-1/2\}\$ under \$W\_1\$ for the exact-fitted and \$n{\textasciicircum}\{-1/4\}\$ under \$W\_2\$ for the over-fitted setting, where \$n\$ is the sample size. This theory, however, is not applicable to several important model classes, including location-scale multivariate Gaussian mixtures, shape-scale Gamma mixtures and location-scale-shape skew-normal mixtures. The second part of this work is devoted to demonstrating that for these "weakly identifiable" classes, algebraic structures of the density family play a fundamental role in determining convergence rates of the model parameters, which display a very rich spectrum of behaviors. For instance, the optimal rate of parameter estimation in an over-fitted location-covariance Gaussian mixture is precisely determined by the order of a solvable system of polynomial equations --- these rates deteriorate rapidly as more extra components are added to the model. The established rates for a variety of settings are illustrated by a simulation study.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/gkonkamking/Zotero/storage/DZEX8GCG/Ho and Nguyen - 2015 - Identifiability and optimal rates of convergence f.pdf}
}

@article{holmes2012dirichlet,
  title = {Dirichlet Multinomial Mixtures: {{Generative}} Models for Microbial Metagenomics},
  author = {Holmes, Ian and Harris, Keith and Quince, Christopher},
  year = {2012},
  journal = {PLoS ONE},
  volume = {7},
  number = {2},
  pages = {e30126},
  publisher = {Public Library of Science},
  issn = {19326203},
  doi = {10.1371/journal.pone.0030126},
  abstract = {We introduce Dirichlet multinomial mixtures (DMM) for the probabilistic modelling of microbial metagenomics data. This data can be represented as a frequency matrix giving the number of times each taxa is observed in each sample. The samples have different size, and the matrix is sparse, as communities are diverse and skewed to rare taxa. Most methods used previously to classify or cluster samples have ignored these features. We describe each community by a vector of taxa probabilities. These vectors are generated from one of a finite number of Dirichlet mixture components each with different hyperparameters. Observed samples are generated through multinomial sampling. The mixture components cluster communities into distinct 'metacommunities', and, hence, determine envirotypes or enterotypes, groups of communities with a similar composition. The model can also deduce the impact of a treatment and be used for classification. We wrote software for the fitting of DMM models using the 'evidence framework' (http://code.google.com/p/microbedmm/). This includes the Laplace approximation of the model evidence. We applied the DMM model to human gut microbe genera frequencies from Obese and Lean twins. From the model evidence four clusters fit this data best. Two clusters were dominated by Bacteroides and were homogenous; two had a more variable community composition. We could not find a significant impact of body mass on community structure. However, Obese twins were more likely to derive from the high variance clusters. We propose that obesity is not associated with a distinct microbiota but increases the chance that an individual derives from a disturbed enterotype. This is an example of the 'Anna Karenina principle (AKP)' applied to microbial communities: disturbed states having many more configurations than undisturbed. We verify this by showing that in a study of inflammatory bowel disease (IBD) phenotypes, ileal Crohn's disease (ICD) is associated with a more variable community.},
  isbn = {1932-6203 (Electronic){\textbackslash}n1932-6203 (Linking)},
  pmid = {22319561},
  file = {/home/gkonkamking/Zotero/storage/BMSTDC3Q/Holmes et al. - 2012 - Dirichlet multinomial mixtures Generative models .pdf}
}

@article{Hooper2005,
  title = {Effects of Biodiversity on Ecosystem Functioning: {{A}} Consensus of Current Knowledge},
  author = {Hooper, D. U. and Chapin, F. S. and Ewel, John J and Hector, A. and Inchausti, P. and Lavorel, S. and Lawton, J. H. and Lodge, D. M. and Loreau, M. and Naeem, S. and Schmid, B. and Set{\"a}l{\"a}, H. and Symstad, A. J. and Vandermeer, J. and Wardle, D. A.},
  year = {2005},
  journal = {Ecological Monographs},
  volume = {75},
  number = {1},
  pages = {3--35},
  issn = {00129615},
  doi = {10.1890/04-0922},
  abstract = {Humans are altering the composition of biological communities through a variety of activities that increase rates of species invasions and species extinctions, at all scales, from local to global. These changes in components of the Earth's biodiversity cause concern for ethical and aesthetic reasons, but they also have a strong potential to alter ecosystem properties and the goods and services they provide to humanity. Ecological experiments, observations, and theoretical developments show that ecosystem properties depend greatly on biodiversity in terms of the functional characteristics of organisms present in the ecosystem and the distribution and abundance of those organisms over space and time. Species effects act in concert with the effects of climate, resource availability, and disturbance regimes in influencing ecosystem properties. Human activities can modify all of the above factors; here we focus on modification of these biotic controls. The scientific community has come to a broad consensus on many aspects of the relationship between biodiversity and ecosystem functioning, including many points relevant to management of ecosystems. Further progress will require integration of knowledge about biotic and abiotic controls on ecosystem properties, how ecological communities are structured, and the forces driving species extinctions and invasions. To strengthen links to policy and management, we also need to integrate our ecological knowledge with understanding of the social and economic constraints of potential management practices. Understanding this complexity, while taking strong steps to minimize current losses of species, is necessary for responsible management of Earth's ecosystems and the diverse biota they contain. Based on our review of the scientific literature, we are certain of the following conclusions: 1)?Species' functional characteristics strongly influence ecosystem properties. Functional characteristics operate in a variety of contexts, including effects of dominant species, keystone species, ecological engineers, and interactions among species (e.g., competition, facilitation, mutualism, disease, and predation). Relative abundance alone is not always a good predictor of the ecosystem-level importance of a species, as even relatively rare species (e.g., a keystone predator) can strongly influence pathways of energy and material flows. 2)?Alteration of biota in ecosystems via species invasions and extinctions caused by human activities has altered ecosystem goods and services in many well-documented cases. Many of these changes are difficult, expensive, or impossible to reverse or fix with technological solutions. 3)?The effects of species loss or changes in composition, and the mechanisms by which the effects manifest themselves, can differ among ecosystem properties, ecosystem types, and pathways of potential community change. 4)?Some ecosystem properties are initially insensitive to species loss because (a) ecosystems may have multiple species that carry out similar functional roles, (b) some species may contribute relatively little to ecosystem properties, or (c) properties may be primarily controlled by abiotic environmental conditions. 5)?More species are needed to insure a stable supply of ecosystem goods and services as spatial and temporal variability increases, which typically occurs as longer time periods and larger areas are considered. We have high confidence in the following conclusions: 1)?Certain combinations of species are complementary in their patterns of resource use and can increase average rates of productivity and nutrient retention. At the same time, environmental conditions can influence the importance of complementarity in structuring communities. Identification of which and how many species act in a complementary way in complex communities is just beginning. 2)?Susceptibility to invasion by exotic species is strongly influenced by species composition and, under similar environmental conditions, generally decreases with increasing species richness. However, several other factors, such as propagule pressure, disturbance regime, and resource availability also strongly influence invasion success and often override effects of species richness in comparisons across different sites or ecosystems. 3)?Having a range of species that respond differently to different environmental perturbations can stabilize ecosystem process rates in response to disturbances and variation in abiotic conditions. Using practices that maintain a diversity of organisms of different functional effect and functional response types will help preserve a range of management options. Uncertainties remain and further research is necessary in the following areas: 1)?Further resolution of the relationships among taxonomic diversity, functional diversity, and community structure is important for identifying mechanisms of biodiversity effects. 2)?Multiple trophic levels are common to ecosystems but have been understudied in biodiversity/ecosystem functioning research. The response of ecosystem properties to varying composition and diversity of consumer organisms is much more complex than responses seen in experiments that vary only the diversity of primary producers. 3)?Theoretical work on stability has outpaced experimental work, especially field research. We need long-term experiments to be able to assess temporal stability, as well as experimental perturbations to assess response to and recovery from a variety of disturbances. Design and analysis of such experiments must account for several factors that covary with species diversity. 4)?Because biodiversity both responds to and influences ecosystem properties, understanding the feedbacks involved is necessary to integrate results from experimental communities with patterns seen at broader scales. Likely patterns of extinction and invasion need to be linked to different drivers of global change, the forces that structure communities, and controls on ecosystem properties for the development of effective management and conservation strategies. 5)?This paper focuses primarily on terrestrial systems, with some coverage of freshwater systems, because that is where most empirical and theoretical study has focused. While the fundamental principles described here should apply to marine systems, further study of that realm is necessary. Despite some uncertainties about the mechanisms and circumstances under which diversity influences ecosystem properties, incorporating diversity effects into policy and management is essential, especially in making decisions involving large temporal and spatial scales. Sacrificing those aspects of ecosystems that are difficult or impossible to reconstruct, such as diversity, simply because we are not yet certain about the extent and mechanisms by which they affect ecosystem properties, will restrict future management options even further. It is incumbent upon ecologists to communicate this need, and the values that can derive from such a perspective, to those charged with economic and policy decision-making.},
  isbn = {9789012082754},
  pmid = {227254000001},
  keywords = {Biodiversity,Complementary resource use,Ecosystem goods and services,Ecosystem processes,Ecosystem properties,Functional characteristics,Functional diversity,Net primary production,nosource,Sampling effect,Species extinction,Species invasions,Species richness}
}

@article{hoppe1984polya,
  title = {P{\'o}lya-like Urns and the {{Ewens}}' Sampling Formula},
  author = {Hoppe, Fred M},
  year = {1984},
  journal = {Journal of Mathematical Biology},
  volume = {20},
  number = {1},
  pages = {91--94},
  publisher = {Springer},
  keywords = {duplicate-citation-key,nosource}
}

@article{hoppe1984polya,
  title = {Polya-like Urns and the \{\vphantom\}{{E}}\vphantom\{\}wens' Sampling Formula},
  author = {Hoppe, F},
  year = {1984},
  journal = {J. Math. Biol.},
  volume = {20},
  number = {1},
  pages = {91--94},
  publisher = {Springer},
  keywords = {duplicate-citation-key,nosource}
}

@article{Hose2004,
  title = {Confirming the Species-Sensitivity Distribution Concept for Endosulfan Using Laboratory, Mesocosm, and Field Data.},
  author = {Hose, G C and {Van den Brink}, P J},
  year = {2004},
  month = nov,
  journal = {Archives of environmental contamination and toxicology},
  volume = {47},
  number = {4},
  eprint = {15499502},
  eprinttype = {pubmed},
  pages = {511--20},
  issn = {0090-4341},
  doi = {10.1007/s00244-003-3212-5},
  abstract = {In Australia, water-quality trigger values for toxicants are derived using protective concentration values based on species-sensitivity distribution (SSD) curves. SSD curves are generally derived from laboratory data with an emphasis on using local or site-specific data. In this study, Australian and non-Australian laboratory-species based SSD curves were compared and the concept of species protection confirmed by comparison of laboratory-based SSD curves with local mesocosm experiments and field monitoring data. Acute LC50 data for the organochlorine pesticide endosulfan were used for these comparisons; SSD curves were fitted using the Burr type III distribution. SSD curves indicated that the sensitivities of Australian fish and arthropods were not significantly different from those of corresponding non-Australian taxa. Arthropod taxa in the mesocosm were less sensitive than taxa in laboratory tests, which suggests that laboratory-generated single-species data may be used to predict concentrations protective of semifield (mesocosm) systems. SSDs based on laboratory data were also protective of field populations.},
  isbn = {0090-4341},
  pmid = {15499502},
  keywords = {Animals,Arthropods,Australia,Chlorinated,duplicate-citation-key,Endosulfan,Endosulfan: toxicity,Hydrocarbons,Insecticides,Insecticides: toxicity,Lethal Dose 50,Models,nosource,Reference Values,Theoretical}
}

@article{Hose2005,
  title = {Assessing the Need for Groundwater Quality Guidelines for Pesticides Using the Species Sensitivity Distribution Approach},
  author = {Hose, Grant C.},
  year = {2005},
  journal = {Human and Ecological Risk Assessment},
  volume = {11},
  number = {5},
  pages = {951--966},
  issn = {10807039},
  doi = {10.1080/10807030500257788},
  keywords = {Groundwater contamination,Hazardous concentration,nosource,Pesticides,Protective concentration,Species sensitivity distribution}
}

@article{hoStrongIdentifiabilityConvergence2016,
  title = {On Strong Identifiability and Convergence Rates of Parameter Estimation in Finite Mixtures},
  author = {Ho, Nhat and Nguyen, XuanLong},
  year = {2016},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {10},
  number = {1},
  issn = {1935-7524},
  doi = {10.1214/16-EJS1105},
  urldate = {2022-06-13},
  abstract = {This paper studies identifiability and convergence behaviors for parameters of multiple types, including matrix-variate ones, that arise in finite mixtures, and the effects of model fitting with extra mixing components. We consider several notions of strong identifiability in a matrix-variate setting, and use them to establish sharp inequalities relating the distance of mixture densities to the Wasserstein distances of the corresponding mixing measures. Characterization of identifiability is given for a broad range of mixture models commonly employed in practice, including locationcovariance mixtures and location-covariance-shape mixtures, for mixtures of symmetric densities, as well as some asymmetric ones. Minimax lower bounds and rates of convergence for the maximum likelihood estimates are established for such classes, which are also confirmed by simulation studies.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/GCS332UD/Ho and Nguyen - 2016 - On strong identifiability and convergence rates of.pdf}
}

@article{houssineau2018large,
  title = {On Large Lag Smoothing for Hidden Markov Models},
  author = {Houssineau, Jeremie and Jasra, Ajay and Singh, Sumeetpal S},
  year = {2018},
  journal = {arXiv preprint arXiv:1804.07117},
  eprint = {1804.07117},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@article{HPOEPP16,
  title = {{{RNA}} Editing Generates Cellular Subsets with Diverse Sequence within Populations},
  author = {Harjanto, Dewi and Papamarkou, Theodore and Oates, Chris J and {Rayon-Estrada}, Violeta and Papavasiliou, F Nina and Papavasiliou, Anastasia},
  year = {2016},
  journal = {Nature Communications},
  volume = {7},
  number = {12145},
  keywords = {nosource}
}

@article{hronAnalysingPairwiseLogratios2021,
  title = {Analysing {{Pairwise Logratios Revisited}}},
  author = {Hron, Karel and Coenders, Germ{\'a} and Filzmoser, Peter and {Palarea-Albaladejo}, Javier and Fam{\v e}ra, Martin and Matys Grygar, Tom{\'a}{\v s}},
  year = {2021},
  month = oct,
  journal = {Mathematical Geosciences},
  volume = {53},
  number = {7},
  pages = {1643--1666},
  issn = {1874-8953},
  doi = {10.1007/s11004-021-09938-w},
  urldate = {2024-07-16},
  abstract = {Even though the logratio methodology provides a range of both generic, mostly exploratory, and purpose-built coordinate representations of compositional data, simple pairwise logratios are preferred by many for multivariate analysis in the geochemical practice, principally because of their simpler interpretation. However, the logratio coordinate systems that incorporate them are predominantly oblique, resulting in both conceptual and practical problems. We propose a new approach, called backwards pivot coordinates, where each pairwise logratio is linked to one orthogonal coordinate system, and these systems are then used together to produce a concise output. In this work, principal component analysis and regression with compositional explanatory variables are used as primary methods to demonstrate the methodological and interpretative advantages of the proposal. In the applied part of this study, sediment compositions from the Jizera River, Czech Republic, were analysed using these techniques through backwards pivot coordinates. This allowed us to discuss grain size control of the element composition of sediments and clearly distinguish anthropogenically contaminated and uncontaminated strata in sediment depth profiles.},
  langid = {english},
  keywords = {Additive logratio coordinates,Linear regression with compositional covariates,Pairwise logratios,Pivot coordinates,Principal component analysis},
  file = {/home/gkonkamking/pCloudDrive/papers/Hron et al_2021_Analysing Pairwise Logratios Revisited.pdf}
}

@article{hseeLessBetterWhen1998,
  title = {Less Is Better: When Low-Value Options Are Valued More Highly than High-Value Options},
  shorttitle = {Less Is Better},
  author = {Hsee, Christopher K.},
  year = {1998},
  journal = {Journal of Behavioral Decision Making},
  volume = {11},
  number = {2},
  pages = {107--121},
  issn = {1099-0771},
  doi = {10.1002/(SICI)1099-0771(199806)11:2<107::AID-BDM292>3.0.CO;2-Y},
  urldate = {2020-07-07},
  abstract = {This research demonstrates a less-is-better effect in three contexts: (1) a person giving a \$45 scarf as a gift was perceived to be more generous than one giving a \$55 coat; (2) an overfilled ice cream serving with 7 oz of ice cream was valued more than an underfilled serving with 8 oz of ice cream; (3) a dinnerware set with 24 intact pieces was judged more favourably than one with 31 intact pieces (including the same 24) plus a few broken ones. This less-is-better effect occurred only when the options were evaluated separately, and reversed itself when the options were juxtaposed. These results are explained in terms of the evaluability hypothesis, which states that separate evaluations of objects are often influenced by attributes which are easy to evaluate rather than by those which are important. {\copyright} 1998 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 1998 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {dominance violation,evaluability,joint and separate evaluation,nosource,preference reversal,willingness to pay}
}

@inproceedings{HT14,
  title = {Julia and the Numerical Homogenization of \{\vphantom\}{{PDE}}\vphantom\{\}s},
  booktitle = {{{HPTCDL}}'14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages},
  author = {Heitzinger, Clemens and Tulzer, Gerhard},
  year = {2014},
  pages = {36--40},
  publisher = {ACM},
  address = {New York},
  doi = {10.1109/HPTCDL.2014.8},
  abstract = {We discuss the advantages of using Julia for solving multiscale problems involving partial differential equations (PDEs). Multiscale problems are problems where the coefficients of a PDE oscillate rapidly on a microscopic length scale, but solutions are sought on a much larger, macroscopic domain. Solving multiscale problems requires both a theoretic result, i.e., a homogenization result yielding effective coefficients, as well as numerical solutions of the PDE at the microscopic and the macroscopic length scales. Numerical homogenization of PDEs with stochastic coefficients is especially computationally expensive. Under certain assumptions, effective coefficients can be found, but their calculation involves subtle numerical problems. The computational cost is huge due to the generally large number of stochastic dimensions. Multiscale problems arise in many applications, e.g., in uncertainty quantification, in the rational design of nanoscale sensors, and in the rational design of materials. Our code for the numerical stochastic homogenization of elliptic problems is implemented in Julia. Since multiscale problems pose new numerical problems, it is in any case necessary to develop new numerical codes. Julia is a dynamic language inspired by the Lisp family of languages, it is open-source, and it provides native-code compilation, access to highly optimized linear-algebra routines, support for parallel computing, and a powerful macro system. We describe our experience in using Julia and discuss the advantages of Julia's features in this problem domain.},
  keywords = {nosource}
}

@article{https://doi.org/10.1111/rssb.12404,
  title = {Gibbs Flow for Approximate Transport with Applications to {{Bayesian}} Computation},
  author = {Heng, Jeremy and Doucet, Arnaud and Pokern, Yvo},
  year = {2021},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {83},
  number = {1},
  eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12404},
  pages = {156--187},
  doi = {10.1111/rssb.12404},
  file = {/home/gkonkamking/Zotero/storage/852MKQCG/Heng et al. - 2021 - Gibbs flow for approximate transport with applicat.pdf}
}

@article{Huang:2004p7016,
  title = {Convergence Rates for Posterior Distributions and Adaptive Estimation},
  author = {Huang, Tzee-Ming},
  year = {2004},
  journal = {The Annals of Statistics},
  volume = {32},
  number = {4},
  pages = {1556--1593},
  issn = {0090-5364},
  doi = {10.1214/009053604000000490},
  abstract = {FOR DISTRIBUTIONS AND ADAPTIVE ESTIMATION on adaptive estimation in the study of . Belitser and},
  arxiv = {0410087v1 [arXiv:math]},
  arxivid = {arXiv:math/0410087v1},
  keywords = {adaptive estimation,and phrases,article published by the,bayesian,convergence rate,density estima-,in the annals of,institute of mathematical statistics,nonparametric regression,nosource,reprint of the original,sieves,statistics,this is an electronic,tion}
}

@article{Huang2014,
  title = {Phylogenetic Gaussian Process Model for the Inference of Functionally Important Regions in Protein Tertiary Structures},
  author = {Huang, Yi Fei and Golding, G. Brian},
  year = {2014},
  journal = {PLoS Computational Biology},
  volume = {10},
  number = {1},
  issn = {1553734X},
  doi = {10.1371/journal.pcbi.1003429},
  abstract = {A critical question in biology is the identification of functionally important amino acid sites in proteins. Because functionally important sites are under stronger purifying selection, site-specific substitution rates tend to be lower than usual at these sites. A large number of phylogenetic models have been developed to estimate site-specific substitution rates in proteins and the extraordinarily low substitution rates have been used as evidence of function. Most of the existing tools, e.g. Rate4Site, assume that site-specific substitution rates are independent across sites. However, site-specific substitution rates may be strongly correlated in the protein tertiary structure, since functionally important sites tend to be clustered together to form functional patches. We have developed a new model, GP4Rate, which incorporates the Gaussian process model with the standard phylogenetic model to identify slowly evolved regions in protein tertiary structures. GP4Rate uses the Gaussian process to define a nonparametric prior distribution of site-specific substitution rates, which naturally captures the spatial correlation of substitution rates. Simulations suggest that GP4Rate can potentially estimate site-specific substitution rates with a much higher accuracy than Rate4Site and tends to report slowly evolved regions rather than individual sites. In addition, GP4Rate can estimate the strength of the spatial correlation of substitution rates from the data. By applying GP4Rate to a set of mammalian B7-1 genes, we found a highly conserved region which coincides with experimental evidence. GP4Rate may be a useful tool for the in silico prediction of functionally important regions in the proteins with known structures.},
  isbn = {10.1371/journal.pcbi.1003429},
  pmid = {24453956},
  keywords = {nosource},
  file = {/home/gkonkamking/pCloudDrive/papers/Huang_Golding_2014_Phylogenetic gaussian process model for the inference of functionally important.pdf}
}

@article{huangLivingNorthNot2014,
  title = {Living in the North Is Not Necessarily Favorable: {{Different}} Metaphoric Associations between Cardinal Direction and Valence in {{Hong Kong}} and in the {{United States}}},
  shorttitle = {Living in the North Is Not Necessarily Favorable},
  author = {Huang, Yanli and Tse, Chi-Shing and Cho, Kit W.},
  year = {2014},
  journal = {European Journal of Social Psychology},
  volume = {44},
  number = {4},
  pages = {360--369},
  issn = {1099-0992},
  doi = {10.1002/ejsp.2013},
  urldate = {2020-06-09},
  abstract = {AbstractThe Conceptual Metaphor Theory (e.g., Lakoff \& Johnson, ) suggests that people represent abstract concepts in terms of concrete concepts via metaphoric association. Participants in the United States (US) showed that cardinal direction (north/south) is metaphorically associated with valence (positive/negative), as reflected by their estimate for where a person with high or low socioeconomic status (SES) lives in a fictional city or their own living preference (Meier, Moller, Chen, \& Riemer-Peltz, ). The present study tested whether the cardinal direction--valence metaphoric association could be moderated by cultural differences. Although US participants believed that high-SES and low-SES individuals were more likely to live in the northern and southern part of the city, respectively, the reverse was so for Hong Kong (HK) participants (Study 1). When asked where they themselves would like to live, HK participants preferred to live in a southern area, whereas US participants showed no preference (Studies 2 and 3). These findings demonstrate cultural differences in metaphoric associations between cardinal direction and valence for HK and US participants. Copyright {\copyright} 2014 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2014 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {nosource}
}

@article{huangSimpleMarginallyNoninformative2013,
  title = {Simple {{Marginally Noninformative Prior Distributions}} for {{Covariance Matrices}}},
  author = {Huang, Alan and Wand, M. P.},
  year = {2013},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {8},
  number = {2},
  pages = {439--452},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/13-BA815},
  urldate = {2024-11-07},
  abstract = {A family of prior distributions for covariance matrices is studied. Members of the family possess the attractive property of all standard deviation and correlation parameters being marginally noninformative for particular hyperparameter choices. Moreover, the family is quite simple and, for approximate Bayesian inference techniques such as Markov chain Monte Carlo and mean field variational Bayes, has tractability on par with the Inverse-Wishart conjugate family of prior distributions. A simulation study shows that the new prior distributions can lead to more accurate sparse covariance matrix estimation.},
  keywords = {Bayesian inference,Gibbs sampling,Markov chain Monte Carlo,Mean field variational Bayes},
  file = {/home/gkonkamking/pCloudDrive/papers/Huang and Wand - 2013 - Simple Marginally Noninformative Prior Distributions for Covariance Matrices.pdf}
}

@article{hubbard2003p,
  ids = {Hubbard2003},
  title = {P Values Are Not Error Probabilities},
  author = {Hubbard, Raymond and Bayarri, {\relax MJ}},
  year = {2003},
  journal = {Institute of Statistics and Decision Sciences, Working Paper},
  number = {03-26},
  pages = {27708--0251},
  keywords = {nosource}
}

@article{huelsenbeckInferencePopulationStructure2007,
  title = {Inference of {{Population Structure Under}} a {{Dirichlet Process Model}}},
  author = {Huelsenbeck, John P. and Andolfatto, Peter},
  year = {2007},
  month = apr,
  journal = {Genetics},
  volume = {175},
  number = {4},
  pages = {1787--1802},
  publisher = {Genetics},
  issn = {0016-6731, 1943-2631},
  doi = {10.1534/genetics.106.061317},
  urldate = {2020-10-05},
  abstract = {Inferring population structure from genetic data sampled from some number of individuals is a formidable statistical problem. One widely used approach considers the number of populations to be fixed and calculates the posterior probability of assigning individuals to each population. More recently, the assignment of individuals to populations and the number of populations have both been considered random variables that follow a Dirichlet process prior. We examined the statistical behavior of assignment of individuals to populations under a Dirichlet process prior. First, we examined a best-case scenario, in which all of the assumptions of the Dirichlet process prior were satisfied, by generating data under a Dirichlet process prior. Second, we examined the performance of the method when the genetic data were generated under a population genetics model with symmetric migration between populations. We examined the accuracy of population assignment using a distance on partitions. The method can be quite accurate with a moderate number of loci. As expected, inferences on the number of populations are more accurate when {\texttheta} = 4Neu is large and when the migration rate (4Nem) is low. We also examined the sensitivity of inferences of population structure to choice of the parameter of the Dirichlet process model. Although inferences could be sensitive to the choice of the prior on the number of populations, this sensitivity occurred when the number of loci sampled was small; inferences are more robust to the prior on the number of populations when the number of sampled loci is large. Finally, we discuss several methods for summarizing the results of a Bayesian Markov chain Monte Carlo (MCMC) analysis of population structure. We develop the notion of the mean population partition, which is the partition of individuals to populations that minimizes the squared partition distance to the partitions sampled by the MCMC algorithm.},
  chapter = {Investigations},
  copyright = {Copyright {\copyright} 2007 by the Genetics Society of America},
  langid = {english},
  pmid = {17237522},
  keywords = {nosource}
}

@article{Huijbregts2011,
  title = {Do We Need a Paradigm Shift in Life Cycle Impact Assessment ?},
  author = {Huijbregts, Mark a J and Hellweg, Stefanie and Hertwich, Edgar},
  year = {2011},
  journal = {Environmental Science \& Technology},
  volume = {45},
  pages = {3833--3834},
  issn = {1520-5851},
  doi = {201110.1002/ieam.141.(5)},
  isbn = {1002100992},
  pmid = {21449583},
  keywords = {nosource}
}

@misc{HumanitarianDataExchange,
  title = {Humanitarian {{Data Exchange}}},
  urldate = {2023-10-25},
  howpublished = {https://data.humdata.org/},
  langid = {australian}
}

@article{Huson,
  title = {Bayesian Fitting of a Logistic Dose-Response Curve with Numerically Derived Priors},
  author = {Huson, L. W. and Kinnersley, N.},
  year = {2009},
  journal = {Pharmaceutical Statistics},
  volume = {8},
  number = {4},
  eprint = {18819118},
  eprinttype = {pubmed},
  pages = {279--286},
  issn = {15391604},
  doi = {10.1002/pst.348},
  abstract = {In this report we describe the Bayesian analysis of a logistic dose-response curve in a Phase I study, and we present two simple and intuitive numerical approaches to construction of prior probability distributions for the model parameters. We combine these priors with the expert prior opinion and compare the results of the analyses with those obtained from the use of alternative prior formulations.},
  pmid = {18819118},
  keywords = {Bayesian,Bootstrap,nosource,Simulation}
}

@article{Hutchinson2006,
  title = {Acute and Chronic Effects of Carrier Solvents in Aquatic Organisms: {{A}} Critical Review},
  author = {Hutchinson, T. H. and Shillabeer, N. and Winter, M. J. and Pickford, D. B.},
  year = {2006},
  journal = {Aquatic Toxicology},
  volume = {76},
  number = {1},
  pages = {69--92},
  issn = {0166445X},
  doi = {10.1016/j.aquatox.2005.09.008},
  abstract = {Recognising the scientific and regulatory need for testing relatively hydrophobic or 'difficult substances', the OECD currently recommends that selected organic solvents may be used in aquatic toxicity testing in order to help achieve more effective dispersion of the toxicant. The OECD recommends a maximum solvent concentration of 100 ??l l-1 (with specific gravity equivalents to 100 ??l l-1 in parentheses) for acetone (79 mg l-1), dimethylformamide (95 mg l-1), dimethylsulfoxide (1.10 mg l-1), ethanol (78.9 mg l-1), methanol (79.2 mg l-1) and triethylene glycol (1.12 mg l-1). While this recommendation is supported by historical data, we have recently observed evidence that some solvents may affect the reproduction of certain fish species, and also impact biomarkers of endocrine disruption. This review presents available data on the effects of solvents in aquatic organisms, supplemented by relevant information from mammalian studies (e.g. effects on liver enzyme induction potentially altering the metabolism of sex hormones). In conclusion, it is recommended that maximum effort should be given to avoiding the use of carrier solvents wherever possible, for example through the use of saturation columns or other physical methods (e.g. stirring or ultrasonification). Where solvent use is necessary, however, it is recommended that in reproduction studies with aquatic organisms, the maximum solvent concentration should not exceed 20 ??l l-1 of dilution water. ?? 2005 Elsevier B.V. All rights reserved.},
  isbn = {0166-445X},
  pmid = {16290221},
  keywords = {Acetone,Carrier solvents,Dimethylformamide,Dimethylsulfoxide,Ecotoxicological experiments,Ethanol,Methanol,nosource,Triethylene glycol}
}

@book{iacus2009simulation,
  title = {Simulation and Inference for Stochastic Differential Equations: With {{R}} Examples},
  author = {Iacus, Stefano M},
  year = {2009},
  publisher = {Springer Science \& Business Media},
  keywords = {nosource}
}

@article{inbar2009disgust,
  title = {Disgust Sensitivity Predicts Intuitive Disapproval of Gays.},
  author = {Inbar, Yoel and Pizarro, David A and Knobe, Joshua and Bloom, Paul},
  year = {2009},
  journal = {Emotion},
  volume = {9},
  number = {3},
  pages = {435},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{Ioannidis2005,
  title = {Why Most Published Research Findings Are False},
  author = {Ioannidis, John P A},
  year = {2005},
  month = aug,
  journal = {PLoS Medicine},
  volume = {2},
  number = {8},
  pages = {0696--0701},
  issn = {15491277},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary{\textbackslash}n There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  arxiv = {0208024 [gr-qc]},
  arxivid = {gr-qc/0208024},
  isbn = {3540239081},
  pmid = {16060722},
  keywords = {Bias (Epidemiology),Data Interpretation,Likelihood Functions,Meta-Analysis as Topic,nosource,Odds Ratio,Publishing,Reproducibility of Results,Research Design,Sample Size,Statistical}
}

@article{ishwaran2001gibbs,
  title = {Gibbs Sampling Methods for Stick-Breaking Priors},
  author = {Ishwaran, Hemant and James, Lancelot F},
  year = {2001},
  journal = {Journal of the American Statistical Association},
  volume = {96},
  number = {453},
  pages = {161--173},
  publisher = {ASA},
  issn = {0162-1459},
  doi = {10.1198/016214501750332758},
  abstract = {A rich and flexible class of random probability measures, which we call stick-breaking priors, can be constructed using a sequence of independent beta random variables. Examples of random measures that have this characterization include the Dirichlet process, its two-parameter extension, the two-parameter Poisson--Dirichlet process, finite dimensional Dirichlet priors, and beta two-parameter processes. The rich nature of stick-breaking priors offers Bayesians a useful class of priors for nonparametric problems, while the similar construction used in each prior can be exploited to develop a general computational procedure for fitting them. In this article we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick-breaking priors. The first type of Gibbs sampler, referred to as a P{\'o}lya urn Gibbs sampler, is a generalized version of a widely used Gibbs sampling method currently employed for Dirichlet process computing. This method applies t...},
  keywords = {blocked gibbs sampler,Blocked Gibbs sampler,dirichlet process,Dirichlet process,generalized dirichlet distribution,Generalized Dirichlet distribution,nosource,p{\'o}lya urn gibbs,P{\'o}lya urn Gibbs sampler,pitman,Pitman--Yor process,prediction rule,Prediction rule,random probability measure,Random probability measure,random weights,Random weights,sampler,stable law,Stable law,yor process},
  file = {/home/gkonkamking/pCloudDrive/papers/Ishwaran_James_2001_Gibbs sampling methods for stick-breaking priors.pdf}
}

@article{Ishwaran2003,
  title = {Generalized Weighted Chinese Restaurant Processes for Species Sampling Mixture Models},
  author = {Ishwaran, Hemant and James, Lancelot F},
  year = {2003},
  journal = {Statistica Sinica},
  volume = {13},
  pages = {1211--1235},
  issn = {10170405},
  abstract = {The class of species sampling mixture models is introduced as an exten-sion of semiparametric models based on the Dirichlet process to models based on the general class of species sampling priors, or equivalently the class of all exchangeable urn distributions. Using Fubini calculus in conjunction with Pitman (1995, 1996), we derive characterizations of the posterior distribution in terms of a posterior par-tition distribution that extend the results of Lo (1984) for the Dirichlet process. These results provide a better understanding of models and have both theoretical and practical applications. To facilitate the use of our models we generalize the work in Brunner, Chan, James and Lo (2001) by extending their weighted Chinese restaurant (WCR) Monte Carlo procedure, an i.i.d. sequential importance sampling (SIS) procedure for approximating posterior mean functionals based on the Dirich-let process, to the case of approximation of mean functionals and additionally their posterior laws in species sampling mixture models. We also discuss collapsed Gibbs sampling, P{\'o}lya urn Gibbs sampling and a P{\'o}lya urn SIS scheme. Our framework allows for numerous applications, including multiplicative counting process models subject to weighted gamma processes, as well as nonparametric and semiparamet-ric hierarchical models based on the Dirichlet process, its two-parameter extension, the Pitman-Yor process and finite dimensional Dirichlet priors.},
  keywords = {and phrases,Dirichlet process,exchangeable partition,finite dimen-sional Dirichlet prior,nosource,prediction rule,ran-dom probability measure,species sampling sequence,two-parameter Poisson-Dirichlet process}
}

@article{ishwaranConsistencySpikeSlab2011,
  title = {Consistency of Spike and Slab Regression},
  author = {Ishwaran, Hemant and Sunil Rao, J.},
  year = {2011},
  month = dec,
  journal = {Statistics \& Probability Letters},
  volume = {81},
  number = {12},
  pages = {1920--1928},
  issn = {01677152},
  doi = {10.1016/j.spl.2011.08.005},
  urldate = {2021-09-21},
  abstract = {Spike and slab models are a popular and attractive variable selection approach in regression settings. Applications for these models have blossomed over the last decade and they are increasingly being used in challenging problems. At the same time, theory for spike and slab models has not kept pace with the applications. There are many gaps in what we know about their theoretical properties. An important property known to hold in these models is selective shrinkage: a unique property whereby the posterior mean is shrunk toward zero for non-informative variables only. This property has been shown to hold under orthogonality for continuous priors under the modified class of rescaled spike and slab models. In this paper, we extend this result to the general case and prove an oracle property for the posterior mean under a discrete two-component prior. An immediate consequence is that a strong selective shrinkage property holds. Interestingly, the conditions needed for our result to hold in the non-orthogonal setting are more stringent than in the orthogonal case and amount to a type of enforced sparsity condition that must be met by the prior.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/MF5TNEXP/Ishwaran and Sunil Rao - 2011 - Consistency of spike and slab regression.pdf}
}

@article{ishwaranSpikeSlabVariable2005,
  title = {Spike and Slab Variable Selection: {{Frequentist}} and {{Bayesian}} Strategies},
  shorttitle = {Spike and Slab Variable Selection},
  author = {Ishwaran, Hemant and Rao, J. Sunil},
  year = {2005},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {33},
  number = {2},
  issn = {0090-5364},
  doi = {10.1214/009053604000001147},
  urldate = {2021-09-21},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/WQRCZVIK/Ishwaran and Rao - 2005 - Spike and slab variable selection Frequentist and.pdf}
}

@article{Jager2011,
  title = {General Unified Threshold Model of Survival - {{A}} Toxicokinetic-Toxicodynamic Framework for Ecotoxicology},
  author = {Jager, Tjalling and Albert, Carlo and Preuss, Thomas G. and Ashauer, Roman},
  year = {2011},
  month = apr,
  journal = {Environmental Science and Technology},
  volume = {45},
  number = {7},
  eprint = {21366215},
  eprinttype = {pubmed},
  pages = {2529--2540},
  issn = {0013936X},
  doi = {10.1021/es103092a},
  abstract = {Toxicokinetic-toxicodynamic models (TKTD models) simulate the time-course of processes leading to toxic effects on organisms. Even for an apparently simple endpoint as survival, a large number of very different TKTD approaches exist. These differ in their underlying hypotheses and assumptions, although often the assumptions are not explicitly stated. Thus, our first objective was to illuminate the underlying assumptions (individual tolerance or stochastic death, speed of toxicodynamic damage recovery, threshold distribution) of various existing modeling approaches for survival and show how they relate to each other (e.g., critical body residue, critical target occupation, damage assessment, DEBtox survival, threshold damage). Our second objective was to develop a general unified threshold model for survival (GUTS), from which a large range of existing models can be derived as special cases. Specific assumptions to arrive at these special cases are made and explained. Finally, we illustrate how special cases of GUTS can be fitted to survival data. We envision that GUTS will help increase the application of TKTD models in ecotoxicological research as well as environmental risk assessment of chemicals. It unifies a wide range of previously unrelated approaches, clarifies their underlying assumptions, and facilitates further improvement in the modeling of survival under chemical stress.},
  isbn = {0013-936X},
  pmid = {21366215},
  keywords = {Amphipoda,Amphipoda: drug effects,Amphipoda: metabolism,Animals,Biological,Chemical,Dose-Response Relationship,Drug,Ecotoxicology,Environmental Pollutants,Environmental Pollutants: metabolism,Environmental Pollutants: toxicity,Models,Pharmacokinetics,Risk Assessment,Survival Analysis,Toxicity Tests,Toxicity Tests: methods,Toxicity Tests: standards},
  file = {/home/gkonkamking/Zotero/storage/ID3YG8R9/Jager et al. - 2011 - General unified threshold model of survival - A to.pdf}
}

@article{jager2011some,
  title = {Some Good Reasons to Ban {{ECx}} and Related Concepts in Ecotoxicology},
  author = {Jager, Tjalling},
  year = {2011},
  journal = {Environmental Science and Technology},
  volume = {45},
  number = {19},
  pages = {8180--8181},
  publisher = {ACS Publications},
  issn = {0013936X},
  doi = {10.1021/es2030559},
  isbn = {0013-936X},
  pmid = {21919475},
  keywords = {nosource}
}

@article{Jager2013,
  title = {Hormesis on Life-History Traits: {{Is}} There Such Thing as a Free Lunch?},
  author = {Jager, Tjalling and Barsi, Alpar and Ducrot, Virginie},
  year = {2013},
  journal = {Ecotoxicology},
  volume = {22},
  number = {2},
  pages = {263--270},
  issn = {09639292},
  doi = {10.1007/s10646-012-1022-0},
  abstract = {The term "hormesis" is used to describe dose-response relationships where the response is reversed between low and high doses of a stressor (generally, stimulation at low doses and inhibition at high ones). A mechanistic explanation is needed to interpret the relevance of such responses, but there does not appear to be a single universal mechanism underlying hormesis. When the endpoint is a life-history trait such as growth or reproduction, a stimulation of the response comes with costs in terms of resources. Organisms have to obey the conservation laws for mass and energy; there is no such thing as a free lunch. Based on the principles of Dynamic Energy Budget theory, we introduce three categories of explanations for hormesis that obey the conservation laws: acquisition (i.e., increasing the input of energy into the individual), allocation (i.e., rearranging the energy flows over various traits) and medication (e.g., the stressor is an essential element or acts as a cure for a disease or infection). In this discussion paper, we illustrate these explanations with cases where they might apply, and elaborate on the potential consequences for field populations.},
  isbn = {0963-9292},
  pmid = {23179410},
  keywords = {Energy budget,Hormesis,Life-history traits,Mechanisms,nosource,Trade off}
}

@article{jager2018modelling,
  title = {Modelling Survival under Chemical Stress. {{A}} Comprehensive Guide to the {{GUTS}} Framework},
  author = {Jager, Tjalling and Ashauer, Roman},
  year = {2018},
  journal = {Oakland, CA: Leanpub},
  keywords = {nosource}
}

@article{Jagoe1997,
  title = {Bootstrap Estimation of Community {{NOEC}} Values},
  author = {Jagoe, Rosemary H. and Newman, Michael C.},
  year = {1997},
  journal = {Ecotoxicology},
  volume = {6},
  number = {5},
  pages = {293--306},
  publisher = {Kluwer Academic Publishers},
  issn = {0963-9292},
  doi = {10.1023/A:1018639113818},
  isbn = {0963-9292},
  langid = {english},
  keywords = {bootstrapping,Bootstrapping,duplicate-citation-key,extrapolation,Extrapolation,NOEC,nosource,prediction,Prediction,toxicity,Toxicity}
}

@article{jainSplittingMergingComponents2007,
  title = {Splitting and Merging Components of a Nonconjugate {{Dirichlet}} Process Mixture Model},
  author = {Jain, Sonia and Neal, Radford M.},
  year = {2007},
  month = sep,
  journal = {Bayesian Analysis},
  volume = {2},
  number = {3},
  pages = {445--472},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/07-BA219},
  urldate = {2022-03-17},
  abstract = {The inferential problem of associating data to mixture components is difficult when components are nearby or overlapping. We introduce a new split-merge Markov chain Monte Carlo technique that efficiently classifies observations by splitting and merging mixture components of a nonconjugate Dirichlet process mixture model. Our method, which is a Metropolis-Hastings procedure with split-merge proposals, samples clusters of observations simultaneously rather than incrementally assigning observations to mixture components. Split-merge moves are produced by exploiting properties of a restricted Gibbs sampling scan. A simulation study compares the new split-merge technique to a nonconjugate version of Gibbs sampling and an incremental Metropolis-Hastings technique. The results demonstrate the improved performance of the new sampler.},
  keywords = {Bayesian model,Markov chain Monte Carlo,nonconjugate prior,split-merge moves},
  file = {/home/gkonkamking/Zotero/storage/BZ3Z7PNS/Jain and Neal - 2007 - Splitting and merging components of a nonconjugate.pdf}
}

@article{Jaloustre2011,
  title = {Bayesian Modeling of {{Clostridium}} Perfringens Growth in Beef-in-Sauce Products},
  author = {Jaloustre, S. and Cornu, Marie and Morelli, E. and No??l, V. and {Delignette-Muller}, Marie Laure},
  year = {2011},
  month = apr,
  journal = {Food Microbiology},
  volume = {28},
  number = {2},
  eprint = {21315989},
  eprinttype = {pubmed},
  pages = {311--320},
  issn = {07400020},
  doi = {10.1016/j.fm.2010.04.002},
  abstract = {Models on Clostridium perfringens growth which have been published to date have all been deterministic. A probabilistic model describing growth under non-isothermal conditions was thus proposed for predicting C. perfringens growth in beef-in-sauce products cooked and distributed in a French hospital. Model parameters were estimated from different types of data from various studies. A Bayesian approach was proposed to model the overall uncertainty regarding parameters and potential variability on the 'work to be done' (h0) during the germination, outgrowth and lag phase. Three models which differed according to their description of this parameter h0 were tested. The model with inter-curve variability on h0 was found to be the best one, on the basis of goodness-of-fit assessment and validation with literature data on results obtained under non-isothermal conditions. This model was used in two-dimensional Monte Carlo simulations to predict C. perfringens growth throughout the preparation of beef-in-sauce products, using temperature profiles recorded in a hospital kitchen. The median predicted growth was 7.8.10-2 log10 cfu.g-1 (95\% credibility interval [2.4.10-2,0.8]) despite the fact that for more than 50\% of the registered temperature profiles cooling steps were longer than those required by French regulations. ?? 2010 Elsevier Ltd.},
  isbn = {1095-9998},
  pmid = {21315989},
  keywords = {Bayes Theorem,Biological,Clostridium perfringens,Clostridium perfringens: growth \& development,Colony Count,Consumer Product Safety,Cooking,Cooking: methods,Food Handling,Food Handling: methods,Food Preservation,Food Preservation: methods,Humans,Kinetics,Meat Products,Meat Products: microbiology,Microbial,Models,Monte Carlo Method,nosource,Predictive Value of Tests,Temperature}
}

@article{Jaloustre2012,
  title = {Modeling of {{Clostridium}} Perfringens Vegetative Cell Inactivation in Beef-in-Sauce Products: {{A}} Meta-Analysis Using Mixed Linear Models},
  author = {Jaloustre, S. and Guillier, L. and Morelli, E. and No??l, V. and {Delignette-Muller}, Marie Laure},
  year = {2012},
  month = mar,
  journal = {International Journal of Food Microbiology},
  volume = {154},
  number = {1-2},
  eprint = {22236760},
  eprinttype = {pubmed},
  pages = {44--51},
  publisher = {Elsevier B.V.},
  issn = {01681605},
  doi = {10.1016/j.ijfoodmicro.2011.12.013},
  abstract = {The aim of the present study was to predict Clostridium perfringens vegetative cell inactivation during the final reheating step of two beef-in-sauce products prepared and distributed in a French hospital for exposure in risk assessment. In order to account for variability according to experts and international organization recommendations, published data were used to estimate the thermal inactivation parameters of a probabilistic model. Mixed effects models were proposed to describe variability on Dref the decimal reduction time at temperature Tref. Many models differing by their description of variability on Dref were tested. Based on goodness-of-fit and parsimony of the model, the one including three random effects was chosen. That model describes random effects of vegetative cell culture conditions, strains and other uncontrolled experimental factors. In order to check the ability of the model to predict inactivation under dynamic thermal conditions, model validation was carried out on published non isothermal data. This model was then used to predict C. perfringens vegetative cell inactivation using temperature profiles inside beef-in-sauce products registered in a French hospital and to explore control measures easier to apply than French regulations. ?? 2011 Elsevier B.V.},
  isbn = {1879-3460 (Electronic){\textbackslash}r0168-1605 (Linking)},
  pmid = {22236760},
  keywords = {Clostridium perfringens,Meta-analysis,Mixed effects model,nosource,Thermal inactivation}
}

@article{Jaloustre2013,
  title = {Efficiency of a Reheating Step to Inactivate {{Clostridium}} Perfringens Vegetative Cells: {{How}} to Measure It?},
  author = {Jaloustre, S. and Guillier, L. and Poumeyrol, G. and Morelli, E. and {Delignette-Muller}, Marie Laure},
  year = {2013},
  month = feb,
  journal = {Food Control},
  volume = {29},
  number = {2},
  pages = {422--428},
  publisher = {Elsevier Ltd},
  issn = {09567135},
  doi = {10.1016/j.foodcont.2012.07.003},
  abstract = {Clostridium perfringens is responsible for foodborne diseases often associated with processed meats in institutions. This study investigated the behavior of C. perfringens in beef-in-sauce products in a French hospital. In the studied process, the final reheating step makes the inactivation of C. perfringens vegetative cells possible. The aim of this study was thus to combine microbial, thermal and dose response modeling to propose, for this reheating step, three new control measures which are as efficient as current French regulation while also being easier to apply. These measures were based on thermal process duration (DA53), final temperature in food (FTF) and sum of temperatures-minutes above 53 ??C (ST53) required to achieve food safety. A criterion was defined to describe the ability of the proposed control measures to lower the risk of foodborne illness related to the consumption of products with high populations of C. perfringens vegetative cells before reheating. To estimate acceptable values of DA53, FTF and ST53, variability and uncertainty were taken into account separately. A two dimensional Monte Carlo simulation enabled the definition of 14 measure thresholds which made sure with a 97.5\% confidence that less than 20\%, 10\% or 5\% of exposed consumers in a specified population would experience diarrhea. The three proposed control measures, single and combined, were then compared by estimating the duration required to reach control measure thresholds. This required duration differs significantly from a control measure to the other, with the lowest duration for the combination of FTF and DA53. ?? 2012 Elsevier Ltd.},
  keywords = {Clostridium perfringens,Control measures,nosource,Thermal inactivation}
}

@inproceedings{jamar2006strategies,
  title = {Strategies to Reduce Copper Use in Organic Apple Production},
  booktitle = {I International Symposium on Organic Apple and Pear 737},
  author = {Jamar, L and Lateur, M},
  year = {2006},
  pages = {113--120}
}

@inproceedings{jamar2006strategies,
  title = {Strategies to Reduce Copper Use in Organic Apple Production},
  booktitle = {I International Symposium on Organic Apple and Pear 737},
  author = {Jamar, L and Lateur, M},
  year = {2006},
  pages = {113--120}
}

@article{james2006conjugacy,
  title = {Conjugacy as a Distinctive Feature of the {{Dirichlet}} Process},
  author = {James, Lancelot F. and Lijoi, Antonio and Pr{\"u}nster, Igor},
  year = {2006},
  journal = {Scandinavian Journal of Statistics},
  volume = {33},
  number = {1},
  pages = {105--120},
  publisher = {Wiley Online Library},
  issn = {03036898},
  doi = {10.1111/j.1467-9469.2005.00486.x},
  abstract = {TeX output 2006.01.11:1424},
  keywords = {Bayesian non-parametrics,Conjugacy,Dirichlet process,Increasing L??vy process,Normalized random measure with independent increme,nosource,predictive distribution}
}

@article{james2009posterior,
  title = {Posterior Analysis for Normalized Random Measures with Independent Increments},
  author = {James, Lancelot F. and Lijoi, Antonio and Pr{\"u}nster, Igor},
  year = {2009},
  journal = {Scandinavian Journal of Statistics},
  volume = {36},
  number = {1},
  pages = {76--97},
  publisher = {Wiley Online Library},
  issn = {03036898},
  doi = {10.1111/j.1467-9469.2008.00609.x},
  abstract = {TeX output 2009.02.03:1002},
  keywords = {Bayesian Nonparametrics,Dirichlet process,Normalized random measure,Poisson random measure,Posterior distribution,Predictive distribution},
  file = {/home/gkonkamking/Zotero/storage/YUN9C9EQ/James et al. - 2009 - Posterior analysis for normalized random measures .pdf}
}

@article{James2010,
  title = {On the Posterior Distribution of Classes of Random Means},
  author = {James, Lancelot F and Lijoi, Antonio and Pr{\"u}nster, Igor},
  year = {2010},
  journal = {Bernoulli},
  volume = {16},
  number = {1},
  eprint = {1002.4276},
  pages = {155--180},
  issn = {1350-7265},
  doi = {10.3150/09-BEJ200},
  abstract = {The study of properties of mean functionals of random probability measures is an important area of research in the theory of Bayesian nonparametric statistics. Many results are now known for random Dirichlet means, but little is known, especially in terms of posterior distributions, for classes of priors beyond the Dirichlet process. In this paper, we consider normalized random measures with independent increments (NRMI's) and mixtures of NRMI. In both cases, we are able to provide exact expressions for the posterior distribution of their means. These general results are then specialized, leading to distributional results for means of two important particular cases of NRMI's and also of the two-parameter Poisson--Dirichlet process.{\textbackslash}n{\textbackslash}nPublished in: Bernoulli 2010, Vol. 16, No. 1, 155-180},
  archiveprefix = {arXiv},
  arxivid = {1002.4276},
  keywords = {bayesian nonparametrics,completely random measures,dirichlet process,means of random probability,measures,normalized random measures,nosource,poisson,posterior distribution,species}
}

@article{jang2010posterior,
  title = {Posterior Consistency of Species Sampling Priors},
  author = {Jang, G H and Lee, J and Lee, S},
  year = {2010},
  journal = {Statistica Sinica},
  volume = {20},
  number = {2},
  pages = {581--593},
  issn = {10170405},
  keywords = {nosource}
}

@article{janiszewskiPrecisionAnchorInfluences2008,
  title = {Precision of the {{Anchor Influences}} the {{Amount}} of {{Adjustment}}:},
  shorttitle = {Precision of the {{Anchor Influences}} the {{Amount}} of {{Adjustment}}},
  author = {Janiszewski, Chris and Uy, Dan},
  year = {2008},
  month = feb,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {1467-9280},
  urldate = {2020-07-08},
  abstract = {The anchoring-and-adjustment heuristic has been used to account for a wide variety of numerical judgments. Five studies show that adjustment away from a numeric...},
  langid = {english},
  keywords = {nosource}
}

@article{jankowski2020influence,
  title = {Influence of Moisture on Maturation Rate of the {{Venturia}} Inaequalis ({{Cooke}}) {{Wint}}. Ascospores in Central {{Poland}}},
  author = {Jankowski, Pawe{\l} and Masny, Sylwester},
  year = {2020},
  journal = {Journal of Plant Diseases and Protection},
  volume = {127},
  number = {2},
  pages = {155--163},
  publisher = {Springer}
}

@article{jansen2014notion,
  title = {On the Notion (s) of Duality for {{Markov}} Processes},
  author = {Jansen, Sabine and Kurt, Noemi},
  year = {2014},
  journal = {Probability surveys},
  volume = {11},
  pages = {59--120},
  publisher = {{The Institute of Mathematical Statistics and the Bernoulli Society}},
  doi = {doi:10.1214/12-PS206},
  file = {/home/gkonkamking/pCloudDrive/papers/Jansen_Kurt_2014_On the notion (s) of duality for Markov processes.pdf}
}

@article{janssen2008semantic,
  title = {Semantic Interference in a Delayed Naming Task: Evidence for the Response Exclusion Hypothesis.},
  author = {Janssen, Niels and Schirm, Walter and Mahon, Bradford Z and Caramazza, Alfonso},
  year = {2008},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {34},
  number = {1},
  pages = {249},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{JAT:JAT1660,
  title = {Triclosan: {{Environmental}} Exposure, Toxicity and Mechanisms of Action},
  author = {Dann, A. B. and Hontela, Alice},
  year = {2011},
  journal = {Journal of Applied Toxicology},
  volume = {31},
  number = {4},
  pages = {285--311},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {0260437X},
  doi = {10.1002/jat.1660},
  abstract = {Triclosan [5-chloro-2-(2,4-dichlorophenoxy)phenol; TCS] is a broad spectrum antibacterial agent used in personal care, veterinary, industrial and household products. TCS is commonly detected in aquatic ecosystems, as it is only partially removed during the wastewater treatment process. Sorption, biodegradation and photolytic degradation mitigate the availability of TCS to aquatic biota; however the by-products such as methyltriclosan and other chlorinated phenols may be more resistant to degradation and have higher toxicity than the parent compound. The continuous exposure of aquatic organisms to TCS, coupled with its bioaccumulation potential, have led to detectable levels of the antimicrobial in a number of aquatic species. TCS has been also detected in breast milk, urine and plasma, with levels of TCS in the blood correlating with consumer use patterns of the antimicrobial. Mammalian systemic toxicity studies indicate that TCS is neither acutely toxic, mutagenic, carcinogenic, nor a developmental toxicant. Recently, however, concern has been raised over TCS's potential for endocrine disruption, as the antimicrobial has been shown to disrupt thyroid hormone homeostasis and possibly the reproductive axis. Moreover, there is strong evidence that aquatic species such as algae, invertebrates and certain types of fish are much more sensitive to TCS than mammals. TCS is highly toxic to algae and exerts reproductive and developmental effects in some fish. The potential for endocrine disruption and antibiotic cross-resistance highlights the importance of the judicious use of TCS, whereby the use of TCS should be limited to applications where it has been shown to be effective.},
  isbn = {1099-1263 (Electronic){\textbackslash}n0260-437X (Linking)},
  pmid = {21462230},
  keywords = {Aquatic species,Effects,Endocrine disruption,Exposure,Human,Irgasan,Levels,Mammals,nosource,Reproduction,Review,Toxicity,Triclosan}
}

@incollection{jaynes2003probability,
  title = {Preface},
  booktitle = {Probability Theory: The Logic of Science},
  author = {Jaynes, Edwin T},
  year = {2003},
  pages = {xxv----------------xxvi},
  publisher = {Cambridge university press},
  keywords = {nosource}
}

@article{jenkins2017exact,
  title = {Exact Simulation of the {{Wright}}--{{Fisher}} Diffusion},
  author = {Jenkins, Paul A and Spano, Dario},
  year = {2017},
  journal = {The Annals of Applied Probability},
  volume = {27},
  number = {3},
  pages = {1478--1509},
  publisher = {Institute of Mathematical Statistics},
  doi = {doi:10.1214/16-AAP1236},
  file = {/home/gkonkamking/pCloudDrive/papers/Jenkins_Spano_2017_Exact simulation of the Wright–Fisher diffusion.pdf}
}

@book{jensen2020handbook,
  title = {Handbook of Urban Mobilities},
  author = {Jensen, Ole B and Lassen, Claus and Kaufmann, Vincent and {Freudendal-Pedersen}, Malene and Lange, Ida Sofie G{\o}tzsche},
  year = {2020},
  publisher = {Routledge},
  keywords = {nosource}
}

@article{jiangComputationCarlsonsMultiple1992,
  title = {Computation of {{Carlson}}'s {{Multiple Hypergeometric Function R}} for {{Bayesian Applications}}},
  author = {Jiang, Thomas J. and Kadane, Joseph B. and Dickey, James M.},
  year = {1992},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {1},
  number = {3},
  eprint = {1390718},
  eprinttype = {jstor},
  pages = {231--251},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
  issn = {1061-8600},
  doi = {10.2307/1390718},
  urldate = {2021-10-18},
  abstract = {Carlson's multiple hypergeometric functions arise in Bayesian inference, including methods for multinomial data with missing category distinctions and for local smoothing of histograms. To use these methods one needs to calculate Carlson functions and their ratios. We discuss properties of the functions and explore computational methods for them, including closed form methods, expansion methods, Laplace approximations, and Monte Carlo methods. Examples are given to illustrate and compare methods.},
  file = {/home/gkonkamking/Zotero/storage/7MQYW8EF/Jiang et al. - 1992 - Computation of Carlson's Multiple Hypergeometric F.pdf}
}

@incollection{jiEstimatingLatentCell2015,
  title = {Estimating {{Latent Cell Subpopulations}} with {{Bayesian Feature Allocation Models}}},
  booktitle = {Nonparametric {{Bayesian Inference}} in {{Biostatistics}}},
  author = {Ji, Yuan and Sengupta, Subhajit and Lee, Juhee and M{\"u}ller, Peter and Gulukota, Kamalakar},
  editor = {Mitra, Riten and M{\"u}ller, Peter},
  year = {2015},
  series = {Frontiers in {{Probability}} and the {{Statistical Sciences}}},
  pages = {77--95},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-19518-6_4},
  urldate = {2021-05-04},
  abstract = {Tumor cells are genetically heterogeneous. The collection of the entire tumor cell population consists of different subclones that can be characterized by mutations in sequence and structure at various genomic locations. Using next-generation sequencing data, we characterize tumor heterogeneity using Bayesian nonparametric inference. Specifically, we estimate the number of subclones in a tumor sample, and for each subclone, we estimate the subclonal copy number and single nucleotide mutations at a selected set of loci. Posterior summaries are presented in three matrices, namely, the matrix of subclonal copy numbers ({$L$}L{\textbackslash}boldsymbol\{L\}), subclonal variant alleles ({$Z$}Z{\textbackslash}boldsymbol\{Z\}), and the population frequencies of the subclones ({$w$}w{\textbackslash}boldsymbol\{w\}). The proposed method can handle a single or multiple tumor samples. Computation via Markov chain Monte Carlo yields posterior Monte Carlo samples of all three matrices, allowing for the assessment of any desired inference summary. Simulation and real-world examples are provided as illustration. An R package is available at http://www.cran.r-project.org/web/packages/BayClone2/index.html.},
  isbn = {978-3-319-19518-6},
  langid = {english},
  keywords = {Binomial Sampling Model,Cellular Prevalence,Indian Buffet Process (IBP),Single Nucleotide Variants (SNVs),Subclones}
}

@article{jingCommunityDetectionMixture2021,
  title = {Community Detection on Mixture Multilayer Networks via Regularized Tensor Decomposition},
  author = {Jing, Bing-Yi and Li, Ting and Lyu, Zhongyuan and Xia, Dong},
  year = {2021},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {49},
  number = {6},
  pages = {3181--3205},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/21-AOS2079},
  urldate = {2023-03-02},
  abstract = {We study the problem of community detection in multilayer networks, where pairs of nodes can be related in multiple modalities. We introduce a general framework, that is, mixture multilayer stochastic block model (MMSBM), which includes many earlier models as special cases. We propose a tensor-based algorithm (TWIST) to reveal both global/local memberships of nodes, and memberships of layers. We show that the TWIST procedure can accurately detect the communities with small misclassification error as the number of nodes and/or number of layers increases. Numerical studies confirm our theoretical findings. To our best knowledge, this is the first systematic study on the mixture multilayer networks using tensor decomposition. The method is applied to two real datasets: worldwide trading networks and malaria parasite genes networks, yielding new and interesting findings.},
  keywords = {62H30,91C20,multilayer network,Network community detection,tensor,Tucker decomposition},
  file = {/home/gkonkamking/pCloudDrive/papers/Jing et al_2021_Community detection on mixture multilayer networks via regularized tensor.pdf}
}

@article{jochmannWhatBelongsWhere2013,
  title = {What Belongs Where? {{Variable}} Selection for Zero-Inflated Count Models with an Application to the Demand for Health Care},
  shorttitle = {What Belongs Where?},
  author = {Jochmann, Markus},
  year = {2013},
  month = oct,
  journal = {Computational Statistics},
  volume = {28},
  number = {5},
  pages = {1947--1964},
  issn = {1613-9658},
  doi = {10.1007/s00180-012-0388-z},
  urldate = {2020-04-10},
  abstract = {This paper develops a Bayesian spike and slab model for zero-inflated count models which are commonly used in health economics. We account for model uncertainty and allow for model averaging in situations with many potential regressors. The proposed techniques are applied to a German data set analyzing the demand for health care. An accompanying package for the free statistical software environment R is provided.},
  langid = {english}
}

@article{Joe2006,
  title = {Generating Random Correlation Matrices Based on Partial Correlations},
  author = {Joe, Harry},
  year = {2006},
  journal = {Journal of Multivariate Analysis},
  volume = {97},
  number = {10},
  pages = {2177--2189},
  issn = {0047259X},
  doi = {10.1016/j.jmva.2005.05.010},
  abstract = {A d-dimensional positive definite correlation matrix R = ({$\rho$}ij) can be parametrized in terms of the correlations {$\rho$}i, i + 1 for i = 1, ..., d - 1, and the partial correlations {$\rho$}ij {\textbar} i + 1, ... j - 1 for j - i {$\geq$} 2. These fenced(frac(d, 2)) parameters can independently take values in the interval (- 1, 1). Hence we can generate a random positive definite correlation matrix by choosing independent distributions Fij, 1 {$\leq$} i {$<$} j {$\leq$} d, for these fenced(frac(d, 2)) parameters. We obtain conditions on the Fij so that the joint density of ({$\rho$}ij) is proportional to a power of det (R) and hence independent of the order of indices defining the sequence of partial correlations. As a special case, we have a simple construction for generating R that is uniform over the space of positive definite correlation matrices. As a byproduct, we determine the volume of the set of correlation matrices in fenced(frac(d, 2))-dimensional space. To prove our results, we obtain a simple remarkable identity which expresses det (R) as a function of {$\rho$}i, i + 1 for i = 1, ..., d - 1, and {$\rho$}ij {\textbar} i + 1, ... j - 1 for j - i {$\geq$} 2. {\copyright} 2005 Elsevier Inc. All rights reserved.},
  isbn = {0047-259X},
  keywords = {Beta distribution,Determinant of correlation matrix,nosource}
}

@article{johansenParticleMethodsMaximum2008,
  title = {Particle Methods for Maximum Likelihood Estimation in Latent Variable Models},
  author = {Johansen, Adam M. and Doucet, Arnaud and Davy, Manuel},
  year = {2008},
  month = mar,
  journal = {Statistics and Computing},
  volume = {18},
  number = {1},
  pages = {47--57},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-007-9037-8},
  urldate = {2021-07-08},
  abstract = {Standard methods for maximum likelihood parameter estimation in latent variable models rely on the Expectation-Maximization algorithm and its Monte Carlo variants. Our approach is different and motivated by similar considerations to simulated annealing; that is we build a sequence of artificial distributions whose support concentrates itself on the set of maximum likelihood estimates. We sample from these distributions using a sequential Monte Carlo approach. We demonstrate state-of-the-art performance for several applications of the proposed approach.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/9W4RLHN6/Johansen et al. - 2008 - Particle methods for maximum likelihood estimation.pdf}
}

@book{johnson1997discrete,
  title = {Discrete Multivariate Distributions},
  author = {Johnson, Norman Lloyd and Kotz, Samuel and Balakrishnan, Narayanaswamy},
  year = {1997},
  volume = {165},
  publisher = {Wiley New York},
  keywords = {duplicate-citation-key,nosource}
}

@incollection{johnson1997discrete,
  title = {Discrete Multivariate Distributions},
  booktitle = {Technometrics},
  author = {Poston, Wendy L.},
  year = {1998},
  volume = {40},
  eprint = {0811.0406},
  pages = {161--162},
  publisher = {Wiley New York},
  issn = {0040-1706},
  doi = {10.2307/1270659},
  abstract = {This article brings in two new discrete distributions: multidimensional Binomial distribution and multidimensional Poisson distribution. Those distributions were created in eventology as more correct generalizations of Binomial and Poisson distributions. Accordingly to eventology new laws take into account full distribution of events. Also, in article its characteristics and properties are described},
  archiveprefix = {arXiv},
  arxivid = {0811.0406},
  isbn = {0-471-12844-9},
  pmid = {3052639},
  keywords = {duplicate-citation-key,nosource}
}

@article{Jones1999,
  title = {Ecological Risk Assessment in a Large River-Reservoir: 3. {{Benthic}} Invertebrates},
  author = {Jones, D S and Barnthouse, Lawrence W and Suter II, Glenn W and Efroymson, Rebecca A and Field, J M and Beauchamp, J J},
  year = {1999},
  journal = {Environmental Toxicology and Chemistry},
  volume = {18},
  number = {4},
  pages = {599--609},
  abstract = {The sediments of Poplar Creek and the Clinch River are contaminated with a wide variety of chemicals, including heavy metals, polycyclic aromatic hydrocarbons, and PCBs. Sources include the U.S. Department of Energy's Oak Ridge Reservation as well as both known and unidentified upstream activities. We investigated the risks to benthic invertebrates posed by chemicals in these sediments as part of a comprehensive ecological risk assessment performed to support Superfund clean-up decisions. Poplar Creek was the only river reach for which significant risks were determined. This conclusion was based on several lines of reasoning: sediment-associated organisms at most sites were exposed to levels of several contaminants that have been observed to be toxic; the biosurvey results show a greater than 20\{\%\} reduction relative to reference sites in taxa richness and abundance; the statistical analysis of the physical, contaminant, and biosurvey data did not exclude contaminants as possible causal factors; and the sediment toxicity tests were too ambiguous to definitively exclude impacts in this reach. This assessment demonstrates the importance of collecting biological data, including sediment toxicity tests and biological surveys; statistically analyzing the relationships of chemicals, physical variables, and measured effects (e.g., toxicity or benthic invertebrate densities); and using sediment chemical and effects distributions in addition to point estimates of exposure and screening benchmarks.},
  isbn = {0730-7268},
  keywords = {benthic invertebrates,ecological risk assessment,metals,nosource,organic chemicals,sediment}
}

@article{Jones2012,
  title = {{{PSICOV}}: {{Precise}} Structural Contact Prediction Using Sparse Inverse Covariance Estimation on Large Multiple Sequence Alignments},
  author = {Jones, David T. and Buchan, Daniel W A and Cozzetto, Domenico and Pontil, Massimiliano},
  year = {2012},
  journal = {Bioinformatics},
  volume = {28},
  number = {2},
  pages = {184--190},
  issn = {13674803},
  doi = {10.1093/bioinformatics/btr638},
  abstract = {MOTIVATION: The accurate prediction of residue-residue contacts, critical for maintaining the native fold of a protein, remains an open problem in the field of structural bioinformatics. Interest in this long-standing problem has increased recently with algorithmic improvements and the rapid growth in the sizes of sequence families. Progress could have major impacts in both structure and function prediction to name but two benefits. Sequence-based contact predictions are usually made by identifying correlated mutations within multiple sequence alignments (MSAs), most commonly through the information-theoretic approach of calculating mutual information between pairs of sites in proteins. These predictions are often inaccurate because the true covariation signal in the MSA is often masked by biases from many ancillary indirect-coupling or phylogenetic effects. Here we present a novel method, PSICOV, which introduces the use of sparse inverse covariance estimation to the problem of protein contact prediction. Our method builds on work which had previously demonstrated corrections for phylogenetic and entropic correlation noise and allows accurate discrimination of direct from indirectly coupled mutation correlations in the MSA.{\textbackslash}n{\textbackslash}nRESULTS: PSICOV displays a mean precision substantially better than the best performing normalized mutual information approach and Bayesian networks. For 118 out of 150 targets, the L/5 (i.e. top-L/5 predictions for a protein of length L) precision for long-range contacts (sequence separation {$>$}23) was {$\geq$} 0.5, which represents an improvement sufficient to be of significant benefit in protein structure prediction or model quality assessment.{\textbackslash}n{\textbackslash}nAVAILABILITY: The PSICOV source code can be downloaded from http://bioinf.cs.ucl.ac.uk/downloads/PSICOV.},
  arxiv = {/bioinformatics.oxfordjournals.org/content/suppl/2011/11/29/btr638.DC1/target.txt [http:]},
  arxivid = {http://bioinformatics.oxfordjournals.org/content/suppl/2011/11/29/btr638.DC1/target.txt},
  isbn = {1367-4811 (Electronic){\textbackslash}r1367-4803 (Linking)},
  pmid = {22101153},
  keywords = {nosource}
}

@incollection{Jordan2010hierarchical,
  title = {Hierarchical Models, Nested Models and Completely Random Measures},
  booktitle = {Frontiers of Statistical Decision Making and {{Bayesian}} Analysis: {{In}} Honor of {{James O}}. {{Berger}}.},
  author = {Jordan, Michael I.},
  year = {2010},
  eprint = {1312.6184v5},
  pages = {207--218},
  publisher = {cs.berkeley.edu},
  address = {New York},
  doi = {10.1007/978-1-4419-6944-6},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1312.6184v5},
  isbn = {978-1-4419-6944-6},
  keywords = {nosource}
}

@article{jostmannWeightEmbodimentImportance2009,
  title = {Weight as an {{Embodiment}} of {{Importance}}},
  author = {Jostmann, Nils B. and Lakens, Dani{\"e}l and Schubert, Thomas W.},
  year = {2009},
  month = sep,
  journal = {Psychological Science},
  volume = {20},
  number = {9},
  pages = {1169--1174},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.2009.02426.x},
  urldate = {2020-05-15},
  abstract = {Four studies show that the abstract concept of importance is grounded in bodily experiences of weight. Participants provided judgments of importance while they held either a heavy or a light clipboard. Holding a heavy clipboard increased judgments of monetary value (Study 1) and made participants consider fair decision-making procedures to be more important (Study 2). It also caused more elaborate thinking, as indicated by higher consistency between related judgments (Study 3) and by greater polarization of agreement ratings for strong versus weak arguments (Study 4). In line with an embodied perspective on cognition, these findings suggest that, much as weight makes people invest more physical effort in dealing with concrete objects, it also makes people invest more cognitive effort in dealing with abstract issues.},
  langid = {english},
  keywords = {nosource}
}

@article{JS15,
  title = {Forward Stable Computation of Roots of Real Polynomials with Only Real Distinct Roots},
  author = {Stor, N Jakovcevic and Slapnicar, I},
  year = {2015},
  pages = {1--15},
  abstract = {As showed in (Fiedler, 1990), any polynomial can be expressed as a characteristic polynomial of a complex symmetric arrowhead matrix. This expression is not unique. If the polynomial is real with only real distinct roots, the matrix can be chosen real. By using accurate forward stable algorithm for computing eigenvalues of real symmetric arrowhead matrices from (Jakovcevic Stor, Slapnicar, Barlow, 2015), we derive a forward stable algorithm for computation of roots of such polynomials in O(n2) operations. The algorithm computes each root to almost full accuracy. In some cases, the algorithm invokes extended precision routines, but only in the non-iterative part. Our examples include numerically difficult problems, like the well-known Wilkinson's polynomials. Our algorithm compares favourably to other method for polynomial root-finding, like MPSolve or Newton's method.},
  keywords = {nosource}
}

@article{JSB15,
  title = {Accurate Eigenvalue Decomposition of Real Symmetric Arrowhead Matrices and Applications},
  author = {N. Jakovcevic Stor, I Slapnicar and Barlow, J L},
  year = {2015},
  journal = {Linear Algebra and Its Applications},
  volume = {464},
  pages = {62--89},
  doi = {10.1016/j.laa.2013.10.007},
  abstract = {We present a new algorithm for solving the eigenvalue problem for an n {\texttimes} n real symmetric arrowhead matrix. The algorithm computes all eigenvalues and all components of the corresponding eigenvectors with high relative accuracy in O(n{\textasciicircum}2) operations under certain circumstances. The algorithm is based on a shift-and-invert approach. Only a single element of the inverse of the shifted matrix eventually needs to be computed with double the working precision. Each eigenvalue and the corresponding eigenvector can be computed separately, which makes the algorithm adaptable for parallel computing. Our results extend to Hermitian arrowhead matrices, real symmetric diagonal-plus-rank-one matrices and singular value decomposition of real triangular arrowhead matrices.},
  keywords = {nosource}
}

@article{JSB15a,
  title = {Forward Stable Eigenvalue Decomposition of Rank-One Modifications of Diagonal Matrices},
  author = {N. Jakovcevic Stor, I Slapnicar and Barlow, J L},
  year = {2015},
  journal = {Linear Algebra and Its Applications},
  volume = {487},
  pages = {301--315},
  doi = {10.1016/j.laa.2015.09.025},
  abstract = {We present a new algorithm for solving an eigenvalue problem for a real symmetric matrix which is a rank-one modification of a diagonal matrix. The algorithm computes each eigenvalue and all components of the corresponding eigenvector with high relative accuracy in O(n) operations. The algorithm is based on a shift-and-invert approach. Only a single element of the inverse of the shifted matrix eventually needs to be computed with double the working precision. Each eigenvalue and the corresponding eigenvector can be computed separately, which makes the algorithm adaptable for parallel computing. Our results extend to the complex Hermitian case. The algorithm is similar to the algorithm for solving the eigenvalue problem for real symmetric arrowhead matrices from N. Jakov{\v c}evi{\'c} Stor et al. (2015) [16].},
  keywords = {nosource}
}

@article{JSSv076i01,
  title = {Stan: {{A}} Probabilistic Programming Language},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  journal = {Journal of Statistical Software, Articles},
  volume = {76},
  number = {1},
  pages = {1--32},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
  keywords = {alg,Bayesian inference,nosource,probabilistic programming}
}

@article{ju_2010,
  title = {Bayesian Nonparametric Estimation of the Spectral Density of a Long or Intermediate Memory Gaussian Process},
  author = {Rousseau, Judith and Chopin, Nicolas and Liseo, Brunero},
  year = {2012},
  journal = {Annals of Statistics},
  volume = {40},
  number = {2},
  eprint = {1007.3823v2},
  pages = {964--995},
  publisher = {Institute of Mathematical Statistics},
  issn = {00905364},
  doi = {10.1214/11-AOS955},
  abstract = {A stationary Gaussian process is said to be long-range dependent (resp. anti-persistent) if its spectral density \$f()\$ can be written as \$f()={\textbar}{\textbar}{\textasciicircum}\{-2d\}g({\textbar}{\textbar})\$, where \$0{$<$} d {$<$} 1/2 (resp. -1/2 {$<$} d {$<$} 0), and g is continuous. We propose a novel Bayesian nonparametric approach for the estimation of the spectral density of such processes. Within this approach, we prove posterior consistency for both d and g, under appropriate conditions on the prior distribution. We establish the rate of convergence for a general class of priors, and apply our results to the family of fractionally exponential priors. Our approach is based on the true likelihood function, and does not resort to Whittle's approximation, which is not valid in a long memory set-up.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1007.3823v2},
  keywords = {Bayesian nonparametric,Consistency,FEXP priors,Gaussian long memory processes,nosource,Rates of convergence}
}

@inproceedings{K14,
  title = {Experimental Multi-Threading Support for the \{\vphantom\}{{J}}\vphantom\{\}ulia Programming Language},
  booktitle = {{{HPTCDL}}'14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages},
  author = {Knopp, Tobias},
  year = {2014},
  pages = {1--5},
  publisher = {ACM},
  address = {New York},
  doi = {10.1109/HPTCDL.2014.11},
  abstract = {Julia is a young programming language that is designed for technical computing. Although Julia is dynamically typed it is very fast and usually yields C speed by utilizing a just-in-time compiler. Still, Julia has a simple syntax that is similar to Matlab, which is widely known as an easy-to-use programming environment. While Julia is very versatile and provides asynchronous programming facilities in the form of tasks (coroutines) as well as distributed multi-process parallelism, one missing feature is shared memory multi-threading. In this paper we present our experiment on introducing multi-threading support in the Julia programming environment. While our implementation has some restrictions that have to be taken into account when using threads, the results are promising yielding almost full speedup for perfectly parallelizable tasks.},
  keywords = {nosource}
}

@article{Kalli2015,
  title = {Bayesian Nonparametric Vector Autoregressive Models},
  author = {Kalli, Maria and Griffin, Jim E.},
  year = {2015},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.2665709},
  abstract = {Vector autoregressive (VAR) models are the main work-horse model for macroeconomic forecasting, and provide a framework for the analysis of complex dynamics that are present between macroeconomic variables. Whether a classical or a Bayesian approach is adopted, most VAR models are linear with Gaussian innovations. This can limit the model's ability to explain the relationships in macroeconomic series. We propose a nonparametric VAR model that allows for nonlinearity in the conditional mean, heteroscedasticity in the conditional variance, and non-Gaussian innovations. Our approach differs to that of previous studies by modelling the stationary and transition densities using Bayesian nonparametric methods. Our Bayesian nonparametric VAR (BayesNP-VAR) model is applied to USA and Eurozone macroeconomic time series, and compared to other Bayesian VAR models. We show that BayesNP-VAR is a flexible model that is able to account for nonlinear relationships as well as heteroscedasticity in the data. In terms of short-run out-of-sample predictions, we show that BayesNP-VAR predictively outperforms competing models.},
  keywords = {Dirichlet Process Prior,Infinite Mixtures,Inflation,Markov chain Monte Carlo,nosource,Vector Autoregressive Models}
}

@article{Kamisetty2013,
  title = {Assessing the Utility of Coevolution-Based Residue-Residue Contact Predictions in a Sequence- and Structure-Rich Era.},
  author = {Kamisetty, Hetunandan and Ovchinnikov, Sergey and Baker, David},
  year = {2013},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {110},
  number = {39},
  pages = {15674--9},
  issn = {1091-6490},
  doi = {10.1073/pnas.1314045110},
  abstract = {Recently developed methods have shown considerable promise in predicting residue-residue contacts in protein 3D structures using evolutionary covariance information. However, these methods require large numbers of evolutionarily related sequences to robustly assess the extent of residue covariation, and the larger the protein family, the more likely that contact information is unnecessary because a reasonable model can be built based on the structure of a homolog. Here we describe a method that integrates sequence coevolution and structural context information using a pseudolikelihood approach, allowing more accurate contact predictions from fewer homologous sequences. We rigorously assess the utility of predicted contacts for protein structure prediction using large and representative sequence and structure databases from recent structure prediction experiments. We find that contact predictions are likely to be accurate when the number of aligned sequences (with sequence redundancy reduced to 90\%) is greater than five times the length of the protein, and that accurate predictions are likely to be useful for structure modeling if the aligned sequences are more similar to the protein of interest than to the closest homolog of known structure. These conditions are currently met by 422 of the protein families collected in the Pfam database.},
  isbn = {1091-6490 (Electronic){\textbackslash}r0027-8424 (Linking)},
  pmid = {24009338},
  keywords = {Algorithms,Amino Acid Sequence,Amino Acids,Amino Acids: chemistry,Computational Biology,Computational Biology: methods,Databases,Evolution,Models,Molecular,nosource,Protein,Proteins,Proteins: chemistry,Proteins: genetics}
}

@article{kamstrup-nielsenCoreConsistencyDiagnostic2013,
  title = {Core Consistency Diagnostic in {{PARAFAC2}}},
  author = {{Kamstrup-Nielsen}, Maja H. and Johnsen, Lea G. and Bro, Rasmus},
  year = {2013},
  journal = {Journal of Chemometrics},
  volume = {27},
  number = {5},
  pages = {99--105},
  issn = {1099-128X},
  doi = {10.1002/cem.2497},
  urldate = {2022-07-22},
  abstract = {PARAFAC2 is applied in multiple research areas, for example, where data containing shifts are analysed, but it is a challenge to determine the appropriate number of components in the model. In this paper, it is hypothesized that the core consistency diagnostic, which is currently applied in, for example, PARAFAC1 can be used to determine model complexity in PARAFAC2. Theoretically, a PARAFAC1 model is fitted `inside' the PARAFAC2 algorithm, and it should therefore be possible to apply the core consistency diagnostic from PARAFAC1 in PARAFAC2. To support this hypothesis, three different datasets, as well as simulated datasets, have been evaluated by means of PARAFAC2, and the core consistencies have been investigated. There is a general trend that if the core consistency is low, the model is overfitted as in PARAFAC1. Also, core consistency captures the true variation in the data, whereas small peaks are easily overlooked by visual inspection of noisy models. However, for determining the number of components in a PARAFAC2 model, we suggest usage of the core consistency in combination with other model parameters such as residuals, loadings, and split-half analysis. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {core consistency,model complexity,number of components,PARAFAC2},
  file = {/home/gkonkamking/pCloudDrive/papers/Kamstrup-Nielsen et al_2013_Core consistency diagnostic in PARAFAC2.pdf}
}

@article{kaniadakis2005two,
  title = {Two-Parameter Deformations of Logarithm, Exponential, and Entropy: {{A}} Consistent Framework for Generalized Statistical Mechanics},
  author = {Kaniadakis, G. and Lissia, M. and Scarfone, A. M.},
  year = {2005},
  journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  volume = {71},
  number = {4},
  pages = {46128},
  publisher = {APS},
  issn = {15393755},
  doi = {10.1103/PhysRevE.71.046128},
  abstract = {A consistent generalization of statistical mechanics is obtained by{\textbackslash}napplying the maximum entropy principle to a trace-form entropy and{\textbackslash}nby requiring that physically motivated mathematical properties are{\textbackslash}npreserved. The emerging differential-functional equation yields a{\textbackslash}ntwo-parameter class of generalized logarithms, from which entropies{\textbackslash}nand power-law distributions follow: these distributions could be{\textbackslash}nrelevant in many anomalous systems. Within the specified range of{\textbackslash}nparameters, these entropies possess positivity, continuity, symmetry,{\textbackslash}nexpansibility, decisivity, maximality, concavity, and are Lesche{\textbackslash}nstable. The Boltzmann-Shannon entropy and some one-parameter generalized{\textbackslash}nentropies already known belong to this class. These entropies and{\textbackslash}ntheir distribution functions are compared, and the corresponding{\textbackslash}ndeformed algebras are discussed.},
  pmid = {15903747},
  keywords = {nosource}
}

@book{kaperMathematicsPlanetEarth2015,
  title = {Mathematics of Planet {{Earth}}: Mathematicians Reflect on How to Discover, Organize, and Protect Our Planet},
  shorttitle = {Mathematics of Planet {{Earth}}},
  editor = {Kaper, H. G. and Rousseau, Christiane},
  year = {2015},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia},
  isbn = {978-1-61197-370-9},
  langid = {english},
  lccn = {QH541.15.M34 M385 2015},
  keywords = {Ecology,Environmental sciences,Mathematics},
  file = {/home/gkonkamking/Zotero/storage/GUZNVSS7/Kaper and Rousseau - 2015 - Mathematics of planet Earth mathematicians reflec.pdf}
}

@article{kaplan1958nonparametric,
  title = {Nonparametric Estimation from Incomplete Observations},
  author = {Kaplan, E L and Meier, Paul},
  year = {1958},
  journal = {Journal of the American Statistical Association},
  volume = {53},
  number = {282},
  pages = {457--481},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.2307/2281868},
  abstract = {In lifetesting, medical follow-up, and other fields the observation of the time of occurrence of the event of interest (called a death) may be prevented for some of the items of the sample by the previous occur- rence of some other event (called a loss). Losses may be either accidental or controlled, the latter resulting from a decision to terminate certain observations. In either case it is usually assumed in this paper that the lifetime (age at death) is independent of the potential loss time; in practice this assumption deserves careful scrutiny. Despite the resulting incompleteness of the data, it is desired to estimate the proportion P(t) of items in the population whose lifetimes would exceed t (in the absence of such losses), without making any assumption about the form of the function P(t). The observation for each item of a suitable initial event, marking the beginning of its lifetime, is presupposed. For random samples of size N the product-limit (PL) estimate can be defined as follows: List and label the N observed lifetimes (whether to death or loss) in order of increasing magnitude, so that one has ? o},
  isbn = {01621459},
  pmid = {252},
  keywords = {nosource}
}

@article{karabatsos2015menu,
  title = {A Menu-Driven Software Package of Bayesian Nonparametric (and Parametric) Mixed Models for Regression Analysis and Density Estimation},
  author = {Karabatsos, George},
  year = {2017},
  journal = {Behavior Research Methods},
  volume = {49},
  eprint = {1506.05435},
  pages = {335--362},
  publisher = {Springer},
  abstract = {Most of applied statistics involves regression analysis of data. This paper presents a stand-alone and menu-driven software package, Bayesian Regression: Nonparametric and Parametric Models. Currently, this package gives the user a choice from 83 Bayesian models for data analysis. They include 47 Bayesian nonparametric (BNP) infinite-mixture regression models; 5 BNP infinite-mixture models for density estimation; and 31 normal random effects models (HLMs), including normal linear models. Each of the 78 regression models handles either a continuous, binary, or ordinal dependent variable, and can handle multi-level (grouped) data. All 83 Bayesian models can handle the analysis of weighted observations (e.g., for meta-analysis), and the analysis of left-censored, right-censored, and/or interval-censored data. Each BNP infinite-mixture model has a mixture distribution assigned one of various BNP prior distributions, including priors defined by either the Dirichlet process, Pitman-Yor process (including the normalized stable process), beta (two-parameter) process, normalized inverse-Gaussian process, geometric weights prior, dependent Dirichlet process, or the dependent infinite-probits prior. The software user can mouse-click to select a Bayesian model and perform data analysis via Markov chain Monte Carlo (MCMC) sampling. After the sampling completes, the software automatically opens text output that reports MCMC-based estimates of the model's posterior distribution and model predictive fit to the data. Additional text and/or graphical output can be generated by mouse-clicking other menu options. This includes output of MCMC convergence analyses, and estimates of the model's posterior predictive distribution, for selected functionals and values of covariates. The software, constructed from MATLAB Compiler, is illustrated through the BNP regression analysis of real data.},
  archiveprefix = {arXiv},
  arxivid = {1506.05435},
  keywords = {nosource}
}

@article{KarabatsosISBAbull15,
  title = {A Menu-Driven Software Package for \{\vphantom\}{{B}}\vphantom\{\}ayesian Regression Analysis},
  author = {Karabatsos, G},
  year = {2015},
  journal = {The ISBA Bulletin},
  volume = {22},
  pages = {13--16},
  keywords = {nosource}
}

@article{karlinskyTrackingExcessMortality2021,
  title = {Tracking Excess Mortality across Countries during the {{COVID-19}} Pandemic with the {{World Mortality Dataset}}},
  author = {Karlinsky, Ariel and Kobak, Dmitry},
  editor = {Davenport, Miles P and Lipsitch, Marc and Lipsitch, Marc and Simonsen, Lone and Mahmud, Ayesha},
  year = {2021},
  month = jun,
  journal = {eLife},
  volume = {10},
  pages = {e69336},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.69336},
  urldate = {2021-10-20},
  abstract = {Comparing the impact of the COVID-19 pandemic between countries or across time is difficult because the reported numbers of cases and deaths can be strongly affected by testing capacity and reporting policy. Excess mortality, defined as the increase in all-cause mortality relative to the expected mortality, is widely considered as a more objective indicator of the COVID-19 death toll. However, there has been no global, frequently updated repository of the all-cause mortality data across countries. To fill this gap, we have collected weekly, monthly, or quarterly all-cause mortality data from 103 countries and territories, openly available as the regularly updated World Mortality Dataset. We used this dataset to compute the excess mortality in each country during the COVID-19 pandemic. We found that in several worst-affected countries (Peru, Ecuador, Bolivia, Mexico) the excess mortality was above 50\% of the expected annual mortality (Peru, Ecuador, Bolivia, Mexico) or above 400 excess deaths per 100,000 population (Peru, Bulgaria, North Macedonia, Serbia). At the same time, in several other countries (e.g. Australia and New Zealand) mortality during the pandemic was below the usual level, presumably due to social distancing measures decreasing the non-COVID infectious mortality. Furthermore, we found that while many countries have been reporting the COVID-19 deaths very accurately, some countries have been substantially underreporting their COVID-19 deaths (e.g. Nicaragua, Russia, Uzbekistan), by up to two orders of magnitude (Tajikistan). Our results highlight the importance of open and rapid all-cause mortality reporting for pandemic monitoring.},
  keywords = {COVID,data,international,mortality},
  file = {/home/gkonkamking/Zotero/storage/3WF5DCWM/Karlinsky and Kobak - 2021 - Tracking excess mortality across countries during .pdf}
}

@article{kayFunctionalBasisStructureseeking2014,
  title = {A Functional Basis for Structure-Seeking: {{Exposure}} to Structure Promotes Willingness to Engage in Motivated Action.},
  shorttitle = {A Functional Basis for Structure-Seeking},
  author = {Kay, Aaron C. and Laurin, Kristin and Fitzsimons, Gr{\'a}inne M. and Landau, Mark J.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {2},
  pages = {486--491},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0034462},
  urldate = {2020-06-09},
  abstract = {A recurring observation of experimental psychologists is that people prefer, seek out, and even selectively ``see'' structure in their social and natural environments. Structure-seeking has been observed across a wide range of phenomena---from the detection of patterns in random arrays to affinities for order-providing political, religious, social, and scientific worldviews---and is exacerbated under psychological threat. Why are people motivated for structure? An intriguing, but untested, explanation holds that perceiving structure, even in domains unrelated to one's current behavioral context, can facilitate willingness to take goal-directed actions. Supporting this, in 5 studies, reminders of structure in nature or society increase willingness to engage in goal pursuit.},
  langid = {english},
  keywords = {nosource}
}

@article{Kefford2003,
  title = {Relative Salinity Tolerance of Macroinvertebrates from the {{Barwon River}}, {{Victoria}}, {{Australia}}},
  author = {Kefford, Ben J. and Papas, Phil J. and Nugegoda, Dayanthi},
  year = {2003},
  journal = {Marine and Freshwater Research},
  volume = {54},
  number = {6},
  pages = {755--765},
  issn = {13231650},
  doi = {10.1071/MF02081},
  abstract = {Salinity levels are rising in many freshwater environments, yet there are few direct measurements of salinity tolerance of organisms likely to be salt sensitive. The relative salinity tolerance to artificial seawater of macroinvertebrates from the Barwon River in Victoria, Australia, was assessed by measuring the 72-h lethal concentrations required to kill 50\% of individuals (LC\textsubscript{50}). LC\textsubscript{50} values ranged from an electrical conductivity of 5.5 to 76 mS cm\textsuperscript{\&\#8211;1} (mean 31 mS cm\textsuperscript{\&\#8211;1}, \emph{n} = 57) and followed a log-normal distribution. The most salt-sensitive groups tested were Baetidae (LC\textsubscript{50} value range: 5.5\&\#8211;6.2 mS cm\textsuperscript{\&\#8211;1}), Chironomidae (10 mS cm\textsuperscript{\&\#8211;1}) and several soft-bodied non-arthropods (Oligochaeta, Gastropoda, Nematomorpha, Tricladida and Hirudinea; 9\&\#8211;14 mS cm\textsuperscript{\&\#8211;1}). Other groups, from least to most tolerant, were non-baetid Ephmeroptera (\&\#62;12.6\&\#8211;15 mS cm\textsuperscript{\&\#8211;1}), Plecoptera (\&\#62;12.6\&\#8211;\&\#62;20 mS cm\textsuperscript{\&\#8211;1}), Trichoptera (9\&\#8211;\&\#62;26 mS cm\textsuperscript{\&\#8211;1}), Corixidae (18\&\#8211;26 mS cm\textsuperscript{\&\#8211;1}), non-corixid Hemiptera (33\&\#8211;44 mS cm\textsuperscript{\&\#8211;1}), Coleoptera (19\&\#8211;54 mS cm\textsuperscript{\&\#8211;1}), Hydracarina (39 mS cm\textsuperscript{\&\#8211;1}) and Odonata (30\&\#8211;55 mS cm\textsuperscript{\&\#8211;1}), and macrocrustaceans (Decapoda, Isopoda and Amphipoda; 38\&\#8211;76 mS cm\textsuperscript{\&\#8211;1}).},
  isbn = {1323-1650},
  pmid = {165},
  keywords = {Acute salinity tolerance,Ecotoxicity,nosource,Stream invertebrates}
}

@article{Kefford2005,
  title = {What Is Meant by ``95\% of Species''? {{An}} Argument for the Inclusion of Rapid Tolerance Testing},
  booktitle = {Human and Ecological Risk Assessment: {{An}} International Journal},
  author = {Kefford, Ben J. and Palmer, Carolyn G. and Jooste, Sebastian and Warne, Michael St. J. and Nugegoda, Dayanthi},
  year = {2005},
  volume = {11},
  number = {5},
  pages = {1025--1046},
  issn = {1080-7039},
  doi = {10.1080/10807030500257770},
  abstract = {It is increasingly common for water quality guidelines and risk assessments to consider the proportion of species at risk from a particular toxicant, based on the species sensitivity distribution (SSD) for that toxicant. There is a premise that the sensitivity data from species included in the SSD are sufficient to predict the effect on species for which there are no data. We discuss and review assumptions that follow this premise and find that for most toxicant SSDs include too few species, and that component species are biased toward particular taxonomic groups, common species and species from North America and western Europe. Consequently, protecting a given percentage, for example, 95\%, of species in an SSD will likely protect more or less than 95\% of species in nature, by an unknown amount. For the assumptions of SSDs to be better met, there is a need for tolerance data on more species, from more taxonomic and other groups, including rare species and those from widespread localities. In order to achieve this, we argue for the inclusion of rapid tests, which we define as toxicity tests designed to require less effort to conduct, relative to traditional tests, so sensitivity can be quickly and approximately determine in many species. Their use will allow for more species, more representative of natural communities, to be tested and therefore allow the construction of less biased SSDs and thus more accurate guidelines and assessments of risk.},
  isbn = {1080-7039},
  keywords = {nosource}
}

@article{Kefford2005a,
  title = {Relative Salinity Tolerance of Freshwater Macroinvertebrates from the South-East {{Eastern Cape}}, {{South Africa}} Compared with the {{Barwon Catchment}}, {{Victoria}}, {{Australia}}},
  author = {Kefford, Ben J. and Palmer, Carolyn G. and Nugegoda, Dayanthi},
  year = {2005},
  journal = {Marine and Freshwater Research},
  volume = {56},
  number = {2},
  pages = {163--171},
  issn = {13231650},
  doi = {10.1071/MF04098},
  abstract = {Salinity is rising in many southern African and Australian rivers with unknown effects on aquatic organisms. The extent of spatial variation, at any scale, in salt tolerances of aquatic organisms is unknown, so whether data from one location is applicable elsewhere is also unknown. The acute tolerances (72-h median lethal concentration (LC50)) to sea salt of 49 macroinvertebrate taxa from the south-east Eastern Cape (SEEC), South Africa were compared with those of 57 species from the Barwon Catchment, Victoria, Australia. The mean LC50 values from both locations were similar (Barwon: 31 and SEEC: 32mScm-1) and less abundant (rare) taxa tended to be more tolerant than more abundant (common) taxa. There was, however, a greater range of LC50 values (5.5--76mScm-1) in the Barwon Catchment than in theSEEC(11--47mScm-1).The species sensitivity distribution (SSD) for SEEC taxa was bimodal whereas the Barwon Catchment's SSD had a single peak.With few exceptions, members of an order had similar tolerances in both locations.The differences in SSD between locationswere related to crustacean, odonate and non-arthropod relative richness. Although it is not ideal to extrapolate SSDs from one location to another, it may be reasonable to assume similar salinity tolerances among related taxa.},
  isbn = {1323-1650},
  pmid = {34},
  keywords = {Acute salinity tolerance,Ecotoxicity,nosource,Rarity,Stream invertebrates}
}

@article{Kefford2006a,
  ids = {Kefford2006},
  title = {Validating Species Sensitivity Distributions Using Salinity Tolerance of Riverine Macroinvertebrates in the Southern {{Murray-Darling Basin}} ({{Victoria}}, {{Australia}})},
  author = {Kefford, {\relax BJ} and Nugegoda, Dayanthi},
  year = {2006},
  journal = {Can. J. Fish. Aquat. Sci.},
  volume = {1877},
  pages = {1865--1877},
  doi = {10.1139/F06-080},
  isbn = {0706-652X},
  keywords = {nosource}
}

@article{Kefford2008a,
  title = {Is the Integration of Hormesis and Essentiality into Ecotoxicology Now Opening {{Pandora}}'s {{Box}}?},
  author = {Kefford, Ben J. and Zalizniak, Liliana and Warne, Michael St J and Nugegoda, Dayanthi},
  year = {2008},
  month = feb,
  journal = {Environmental Pollution},
  volume = {151},
  number = {3},
  eprint = {17559995},
  eprinttype = {pubmed},
  pages = {516--523},
  issn = {02697491},
  doi = {10.1016/j.envpol.2007.04.019},
  abstract = {Hormesis and essentiality are likely real and common effects at the level of the individual. However, the widespread incorporation of stimulatory effects into applications of ecotoxicology requires the acceptance of assumptions, value judgements and possibly lowering of water/sediment quality standards. There is also currently little data appropriate for considering hormetic effects in the ecotoxicological context. Except perhaps in the case of fitting concentration-response curves, it is not clear that incorporation of hormetic and essentiality type responses into ecotoxicology is necessary. Furthermore, its incorporation presents considerable intellectual and practical changes for ecotoxicology and could have unanticipated consequences. ?? 2007 Elsevier Ltd. All rights reserved.},
  isbn = {0269-7491},
  pmid = {17559995},
  keywords = {Concentration-response curve,Ecotoxicology theory,Essential elements,Hormesis,nosource}
}

@article{Kefford2011,
  title = {The Definition of Species Richness Used by Species Sensitivity Distributions Approximates Observed Effects of Salinity on Stream Macroinvertebrates.},
  author = {Kefford, Ben J and Marchant, Richard and Sch{\"a}fer, Ralf B and Metzeling, Leon and Dunlop, Jason E and Choy, Satish C and Goonan, Peter},
  year = {2011},
  month = jan,
  journal = {Environmental pollution (Barking, Essex : 1987)},
  volume = {159},
  number = {1},
  eprint = {20932614},
  eprinttype = {pubmed},
  pages = {302--10},
  publisher = {Elsevier Ltd},
  issn = {1873-6424},
  doi = {10.1016/j.envpol.2010.08.025},
  abstract = {The risk of chemicals for ecological communities is often forecast with species sensitivity distributions (SSDs) which are used to predict the concentration which will protect p\% of species (PCp value). However, at the PCp value, species richness in nature would not necessary be p\% less than at uncontaminated sites. The definition of species richness inherent to SSDs (contaminant category richness) contrasts with species richness typically measured in most field studies (point richness). We determine, for salinity in eastern Australia, whether these definitions of stream macroinvertebrate species richness are commensurable. There were strong relationships (r2{$\geq$}0.87) between mean point species, family and Ephemeroptera, Trichoptera and Plecoptera species richness and their respective contamination category richness. Despite differences in the definition of richness used by SSDs and field biomonitoring, their results in terms of relative species loss from salinity in south-east Australia are similar. We conclude that in our system both definitions are commensurable.},
  isbn = {1873-6424 (Electronic){\textbackslash}r0269-7491 (Linking)},
  pmid = {20932614},
  keywords = {Animals,Australia,Biomonitoring,Chemical,Chemical: toxicity,duplicate-citation-key,Environmental Monitoring,Environmental Monitoring: methods,Invertebrates,Invertebrates: drug effects,nosource,Risk assessment,Risk Assessment,Risk Assessment: methods,Rivers,Rivers: chemistry,Salinity,Species richness,Stream invertebrate,Water Pollutants}
}

@article{Kefford2012,
  title = {Risk Assessment of Salinity and Turbidity in {{Victoria}} ({{Australia}}) to Stream Insects' Community Structure Does Not Always Protect Functional Traits.},
  author = {Kefford, Ben J and Sch{\"a}fer, Ralf B and Metzeling, Leon},
  year = {2012},
  month = jan,
  journal = {The Science of the total environment},
  volume = {415},
  eprint = {21714988},
  eprinttype = {pubmed},
  pages = {61--8},
  publisher = {Elsevier B.V.},
  issn = {1879-1026},
  doi = {10.1016/j.scitotenv.2011.05.056},
  abstract = {Ecological risk assessments mostly consider measures of community composition (structure) across large spatial scales. These assessments, using species sensitivity distributions (SSDs) or the relative species retention (RSR), may not be protective of ecosystem functions and services at smaller spatial scales. Here we examine how changes in biological traits, as proxy for ecosystem functions/services, at a fine spatial scale relate to larger scale assessment of structure. We use functional traits of stream insect species in south-east Australia in two habitats (riffle and edge/pool). We find that the protection of community structure in terms of 95\% of species over multiple sites against adverse effects of salinity (as electrical conductivity) and turbidity will mostly, but not always, protect traits at smaller scales. Considering different combinations of trait modalities, contaminants and habitat, a mean of 17.5\% (range 0\%-36.8) of cases would result in under-protection of trait modalities despite protecting species composition (in terms of Jaccard's Index). This under-protection of trait modalities is only because of the different spatial scales that community structure and the traits were considered. We recommend that where the protection of biological traits, ecosystem functions or ecosystem services from stressors is a management goal, protective targets should not be solely set using measures of community structure such as SSDs or RSR. To protect both structural and functional attributes separate risk assessments should be done.},
  isbn = {0048-9697},
  pmid = {21714988},
  keywords = {Animals,Biological traits,Conservation of Natural Resources,duplicate-citation-key,Ecosystem,Environmental Monitoring,Insects,Insects: physiology,nosource,Population Dynamics,Risk Assessment,Rivers,Rivers: chemistry,Salinization,Sodium Chloride,Sodium Chloride: analysis,Spatial scale,Stream macroinvertebrates,Suspended sediments,Victoria,Water Movements}
}

@article{Kefford2012a,
  title = {Global Scale Variation in the Salinity Sensitivity of Riverine Macroinvertebrates: Eastern {{Australia}}, {{France}}, {{Israel}} and {{South Africa}}.},
  author = {Kefford, Ben J and Hickey, Graeme L and Gasith, Avital and {Ben-David}, Elad and Dunlop, Jason E and Palmer, Carolyn G and Allan, Kaylene and Choy, Satish C and Piscart, Christophe},
  year = {2012},
  month = jan,
  journal = {PloS one},
  volume = {7},
  number = {5},
  pages = {e35224},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0035224},
  abstract = {Salinity is a key abiotic property of inland waters; it has a major influence on biotic communities and is affected by many natural and anthropogenic processes. Salinity of inland waters tends to increase with aridity, and biota of inland waters may have evolved greater salt tolerance in more arid regions. Here we compare the sensitivity of stream macroinvertebrate species to salinity from a relatively wet region in France (Lorraine and Brittany) to that in three relatively arid regions eastern Australia (Victoria, Queensland and Tasmania), South Africa (south-east of the Eastern Cape Province) and Israel using the identical experimental method in all locations. The species whose salinity tolerance was tested, were somewhat more salt tolerant in eastern Australia and South Africa than France, with those in Israel being intermediate. However, by far the greatest source of variation in species sensitivity was between taxonomic groups (Order and Class) and not between the regions. We used a bayesian statistical model to estimate the species sensitivity distributions (SSDs) for salinity in eastern Australia and France adjusting for the assemblages of species in these regions. The assemblage in France was slightly more salinity sensitive than that in eastern Australia. We therefore suggest that regional salinity sensitivity is therefore likely to depend most on the taxonomic composition of respective macroinvertebrate assemblages. On this basis it would be possible to screen rivers globally for risk from salinisation.},
  isbn = {1932-6203},
  pmid = {22567097},
  keywords = {Animals,Australia,Bayes Theorem,duplicate-citation-key,Ecosystem,Environmental Monitoring,France,Invertebrates,Israel,nosource,Queensland,Rivers,Salinity,South Africa,Tasmania,Victoria}
}

@article{kefford2022exploring,
  title = {Exploring the Interplay of Biotic Interactions and Salinity Stress in Freshwater Invertebrate Assemblages: Reply to {{Chessman}} (2022)},
  author = {Kefford, Ben J and Bray, Jon P and Nichols, Susan J and Reich, Jollene and Mac Nally, Ralph and {O'Reilly-Nugent}, Andrew and King, Guillaume Kon Kam and Thompson, Ross},
  year = {2022},
  journal = {Marine and Freshwater Research},
  volume = {73},
  number = {5},
  pages = {585--587},
  publisher = {CSIRO Publishing},
  doi = {10.1071/MF22043}
}

@article{keffordUnderstandingSalttoleranceBiota2021,
  title = {Understanding Salt-Tolerance and Biota--Stressor Interactions in Freshwater Invertebrate Communities},
  author = {Kefford, Ben J and Bray, Jon P and Nichols, Susan J and Reich, Jollene and Mac Nally, Ralph and {O'Reilly-Nugent}, Andrew and King, Guillaume Kon Kam and Thompson, Ross},
  year = {2021},
  journal = {Marine and Freshwater Research},
  volume = {73},
  number = {1},
  pages = {140--146},
  publisher = {CSIRO Publishing},
  doi = {10.1071/MF21164},
  keywords = {nosource}
}

@article{kendallWhatUncertaintiesWe,
  title = {What {{Uncertainties Do We Need}} in {{Bayesian Deep Learning}} for {{Computer Vision}}?},
  author = {Kendall, Alex and Gal, Yarin},
  abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/pCloudDrive/papers/Kendall_Gal_What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf}
}

@book{kery2010introduction,
  title = {Introduction to {{WinBUGS}} for Ecologists: {{Bayesian}} Approach to Regression, {{ANOVA}}, Mixed Models and Related Analyses},
  author = {K{\'e}ry, Marc},
  year = {2010},
  publisher = {Academic Press},
  keywords = {nosource}
}

@article{Kielbassa2010,
  title = {Application of a Temperature-Dependent von {{Bertalanffy}} Growth Model to Bullhead ({{Cottus}} Gobio)},
  author = {Kielbassa, J. and {Delignette-Muller}, Marie Laure and Pont, D. and Charles, Sandrine},
  year = {2010},
  month = jan,
  journal = {Ecological Modelling},
  volume = {221},
  number = {20},
  pages = {2475--2481},
  publisher = {Elsevier B.V.},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2010.07.001},
  abstract = {The most studied and commonly applied model of fish growth is the von Bertalanffy model. However, this model does not take water temperature into account, which is one of the most important environmental factors affecting the life cycle of fish, as many physiological processes that determine growth, e.g. metabolic rate and oxygen supply, are directly influenced by temperature. In the present study we propose a version of the von Bertalanffy growth model that includes mean annual water temperatures by correlating the growth coefficient, k, explicitly and the asymptotic length, L{$\infty$}, implicitly to water temperature. All relationships include parameters with an obvious biological relevance that makes them easier to identify. The model is used to fit growth data of bullhead (Cottus gobio) at different locations in the Bez River network (Drme, France). We show that temperature explains much of the growth variability at the different sampling sites of the network. {\copyright} 2010 Elsevier B.V.},
  keywords = {Cottus gobio,Growth model,Model comparison,nosource,River network,Warming scenario}
}

@article{Kielbassa2011,
  title = {The Importance of Incorporating Age and Sex When Backcalculating Length in Bullhead {{Cottus}} Gobio},
  author = {Kielbassa, J. and Charles, Sandrine and {Delignette-Muller}, Marie Laure},
  year = {2011},
  journal = {Journal of Fish Biology},
  volume = {78},
  number = {5},
  pages = {1492--1507},
  issn = {00221112},
  doi = {10.1111/j.1095-8649.2011.02956.x},
  abstract = {The present study backcalculated body length for a data set of a bullhead Cottus gobio population located at different sampling sites in a river network. Model comparison between various growth models, which included successively new parameters, showed the effect and importance of taking sex, age and the location in the river network into account. The data sets obtained by backcalculation were fitted by the von Bertalanffy growth function, which revealed the effect of the backcalculation formula on the estimation of the von Bertalanffy growth parameters. Fitting results and parameter estimates showed again the importance of incorporating age and sex when backcalculating body length in the C. gobio population studied.},
  pmid = {21539555},
  keywords = {Growth parameters,Model comparison,nosource,Otolith,River network,Von Bertalanffy}
}

@article{king2015statistical,
  title = {Statistical Inference for Partially Observed {{Markov}} Processes via the {{R}} Package Pomp},
  author = {King, Aaron A and Nguyen, Dao and Ionides, Edward L},
  year = {2015},
  journal = {arXiv preprint arXiv:1509.00503},
  eprint = {1509.00503},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@article{kingman1967completely,
  title = {Completely Random Measures},
  author = {Kingman, J F C},
  year = {1967},
  journal = {Pacific Journal Of Mathematics},
  volume = {21},
  number = {1},
  pages = {59--78},
  issn = {0030-8730},
  doi = {10.2140/pjm.1967.21.59},
  abstract = {The theory of stochastic processes is concerned with random functions defined on some parameter set. This paper is concerned with the case, which occurs naturally in some practical situations, in which the parameter set is a sigma-algebra of subsets of some space, and the random functions are all measures on this space. Among all such random measures are distinguished some which are called completely random, which have the property that the values they take on disjoint subsets are independent. A representation theorem is proved for all completely random measures satisfying a weak finiteness condition, and as a consequence it is shown that all such measures are necessarily purely atomic.},
  keywords = {nosource}
}

@article{kingman1975random,
  title = {Random Discrete Distributions},
  author = {Kingman, J},
  year = {1975},
  journal = {Journal of the Royal Statistical Society. Series B},
  volume = {37},
  number = {1},
  eprint = {2984986\{\%\}5Cnpapers2://publication/uuid/994B38DD-AA81-4084-932F-A36522878C9A},
  eprinttype = {jstor},
  pages = {1--15},
  publisher = {JSTOR},
  issn = {0035-9246},
  abstract = {Random Discrete Distributions By JF {\cyrchar\CYRS} Kingman University of Oxford [Read before the Royal Statistical Society at a meeting organized by the Research Section, on Wednesday, October 16th, 1974, Professor RL Plackett in the Chair] Summary},
  keywords = {nosource}
}

@article{kingman1977population,
  title = {The Population Structure Associated with the {{Ewens}} Sampling Formula},
  author = {Kingman, J. F C},
  year = {1977},
  journal = {Theoretical Population Biology},
  volume = {11},
  number = {2},
  pages = {274--283},
  publisher = {Elsevier},
  issn = {10960325},
  doi = {10.1016/0040-5809(77)90029-6},
  abstract = {Models for selectively neutral mutation, in which mutation always yields a new allele, seem always to lead, in the limit of large population size, to a sampling formula first propounded by Ewens in 1972. It is shown that the asymptotic validity of the Ewens formula is equivalent to a certain limiting joint distribution for the allele proportions in the population, arranged in descending order. The familiar diffusion approximations are corollaries of this limiting distribution, and therefore share the apparent robustness of the sampling formula. ?? 1977.},
  pmid = {867290},
  keywords = {nosource}
}

@book{kingman1992poisson,
  title = {Poisson Processes},
  author = {Kingman, John Frank Charles},
  year = {1992},
  volume = {3},
  publisher = {Oxford university press},
  keywords = {duplicate-citation-key,nosource}
}

@book{kingman1992poisson,
  title = {Poisson Processes},
  author = {Kingman, J.F.C.},
  year = {1983},
  volume = {3},
  publisher = {Oxford university press},
  keywords = {duplicate-citation-key,nosource}
}

@article{kinneyFixedRandomEffects2007,
  title = {Fixed and {{Random Effects Selection}} in {{Linear}} and {{Logistic Models}}},
  author = {Kinney, Satkartar K. and Dunson, David B.},
  year = {2007},
  journal = {Biometrics},
  volume = {63},
  number = {3},
  pages = {690--698},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2007.00771.x},
  urldate = {2022-03-09},
  abstract = {We address the problem of selecting which variables should be included in the fixed and random components of logistic mixed effects models for correlated data. A fully Bayesian variable selection is implemented using a stochastic search Gibbs sampler to estimate the exact model-averaged posterior distribution. This approach automatically identifies subsets of predictors having nonzero fixed effect coefficients or nonzero random effects variance, while allowing uncertainty in the model selection process. Default priors are proposed for the variance components and an efficient parameter expansion Gibbs sampler is developed for posterior computation. The approach is illustrated using simulated data and an epidemiologic example.},
  langid = {english},
  keywords = {Bayesian model selection,Logistic regression,Mixed effects model,Model averaging,Parameter expansion,Random effects,Variable selection,Variance components test},
  file = {/home/gkonkamking/Zotero/storage/DVI6VSJZ/Kinney and Dunson - 2007 - Fixed and Random Effects Selection in Linear and L.pdf}
}

@article{kitagawa1987non,
  title = {Non-Gaussian State---{{Space}} Modeling of Nonstationary Time Series},
  author = {Kitagawa, Genshiro},
  year = {1987},
  journal = {Journal of the American statistical association},
  volume = {82},
  number = {400},
  pages = {1032--1041},
  publisher = {Taylor \& Francis},
  keywords = {nosource}
}

@article{kitagawa1994two,
  title = {The Two-Filter Formula for Smoothing and an Implementation of the {{Gaussian-sum}} Smoother},
  author = {Kitagawa, Genshiro},
  year = {1994},
  journal = {Annals of the Institute of Statistical Mathematics},
  volume = {46},
  number = {4},
  pages = {605--623},
  publisher = {Springer},
  keywords = {nosource}
}

@article{klanjvcic2021identifying,
  title = {Identifying Urban Features for Vulnerable Road User Safety in {{Europe}}},
  author = {Klanj{\v c}i{\'c}, Marina and Gauvin, Laetitia and Tizzoni, Michele and Szell, Michael},
  year = {2021},
  publisher = {SocArXiv},
  doi = {10.31235/osf.io/89cyu},
  keywords = {nosource}
}

@book{klein2013survival,
  title = {Survival Analysis: State of the Art},
  author = {Klein, John P and Goel, Prem},
  year = {2013},
  volume = {211},
  publisher = {Springer Science \& Business Media},
  isbn = {978-90-481-4133-3},
  keywords = {nosource}
}

@article{kleinInvestigatingVariationReplicability2014,
  ids = {kleinInvestigatingVariationReplicability2014a},
  title = {Investigating {{Variation}} in {{Replicability}}},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and {van `t Veer}, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  year = {2014},
  month = jan,
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {142--152},
  publisher = {Hogrefe Publishing},
  issn = {1864-9335},
  doi = {10.1027/1864-9335/a000178},
  urldate = {2020-04-14},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect -- imagined contact reducing prejudice -- showed weak support for replicability. And two effects -- flag priming influencing conservatism and currency priming influencing system justification -- did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  file = {/home/gkonkamking/Zotero/storage/7Q8WG2LS/Klein et al. - 2014 - Investigating Variation in Replicability.pdf;/home/gkonkamking/Zotero/storage/AA8YYLA3/Klein et al. - 2014 - Investigating Variation in Replicability.pdf}
}

@article{Kleinman2006,
  title = {A Maximum Likelihood Framework for Protein Design.},
  author = {Kleinman, Claudia L and Rodrigue, Nicolas and Bonnard, C{\'e}cile and Philippe, Herv{\'e} and Lartillot, Nicolas},
  year = {2006},
  month = jan,
  journal = {BMC bioinformatics},
  volume = {7},
  number = {1},
  pages = {326},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-7-326},
  abstract = {BACKGROUND: The aim of protein design is to predict amino-acid sequences compatible with a given target structure. Traditionally envisioned as a purely thermodynamic question, this problem can also be understood in a wider context, where additional constraints are captured by learning the sequence patterns displayed by natural proteins of known conformation. In this latter perspective, however, we still need a theoretical formalization of the question, leading to general and efficient learning methods, and allowing for the selection of fast and accurate objective functions quantifying sequence/structure compatibility.{\textbackslash}n{\textbackslash}nRESULTS: We propose a formulation of the protein design problem in terms of model-based statistical inference. Our framework uses the maximum likelihood principle to optimize the unknown parameters of a statistical potential, which we call an inverse potential to contrast with classical potentials used for structure prediction. We propose an implementation based on Markov chain Monte Carlo, in which the likelihood is maximized by gradient descent and is numerically estimated by thermodynamic integration. The fit of the models is evaluated by cross-validation. We apply this to a simple pairwise contact potential, supplemented with a solvent-accessibility term, and show that the resulting models have a better predictive power than currently available pairwise potentials. Furthermore, the model comparison method presented here allows one to measure the relative contribution of each component of the potential, and to choose the optimal number of accessibility classes, which turns out to be much higher than classically considered.{\textbackslash}n{\textbackslash}nCONCLUSION: Altogether, this reformulation makes it possible to test a wide diversity of models, using different forms of potentials, or accounting for other factors than just the constraint of thermodynamic stability. Ultimately, such model-based statistical analyses may help to understand the forces shaping protein sequences, and driving their evolution.},
  isbn = {1471-2105},
  pmid = {16808841},
  keywords = {Amino Acid Sequence,Binding Sites,Computer Simulation,Drug Delivery Systems,Drug Delivery Systems: methods,Drug Design,Likelihood Functions,Models,Molecular,Molecular Sequence Data,nosource,Protein,Protein Binding,Protein Engineering,Protein Engineering: methods,Protein: methods,Proteins,Proteins: chemistry,Proteins: classification,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Statistical}
}

@article{kleinman2010statistical,
  title = {Statistical Potentials for Improved Structurally Constrained Evolutionary Models},
  author = {Kleinman, Claudia L. and Rodrigue, Nicolas and Lartillot, Nicolas and Philippe, Herv??},
  year = {2010},
  journal = {Molecular Biology and Evolution},
  volume = {27},
  number = {7},
  pages = {1546--1560},
  publisher = {SMBE},
  issn = {15371719},
  doi = {10.1093/molbev/msq047},
  abstract = {Assessing the influence of three-dimensional protein structure on sequence evolution is a difficult task, mainly because of the assumption of independence between sites required by probabilistic phylogenetic methods. Recently, models that include an explicit treatment of protein structure and site interdependencies have been developed: a statistical potential (an energy-like scoring system for sequence-structure compatibility) is used to evaluate the probability of fixation of a given mutation, assuming a coarse-grained protein structure that is constant through evolution. Yet, due to the novelty of these models and the small degree of overlap between the fields of structural and evolutionary biology, only simple representations of protein structure have been used so far. In this work, we present new forms of statistical potentials using a probabilistic framework recently developed for evolutionary studies. Terms related to pairwise distance interactions, torsion angles, solvent accessibility, and flexibility of the residues are included in the potentials, so as to study the effects of the main factors known to influence protein structure. The new potentials, with a more detailed representation of the protein structure, yield a better fit than the previously used scoring functions, with pairwise interactions contributing to more than half of this improvement. In a phylogenetic context, however, the structurally constrained models are still outperformed by some of the available site-independent models in terms of fit, possibly indicating that alternatives to coarse-grained statistical potentials should be explored in order to better model structural constraints.},
  isbn = {0737-4038},
  pmid = {20159780},
  keywords = {Bayes factor,Maximum likelihood,Molecular evolution,nosource,Protein structure,Statistical potentials}
}

@article{kleinManyLabs22018,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability Across Samples}} and {{Settings}}},
  shorttitle = {Many {{Labs}} 2},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and {de Bruijn}, Maaike and De Schutter, Leander and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\AA}se H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\dj}edovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Lee Nichols, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'a}squez- Echeverr{\'i}a}, Alejandro and Ann Vaughn, Leigh and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {4},
  pages = {443--490},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918810225},
  urldate = {2020-04-16},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {$<$} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {$<$} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({$<$} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/LVGAHKK9/Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf}
}

@article{knightsBayesianCommunitywideCultureindependent2011,
  title = {Bayesian Community-Wide Culture-Independent Microbial Source Tracking},
  author = {Knights, Dan and Kuczynski, Justin and Charlson, Emily S. and Zaneveld, Jesse and Mozer, Michael C. and Collman, Ronald G. and Bushman, Frederic D. and Knight, Rob and Kelley, Scott T.},
  year = {2011},
  month = jul,
  journal = {Nature methods},
  volume = {8},
  number = {9},
  pages = {761--763},
  issn = {1548-7091},
  doi = {10.1038/nmeth.1650},
  urldate = {2024-01-24},
  abstract = {Contamination is a critical issue in high-throughput metagenomic studies, yet progress towards a comprehensive solution has been limited. We present SourceTracker, a Bayesian approach to estimating the proportion of a novel community that comes from a set of source environments. We apply SourceTracker to new microbial surveys from neonatal intensive care units (NICUs), offices, and molecular biology laboratories, and provide a database of known contaminants for future testing.},
  pmcid = {PMC3791591},
  pmid = {21765408},
  file = {/home/gkonkamking/pCloudDrive/papers/Knights et al_2011_Bayesian community-wide culture-independent microbial source tracking.pdf}
}

@article{knobeIntentionalActionFolk2003,
  title = {Intentional Action in Folk Psychology: {{An}} Experimental Investigation},
  shorttitle = {Intentional Action in Folk Psychology},
  author = {Knobe, Joshua},
  year = {2003},
  month = jun,
  journal = {Philosophical Psychology},
  volume = {16},
  number = {2},
  pages = {309--324},
  publisher = {Routledge},
  issn = {0951-5089},
  doi = {10.1080/09515080307771},
  urldate = {2020-04-16},
  abstract = {Four experiments examined people's folk-psychological concept of intentional action. The chief question was whether or not evaluative considerations--considerations of good and bad, right and wrong, praise and blame--played any role in that concept. The results indicated that the moral qualities of a behavior strongly influence people's judgments as to whether or not that behavior should be considered "intentional." After eliminating a number of alternative explanations, the author concludes that this effect is best explained by the hypothesis that evaluative considerations do play some role in people's concept of intentional action.},
  keywords = {nosource}
}

@article{knobeIntentionalActionSide2003,
  ids = {knobeIntentionalActionSide2003a},
  title = {Intentional Action and Side Effects in Ordinary Language},
  author = {Knobe, Joshua},
  year = {2003},
  month = jul,
  journal = {Analysis},
  volume = {63},
  number = {3},
  pages = {190--194},
  publisher = {Oxford Academic},
  issn = {0003-2638},
  doi = {10.1093/analys/63.3.190},
  urldate = {2020-04-16},
  abstract = {Joshua Knobe;  Intentional action and side effects in ordinary language, Analysis, Volume 63, Issue 3, 1 July 2003, Pages 190--194, https://doi.org/10.1093/analy},
  langid = {english},
  keywords = {nosource}
}

@article{knobeIntentionIntentionalAction2004,
  title = {Intention, Intentional Action and Moral Considerations},
  author = {Knobe, Joshua},
  year = {2004},
  month = apr,
  journal = {Analysis},
  volume = {64},
  number = {2},
  pages = {181--187},
  publisher = {Oxford Academic},
  issn = {0003-2638},
  doi = {10.1093/analys/64.2.181},
  urldate = {2020-04-16},
  abstract = {Joshua Knobe;  Intention, intentional action and moral considerations, Analysis, Volume 64, Issue 2, 1 April 2004, Pages 181--187, https://doi.org/10.1093/analys},
  langid = {english},
  keywords = {nosource}
}

@article{knoll2004jacobian,
  title = {Jacobian-Free {{Newton}}--{{Krylov}} Methods: A Survey of Approaches and Applications},
  author = {Knoll, Dana A and Keyes, David E},
  year = {2004},
  journal = {Journal of Computational Physics},
  volume = {193},
  number = {2},
  pages = {357--397},
  publisher = {Elsevier},
  keywords = {nosource}
}

@incollection{koller2009probabilistic,
  title = {Probabilistic Graphical Models: {{Principles}} and Techniques},
  booktitle = {Igarss 2014},
  author = {Koller, Daphne and Friedman, Nir},
  year = {2014},
  number = {1},
  eprint = {1011.1669v3},
  pages = {1--5},
  publisher = {MIT press},
  issn = {13514180},
  doi = {10.1007/s13398-014-0173-7.2},
  abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1011.1669v3},
  isbn = {978-0-87421-656-1},
  pmid = {15991970},
  keywords = {high resolution images,nosource,research,risks management,sustainable reconstruction}
}

@article{kolossiatis2013bayesian,
  title = {On {{Bayesian}} Nonparametric Modelling of Two Correlated Distributions},
  author = {Kolossiatis, Michalis and Griffin, Jim E. and Steel, Mark F J},
  year = {2013},
  journal = {Statistics and Computing},
  volume = {23},
  number = {1},
  pages = {1--15},
  publisher = {Springer},
  issn = {09603174},
  doi = {10.1007/s11222-011-9283-7},
  isbn = {1122201192},
  keywords = {Dependent Dirichlet process,Markov chain Monte Carlo,Normalised random measures,nosource,P??lya-urn scheme,Split-merge move}
}

@article{kominskyCausalSuperseding2015,
  title = {Causal Superseding},
  author = {Kominsky, Jonathan F. and Phillips, Jonathan and Gerstenberg, Tobias and Lagnado, David and Knobe, Joshua},
  year = {2015},
  month = apr,
  journal = {Cognition},
  volume = {137},
  pages = {196--209},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2015.01.013},
  urldate = {2020-05-11},
  abstract = {When agents violate norms, they are typically judged to be more of a cause of resulting outcomes. In this paper, we suggest that norm violations also affect the causality attributed to other agents, a phenomenon we refer to as ``causal superseding.'' We propose and test a counterfactual reasoning model of this phenomenon in four experiments. Experiments 1 and 2 provide an initial demonstration of the causal superseding effect and distinguish it from previously studied effects. Experiment 3 shows that this causal superseding effect is dependent on a particular event structure, following a prediction of our counterfactual model. Experiment 4 demonstrates that causal superseding can occur with violations of non-moral norms. We propose a model of the superseding effect based on the idea of counterfactual sufficiency.},
  langid = {english},
  keywords = {Causal reasoning,Counterfactuals,Morality,nosource,Superseding}
}

@article{Kompany-Zareh2012,
  title = {Tucker Core Consistency for Validation of Restricted {{Tucker3}} Models},
  author = {{Kompany-Zareh}, Mohsen and Akhlaghi, Yousef and Bro, Rasmus},
  year = {2012},
  journal = {Analytica Chimica Acta},
  volume = {723},
  pages = {18--26},
  publisher = {Elsevier B.V.},
  issn = {00032670},
  doi = {10.1016/j.aca.2012.02.028},
  abstract = {In Tucker3 analysis of three-way data array obtained from a chemical or biological system, it is sometimes possible to use a priori knowledge about the system to specify what is called a restricted Tucker3 model. Often, the restricted Tucker3 model is characterized by having some elements of the core forced to zero. As a simple example, an F-component PARAFAC model can be seen as a restricted (. F, F, F) Tucker3 model in which only superdiagonal elements of the core are allowed to be nonzero. The core consistency diagnostic was previously introduced by Bro and Kiers for determining the proper number of components in PARAFAC analysis. In the current study, this diagnostic is extended to other restricted Tucker3 models to validate the appropriateness of the applied constraints. The new diagnostic is named Tucker core consistency (. TuckCorCon). When the dimensionality and the pattern of the restricted core is valid, the simple core of restricted Tucker3 model and a corresponding unrestricted core will be similar and in this case the TuckCorCon will be close to maximum (100\%). A simulated chemical equilibrium data set and two experimental data sets were used to evaluate the applicability of the TuckCorCon to decide about the appropriateness of dimensionality and pattern of the core nonzero elements in the restricted Tucker3 models. ?? 2012 Elsevier B.V..},
  pmid = {22444568},
  keywords = {nosource,Rank deficiency,Restricted Tucker3,Three-way data,Tucker core consistency}
}

@article{kon2016bayesian,
  title = {Species Sensitivity Distribution Revisited: A Bayesian Nonparametric Approach},
  author = {Kon Kam King, Guillaume and Arbel, Julyan and Pr{\"u}nster, Igor},
  year = {2019},
  journal = {In preparation},
  keywords = {nosource}
}

@article{konconstructing,
  title = {Constructing Time-Resolved Species Sensitivity Distributions Using a Hierarchical Toxico-Dynamic Model},
  author = {Kon Kam King, Guillaume and {Delignette-Muller}, Marie Laure and Kefford, Ben J. and Piscart, Christophe and Charles, Sandrine},
  year = {2015},
  journal = {Environmental Science and Technology},
  volume = {49},
  number = {20},
  pages = {12465--12473},
  publisher = {ACS Publications},
  issn = {15205851},
  doi = {10.1021/acs.est.5b02142},
  copyright = {All rights reserved},
  file = {/home/gkonkamking/Downloads/KonKamKing2015_SSDTKTD.pdf}
}

@article{KonKamKing2014,
  ids = {kingMOSAICSSDNew2014},
  title = {{{MOSAIC}}\_{{SSD}}: {{A}} New Web Tool for Species Sensitivity Distribution to Include Censored Data by Maximum Likelihood},
  author = {Kon Kam King, Guillaume and Veber, Philippe and Charles, Sandrine and {Delignette-Muller}, Marie Laure},
  year = {2014},
  month = sep,
  journal = {Environmental Toxicology and Chemistry},
  volume = {33},
  number = {9},
  eprint = {24863265},
  eprinttype = {pubmed},
  pages = {2133--2139},
  issn = {15528618},
  doi = {10.1002/etc.2644},
  abstract = {Censored data are seldom taken into account in species sensitivity distribution (SSD) analysis. However, they are found in virtually every dataset and sometimes represent the better part of the data. Stringent recommendations on data quality often entail discarding a lot of these meaningful data, resulting in datasets of reduced size which lack representativeness of any realistic community. However, it is reasonably simple to include censored data in SSD by using an extension of the standard maximum likelihood method. The authors detail this approach based on the use of the R-package fitdistrplus, dedicated to the fit of parametric probability distributions. The authors present the new Web tool MOSAIC\_SSD, that can fit an SSD on datasets containing any type of data, censored or not. The MOSAIC\_SSD Web tool predicts any hazardous concentration and provides bootstrap confidence intervals on the predictions. Finally, the authors illustrate the added value of including censored data in SSD, taking examples from published data.},
  copyright = {All rights reserved},
  pmid = {24863265},
  keywords = {Bootstrap,Confidence interval,Fitdistrplus,Hazardous concentration,Web interface},
  file = {/home/gkonkamking/Downloads/King_et_al-2014-Environmental_Toxicology_and_Chemistry.pdf}
}

@article{KonKamKing2015,
  title = {Hierarchical Modelling of Species Sensitivity Distribution: {{Development}} and Application to the Case of Diatoms Exposed to Several Herbicides},
  author = {Kon Kam King, Guillaume and Larras, Floriane and Charles, Sandrine and {Delignette-Muller}, Marie Laure},
  year = {2015},
  journal = {Ecotoxicology and Environmental Safety},
  volume = {114},
  eprint = {1501.05230v1},
  pages = {212--221},
  publisher = {Elsevier},
  issn = {10902414},
  doi = {10.1016/j.ecoenv.2015.01.022},
  abstract = {The species sensitivity distribution (SSD) is a key tool to assess the ecotoxicological threat of contaminants to biodiversity. For a contaminant, it predicts which concentration is safe for a community of species. Widely used, this approach suffers from several drawbacks: (i) summarizing the sensitivity of each species by a single value entails a loss of valuable information about the other parameters characterizing the concentration-effect curves; (ii) it does not propagate the uncertainty on estimated sensitivities into the SSD; (iii) the hazardous concentration estimated with SSD only indicates the threat to biodiversity, without any insight about a global response of the community related to the measured endpoint. To remedy these drawbacks, we built a global hierarchical model including the concentration-effect model together with the distribution law of the SSD. We revisited the current SSD approach to account for more sources of variability and uncertainty into the prediction than the traditional analysis and to assess a global response for the community. Working within a Bayesian framework, we were able to compute an SSD taking into account the uncertainty from the original raw data. We also developed a quantitative indicator of a global response of the community to the contaminant. We applied this methodology to study the toxicity and the risk of six herbicides to benthic diatoms from Lake Geneva, based on the biomass endpoint. Our approach highlighted a wide variability within the set of diatom species for all the parameters of the concentration-effect model and a potential correlation between them. Remarkably, variability of the shape parameter of the model and correlation had not been considered before. Comparison between the SSD and the global response of the community revealed that protecting 95\% of the species might preserve only 80-86\% of the global response. Finally, propagating the uncertainty on the estimated sensitivity showed that building an SSD on a low level of effect, such as EC10, might be unreasonable as it induces a large uncertainty on the result.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1501.05230v1},
  copyright = {All rights reserved},
  pmid = {25656423},
  keywords = {Bayesian statistics,Diatoms,Ecological risk assessment,Global response,Hazardous concentration,Uncertainty},
  file = {/home/gkonkamking/pCloudDrive/papers/Kon Kam King et al_2015_Hierarchical modelling of species sensitivity distribution.pdf;/home/gkonkamking/Zotero/storage/VSB2Q4ZF/Kon Kam King et al. - 2015 - Hierarchical modelling of species sensitivity dist.pdf}
}

@phdthesis{KonKamKing2015a,
  title = {Revisiting {{Species Sensitivity Distribution}}: Modelling Species Variability for the Protection of Communities},
  author = {Kon Kam King, Guillaume},
  year = {2015},
  school = {Universit{\'e} Claude Bernard - Lyon 1},
  keywords = {nosource}
}

@incollection{KonKamKing2017,
  title = {A Bayesian Nonparametric Approach to Ecological Risk Assessment},
  booktitle = {Bayesian Statistics in Action: {{BAYSM}} 2016, {{Florence}}, {{Italy}}, {{June}} 19-21},
  author = {Kon Kam King, Guillaume and Arbel, Julyan and Pr{\"u}nster, Igor},
  editor = {Argiento, Raffaele and Lanzarone, Ettore and Antoniano Villalobos, Isadora and Mattei, Alessandra},
  year = {2017},
  pages = {151--159},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-54084-9_14},
  copyright = {All rights reserved},
  isbn = {978-3-319-54084-9},
  file = {/home/gkonkamking/Zotero/storage/YK6PSU36/Kon Kam King et al. - 2017 - A bayesian nonparametric approach to ecological ri.pdf}
}

@inproceedings{KonKamKing2018,
  title = {Bayesian Inference for Hidden {{Markov}} Models Using Duality and Approximate Filtering Distributions},
  booktitle = {Book of Short Papers {{SIS}} 2018},
  author = {Kon Kam King, Guillaume and Papaspiliopoulos, Omiros and Ruggiero, Matteo},
  editor = {Abbruzzo, Antonio and Brentari, E and Piacentino, D},
  year = {2018},
  pages = {248--255},
  publisher = {Pearson},
  copyright = {All rights reserved},
  file = {/home/gkonkamking/Downloads/2018-SIS.pdf;/home/gkonkamking/Zotero/storage/M5RXJKJQ/2018-SIS.pdf}
}

@article{konkamking2018bayesian,
  ids = {konkamkingBayesianFunctionalForecasting2019},
  title = {Bayesian Functional Forecasting with Locally-Autoregressive Dependent Processes},
  author = {Kon Kam King, Guillaume and Canale, Antonio and Ruggiero, Matteo},
  year = {2019},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {14},
  number = {4},
  pages = {1121--1141},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975},
  doi = {10.1214/18-BA1140},
  copyright = {All rights reserved},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/4227D2VZ/Kon Kam King et al. - 2019 - Bayesian Functional Forecasting with Locally-Autor.pdf}
}

@article{konkamkingApproximateFilteringDiscrete2024,
  title = {Approximate Filtering via Discrete Dual Processes},
  author = {Kon Kam King, Guillaume and Pandolfi, Andrea and Piretto, Marco and Ruggiero, Matteo},
  year = {2024},
  month = feb,
  journal = {Stochastic Processes and their Applications},
  volume = {168},
  pages = {104268},
  issn = {0304-4149},
  doi = {10.1016/j.spa.2023.104268},
  urldate = {2023-11-30},
  abstract = {We consider the task of filtering a dynamic parameter evolving as a diffusion process, given data collected at discrete times from a likelihood which is conjugate to the reversible law of the diffusion, when a generic dual process on a discrete state space is available. Recently, it was shown that duality with respect to a death-like process implies that the filtering distributions are finite mixtures, making exact filtering and smoothing feasible through recursive algorithms with polynomial complexity in the number of observations. Here we provide general results for the case where the dual is a regular jump continuous-time Markov chain on a discrete state space, which typically leads to filtering distribution given by countable mixtures indexed by the dual process state space. We investigate the performance of several approximation strategies on two hidden Markov models driven by Cox--Ingersoll--Ross and Wright--Fisher diffusions, which admit duals of birth-and-death type, and compare them with the available exact strategies based on death-type duals and with bootstrap particle filtering on the diffusion state space as a general benchmark.},
  keywords = {Bayesian inference,Diffusion,Duality,Hidden Markov models,Particle filtering,Smoothing},
  file = {/home/gkonkamking/pCloudDrive/papers/Kon Kam King et al_2024_Approximate filtering via discrete dual processes.pdf}
}

@article{konkamkingExactInferenceClass2021,
  title = {Exact Inference for a Class of Hidden {{Markov}} Models on General State Spaces},
  author = {Kon Kam King, Guillaume and Papaspiliopoulos, Omiros and Ruggiero, Matteo},
  year = {2021},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {15},
  number = {1},
  pages = {2832--2875},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1935-7524, 1935-7524},
  doi = {10.1214/21-EJS1841},
  urldate = {2021-05-19},
  abstract = {Exact inference for hidden Markov models requires the evaluation of all distributions of interest -- filtering, prediction, smoothing and likelihood -- with a finite computational effort. This article provides sufficient conditions for exact inference for a class of hidden Markov models on general state spaces given a set of discretely collected indirect observations linked non linearly to the signal, and a set of practical algorithms for inference. The conditions we obtain are concerned with the existence of a certain type of dual process, which is an auxiliary process embedded in the time reversal of the signal, that in turn allows to represent the distributions and functions of interest as finite mixtures of elementary densities or products thereof. We describe explicitly how to update recursively the parameters involved, yielding qualitatively similar results to those obtained with Baum--Welch filters on finite state spaces. We then provide practical algorithms for implementing the recursions, as well as approximations thereof via an informed pruning of the mixtures, and we show superior performance to particle filters both in accuracy and computational efficiency. The code for optimal filtering, smoothing and parameter inference is made available in the Julia package DualOptimalFiltering.},
  keywords = {Cox--Ingersoll--Ross,diffusion process,Hidden Markov models,optimal filtering,smoothing,Wright--Fisher},
  file = {/home/gkonkamking/pCloudDrive/papers/Kon Kam King et al_2021_Exact inference for a class of hidden Markov models on general state spaces.pdf}
}

@incollection{konrathBayesianSmoothingShrinkage2013,
  title = {Bayesian {{Smoothing}}, {{Shrinkage}} and {{Variable Selection}} in {{Hazard Regression}}},
  booktitle = {Robustness and {{Complex Data Structures}}: {{Festschrift}} in {{Honour}} of {{Ursula Gather}}},
  author = {Konrath, Susanne and Fahrmeir, Ludwig and Kneib, Thomas},
  editor = {Becker, Claudia and Fried, Roland and Kuhnt, Sonja},
  year = {2013},
  pages = {149--170},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35494-6_10},
  urldate = {2020-04-10},
  abstract = {This contribution deals with a unified Bayesian framework to combine regularization of high-dimensional linear covariate effects and semiparametric smoothing of nonlinear functional effects for a broad class of hazard regression models. While penalized splines with conditionally Gaussian smoothness priors form the basis for estimating nonparametric and flexible time-varying effects, regularization of high-dimensional covariate vectors is based on scale mixture of normals priors, including among others the Bayesian ridge and lasso as well as a spike and slab prior for shrinkage variances. This class of priors allows us to keep a conditionally Gaussian prior for regression coefficients on the predictor stage of the model but introduces suitable mixture distributions for the Gaussian variance to achieve regularization. The scale mixture property allows to device general and adaptive Markov chain Monte Carlo simulation algorithms for fitting a variety of hazard regression models. In particular, unifying Metropolis-Hastings-algorithms based on iteratively weighted least squares proposals can be employed both for regularization and penalized semiparametric function estimation. We demonstrate performance through simulation studies and an application to data on acute myeloid leukemia (AML) survival.},
  isbn = {978-3-642-35494-6},
  langid = {english},
  keywords = {Baseline Hazard,Full Likelihood,Metropolis Hastings,nosource,Partial Likelihood,Shrinkage Parameter}
}

@article{Kooijman1987,
  ids = {Kooijman1987a},
  title = {A Safety Factor for {{LC}} 50 Values Allowing for Differences in Sensitivity among Species},
  author = {Kooijman, S},
  year = {1987},
  journal = {Water Research},
  volume = {21},
  number = {3},
  pages = {269--276},
  keywords = {---safety factor,lcs0,log-logistic distribution,most sensitive species,nosource,number of best species,number of community species}
}

@article{Kooijman1996,
  title = {No-Effect Concentration as a Basis for Ecological Risk Assessment.},
  author = {Kooijman, S A and Bedaux, Jacques J. M. and Slob, W},
  year = {1996},
  month = aug,
  journal = {Risk analysis : an official publication of the Society for Risk Analysis},
  volume = {16},
  number = {4},
  eprint = {8819337},
  eprinttype = {pubmed},
  pages = {445--447},
  issn = {0272-4332},
  isbn = {0272-4332 (Print){\textbackslash}r0272-4332 (Linking)},
  pmid = {8819337},
  keywords = {Animals,Ecosystem,Environmental Pollutants,Environmental Pollutants: toxicity,No-Observed-Adverse-Effect Level,nosource,Risk Assessment}
}

@book{kooijman1996analysis_chap,
  title = {The Analysis of Aquatic Toxicity Data},
  author = {Kooijman, {\relax SALM} and Bedaux, Jacques J. M.},
  year = {1996},
  publisher = {VU University press Amsterdam},
  address = {Amsterdam},
  chapter = {3},
  isbn = {90-5383-477-X},
  keywords = {nosource}
}

@incollection{kooijman2010dynamic,
  title = {Dynamic {{Energy Budget}} Theory for Metabolic Organisation : {{Summary}} of Concepts of the Third Edition},
  booktitle = {Water},
  author = {Kooijman, S. a. L. M.},
  year = {2010},
  volume = {365},
  pages = {68},
  publisher = {Cambridge university press},
  issn = {14712970},
  doi = {10.1098/rstb.2010.0167},
  abstract = {Dynamic energy budget (DEB) theory offers a perspective on population ecology whose starting point is energy utilization by, and homeostasis within, individual organisms. It is natural to ask what it adds to the existing large body of individual-based ecological theory. We approach this question pragmatically-through detailed study of the individual physiology and population dynamics of the zooplankter Daphnia and its algal food. Standard DEB theory uses several state variables to characterize the state of an individual organism, thereby making the transition to population dynamics technically challenging, while ecologists demand maximally simple models that can be used in multi-scale modelling. We demonstrate that simpler representations of individual bioenergetics with a single state variable (size), and two life stages (juveniles and adults), contain sufficient detail on mass and energy budgets to yield good fits to data on growth, maturation and reproduction of individual Daphnia in response to food availability. The same simple representations of bioenergetics describe some features of Daphnia mortality, including enhanced mortality at low food that is not explicitly incorporated in the standard DEB model. Size-structured, population models incorporating this additional mortality component resolve some long-standing questions on stability and population cycles in Daphnia. We conclude that a bioenergetic model serving solely as a 'regression' connecting organismal performance to the history of its environment can rest on simpler representations than those of standard DEB. But there are associated costs with such pragmatism, notably loss of connection to theory describing interspecific variation in physiological rates. The latter is an important issue, as the type of detailed study reported here can only be performed for a handful of species.},
  isbn = {978-0-521-13191-9},
  pmid = {20921052},
  keywords = {nosource}
}

@article{Kooijman2011,
  title = {A {{Bicycle Can Be Self-Stable Without Gyroscopic}} or {{Caster Effects}}},
  author = {Kooijman, J D G and Meijaard, J P and Papadopoulos, Jim M and Ruina, Andy and Schwab, A L},
  year = {2011},
  journal = {Science},
  volume = {332},
  number = {5},
  pages = {339--342},
  issn = {0036-8075},
  doi = {10.1126/science.1201959},
  abstract = {A riderless bicycle can automatically steer itself so as to recover from falls. The common view is that this self-steering is caused by gyroscopic precession of the front wheel, or by the wheel contact trailing like a caster behind the steer axis. We show that neither effect is necessary for self-stability. Using linearized stability calculations as a guide, we built a bicycle with extra counter-rotating wheels (canceling the wheel spin angular momentum) and with its front-wheel ground-contact forward of the steer axis (making the trailing distance negative). When laterally disturbed from rolling straight, this bicycle automatically recovers to upright travel. Our results show that various design variables, like the front mass location and the steer axis tilt, contribute to stability in complex interacting ways.},
  isbn = {0036-8075},
  pmid = {21493856},
  keywords = {nosource}
}

@article{KOOP1996119,
  title = {Impulse Response Analysis in Nonlinear Multivariate Models},
  author = {Koop, Gary and Pesaran, M.Hashem and Potter, Simon M},
  year = {1996},
  journal = {Journal of Econometrics},
  volume = {74},
  number = {1},
  pages = {119--147},
  issn = {0304-4076},
  doi = {10.1016/0304-4076(95)01753-4},
  abstract = {This paper presents a unified approach to impulse response analysis which can be used for both linear and nonlinear multivariate models. After discussing the advantages and disadvantages of traditional impulse response functions for nonlinear models, we introduce the concept of a generalized impulse response function which, we argue, is applicable to both linear and nonlinear models. We develop measures of shock persistence and asymmetric effects of shocks derived from the generalized impulse response function. We illustrate the use of these measures for a nonlinear bivariate model of US output and the unemployment rate.},
  keywords = {Impulse response functions,Nonlinear vector autoregressions,nosource,Persistence,Threshold autoregressive models}
}

@article{korwar1973contributions,
  title = {Contributions to the Theory of Dirichlet Processes},
  author = {Korwar, Ramesh and Hollander, Myles},
  year = {1973},
  journal = {The Annals of Probability},
  volume = {1},
  number = {4},
  eprint = {2959441\{\%\}5Cnpapers2://publication/uuid/0A99B181-0755-41BD-952E-68AA65B32DE7},
  eprinttype = {jstor},
  pages = {705--711},
  publisher = {JSTOR},
  issn = {0091-1798},
  abstract = {Consider a sample \$X\_1, , X\_n\$ from a Dirichlet process \$P\$ on an uncountable standard Borel space \$(\{X\}, \{A\})\$ where the parameter \$\$ of the process is assumed to be non-atomic and \$\$-additive. Let \$D(n)\$ be the number of distinct observations in the sample and denote these distinct observations by \$Y\_1, , Y\_\{D(n)\}\$. Our main results are (1) \$D(n)/n \_\{\{a.s.\}\} (\{X\}), n \$, and (2) given \$D(n), Y\_1, , Y\_\{D(n)\}\$ are independent and identically distributed according to \$()/(\{X\})\$. Result (1) shows that \$(\{X\})\$ can be consistently estimated from the sample, and result (2) leads to a strong law for \${\textasciicircum}\{D(n)\}\_\{i=1\} Y\_i/D(n)\$.},
  keywords = {nosource}
}

@article{kottas2002nonparametric,
  title = {A Nonparametric {{Bayesian}} Modeling Approach for Cytogenetic Dosimetry},
  author = {Kottas, Athanasios and Branco, M{\'a}rcia D and Gelfand, Alan E},
  year = {2002},
  journal = {Biometrics},
  volume = {58},
  number = {3},
  pages = {593--600},
  publisher = {Wiley Online Library},
  keywords = {nosource}
}

@article{KOUTMOS2018122,
  title = {Return and Volatility Spillovers among Cryptocurrencies},
  author = {Koutmos, Dimitrios},
  year = {2018},
  journal = {Economics Letters},
  volume = {173},
  pages = {122--127},
  issn = {0165-1765},
  doi = {10.1016/j.econlet.2018.10.004},
  abstract = {This paper measures interdependencies among 18 major cryptocurrencies and shows that (i) Bitcoin is the dominant contributor of return and volatility spillovers among all the sampled cryptocurrencies; (ii) return and volatility spillovers have risen steadily over time; (iii) there are 'spikes' in spillovers during major news events regarding cryptocurrencies. These findings suggest growing interdependence among cryptocurrencies and, by extension, a higher degree of contagion risk. It may be the case that cryptocurrencies are becoming more integrated, albeit this makes for interesting future empirical testing. In addition, the time-varying nature of spillovers reveals a certain dimension of uncertainty regarding the future of these digital currencies.},
  keywords = {Bitcoin,Cryptocurrencies,nosource,Spillovers,Variance decompositions,Vector autoregression}
}

@article{Koyama1996a,
  ids = {Koyama1996},
  title = {Vertebral Deformity Susceptibilities of Marine Fishes Exposed to Herbicide.},
  author = {Koyama, J},
  year = {1996},
  month = apr,
  journal = {Bulletin of environmental contamination and toxicology},
  volume = {56},
  number = {4},
  eprint = {8645926},
  eprinttype = {pubmed},
  pages = {655--62},
  issn = {0007-4861},
  pmid = {8645926},
  keywords = {Animals,Chromatography,Dislocations,Dislocations: chemically induced,Dislocations: veterinary,Fishes,Fishes: injuries,Gas,Herbicides,Herbicides: administration \& dosage,Herbicides: toxicity,Larva,Larva: drug effects,nosource,Seawater,Species Specificity,Spinal Fractures,Spinal Fractures: chemically induced,Spinal Fractures: veterinary,Spine,Spine: drug effects,Trifluralin,Trifluralin: administration \& dosage,Trifluralin: toxicity}
}

@article{Kribs-Zaleta2011,
  title = {Modeling Nosocomial Transmission of Rotavirus in Pediatric Wards},
  author = {{Kribs-Zaleta}, Christopher M. and Jusot, Jean Fran??ois and Vanhems, Philippe and Charles, Sandrine},
  year = {2011},
  month = jul,
  journal = {Bulletin of Mathematical Biology},
  volume = {73},
  number = {7},
  eprint = {20811781},
  eprinttype = {pubmed},
  pages = {1413--1442},
  issn = {00928240},
  doi = {10.1007/s11538-010-9570-z},
  abstract = {Nosocomial transmission of viral and bacterial infections is a major problem worldwide, affecting millions of patients (and causing hundreds of thousands of deaths) per year. Rotavirus infections affect most children worldwide at least once before age five. We present here deterministic and stochastic models for the transmission of rotavirus in a pediatric hospital ward and draw on published data to compare the efficacy of several possible control measures in reducing the number of infections during a 90-day outbreak, including cohorting, changes in healthcare worker-patient ratio, improving compliance with preventive hygiene measures, and vaccination. Although recently approved vaccines have potential to curtail most nosocomial rotavirus transmission in the future, even short-term improvement in preventive hygiene compliance following contact with symptomatic patients may significantly limit transmission as well, and remains an important control measure, especially where resources are limited.},
  isbn = {0092-8240},
  pmid = {20811781},
  keywords = {Dynamical systems,Nosocomial infections,nosource,Preventive measures,Rotavirus,Stochastic model}
}

@article{krollEstimationMomentsQuantiles1996,
  title = {Estimation of Moments and Quantiles Using Censored Data},
  author = {Kroll, Charles N and Stedinger, Jery R},
  year = {1996},
  journal = {Water Resources Research},
  volume = {32},
  number = {4},
  pages = {1005--1012},
  issn = {00431397},
  doi = {10.1029/95WR03294},
  abstract = {Censored data sets are often encountered in water quality investigations and streamflow analyses. A Monte Carlo analysis examined the performance of three techniques for estimating the moments and quantiles of a distribution using censored data sets. These techniques include a lognormal maximum likelihood estimator (MLE), a log-probability plot regression estimator, and a new log-partial probability-weighted moment estimator. Data sets were generated from a number of distributions commonly used to describe water quality and water quantity variables. A ``robust'' fill-in method, which circumvents transformation bias in the real space moments, was implemented with all three estimation techniques to obtain a complete sample for computation of the sample mean and standard deviation. Regardless of the underlying distribution, the MLE generally performed as well as or better than the other estimators, though the moment and quantile estimators using all three techniques had comparable log-space root mean square errors (rmse) for censoring at or below the 20th percentile for samples sizes of n=10, the 40th percentile for n=25, and the 60th percentile for n=50. Comparison of the log-space rmse and real-space rmse indicated that a log-space rmse was a better overall metric of estimator precision.},
  isbn = {1944-7973},
  keywords = {1812 Drought,1860 Streamflow,1871 Surface water quality},
  file = {/home/gkonkamking/Zotero/storage/PXVJGAGD/Kroll and Stedinger - 1996 - Estimation of Moments and Quantiles using Censored.pdf}
}

@article{kruijer_spectral,
  title = {Adaptive Bayesian Estimation of a Spectral Density},
  author = {Rousseau, Judith and Kruijer, Willem},
  year = {2011},
  journal = {Preprint},
  keywords = {nosource}
}

@article{kruijer:vdv,
  title = {Posterior Convergence Rates for {{Dirichlet}} Mixtures of Beta Densities},
  author = {Kruijer, Willem and {van der Vaart}, Aad},
  year = {2008},
  journal = {Journal of Statistical Planning and Inference},
  volume = {138},
  number = {7},
  eprint = {0708.1885},
  pages = {1981--1992},
  publisher = {Elsevier},
  issn = {03783758},
  doi = {10.1016/j.jspi.2007.07.012},
  abstract = {We consider Bayesian density estimation for compactly supported densities using Bernstein mixtures of beta-densities equipped with a Dirichlet prior on the distribution function. We derive the rate of convergence for ??-smooth densities for 0 {$<$} ?? ??? 2 and show that a faster rate of convergence can be obtained by using fewer terms in the mixtures than proposed before. The Bayesian procedure adapts to the unknown value of ??. The modified Bayesian procedure is rate-optimal if ?? is at most one. This result can be extended to two dimensions. ?? 2008 Elsevier B.V. All rights reserved.},
  archiveprefix = {arXiv},
  arxivid = {0708.1885},
  keywords = {Bayesian mixtures,Bernstein approximation,Convergence rates,nosource,Posterior distribution}
}

@article{kruijer2010adaptive,
  title = {Adaptive {{Bayesian}} Density Estimation with Location-Scale Mixtures},
  author = {Kruijer, Willem and Rousseau, Judith and {van der Vaart}, Aad},
  year = {2010},
  journal = {Electronic Journal of Statistics},
  volume = {4},
  number = {0},
  pages = {1225--1257},
  publisher = {Institute of Mathematical Statistics},
  issn = {1935-7524},
  doi = {10.1214/10-EJS584},
  keywords = {and phrases,bayesian,convergence rates,density estimation,location-scale mixtures,nonparametric density estimation,nosource,rate-adaptive density estimation}
}

@inproceedings{krumm2006predestination,
  title = {Predestination: {{Inferring}} Destinations from Partial Trajectories},
  booktitle = {International Conference on Ubiquitous Computing},
  author = {Krumm, John and Horvitz, Eric},
  year = {2006},
  pages = {243--260},
  organization = {Springer},
  keywords = {nosource}
}

@inproceedings{kuangSymmetricNonnegativeMatrix2012,
  title = {Symmetric {{Nonnegative Matrix Factorization}} for {{Graph Clustering}}},
  booktitle = {Proceedings of the 2012 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Kuang, Da and Ding, Chris and Park, Haesun},
  year = {2012},
  month = apr,
  pages = {106--117},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972825.10},
  urldate = {2022-12-14},
  isbn = {978-1-61197-232-0 978-1-61197-282-5},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/S7JSNKYL/Kuang et al. - 2012 - Symmetric Nonnegative Matrix Factorization for Gra.pdf}
}

@article{kucukelbirAutomaticVariationalInference,
  title = {Automatic {{Variational Inference}} in {{Stan}}},
  author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/Zotero/storage/MT8GLX9L/Kucukelbir et al. - Automatic Variational Inference in Stan.pdf}
}

@article{kuziemkoLastPlaceAversionEvidence2014,
  title = {``{{Last-Place Aversion}}'': {{Evidence}} and {{Redistributive Implications}}},
  shorttitle = {``{{Last-Place Aversion}}''},
  author = {Kuziemko, Ilyana and Buell, Ryan W. and Reich, Taly and Norton, Michael I.},
  year = {2014},
  month = feb,
  journal = {The Quarterly Journal of Economics},
  volume = {129},
  number = {1},
  pages = {105--149},
  publisher = {Oxford Academic},
  issn = {0033-5533},
  doi = {10.1093/qje/qjt035},
  urldate = {2020-04-16},
  abstract = {Abstract.   We present evidence from laboratory experiments showing that individuals are ``last-place averse.'' Participants choose gambles with the potential to},
  langid = {english},
  keywords = {nosource}
}

@article{Kwok2007,
  title = {Comparison of Tropical and Temperate Freshwater Animal Species' Acute Sensitivities to Chemicals: Implications for Deriving Safe Extrapolation Factors.},
  author = {Kwok, Kevin W H and Leung, Kenneth M Y and Lui, Gilbert S G and Chu, S Vincent K H and Lam, Paul K S and Morritt, David and Maltby, Lorraine and Brock, Theo C M and {Van den Brink}, Paul J and Warne, Michael St J and Crane, Mark},
  year = {2007},
  journal = {Integrated environmental assessment and management},
  volume = {3},
  number = {1},
  pages = {49--67},
  issn = {1551-3777},
  doi = {10.1897/1551-3793(2007)3[49:cotatf]2.0.co;2},
  abstract = {Toxicity data for tropical species are often lacking for ecological risk assessment. Consequently, tropical and subtropical countries use water quality criteria (WQC) derived from temperate species (e.g., United States, Canada, or Europe) to assess ecological risks in their aquatic systems, leaving an unknown margin of uncertainty. To address this issue, we use species sensitivity distributions of freshwater animal species to determine whether temperate datasets are adequately protective of tropical species assemblages for 18 chemical substances. The results indicate that the relative sensitivities of tropical and temperate species are noticeably different for some of these chemicals. For most metals, temperate species tend to be more sensitive than their tropical counterparts. However, for un-ionized ammonia, phenol, and some pesticides (e.g., chlorpyrifos), tropical species are probably more sensitive. On the basis of the results from objective comparisons of the ratio between temperate and tropical hazardous concentration values for 10\% of species, or the 90\% protection level, we recommend that an extrapolation factor of 10 should be applied when such surrogate temperate WQCs are used for tropical or subtropical regions and a priori knowledge on the sensitivity of tropical species is very limited or not available.},
  isbn = {15513777 (ISSN)},
  pmid = {17283595},
  keywords = {environmental quality standard,nosource,temperature},
  file = {/home/gkonkamking/pCloudDrive/papers/Kwok et al_2007_Comparison of tropical and temperate freshwater animal species' acute.pdf}
}

@article{kypraiosTutorialIntroductionBayesian2017,
  title = {A Tutorial Introduction to {{Bayesian}} Inference for Stochastic Epidemic Models Using {{Approximate Bayesian Computation}}},
  author = {Kypraios, Theodore and Neal, Peter and Prangle, Dennis},
  year = {2017},
  month = may,
  journal = {Mathematical Biosciences},
  volume = {287},
  pages = {42--53},
  issn = {00255564},
  doi = {10.1016/j.mbs.2016.07.001},
  urldate = {2021-03-23},
  abstract = {Likelihood-based inference for disease outbreak data can be very challenging due to the inherent dependence of the data and the fact that they are usually incomplete. In this paper we review recent Approximate Bayesian Computation (ABC) methods for the analysis of such data by fitting to them stochastic epidemic models without having to calculate the likelihood of the observed data. We consider both non-temporal and temporal-data and illustrate the methods with a number of examples featuring different models and datasets. In addition, we present extensions to existing algorithms which are easy to implement and provide an improvement to the existing methodology. Finally, we provide R code to implement the algorithms presented in the paper.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/8ZB5RNR6/Kypraios et al. - 2017 - A tutorial introduction to Bayesian inference for .pdf}
}

@inproceedings{L13,
  title = {Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation},
  booktitle = {Advances in Neural Information Processing Systems 26},
  author = {Lin, Dahua},
  editor = {Burges, C J C and Bottou, L and Welling, M and Ghahramani, Z and Weinberger, K Q},
  year = {2013},
  pages = {395--403},
  publisher = {Curran Associates, Inc.},
  abstract = {Reliance on computationally expensive algorithms for inference has been limiting the use of Bayesian nonparametric models in large scale applications. To tackle this problem, we propose a Bayesian learning algorithm for DP mixture models. Instead of following the conventional paradigm -- random initialization plus iterative update, we take an progressive approach. Starting with a given prior, our method recursively transforms it into an approximate posterior through sequential variational approximation. In this process, new components will be incorporated on the fly when needed. The algorithm can reliably estimate a DP mixture model in one pass, making it particularly suited for applications with massive data. Experiments on both synthetic data and real datasets demonstrate remarkable improvement on efficiency -- orders of magnitude speed-up compared to the state-of-the-art.},
  keywords = {nosource}
}

@article{labadi2014approximation,
  title = {Size-Biased Sampling of {{Poisson}} Point Processes and Excursions},
  author = {Al Labadi, Luai and Zarepour, Mahmoud},
  year = {2014},
  journal = {Arxiv preprint},
  keywords = {nosource}
}

@article{lanFlexibleBayesianDynamic2020,
  title = {Flexible {{Bayesian Dynamic Modeling}} of {{Correlation}} and {{Covariance Matrices}}},
  author = {Lan, Shiwei and Holbrook, Andrew and Elias, Gabriel A. and Fortin, Norbert J. and Ombao, Hernando and Shahbaba, Babak},
  year = {2020},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {15},
  number = {4},
  pages = {1199--1228},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/19-BA1173},
  urldate = {2024-11-07},
  abstract = {Modeling correlation (and covariance) matrices can be challenging due to the positive-definiteness constraint and potential high-dimensionality. Our approach is to decompose the covariance matrix into the correlation and variance matrices and propose a novel Bayesian framework based on modeling the correlations as products of unit vectors. By specifying a wide range of distributions on a sphere (e.g. the squared-Dirichlet distribution), the proposed approach induces flexible prior distributions for covariance matrices (that go beyond the commonly used inverse-Wishart prior). For modeling real-life spatio-temporal processes with complex dependence structures, we extend our method to dynamic cases and introduce unit-vector Gaussian process priors in order to capture the evolution of correlation among components of a multivariate time series. To handle the intractability of the resulting posterior, we introduce the adaptive {$\Delta$} -Spherical Hamiltonian Monte Carlo. We demonstrate the validity and flexibility of our proposed framework in a simulation study of periodic processes and an analysis of rat's local field potential activity in a complex sequence memory task.},
  keywords = {{$\Delta$}-Spherical Hamiltonian Monte Carlo,dynamic covariance modeling,geometric methods,posterior contraction,spatio-temporal models},
  file = {/home/gkonkamking/pCloudDrive/papers/Lan et al. - 2020 - Flexible Bayesian Dynamic Modeling of Correlation and Covariance Matrices.pdf}
}

@article{langrock2015nonparametric,
  title = {Nonparametric Inference in Hidden {{Markov}} Models Using {{P-splines}}},
  author = {Langrock, Roland and Kneib, Thomas and Sohn, Alexander and DeRuiter, Stacy L},
  year = {2015},
  journal = {Biometrics},
  volume = {71},
  number = {2},
  pages = {520--528},
  publisher = {Wiley Online Library},
  keywords = {nosource}
}

@article{Larras2012,
  title = {Using Bioassays and Species Sensitivity Distributions to Assess Herbicide Toxicity towards Benthic Diatoms.},
  author = {Larras, Floriane and Bouchez, Agn{\`e}s and Rimet, Fr{\'e}d{\'e}ric and Montuelle, Bernard},
  year = {2012},
  month = jan,
  journal = {PloS one},
  volume = {7},
  number = {8},
  pages = {e44458},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0044458},
  abstract = {Although benthic diatoms are widely used in ecological studies of aquatic systems, there is still a dearth of data concerning species sensitivities towards several contaminants. Within the same community, different species may respond differently depending on their physiological and ecological characteristics. This lack of knowledge makes specific appropriate risk assessment impossible. To find out whether species sensitivity distribution (SSD) could be used to estimate the risk of herbicide toxicity for diatoms, we need to know whether their sensitivity depends on their physiological and ecological characteristics. We carried out single-species bioassays on 11 diatom species exposed to 8 herbicides. Dose-responses relationships were used to extrapolate the Effective Concentration 5 (EC(5)) and the Effective Concentration 50 (EC(50)) for each exposure. These data were used to fit a SSD curve for each herbicide, and to determine the Hazardous concentration 5 (HC(5)) and 50 (HC(50)). Our results revealed a high level of variability of the sensitivity in the set of species tested. For photosystem-II inhibitor (PSII) herbicides, diatoms species displayed a typical grouping of sensitivity levels consistent with their trophic mode and their ecological guild. N-heterotroph and "motile" guild species were more tolerant of PSII inhibitors, while N-autotroph and "low profile" guild species were more sensitive. Comprehensive SSD curves were obtained for 5 herbicides, but not for sulfonylurea herbicides or for dimetachlor, which had toxicity levels that were below the range of concentration tested. The SSD curves provided the following ranking of toxicity: diuron{$>$} terbutryn{$>$} isoproturon{$>$} atrazine{$>$} metolachlor. The HC that affected 5\% of the species revealed that, even at the usual environmental concentrations of herbicides, diatom assemblages could be affected, especially by isoproturon, terbutryn, and diuron.},
  pmid = {22952981},
  keywords = {duplicate-citation-key,nosource}
}

@article{Larras2014,
  title = {Seasonal Shift in the Sensitivity of a Natural Benthic Microalgal Community to a Herbicide Mixture: {{Impact}} on the Protective Level of Thresholds Derived from Species Sensitivity Distributions},
  author = {Larras, Floriane and Montuelle, Bernard and Rimet, Fr{\'e}d{\'e}ric and Ch{\`e}vre, Nathalie and Bouchez, Agn{\`e}s},
  year = {2014},
  journal = {Ecotoxicology},
  volume = {23},
  number = {6},
  pages = {1109--1123},
  issn = {15733017},
  doi = {10.1007/s10646-014-1254-2},
  abstract = {Seasonal changes in the structure and composition of a benthic microalgal community may lead to different responses to herbicide contamination during different seasons. Consequently, the thresholds derived from risk assessment tools such as species sensitivity distributions (SSDs) must allow for these changes. We built a single-substance SSD for each of four herbicides (atrazine, terbutryn, diuron and isoproturon), which was specific to the sensitivity of the benthic diatoms found in Lake Geneva, in order to derive protective thresholds for a mixture of these four herbicides using the concentration addition model. We then investigated (1) the structural parameters of a Lake Geneva benthic microalgal community during two contrasting seasons (summer 2012 and winter 2013), (2) the response of these communities to a herbicide mixture, and (3) the protective levels of the thresholds derived. The winter community was characterized by having greater biomass, diatom species richness, and diversity metrics, and lower non-diatom species richness than the summer community. The differences in the diatom communities composition in these seasons appeared to be primarily driven by the environmental nitrate concentrations and the temperature. Moreover, the species in the winter community were more resistant to herbicides than those found in the summer community. Consequently, the protective threshold for this herbicide mixture obtained in this study was in fact protective for the winter community, but not for the summer community based on their structural parameters. Thus, the protective level against herbicides of the threshold for the benthic microalgal community should take into account changes in the environmental physico-chemical conditions that strongly influence the structure and composition of the community. The fact that the succession of species over time (i.e., over the seasons) is difficult to predict introduces uncertainties into the estimation of protective thresholds and questions their applicability year round.},
  isbn = {1573-3017 (Electronic){\textbackslash}r0963-9292 (Linking)},
  pmid = {24840105},
  keywords = {Diversity,Hazardous concentration,Herbicide,Microalgae,nosource,Season}
}

@article{Larras2018,
  title = {{{DRomics}}: {{A}} Turnkey Tool to Support the Use of the Dose-Response Framework for Omics Data in Ecological Risk Assessment},
  author = {Larras, Floriane and Billoir, Elise and Baillard, Vincent and Siberchicot, Aur{\'e}lie and Scholz, Stefan and Wubet, Tesfaye and Tarkka, Mika and {Schmitt-Jansen}, Mechthild and {Delignette-Muller}, Marie Laure},
  year = {2018},
  journal = {Environmental Science and Technology},
  volume = {52},
  number = {24},
  pages = {14461--14468},
  issn = {15205851},
  doi = {10.1021/acs.est.8b04752},
  abstract = {Omics approaches (e.g., transcriptomics, metabolomics) are promising for ecological risk assessment (ERA) since they provide mechanistic information and early warning signals. A crucial step in the analysis of omics data is the modeling of concentration-dependency which may have different trends including monotonic (e.g., linear, exponential) or biphasic (e.g., U shape, bell shape) forms. The diversity of responses raises challenges concerning detection and modeling of significant responses and effect concentration (EC) derivation. Furthermore, handling high-throughput data sets is time-consuming and requires effective and automated processing routines. Thus, we developed an open source tool (DRomics, available as an R-package and as a web-based service) which, after elimination of molecular responses (e.g., gene expressions from microarrays) with no concentration-dependency and/or high variability, identifies the best model for concentration--response curve description. Subsequently, an EC (e.g., a benchm...},
  keywords = {nosource},
  file = {/home/gkonkamking/pCloudDrive/papers/Larras et al_2018_DRomics.pdf}
}

@article{Lartillot2004,
  title = {A {{Bayesian}} Mixture Model for Across-Site Heterogeneities in the Amino-Acid Replacement Process},
  author = {Lartillot, Nicolas and Philippe, Herv??},
  year = {2004},
  journal = {Molecular Biology and Evolution},
  volume = {21},
  number = {6},
  pages = {1095--1109},
  issn = {07374038},
  doi = {10.1093/molbev/msh112},
  abstract = {Most current models of sequence evolution assume that all sites of a protein evolve under the same substitution process, characterized by a 20 x 20 substitution matrix. Here, we propose to relax this assumption by developing a Bayesian mixture model that allows the amino-acid replacement pattern at different sites of a protein alignment to be described by distinct substitution processes. Our model, named CAT, assumes the existence of distinct processes (or classes) differing by their equilibrium frequencies over the 20 residues. Through the use of a Dirichlet process prior, the total number of classes and their respective amino-acid profiles, as well as the affiliations of each site to a given class, are all free variables of the model. In this way, the CAT model is able to adapt to the complexity actually present in the data, and it yields an estimate of the substitutional heterogeneity through the posterior mean number of classes. We show that a significant level of heterogeneity is present in the substitution patterns of proteins, and that the standard one-matrix model fails to account for this heterogeneity. By evaluating the Bayes factor, we demonstrate that the standard model is outperformed by CAT on all of the data sets which we analyzed. Altogether, these results suggest that the complexity of the pattern of substitution of real sequences is better captured by the CAT model, offering the possibility of studying its impact on phylogenetic reconstruction and its connections with structure-function determinants.},
  isbn = {0737-4038},
  pmid = {15014145},
  keywords = {Amino-acid replacement,Bayes,Bayes factor,Dirichlet process mixtures,nosource,Phylogeny,Posterior predictive resampling},
  file = {/home/gkonkamking/Zotero/storage/ASWIJUSX/Lartillot and Philippe - 2004 - A Bayesian mixture model for across-site heterogen.pdf}
}

@article{Lartillot2006,
  title = {Conjugate {{Gibbs}} Sampling for {{Bayesian}} Phylogenetic Models.},
  author = {Lartillot, Nicolas},
  year = {2006},
  journal = {Journal of computational biology : a journal of computational molecular cell biology},
  volume = {13},
  number = {10},
  pages = {1701--1722},
  issn = {1066-5277},
  doi = {10.1089/cmb.2006.13.1701},
  abstract = {We propose a new Markov Chain Monte Carlo (MCMC) sampling mechanism for Bayesian phylogenetic inference. This method, which we call conjugate Gibbs, relies on analytical conjugacy properties, and is based on an alternation between data augmentation and Gibbs sampling. The data augmentation step consists in sampling a detailed substitution history for each site, and across the whole tree, given the current value of the model parameters. Provided convenient priors are used, the parameters of the model can then be directly updated by a Gibbs sampling procedure, conditional on the current substitution history. Alternating between these two sampling steps yields a MCMC device whose equilibrium distribution is the posterior probability density of interest. We show, on real examples, that this conjugate Gibbs method leads to a significant improvement of the mixing behavior of the MCMC. In all cases, the decorrelation times of the resulting chains are smaller than those obtained by standard Metropolis Hastings procedures by at least one order of magnitude. The method is particularly well suited to heterogeneous models, i.e. assuming site-specific random variables. In particular, the conjugate Gibbs formalism allows one to propose efficient implementations of complex models, for instance assuming site-specific substitution processes, that would not be accessible to standard MCMC methods.},
  isbn = {1066-5277},
  pmid = {17238840},
  keywords = {algorithms,molecular evolution,monte carlo likelihood,nosource,phylogenetic analyses}
}

@article{latoucheCombiningRelaxedEM2016,
  title = {Combining a Relaxed {{EM}} Algorithm with {{Occam}}'s Razor for {{Bayesian}} Variable Selection in High-Dimensional Regression},
  author = {Latouche, Pierre and Mattei, Pierre-Alexandre and Bouveyron, Charles and Chiquet, Julien},
  year = {2016},
  month = apr,
  journal = {Journal of Multivariate Analysis},
  series = {Special {{Issue}} on {{Statistical Models}} and {{Methods}} for {{High}} or {{Infinite Dimensional Spaces}}},
  volume = {146},
  pages = {177--190},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2015.09.004},
  urldate = {2020-04-10},
  abstract = {We address the problem of Bayesian variable selection for high-dimensional linear regression. We consider a generative model that uses a spike-and-slab-like prior distribution obtained by multiplying a deterministic binary vector, which traduces the sparsity of the problem, with a random Gaussian parameter vector. The originality of the work is to consider inference through relaxing the model and using a type-II log-likelihood maximization based on an EM algorithm. Model selection is performed afterwards relying on Occam's razor and on a path of models found by the EM algorithm. Numerical comparisons between our method, called spinyReg, and state-of-the-art high-dimensional variable selection algorithms (such as lasso, adaptive lasso, stability selection or spike-and-slab procedures) are reported. Competitive variable selection results and predictive performances are achieved on both simulated and real benchmark data sets. An original regression data set involving the prediction of the number of visitors of the Orsay museum in Paris using bike-sharing system data is also introduced, illustrating the efficiency of the proposed approach. The R package spinyReg implementing the method proposed in this paper is available on CRAN.},
  langid = {english},
  keywords = {EM algorithm,High-dimensional data,Linear regression,Occam's razor,Spike-and-slab,Variable selection},
  file = {/home/gkonkamking/Zotero/storage/EQM65UY5/Latouche et al. - 2016 - Combining a relaxed EM algorithm with Occam’s razo.pdf}
}

@article{lau2007bayesian,
  title = {Bayesian Model-Based Clustering Procedures},
  author = {Lau, John W and Green, Peter J},
  year = {2007},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {16},
  number = {3},
  pages = {526--558},
  publisher = {Taylor \& Francis},
  keywords = {nosource}
}

@article{lauStickBreakingRepresentationComputation2015,
  title = {Stick-{{Breaking Representation}} and {{Computation}} for {{Normalized Generalized Gamma Processes}}},
  author = {Lau, John W. and Cripps, Edward},
  year = {2015},
  month = aug,
  journal = {Sankhya A},
  volume = {77},
  number = {2},
  pages = {300--329},
  issn = {0976-8378},
  doi = {10.1007/s13171-015-0070-y},
  urldate = {2020-04-08},
  abstract = {Using fractions of gamma and exponentially titled stable random variables this article develops a stick-breaking representation of a truncated normalized generalized gamma process. Sampling from the posterior of this process requires sampling from gamma titled stable random variables and we develop an algorithm to do so that is readily implemented in the open source software R. A Blocked Gibbs sampling algorithm for a Bayesian kernel mixture model is then described and we compare the performance of our algorithm with an algorithm recently proposed in the literature.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/6VWCYNBC/Lau and Cripps - 2015 - Stick-Breaking Representation and Computation for .pdf}
}

@article{law1999beyond,
  title = {Beyond `Women and Transport': Towards New Geographies of Gender and Daily Mobility},
  author = {Law, Robin},
  year = {1999},
  journal = {Progress in human geography},
  volume = {23},
  number = {4},
  pages = {567--588},
  publisher = {Sage Publications Sage CA: Thousand Oaks, CA},
  keywords = {nosource}
}

@inproceedings{lawless2023clustering,
  title = {Clustering Inconsistency for {{Pitman}}--{{Yor}} Mixture Models with a Prior on the Precision but Fixed Discount Parameter},
  booktitle = {Fifth Symposium on Advances in Approximate Bayesian Inference},
  author = {Lawless, Caroline and Arbel, Julyan and Alamichel, Louise and KING, Guillaume KON KAM},
  year = {2023},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/pCloudDrive/papers/Lawless et al_2023_Clustering inconsistency for Pitman–Yor mixture models with a prior on the.pdf}
}

@article{LD13,
  title = {Computing in Operations Research Using \{\vphantom\}{{J}}\vphantom\{\}ulia},
  author = {Lubin, Miles and Dunning, Iain},
  year = {2015},
  journal = {INFORMS Journal on Computing},
  volume = {27},
  number = {2},
  pages = {238--248},
  doi = {10.1287/ijoc.2014.0623},
  abstract = {The state of numerical computing is currently characterized by a divide between highly efficient yet typically cumbersome low-level languages such as C, C++, and Fortran and highly expressive yet typically slow high-level languages such as Python and MATLAB. This paper explores how Julia, a modern programming language for numerical computing which claims to bridge this divide by incorporating recent advances in language and compiler design (such as just-in-time compilation), can be used for implementing software and algorithms fundamental to the field of operations research, with a focus on mathematical optimization. In particular, we demonstrate algebraic modeling for linear and nonlinear optimization and a partial implementation of a practical simplex code. Extensive cross-language benchmarks suggest that Julia is capable of obtaining state-of-the-art performance.},
  keywords = {nosource}
}

@article{Lee20051241,
  title = {Statistical Analysis of Water-Quality Data Containing Multiple Detection Limits},
  author = {Lee, Lopaka and Helsel, Dennis},
  year = {2005},
  journal = {Computers \& Geosciences},
  volume = {31},
  number = {10},
  pages = {1241--1248},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2005.03.012},
  abstract = {Trace contaminants in water, including metals and organics, often are measured at sufficiently low concentrations to be reported only as values below the instrument detection limit. Interpretation of these ``less thans'' is complicated when multiple detection limits occur. Statistical methods for multiply censored, or multiple-detection limit, datasets have been developed for medical and industrial statistics, and can be employed to estimate summary statistics or model the distributions of trace-level environmental data.We describe S-language-based software tools that perform robust linear regression on order statistics (ROS). The ROS method has been evaluated as one of the most reliable procedures for developing summary statistics of multiply censored data. It is applicable to any dataset that has 0 to 80\% of its values censored. These tools are a part of a software library, or add-on package, for the R environment for statistical computing. This library can be used to generate ROS models and associated summary statistics, plot modeled distributions, and predict exceedance probabilities of water-quality standards.},
  keywords = {nosource,Statistics}
}

@article{lee2008defining,
  title = {Defining Predictive Probability Functions for Species Sampling Models},
  author = {Lee, Jaeyong and Quintana, Fernando A and M{\"u}ller, Peter and Trippa, Lorenzo},
  year = {2013},
  journal = {Statistical science},
  volume = {28},
  number = {2},
  pages = {209--222},
  keywords = {nosource}
}

@article{Lee2013,
  title = {{{NADA}}: {{Nondetects}} and Data Analysis for Environmental Data. {{R}} Package Version 1.5-6},
  author = {Lee, Lopaka},
  year = {2013},
  keywords = {nosource}
}

@inproceedings{leeAlgorithmsNonnegativeMatrix2000,
  title = {Algorithms for {{Non-negative Matrix Factorization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lee, Daniel and Seung, H. Sebastian},
  year = {2000},
  volume = {13},
  publisher = {MIT Press},
  urldate = {2023-10-06},
  abstract = {Non-negative matrix factorization (NMF) has previously been shown to  be a useful decomposition for multivariate data. Two different multi-  plicative algorithms for NMF are analyzed. They differ only slightly in  the multiplicative factor used in the update rules. One algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized Kullback-Leibler divergence. The monotonic  convergence of both algorithms can be proven using an auxiliary func-  tion analogous to that used for proving convergence of the Expectation-  Maximization algorithm. The algorithms can also be interpreted as diag-  onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence.},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/pCloudDrive/papers/Lee_Seung_2000_Algorithms for Non-negative Matrix Factorization.pdf}
}

@article{leeBayesianFeatureAllocation2015,
  title = {A {{Bayesian}} Feature Allocation Model for Tumor Heterogeneity},
  author = {Lee, Juhee and M{\"u}ller, Peter and Gulukota, Kamalakar and Ji, Yuan},
  year = {2015},
  month = jun,
  journal = {The Annals of Applied Statistics},
  volume = {9},
  number = {2},
  pages = {621--639},
  publisher = {Institute of Mathematical Statistics},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/15-AOAS817},
  urldate = {2021-05-04},
  abstract = {We develop a feature allocation model for inference on genetic tumor variation using next-generation sequencing data. Specifically, we record single nucleotide variants (SNVs) based on short reads mapped to human reference genome and characterize tumor heterogeneity by latent haplotypes defined as a scaffold of SNVs on the same homologous genome. For multiple samples from a single tumor, assuming that each sample is composed of some sample-specific proportions of these haplotypes, we then fit the observed variant allele fractions of SNVs for each sample and estimate the proportions of haplotypes. Varying proportions of haplotypes across samples is evidence of tumor heterogeneity since it implies varying composition of cell subpopulations. Taking a Bayesian perspective, we proceed with a prior probability model for all relevant unknown quantities, including, in particular, a prior probability model on the binary indicators that characterize the latent haplotypes. Such prior models are known as feature allocation models. Specifically, we define a simplified version of the Indian buffet process, one of the most traditional feature allocation models. The proposed model allows overlapping clustering of SNVs in defining latent haplotypes, which reflects the evolutionary process of subclonal expansion in tumor samples.},
  keywords = {feature allocation models,Haplotypes,Indian buffet process,Markov chain Monte Carlo,next-generation sequencing,random binary matrices,variant calling},
  file = {/home/gkonkamking/Zotero/storage/DH3NBSGE/Lee et al_2016_Bayesian Feature Allocation Models for Tumor Heterogeneity.pdf}
}

@inproceedings{leeBayesianFeatureAllocation2016,
  title = {Bayesian {{Feature Allocation Models}} for {{Tumor Heterogeneity}}},
  booktitle = {Statistical {{Analysis}} for {{High-Dimensional Data}}},
  author = {Lee, Juhee and M{\"u}ller, Peter and Sengupta, Subhajit and Gulukota, Kamalakar and Ji, Yuan},
  editor = {Frigessi, Arnoldo and B{\"u}hlmann, Peter and Glad, Ingrid K. and Langaas, Mette and Richardson, Sylvia and Vannucci, Marina},
  year = {2016},
  series = {Abel {{Symposia}}},
  pages = {211--232},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-27099-9_10},
  abstract = {Tumor samples are composed of subclones that evolve stochastically by acquiring mutations and by selection of those that are beneficial to the survival of the organism or local environment. This process results in the often observed heterogeneity of tumor samples. We review some recent work on a new class of feature allocation models for statistical inference on this tumor heterogeneity. We use next-generation sequencing data. The developed methods identify cell subpopulations (subclones) in tumor samples and allow us to cluster samples based on these identified subclones. We characterize subclones by latent haplotypes that are defined as a scaffold of single nucleotide variations (SNVs) on the same homologous genome. That is, each subclone is defined by a unique set of SNVs. We formally represent these sets of SNVs in a binary matrix with columns corresponding to subclones and entries indicating the presence or absence of a set of SNVs that characterize each subclone. We use a simplified version of the Indian buffet process (IBP) as a prior model on this latent binary matrix. In a model extension we develop a categorical IBP that allows us to incorporate copy number variants (CNVs) in addition to SNVs to jointly define subclones. We illustrate the proposed methods with several data analyses.},
  isbn = {978-3-319-27099-9},
  langid = {english},
  keywords = {Copy Number Variation,Markov Chain Monte Carlo,Next Generation Sequencing Data,Single Nucleotide Variant,Tumor Heterogeneity},
  file = {/home/gkonkamking/pCloudDrive/papers/Lee et al_2016_Bayesian Feature Allocation Models for Tumor Heterogeneity.pdf}
}

@article{leeBayesianRegressionBased2013,
  title = {Bayesian Regression Based on Principal Components for High-Dimensional Data},
  author = {Lee, Jaeyong and Oh, Hee-Seok},
  year = {2013},
  month = may,
  journal = {Journal of Multivariate Analysis},
  volume = {117},
  pages = {175--192},
  issn = {0047259X},
  doi = {10.1016/j.jmva.2013.02.002},
  urldate = {2020-04-10},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/KZI2SIHC/Lee and Oh - 2013 - Bayesian regression based on principal components .pdf}
}

@misc{leeUnifiedConstructionSeries2019,
  title = {A Unified Construction for Series Representations and Finite Approximations of Completely Random Measures},
  author = {Lee, Juho and Miscouridou, Xenia and Caron, Fran{\c c}ois},
  year = {2019},
  month = may,
  number = {arXiv:1905.10733},
  eprint = {1905.10733},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2022-07-18},
  abstract = {Infinite-activity completely random measures (CRMs) have become important building blocks of complex Bayesian nonparametric models. They have been successfully used in various applications such as clustering, density estimation, latent feature models, survival analysis or network science. Popular infinite-activity CRMs include the (generalized) gamma process and the (stable) beta process. However, except in some specific cases, exact simulation or scalable inference with these models is challenging and finite-dimensional approximations are often considered. In this work, we propose a general and unified framework to derive both series representations and finite-dimensional approximations of CRMs. Our framework can be seen as an extension of constructions based on size-biased sampling of Poisson point process [PPY92]. It includes as special cases several known series representations as well as novel ones. In particular, we show that one can get novel series representations for the generalized gamma process and the stable beta process. We also provide some analysis of the truncation error.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/gkonkamking/pCloudDrive/papers/Lee et al_2019_A unified construction for series representations and finite approximations of.pdf}
}

@article{lelievrePartialDifferentialEquations2016,
  title = {Partial Differential Equations and Stochastic Methods in Molecular Dynamics},
  author = {Leli{\`e}vre, Tony and Stoltz, Gabriel},
  year = {2016},
  month = may,
  journal = {Acta Numerica},
  volume = {25},
  pages = {681--880},
  issn = {0962-4929, 1474-0508},
  doi = {10.1017/S0962492916000039},
  urldate = {2024-10-23},
  abstract = {The objective of molecular dynamics computations is to infer macroscopic properties                of matter from atomistic models via averages with respect to probability measures                dictated by the principles of statistical physics. Obtaining accurate results                requires efficient sampling of atomistic configurations, which are typically                generated using very long trajectories of stochastic differential equations in high                dimensions, such as Langevin dynamics and its overdamped limit. Depending on the                quantities of interest at the macroscopic level, one may also be interested in                dynamical properties computed from averages over paths of these dynamics.This review describes how techniques from the analysis of partial differential                equations can be used to devise good algorithms and to quantify their efficiency and                accuracy. In particular, a crucial role is played by the study of the long-time                behaviour of the solution to the Fokker--Planck equation associated with                the stochastic dynamics.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/5V8XVN4S/Lelièvre and Stoltz - 2016 - Partial differential equations and stochastic methods in molecular dynamics.pdf}
}

@article{LEPAGE2006216,
  title = {Continuous and Tractable Models for the Variation of Evolutionary Rates},
  author = {Lepage, Thomas and Lawi, Stephan and Tupper, Paul and Bryant, David},
  year = {2006},
  journal = {Mathematical Biosciences},
  volume = {199},
  number = {2},
  pages = {216--233},
  issn = {0025-5564},
  doi = {10.1016/j.mbs.2005.11.002},
  abstract = {We propose a continuous model for variation in the evolutionary rate across sites and over the phylogenetic tree. We derive exact transition probabilities of substitutions under this model. Changes in rate are modelled using the CIR process, a diffusion widely used in financial applications. The model directly extends the standard gamma distributed rates across site model, with one additional parameter governing changes in rate down the tree. The parameters of the model can be estimated directly from two well-known statistics: the index of dispersion and the gamma shape parameter of the rates across sites model. The CIR model can be readily incorporated into probabilistic models for sequence evolution. We provide here an exact formula for the likelihood of a three-taxon tree. The likelihoods of larger trees can be evaluated using Monte--Carlo methods.},
  keywords = {CIR process,Covarion,Diffusion processes,Evolutionary rate,Molecular clocks,nosource,Phylogenetics}
}

@article{leroux1992consistent,
  title = {Consistent Estimation of a Mixing Distribution},
  author = {Leroux, Brian G},
  year = {1992},
  journal = {The Annals of Statistics},
  pages = {1350--1360},
  publisher = {JSTOR},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/pCloudDrive/papers/Leroux_1992_Consistent estimation of a mixing distribution.pdf}
}

@article{Leung2005,
  title = {Deriving Sediment Quality Guidelines from Field-Based Species Sensitivity Distributions.},
  author = {Leung, Kenneth M Y and Bj{\o}rgesaeter, Anders and Gray, John S and Li, W K and Lui, Gilbert C S and Wang, Yuan and Lam, Paul K S},
  year = {2005},
  month = jul,
  journal = {Environmental science \& technology},
  volume = {39},
  number = {14},
  eprint = {18092863},
  eprinttype = {pubmed},
  pages = {5148--56},
  issn = {0013-936X},
  abstract = {The determination of predicted no-effect concentrations (PNECs) and sediment quality guidelines (SQGs) of toxic chemicals in marine sediment is extremely important in ecological risk assessment. However, current methods of deriving sediment PNECs or threshold effect levels (TELs) are primarily based on laboratory ecotoxicity bioassays that may not be ecologically and environmentally relevant. This study explores the possibility of utilizing field data of benthic communities and contaminant loadings concurrently measured in sediment samples collected from the Norwegian continental shelf to derive SQGs. This unique dataset contains abundance data for ca. 2200 benthic species measured at over 4200 sampling stations, along with co-occurring concentration data for {$>$}25 chemical species. Using barium, cadmium, and total polycyclic aromatic hydrocarbons (PAHs) as examples, this paper describes a novel approach that makes use of the above data set for constructing field-based species sensitivity distributions (f-SSDs). Field-based SQGs are then derived based on the f-SSDs and HCx values [hazardous concentration for x\% of species or the (100-x)\% protection level] by the nonparametric bootstrap method. Our results for Cd and total PAHs indicate that there are some discrepancies between the SQGs currently in use in various countries and our field-data-derived SQGs. The field-data-derived criteria appear to be more environmentally relevant and realistic. Here, we suggest that the f-SSDs can be directly used as benchmarks for probabilistic risk assessment, while the field-data-derived SQGs can be used as site-specific guidelines or integrated into current SQGs.},
  isbn = {0013-936X},
  pmid = {16082942},
  keywords = {Animals,Aromatic,Aromatic: toxicity,Barium,Barium: toxicity,Benchmarking,Biological Assay,Cadmium,Cadmium: toxicity,duplicate-citation-key,Ecology,Environmental Monitoring,Geologic Sediments,Geologic Sediments: chemistry,Guidelines as Topic,Invertebrates,nosource,Polycyclic Hydrocarbons,Risk Assessment,Sensitivity and Specificity}
}

@article{Lewandowski20091989,
  title = {Generating Random Correlation Matrices Based on Vines and Extended Onion Method},
  author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
  year = {2009},
  journal = {Journal of Multivariate Analysis},
  volume = {100},
  number = {9},
  pages = {1989--2001},
  issn = {0047259X},
  doi = {10.1016/j.jmva.2009.04.008},
  abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276-294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177-2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions. ?? 2009 Elsevier Inc. All rights reserved.},
  isbn = {0047-259X},
  keywords = {Correlation matrix,Dependence vines,nosource,Onion method,Partial correlation}
}

@article{Li2018,
  title = {On the Asymptotic Efficiency of Approximate {{Bayesian}} Computation Estimators},
  author = {Li, Wentao and Fearnhead, Paul},
  year = {2018},
  journal = {Biometrika},
  volume = {105},
  number = {2},
  eprint = {1506.03481},
  pages = {285--299},
  issn = {14643510},
  doi = {10.1093/biomet/asx078},
  abstract = {Many statistical applications involve models for which it is difficult to evaluate the likelihood, but from which it is relatively easy to sample. Approximate Bayesian computation is a likelihood-free method for implementing Bayesian inference in such cases. We present results on the asymptotic variance of estimators obtained using approximate Bayesian computation in a large-data limit. Our key assumption is that the data are summarized by a fixed-dimensional summary statistic that obeys a central limit theorem. We prove asymptotic normality of the mean of the approximate Bayesian computation posterior. This result also shows that, in terms of asymptotic variance, we should use a summary statistic that is the same dimension as the parameter vector, p; and that any summary statistic of higher dimension can be reduced, through a linear transformation, to dimension p in a way that can only reduce the asymptotic variance of the posterior mean. We look at how the Monte Carlo error of an importance sampling algorithm that samples from the approximate Bayesian computation posterior affects the accuracy of estimators. We give conditions on the importance sampling proposal distribution such that the variance of the estimator will be the same order as that of the maximum likelihood estimator based on the summary statistics used. This suggests an iterative importance sampling algorithm, which we evaluate empirically on a stochastic volatility model.},
  archiveprefix = {arXiv},
  arxivid = {1506.03481},
  keywords = {Approximate Bayesian computation,Dimension reduction,Importance sampling,nosource,Partial information,Proposal distribution}
}

@article{liangMixturesPriorsBayesian2008,
  title = {Mixtures of g {{Priors}} for {{Bayesian Variable Selection}}},
  author = {Liang, Feng and Paulo, Rui and Molina, German and Clyde, Merlise A and Berger, Jim O},
  year = {2008},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {481},
  pages = {410--423},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1198/016214507000001337},
  urldate = {2022-11-09},
  abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures. Please see Arnold Zellner's letter and the author's response.},
  keywords = {AIC,Bayesian model averaging,BIC,Cauchy,Empirical Bayes,Gaussian hypergeometric functions,Model selection,Zellner--Siow priors},
  file = {/home/gkonkamking/pCloudDrive/papers/Liang et al_2008_Mixtures of g Priors for Bayesian Variable Selection.pdf}
}

@article{liaoEmpiricallyInvestigatingImaginative2014,
  title = {Empirically {{Investigating Imaginative Resistance}}},
  author = {Liao, Shen-yi and Strohminger, Nina and Sripada, Chandra Sekhar},
  year = {2014},
  month = jul,
  journal = {The British Journal of Aesthetics},
  volume = {54},
  number = {3},
  pages = {339--355},
  publisher = {Oxford Academic},
  issn = {0007-0904},
  doi = {10.1093/aesthj/ayu027},
  urldate = {2020-05-11},
  abstract = {Abstract.  Imaginative resistance refers to a phenomenon in which people resist engaging in particular prompted imaginative activities. Philosophers have primar},
  langid = {english},
  keywords = {nosource}
}

@article{Liberles2012,
  title = {The Interface of Protein Structure, Protein Biophysics, and Molecular Evolution},
  author = {Liberles, David A. and Teichmann, Sarah A. and Bahar, Ivet and Bastolla, Ugo and Bloom, Jesse and {Bornberg-Bauer}, Erich and Colwell, Lucy J. and De Koning, A. P Jason and Dokholyan, Nikolay V. and Echave, Julian and Elofsson, Arne and Gerloff, Dietlind L. and Goldstein, Richard A. and Grahnen, Johan A. and Holder, Mark T. and Lakner, Clemens and Lartillot, Nicolas and Lovell, Simon C. and Naylor, Gavin and Perica, Tina and Pollock, David D. and Pupko, Tal and Regan, Lynne and Roger, Andrew and Rubinstein, Nimrod and Shakhnovich, Eugene and Sj??lander, Kimmen and Sunyaev, Shamil and Teufel, Ashley I. and Thorne, Jeffrey L. and Thornton, Joseph W. and Weinreich, Daniel M. and Whelan, Simon},
  year = {2012},
  journal = {Protein Science},
  volume = {21},
  number = {6},
  pages = {769--785},
  issn = {09618368},
  doi = {10.1002/pro.2071},
  abstract = {Abstract The interface of protein structural biology, protein biophysics, molecular evolution, and molecular population genetics forms the foundations for a mechanistic understanding of many aspects of protein biochemistry. Current efforts in interdisciplinary protein modeling are in their infancy and the state-of-the art of such models is described. Beyond the relationship between amino acid substitution and static protein structure, protein function, and corresponding organismal fitness, other considerations are also discussed. More complex mutational processes such as insertion and deletion and domain rearrangements and even circular permutations should be evaluated. The role of intrinsically disordered proteins is still controversial, but may be increasingly important to consider. Protein geometry and protein dynamics as a deviation from static considerations of protein structure are also important. Protein expression level is known to be a major determinant of evolutionary rate and several considerations including selection at the mRNA level and the role of interaction specificity are discussed. Lastly, the relationship between modeling and needed high-throughput experimental data as well as experimental examination of protein evolution using ancestral sequence resurrection and in vitro biochemistry are presented, towards an aim of ultimately generating better models for biological inference and prediction.},
  isbn = {0961-8368},
  pmid = {22528593},
  keywords = {Ancestral sequence reconstruction,Domain evolution,Evolutionary modeling,Gene duplication,nosource,Protein dynamics,Protein expression,Protein thermodynamics,Sequence-structure-function relationships}
}

@techreport{liCodonBERTLargeLanguage2023,
  type = {Preprint},
  title = {{{CodonBERT}}: {{Large Language Models}} for {{mRNA}} Design and Optimization},
  shorttitle = {{{CodonBERT}}},
  author = {Li, Sizhen and Moayedpour, Saeed and Li, Ruijiang and Bailey, Michael and Riahi, Saleh and {Kogler-Anele}, Lorenzo and Miladi, Milad and Miner, Jacob and Zheng, Dinghai and Wang, Jun and Balsubramani, Akshay and Tran, Khang and Zacharia, Minnie and Wu, Monica and Gu, Xiaobo and Clinton, Ryan and Asquith, Carla and Skaleski, Joseph and Boeglin, Lianne and Chivukula, Sudha and Dias, Anusha and Montoya, Fernando Ulloa and Agarwal, Vikram and {Bar-Joseph}, Ziv and Jager, Sven},
  year = {2023},
  month = sep,
  institution = {Bioinformatics},
  doi = {10.1101/2023.09.09.556981},
  urldate = {2024-01-09},
  abstract = {A             bstract                      mRNA based vaccines and therapeutics are gaining popularity and usage across a wide range of conditions. One of the critical issues when designing such mRNAs is sequence optimization. Even small proteins or peptides can be encoded by an enormously large number of mRNAs. The actual mRNA sequence can have a large impact on several properties including expression, stability, immunogenicity, and more. To enable the selection of an optimal sequence, we developed CodonBERT, a large language model (LLM) for mRNAs. Unlike prior models, CodonBERT uses codons as inputs which enables it to learn better representations. CodonBERT was trained using more than 10 million mRNA sequences from a diverse set of organisms. The resulting model captures important biological concepts. CodonBERT can also be extended to perform prediction tasks for various mRNA properties. CodonBERT outperforms previous mRNA prediction methods including on a new flu vaccine dataset.},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Li et al_2023_CodonBERT.pdf}
}

@article{lijoi2005bayesian,
  title = {Bayesian Nonparametric Analysis for a Generalized {{Dirichlet}} Process Prior},
  author = {Lijoi, Antonio and Mena, Rams{\'e}s H. and Pr{\"u}nster, Igor},
  year = {2005},
  journal = {Statistical Inference for Stochastic Processes},
  volume = {8},
  number = {3},
  pages = {283--309},
  publisher = {Springer},
  issn = {13870874},
  doi = {10.1007/s11203-005-6071-z},
  abstract = {This paper considers a generalization of the Dirichlet process which is obtained by suitably normalizing superposed independent gamma processes having increasing integer-valued scale parameter. A comprehensive treatment of this random probability measure is provided. We prove results concerning its finite-dimensional distributions, moments, predictive distributions and the distribution of its mean. Most expressions are given in terms of multiple hypergeometric functions, thus highlighting the interplay between Bayesian Nonparametrics and special functions. Finally, a suitable simulation algorithm is applied in order to compute quantities of statistical interest.},
  keywords = {Bayesian nonparametric inference,Dirichlet process,Generalized gamma convolutions,Lauricella hypergeometric functions,Means of random probability measures,nosource,Predictive distributions}
}

@article{lijoi2005consistency,
  title = {On Consistency of Nonparametric Normal Mixtures for {{Bayesian}} Density Estimation},
  author = {Lijoi, Antonio and Pr{\"u}nster, Igor and Walker, S.G.},
  year = {2005},
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {472},
  pages = {1292--1296},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1198/016214505000000358},
  abstract = {The past decade has seen a remarkable development in the area of Bayesian nonparametric inference from both theoretical and applied perspectives. As for the latter, the celebrated Dirichlet process has been successfully exploited within Bayesian mixture models, leading to many interesting applications. As for the former, some new discrete nonparametric priors have been recently proposed in the literature that have natural use as alternatives to the Dirichlet process in a Bayesian hierarchical model for density estimation. When using such models for concrete applications, an investigation of their statistical properties is mandatory. Of these properties, a prominent role is to be assigned to consistency. Indeed, strong consistency of Bayesian nonparametric procedures for density estimation has been the focus of a considerable amount of research; in particular, much attention has been devoted to the normal mixture of Dirichlet process. In this article we improve on previous contributions by establishing strong consistency of the mixture of Dirichlet process under fairly general conditions. Besides the usual Kullback-Leibler support condition, consistency is achieved by finiteness of the mean of the base measure of the Dirichlet process and an exponential decay of the prior on the standard deviation. We show that the same conditions are also sufficient for mixtures based on priors more general than the Dirichlet process. This leads to the easy establishment of consistency for many recently proposed mixture models.},
  keywords = {nosource,QA Mathematics (inc Computing science)}
}

@article{lijoi2005hierarchical,
  title = {Hierarchical Mixture Modeling with Normalized Inverse-{{Gaussian}} Priors},
  author = {Lijoi, Antonio and Mena, Rams{\'e}s H. and Pr{\"u}nster, Igor},
  year = {2005},
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {472},
  pages = {1278--1291},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1198/016214505000000132},
  abstract = {In recent years the Dirichlet process prior has experienced a great success in the context of Bayesian mixture modeling. The idea of overcoming discreteness of its realizations by exploiting it in hierarchical models, combined with the development of suitable sampling techniques, represent one of the reasons of its popularity. In this article we propose the normalized inverse-Gaussian (N-IG) process as an alternative to the Dirichlet process to be used in Bayesian hierarchical models. The N-IG prior is constructed via its finite-dimensional distributions. This prior, although sharing the discreteness property of the Dirichlet prior, is characterized by a more elaborate and sensible clustering which makes use of all the information contained in the data. Whereas in the Dirichlet case the mass assigned to each observation depends solely on the number of times that it occurred, for the N-IG prior the weight of a single observation depends heavily on the whole number of ties in the sample. Moreover, expressions corresponding to relevant statistical quantities, such as a priori moments and the predictive distributions, are as tractable as those arising from the Dirichlet process. This implies that well-established sampling schemes can be easily extended to cover hierarchical models based on the N-IG process. The mixture of N-IG process and the mixture of Dirichlet process are compared using two examples involving mixtures of normals.},
  keywords = {bayesian nonparametrics,density estimation,dirichlet process,inverse-gaussian distribution,mixture models,predic-,semiparametric inference,tive distribution},
  file = {/home/gkonkamking/Zotero/storage/V5DHRL8W/Lijoi et al. - 2005 - Hierarchical mixture modeling with normalized inve.pdf}
}

@article{lijoi2007bayesian,
  title = {Bayesian Nonparametric Estimation of the Probability of Discovering New Species},
  author = {Lijoi, Antonio and Mena, Rams{\'e}s H. and Pr{\"u}nster, Igor},
  year = {2007},
  journal = {Biometrika},
  volume = {94},
  number = {November},
  pages = {769--786},
  publisher = {Biometrika Trust},
  issn = {0006-3444},
  doi = {10.1093/biomet/asm061},
  abstract = {Lijoi, A.},
  keywords = {nosource}
}

@article{lijoi2007controlling,
  title = {Controlling the Reinforcement in {{Bayesian}} Non-Parametric Mixture Models},
  author = {Lijoi, Antonio and Mena, Rams{\'e}s H. and Pr{\"u}nster, Igor},
  year = {2007},
  journal = {J. Roy. Stat. Soc. B Met.},
  volume = {69},
  number = {4},
  pages = {715--740},
  publisher = {Wiley Online Library},
  issn = {13697412},
  doi = {10.1111/j.1467-9868.2007.00609.x},
  abstract = {The paper deals with the problem of determining the number of components{\textbackslash}nin a mixture model. We take a Bayesian non-parametric approach and adopt{\textbackslash}na hierarchical model with a suitable non-parametric prior for the latent{\textbackslash}nstructure. A commonly used model for such a problem is the mixture of{\textbackslash}nDirichlet process model. Here, we replace the Dirichlet process with a{\textbackslash}nmore general non-parametric prior obtained from a generalized gamma{\textbackslash}nprocess. The basic feature of this model is that it yields a partition{\textbackslash}nstructure for the latent variables which is of Gibbs type. This relates{\textbackslash}nto the well-known (exchangeable) product partition models. If compared{\textbackslash}nwith the usual mixture of Dirichlet process model the advantage of the{\textbackslash}ngeneralization that we are examining relies on the availability of an{\textbackslash}nadditional parameter a belonging to the interval (0,1): it is shown that{\textbackslash}nsuch a parameter greatly influences the clustering behaviour of the{\textbackslash}nmodel. A value of a that is close to 1 generates a large number of{\textbackslash}nclusters, most of which are of small size. Then, a reinforcement{\textbackslash}nmechanism which is driven by (T acts on the mass allocation by{\textbackslash}npenalizing clusters of small size and favouring those few groups{\textbackslash}ncontaining a large number of elements. These features turn out to be{\textbackslash}nvery useful in the context of mixture modelling. Since it is difficult{\textbackslash}nto specify a priori the reinforcement rate, it is reasonable to specify{\textbackslash}na prior for sigma. Hence, the strength of the reinforcement mechanism is{\textbackslash}ncontrolled by the data.},
  keywords = {Bayesian clustering,Bayesian non-parametric inference,Dirichlet process,Mixture model,Predictive distribution,Product partition model},
  file = {/home/gkonkamking/Zotero/storage/3R52CZVK/Lijoi et al. - 2007 - Controlling the reinforcement in Bayesian non-para.pdf}
}

@article{Lijoi2008,
  title = {A {{Bayesian}} Nonparametric Approach for Comparing Clustering Structures in {{EST}} Libraries.},
  author = {Lijoi, Antonio and Mena, Rams{\'e}s H. and Pr{\"u}nster, Igor},
  year = {2008},
  journal = {Journal of Computational Biology},
  volume = {15},
  number = {10},
  eprint = {19040366},
  eprinttype = {pubmed},
  pages = {1315--27},
  issn = {1557-8666},
  doi = {10.1089/cmb.2008.0043},
  abstract = {Inference for Expressed Sequence Tags (ESTs) data is considered. We focus on evaluating the redundancy of a cDNA library and, more importantly, on comparing different libraries on the basis of their clustering structure. The numerical results we achieve allow us to assess the effect of an error correction procedure for EST data and to study the compatibility of single EST libraries with respect to merged ones. The proposed method is based on a Bayesian nonparametric approach that allows to understand the clustering mechanism that generates the observed data. As specific nonparametric model we use the two parameter Poisson-Dirichlet (PD) process. The PD process represents a tractable nonparametric prior which is a natural candidate for modeling data arising from discrete distributions. It allows prediction and testing in order to analyze the clustering structure featured by the data. We show how a full Bayesian analysis can be performed and describe the corresponding computational algorithm.},
  pmid = {19040366},
  keywords = {Algorithms,Base Sequence,Bayes Theorem,Cluster Analysis,DNA,DNA: methods,Expressed Sequence Tags,Gene Library,Molecular Sequence Data,nosource,Sequence Analysis}
}

@article{lijoi2008bayesian,
  title = {Bayesian Nonparametric Estimators Derived from Conditional {{Gibbs}} Structures},
  author = {Lijoi, Antonio and Pr{\"u}nster, Igor and Walker, Stephen G.},
  year = {2008},
  journal = {Annals of Applied Probability},
  volume = {18},
  number = {4},
  eprint = {0808.2863},
  pages = {1519--1547},
  publisher = {Institute of Mathematical Statistics},
  issn = {10505164},
  doi = {10.1214/07-AAP495},
  abstract = {We consider discrete nonparametric priors which induce Gibbs-type exchangeable random partitions and investigate their posterior behavior in detail. In particular, we deduce conditional distributions and the corresponding Bayesian nonparametric estimators, which can be readily exploited for predicting various features of additional samples. The results provide useful tools for genomic applications where prediction of future outcomes is required.{\textbackslash}n{\textbackslash}nPublished in: Annals of Applied Probability 2008, Vol. 18, No. 4, 1519-1547},
  archiveprefix = {arXiv},
  arxivid = {0808.2863},
  keywords = {Bayesian nonparametrics,Dirichlet process,duplicate-citation-key,Exchangeable random partitions,Generalized factorial coefficients,Generalized gamma process,nosource,Population genetics,Species sampling models,Two parameter Poisson-Dirichlet process}
}

@article{lijoi2008investigating,
  title = {Investigating Nonparametric Priors with {{Gibbs}} Structure},
  author = {Lijoi, Antonio and Pr{\"u}nster, Igor and Walker, Stephen G},
  year = {2008},
  journal = {Statistica Sinica},
  volume = {18},
  number = {4},
  pages = {1653--1668},
  issn = {10170405},
  abstract = {This paper investigates nonparametric priors that induce infinite Gibbs-type partitions; such a feature is desirable both from a conceptual and a mathematical point of view. Recently it has been shown that Gibbs-type priors, with sigma is an element of (0, 1), are equivalent to sigma-stable Poisson-Kingman models. By looking at solutions to a recursive equation arising through Gibbs partitions, we provide an alternative proof of this fundamental result. Since practical implementation of general sigma-stable Poisson-Kingman models is difficult, we focus on a related class of priors, namely normalized random measures with independent increments; these are easily implementable in complex Bayesian models. We establish the result that the only Gibbs-type priors within this class are those based on a generalized gamma random measure.},
  file = {/home/gkonkamking/Zotero/storage/22N94EKI/Lijoi et al. - 2008 - Investigating nonparametric priors with Gibbs stru.pdf}
}

@incollection{lijoi2010models,
  title = {Models beyond the {{Dirichlet}} Process},
  booktitle = {Bayesian Nonparametrics},
  author = {Lijoi, Antonio and Pr{\"u}nster, Igor},
  editor = {Hjort, N L and Holmes, C C and M{\"u}ller, P and Walker, S G},
  year = {2010},
  volume = {28},
  pages = {80},
  publisher = {Cambridge University Press, Cambridge},
  keywords = {nosource}
}

@article{lijoi2013bayesian,
  title = {Bayesian Inference with Dependent Normalized Completely Random Measures},
  author = {Lijoi, Antonio and Nipoti, Bernardo and Pr{\"u}nster, Igor},
  year = {2014},
  journal = {Bernoulli},
  volume = {20},
  number = {3},
  pages = {1260--1291},
  issn = {13507265},
  doi = {10.3150/13-BEJ521},
  keywords = {nosource}
}

@techreport{lijoi2013dependent,
  title = {Dependent Mixture Models: {{Clustering}} and Borrowing Information},
  booktitle = {Computational Statistics and Data Analysis},
  author = {Lijoi, Antonio and Nipoti, Bernardo and Pr{\"u}nster, Igor},
  year = {2014},
  volume = {71},
  number = {302},
  pages = {417--433},
  institution = {{University of Pavia, Department of Economics and Management}},
  issn = {01679473},
  doi = {10.1016/j.csda.2013.06.015},
  abstract = {Most of the Bayesian nonparametric models for non-exchangeable data that are used in applications are based on some extension to the multivariate setting of the Dirichlet process, the best known being MacEachern's dependent Dirichlet process. A comparison of two recently introduced classes of vectors of dependent nonparametric priors, based on the Dirichlet and the normalized ??-stable processes respectively, is provided. These priors are used to define dependent hierarchical mixture models whose distributional properties are investigated. Furthermore, their inferential performance is examined through an extensive simulation study. The models exhibit different features, especially in terms of the clustering behavior and the borrowing of information across studies. Compared to popular Dirichlet process based models, mixtures of dependent normalized ??-stable processes turn out to be a valid choice being capable of more effectively detecting the clustering structure featured by the data. ?? 2013 Elsevier B.V. All rights reserved.},
  isbn = {0167-9473},
  keywords = {Bayesian nonparametrics,Dependent process,Dirichlet process,Generalized P??lya urn scheme,Mixture models,Normalized ??-stable process,nosource,Partially exchangeable random partition}
}

@article{lijoiPitmanYorMultinomial2020,
  title = {The {{Pitman}}--{{Yor}} Multinomial Process for Mixture Modelling},
  author = {Lijoi, Antonio and Pr{\"u}nster, Igor and Rigon, Tommaso},
  year = {2020},
  month = dec,
  journal = {Biometrika},
  volume = {107},
  number = {4},
  pages = {891--906},
  issn = {0006-3444},
  doi = {10.1093/biomet/asaa030},
  urldate = {2021-11-08},
  abstract = {Discrete nonparametric priors play a central role in a variety of Bayesian procedures, most notably when used to model latent features, such as in clustering, mixtures and curve fitting. They are effective and well-developed tools, though their infinite dimensionality is unsuited to some applications. If one restricts to a finite-dimensional simplex, very little is known beyond the traditional Dirichlet multinomial process, which is mainly motivated by conjugacy. This paper introduces an alternative based on the Pitman--Yor process, which provides greater flexibility while preserving analytical tractability. Urn schemes and posterior characterizations are obtained in closed form, leading to exact sampling methods. In addition, the proposed approach can be used to accurately approximate the infinite-dimensional Pitman--Yor process, yielding improvements over existing truncation-based approaches. An application to convex mixture regression for quantitative risk assessment illustrates the theoretical results and compares our approach with existing methods.},
  file = {/home/gkonkamking/pCloudDrive/papers/Lijoi et al_2020_The Pitman–Yor multinomial process for mixture modelling.pdf}
}

@article{lindley1981role,
  title = {The Role of Exchangeability in Inference},
  author = {Lindley, D. V. and Novick, Melvin R.},
  year = {1981},
  journal = {The Annals of Statistics},
  volume = {9},
  number = {1},
  pages = {45--58},
  publisher = {JSTOR},
  issn = {0090-5364},
  doi = {10.1214/aos/1176345331},
  abstract = {This paper is concerned with basic problems of statistical inference. The thesis is in three parts: (1) that inference is a procedure whereby one passes from a population (or sample) to a new individual; (2) that this connection can be established using de Finetti's idea of exchangeability or Fisher's concept of a subpopulation; (3) in making the connection use must be made of the appropriate probability. These three principles are used in a variety of situations and the topics discussed include analysis of variance and covariance, contingency tables, and calibration. Some comments on randomization are also included.},
  isbn = {0090-5364},
  keywords = {nosource}
}

@article{Liquet2016,
  title = {{{R2GUESS}} : {{A}} Graphics Processing Unit-Based r Package for Bayesian Variable Selection Regression of Multivariate Responses},
  author = {Liquet, Beno{\^i}t and Bottolo, Leonardo and Campanella, Gianluca and Richardson, Sylvia and {Chadeau-Hyam}, Marc},
  year = {2016},
  journal = {Journal of Statistical Software},
  volume = {69},
  number = {2},
  pages = {1--32},
  issn = {1548-7660},
  doi = {10.18637/jss.v069.i02},
  abstract = {Technological advances in molecular biology over the past decade have given rise to high dimensional and complex datasets offering the possibility to investigate biological associations between a range of genomic features and complex phenotypes. The analysis of this novel type of data generated unprecedented computational challenges which ultimately led to the definition and implementation of computationally efficient statistical models that were able to scale to genome-wide data, including Bayesian variable selection approaches. While extensive methodological work has been carried out in this area, only few methods capable of handling hundreds of thousands of predictors were implemented and distributed. Among these we recently proposed GUESS, a computationally optimised algorithm making use of graphics processing unit capabilities, which can accommodate multiple outcomes. In this paper we propose R2GUESS, an R package wrapping the original C++ source code. In addition to providing a user-friendly interface of the original code automating its parametrisation, and data handling, R2GUESS also incorporates many features to explore the data, to extend statistical inferences from the native algorithm (e.g., effect size estimation, significance assessment), and to visualize outputs from the algorithm. We first detail the model and its parametrisation, and describe in details its optimised implementation. Based on two examples we finally illustrate its statistical performances and flexibility.},
  keywords = {Bayesian variable selection,C++,graphics processing unit,multivariate regression,nosource,OMICs data,R}
}

@article{Liquet2016a,
  title = {Bayesian Variable Selection Regression of Multivariate Responses for Group Data},
  author = {Liquet, B and Mengersen, K and Pettitt, A N and Sutton, M},
  year = {2017},
  journal = {Bayesian Analysis},
  volume = {12},
  number = {4},
  pages = {1039--1067},
  issn = {19316690},
  doi = {10.1214/17-BA1081},
  keywords = {Bayesian variable selection,Multivariate regression,slab,Sparsity,Spike},
  file = {/home/gkonkamking/Zotero/storage/73RH9T9C/Liquet et al. - 2017 - Bayesian variable selection regression of multivar.pdf}
}

@incollection{liu2014setting,
  title = {Setting Water Quality Criteria in China: {{Approaches}} for Developing Species Sensitivity Distributions for Metals and Metalloids},
  booktitle = {Reviews of Environmental Contamination and Toxicology Volume},
  author = {Liu, Yuedan and Wu, Fengchang and Mu, Yunsong and Feng, Chenglian and Fang, Yixiang and Chen, Lulu and Giesy, John P.},
  year = {2014},
  pages = {35--57},
  publisher = {Springer},
  keywords = {nosource}
}

@article{livengoodFolkProbablyDont2007,
  title = {The {{Folk Probably Don}}'t {{Think What You Think They Think}}: {{Experiments}} on {{Causation}} by {{Absence}}},
  shorttitle = {The {{Folk Probably Don}}'t {{Think What You Think They Think}}},
  author = {Livengood, Jonathan and Machery, Edouard},
  year = {2007},
  journal = {Midwest Studies In Philosophy},
  volume = {31},
  number = {1},
  pages = {107--127},
  issn = {1475-4975},
  doi = {10.1111/j.1475-4975.2007.00150.x},
  urldate = {2020-04-16},
  langid = {english},
  keywords = {nosource}
}

@article{Liverani2013,
  title = {{{PReMiuM}}: {{An}} r Package for Profile Regression Mixture Models Using Dirichlet Processes},
  author = {Liverani, Silvia and Hastie, David I. and Azizi, Lamiae and Papathomas, Michail and Richardson, Sylvia},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {64},
  number = {7},
  pages = {1--30},
  issn = {1548-7660},
  keywords = {nosource}
}

@article{lo1984class,
  title = {On a Class of Bayesian Nonparametric Estimates: {{I}}. {{Density}} Estimates},
  author = {Lo, Albert Y.},
  year = {1984},
  journal = {The Annals of Statistics},
  volume = {12},
  number = {1},
  pages = {351--357},
  publisher = {JSTOR},
  issn = {0090-5364},
  doi = {10.1214/aos/1176346412},
  abstract = {Given a positive, normalized kernel and a finite measure on an Euclidean space, we construct a random density by convoluting the kernel with Dirichlet random probability indexed by the finite measure. The posterior distribution of the random density given a sample is classified. The Bayes estimator of the density function is given.},
  keywords = {nosource}
}

@article{lockBayesianConsensusClustering2013,
  title = {Bayesian Consensus Clustering},
  author = {Lock, Eric F. and Dunson, David B.},
  year = {2013},
  month = oct,
  journal = {Bioinformatics},
  volume = {29},
  number = {20},
  pages = {2610--2616},
  issn = {1460-2059, 1367-4803},
  doi = {10.1093/bioinformatics/btt425},
  urldate = {2021-03-24},
  abstract = {Motivation: In biomedical research a growing number of platforms and technologies are used to measure diverse but related information, and the task of clustering a set of objects based on multiple sources of data arises in several applications. Most current approaches to multisource clustering either independently determine a separate clustering for each data source or determine a single `joint' clustering for all data sources. There is a need for more flexible approaches that simultaneously model the dependence and the heterogeneity of the data sources.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/9FITSF9A/Lock and Dunson - 2013 - Bayesian consensus clustering.pdf}
}

@article{lomeliHybridSamplerPoissonKingman2015,
  title = {A Hybrid Sampler for {{Poisson-Kingman}} Mixture Models},
  author = {Lomeli, Maria and Favaro, Stefano and Teh, Yee Whye},
  year = {2015},
  journal = {Advances in Neural Information Processing Systems},
  volume = {28},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/Zotero/storage/Z9V8NFV7/Lomeli et al. - 2015 - A hybrid sampler for Poisson-Kingman mixture model.pdf}
}

@article{lomeliMarginalSamplerSstable2017,
  title = {A Marginal Sampler for {$\sigma$}-Stable {{Poisson}}--{{Kingman}} Mixture Models},
  author = {Lomel{\'i}, Mar{\'i}a and Favaro, Stefano and Teh, Yee Whye},
  year = {2017},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {1},
  pages = {44--53},
  publisher = {Taylor \& Francis},
  doi = {10.1080/10618600.2015.1110526},
  keywords = {nosource}
}

@article{Lopes2009,
  title = {Optimal Release Strategies for the Biological Control of Aphids in Melon Greenhouses},
  author = {Lopes, Christelle and Spataro, Thierry and Lapchin, Laurent and Arditi, Roger},
  year = {2009},
  month = jan,
  journal = {Biological Control},
  volume = {48},
  number = {1},
  pages = {12--21},
  publisher = {Elsevier Inc.},
  issn = {10499644},
  doi = {10.1016/j.biocontrol.2008.09.011},
  abstract = {In this paper, we report the use of a new spatially implicit host-parasitoid model to compare different biological control strategies. We demonstrate how this approach could be used as a decision tool for crop protection by testing various strategies for the inoculative release of parasitoids to target a pest in a greenhouse. The results show that a single curative release of parasitoids is more effective than a single preventative release: that is, the best strategy is to release as many parasitoids as possible once the pest has invaded the crop if the parasitoid exhibits a type II functional response, or to do so a few days later for a type III functional response. We also confirm that multiple releases are more effective than single releases, and that the initial distribution of the pests and the functional response of the parasitoids are decisive factors for the effectiveness of the biological control (parasitoids with a type II functional response being more effective than those with a type III response). {\copyright} 2008 Elsevier Inc. All rights reserved.},
  isbn = {1049-9644},
  keywords = {Aphis gossypii,Biological control,Host-parasitoid dynamics,Lysiphlebus testaceipes,nosource,Spatially implicit approach}
}

@article{Lopes2009a,
  title = {Toxicity of Ivermectin on Cladocerans: {{Comparison}} of Toxic Effects on Daphnia and Ceriodaphnia Species},
  author = {Lopes, Christelle and Charles, Sandrine and Vollat, Bernard and Garric, Jeanne},
  year = {2009},
  month = oct,
  journal = {Environmental Toxicology and Chemistry},
  volume = {28},
  number = {10},
  pages = {2160},
  issn = {0730-7268},
  doi = {10.1897/08-607.1},
  abstract = {Interspecies differences in contaminant sensitivity are measured to assess environmental risk based on species sensitivity distribution. The present study was intended to demonstrate the importance of studying the effects of contaminants on the life-history traits of various species. To do this, we compared the effects of ivermectin on the survival, growth, and reproduction of two cladoceran species (Daphnia magna and Ceriodaphnia dubia) and two strains of D. magna (one Japanese and one European). Ivermectin is widely used against endo- and ectoparasites in livestock and pets and is known for its high toxicity. Local aquatic ecosystems can be contaminated due to direct excretion into surface waters, but few data are available about the chronic effects of ivermectin on aquatic organisms. Adult daphnids were exposed to concentrations from 0 to 1 ng/L. Our results show a significant effect on all the life-history traits measured and reveal inter- and intraspecies differences. The no-observed-effect concentration found for growth and reproduction is 0.0003 ng/L for D. magna versus 0.001 ng/L for C. dubia, and the lowest-observed-effect concentration is 0.001 ng/L for D. magna versus 0.01 ng/L for C. dubia. C. dubia is smaller than D. magna and appeared to be less sensitive to ivermectin. The European strain of D. magna exhibited less resistance than the Japanese strain. A bias in the sex ratio was observed for all strains tested.},
  pmid = {19456204},
  keywords = {Animals,Cladocera,Cladocera: drug effects,Cladocera: growth \& development,Daphnia,Daphnia: drug effects,Daphnia: growth \& development,Ivermectin,Ivermectin: toxicity,nosource,Reproduction,Reproduction: drug effects,Species Specificity,Survival Rate,Time Factors,Toxicity Tests}
}

@article{Lopes2010,
  title = {Comparison of Spatially Implicit and Explicit Approaches to Model Plant Infestation by Insect Pests},
  author = {Lopes, Christelle and Spataro, Thierry and Arditi, Roger},
  year = {2010},
  month = mar,
  journal = {Ecological Complexity},
  volume = {7},
  number = {1},
  pages = {1--12},
  publisher = {Elsevier B.V.},
  issn = {1476945X},
  doi = {10.1016/j.ecocom.2009.03.006},
  abstract = {Spatial heterogeneities have been explored in many different ways in population dynamic models. We investigate here the way in which space should be considered in the dynamics of an aphid-parasitoid system in a melon greenhouse, in order to plan a biological control program at a wide scale. By comparing a non-spatial model with a spatially explicit model (a lattice one), we show a strong difference between predictions and we thus confirm that it is essential to take space into account in such closed and structured environments when describing the spatial heterogeneities observed in the field. The way in which space should be considered in such system is tested by comparing the spatially explicit model with a new implicit approach, which describes the level of plant infestation by a continuous variable corresponding to the number of plants with a given density of pests at a given time. When the explicit model needs as many equations as plants in the greenhouse, our novel approach has only a partial differential equation. We infer from the comparisons between the two spatial models that the predicted host-parasitoid dynamics are similar under most conditions. Some differences occur when local dispersal (considered only in the explicit model) is high and it can have a strong impact on population dynamics but does not change the conclusions for crop protection. We conclude that the new spatially implicit model thus generate relevant predictions with a more synthetic formalism than the common plant-by-plant model and it will thus be more adapted to test biological control strategies at a higher scale than the greenhouse. ?? 2009 Elsevier B.V. All rights reserved.},
  isbn = {1476-945X},
  keywords = {Aphis gossypii,Dispersal,Host-parasitoid dynamics,Lysiphlebus testaceipes,nosource,Spatial scales,Spatially structured models}
}

@article{Lopes2011,
  title = {Is {{PCBs}} Concentration Variability between and within Freshwater Fish Species Explained by Their Contamination Pathways?},
  author = {Lopes, Christelle and Perga, M. E. and Peretti, A. and Roger, M. C. and Persat, H. and Babut, Marc},
  year = {2011},
  month = oct,
  journal = {Chemosphere},
  volume = {85},
  number = {3},
  eprint = {21893333},
  eprinttype = {pubmed},
  pages = {502--508},
  issn = {00456535},
  doi = {10.1016/j.chemosphere.2011.08.011},
  abstract = {Many chemical, physiological, and trophic factors are known to affect bioaccumulation of polychlorinated biphenyls (PCBs) in biota. Understanding the primary factors affecting fish contamination is critical for predicting and assessing risks to upper-trophic level consumers, including humans. Here we identify PCB contamination pathways that could explain within- and between-species variability in fish concentration levels. Three freshwater river fish species (barbel, chub and bream) were sampled at three sites along the Rhone River (France) where fish consumption is partially prohibited because of PCB levels exceeding the European health-based benchmark. The trophic position was assessed using an innovative approach based on stable isotope analyses and Bayesian inference, which takes into account both isotope data variability and parameter uncertainty. The effect of foraging habitat on fish contamination was addressed using stable isotope mixing models. The fish trophic position and PCB concentrations were found to be unrelated while the exploitation of sediment detrital carbon as a food source appeared to be a critical factor affecting fish contamination. Fish length, PCB concentration of the sediment, and individual fish foraging habitat (exploitation of detrital versus planktonic carbon sources) explained 80\% of within- and between-species variability observed in PCB concentrations. These results, obtained for species that have overlapping TPs and exploit different carbon sources, reveal that the important factor in fish PCB contamination is not only what fish consume, but also and essentially the feeding location. {\copyright} 2011 Elsevier Ltd.},
  isbn = {0045-6535},
  pmid = {21893333},
  keywords = {Bayesian inference,Freshwater river fish,Mixing model,nosource,PCB contamination,Predictive models,Stable isotopes}
}

@article{Lopes2012,
  title = {Transfer of {{PCBs}} from Bottom Sediment to Freshwater River Fish: {{A}} Food-Web Modelling Approach in the {{RhOne River}} ({{France}}) in Support of Sediment Management},
  author = {Lopes, Christelle and Persat, H. and Babut, Marc},
  year = {2012},
  month = jul,
  journal = {Ecotoxicology and Environmental Safety},
  volume = {81},
  eprint = {22627014},
  eprinttype = {pubmed},
  pages = {17--26},
  publisher = {Elsevier},
  issn = {01476513},
  doi = {10.1016/j.ecoenv.2012.04.007},
  abstract = {Since 2005, restrictions have been because of fish consumption along the Rhone River because of high polychlorobiphenyl (PCB) concentrations, which have resulted inadverse economic consequences for professional fisheries in affected areas. French environmental authorities have expended considerable efforts to research sediment remediation strategies and development of sediment quality guidelines designed to protect the health of humans consuming RhOne River fish. Here we: (1) develop a bioaccumulation food-web model that describes PCB concentrations in three common freshwater fish species of the RhOne River, using Bayesian inference to estimate the input parameters; (2) test the predictive power of the model in terms of risk assessment for fish consumption; and (3) discuss the use of this approach to develop sediment quality guidelines that protect the health of humans consuming RhOne River fish. The bioaccumulation model predictions are protective for human consumer of fish and are efficient for use in risk assessment. For example, 85\% of the predicted values were within a factor of 5 of measured CB153 concentrations in fish. Using sensitivity analyses, the major role played by sediment and diet behaviors on bioaccumulation process is illustrated: the parameters involved in the respiratory process (contamination from water) have little impact on model outputs, whereas the parameters related to diet and digestion processes are the most sensitive. The bioaccumulation model was applied to derive sediment concentrations compatible with safe fish consumption. The resulting PCB sediment thresholds (expressed as the sum of seven PCB indicator congeners) that are protective for the consumption of the fish species ranged from 0.7 to 3. ng/g (dw). {\copyright} 2012 Elsevier Inc.},
  isbn = {0147-6513},
  pmid = {22627014},
  keywords = {Bayesian inference,Food-web bioaccumulation model,nosource,PCB,Risk assessment,Sediment management}
}

@article{lotero2016rich,
  title = {Rich Do Not Rise Early: Spatio-Temporal Patterns in the Mobility Networks of Different Socio-Economic Classes},
  author = {Lotero, Laura and Hurtado, Rafael G and Flor{\'i}a, Luis Mario and {G{\'o}mez-Garde{\~n}es}, Jes{\'u}s},
  year = {2016},
  journal = {Royal Society open science},
  volume = {3},
  number = {10},
  pages = {150654},
  publisher = {The Royal Society},
  keywords = {nosource}
}

@article{Lowell1982,
  title = {The Stability of Bicycles},
  author = {Lowell, J and McKell, {\relax HD}},
  year = {1982},
  journal = {American Journal of Physics},
  pages = {1106--1112},
  issn = {00029505},
  doi = {10.1119/1.12893},
  keywords = {nosource}
}

@article{luca2021survey,
  title = {A Survey on Deep Learning for Human Mobility},
  author = {Luca, Massimiliano and Barlacchi, Gianni and Lepri, Bruno and Pappalardo, Luca},
  year = {2021},
  journal = {ACM Computing Surveys (CSUR)},
  volume = {55},
  number = {1},
  pages = {1--44},
  publisher = {ACM New York, NY},
  doi = {10.1145/3485125},
  keywords = {nosource}
}

@article{lucas2011making,
  title = {Making the Connections between Transport Disadvantage and the Social Exclusion of Low Income Populations in the {{Tshwane Region}} of {{South Africa}}},
  author = {Lucas, Karen},
  year = {2011},
  journal = {Journal of Transport Geography},
  volume = {19},
  number = {6},
  pages = {1320--1334},
  publisher = {Elsevier},
  doi = {10.1016/j.jtrangeo.2011.02.007},
  keywords = {nosource}
}

@article{Luck2018,
  title = {Learning to Rank for Censored Survival Data},
  author = {Luck, Margaux and Sylvain, Tristan and Cohen, Joseph Paul and Cardinal, Heloise and Lodi, Andrea and Bengio, Yoshua},
  year = {2018},
  eprint = {1806.01984},
  abstract = {Survival analysis is a type of semi-supervised ranking task where the target output (the survival time) is often right-censored. Utilizing this information is a challenge because it is not obvious how to correctly incorporate these censored examples into a model. We study how three categories of loss functions, namely partial likelihood methods, rank methods, and our classification method based on a Wasserstein metric (WM) and the non-parametric Kaplan Meier estimate of the probability density to impute the labels of censored examples, can take advantage of this information. The proposed method allows us to have a model that predict the probability distribution of an event. If a clinician had access to the detailed probability of an event over time this would help in treatment planning. For example, determining if the risk of kidney graft rejection is constant or peaked after some time. Also, we demonstrate that this approach directly optimizes the expected C-index which is the most common evaluation metric for ranking survival models.},
  archiveprefix = {arXiv},
  arxivid = {1806.01984},
  keywords = {nosource}
}

@article{luiBayesianFeatureAllocation2020,
  title = {A {{Bayesian Feature Allocation Model}} for {{Identification}} of {{Cell Subpopulations Using Cytometry Data}}},
  author = {Lui, Arthur and Lee, Juhee and Thall, Peter F. and Daher, May and Rezvani, Katy and Barar, Rafet},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.08609 [stat]},
  eprint = {2002.08609},
  primaryclass = {stat},
  urldate = {2021-05-04},
  abstract = {A Bayesian feature allocation model (FAM) is presented for identifying cell subpopulations based on multiple samples of cell surface or intracellular marker expression level data obtained by cytometry by time of flight (CyTOF). Cell subpopulations are characterized by differences in expression patterns of makers, and individual cells are clustered into the subpopulations based on the patterns of their observed expression levels. A finite Indian buffet process is used to model subpopulations as latent features, and a model-based method based on these latent feature subpopulations is used to construct cell clusters within each sample. Non-ignorable missing data due to technical artifacts in mass cytometry instruments are accounted for by defining a static missing data mechanism. In contrast to conventional cell clustering methods based on observed marker expression levels that are applied separately to different samples, the FAM based method can be applied simultaneously to multiple samples, and can identify important cell subpopulations likely to be missed by conventional clustering. The proposed FAM based method is applied to jointly analyze three datasets, generated by CyTOF, to study natural killer (NK) cells. Because the subpopulations identified by the FAM may define novel NK cell subsets, this statistical analysis may provide useful information about the biology of NK cells and their potential role in cancer immunotherapy which may lead, in turn, to development of improved cellular therapies. Simulation studies of the proposed method's behavior under two cases of known subpopulations also are presented, followed by analysis of the CyTOF NK cell surface marker data.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/home/gkonkamking/Zotero/storage/A3I2XY7G/Lui et al. - 2020 - A Bayesian Feature Allocation Model for Identifica.pdf}
}

@article{lunn2000winbugs,
  title = {{{WinBUGS}} -- {{A Bayesian}} Modelling Framework: {{Concepts}}, Structure, and Extensibility},
  author = {Lunn, David J and Thomas, Andrew and Best, Nicky and Spiegelhalter, David},
  year = {2000},
  journal = {Statistics and Computing},
  volume = {10},
  number = {4},
  pages = {325--337},
  publisher = {Springer},
  issn = {09603174},
  doi = {10.1023/A:1008929526011},
  abstract = {WinBUGS is a fully extensible modular framework for constructing and analysing Bayesian full probability models. Models may be specified either textually via the BUGS language or pictorially using a graphical interface called DoodleBUGS. WinBUGS processes the model specification and constructs an object-oriented representation of the model. The software offers a user-interface, based on dialogue boxes and menu commands, throughwhich the modelmay then be analysed using Markov chain Monte Carlo techniques. In this paper we discuss how and why various modern computing concepts, such as object-orientation and run-time linking, feature in the software's design. We also discuss how the framework may be extended. It is possible to write specific applications that form an apparently seamless interface with WinBUGS for users with specialized requirements. It is also possible to interface withWinBUGS at a lower level by incorporating new object types that may be used by WinBUGS without knowledge of the modules in which they are implemented. Neither of these types of extension require access to, or even recompilation of, theWinBUGS source-code.},
  isbn = {0960-3174},
  pmid = {2601},
  keywords = {bugs,directed acyclic graphs,markov chain monte carlo,nosource,object-,orientation,run-time linking,type extension,winbugs}
}

@article{Lunzer2010,
  title = {Pervasive Cryptic Epistasis in Molecular Evolution},
  author = {Lunzer, Mark and Brian Golding, G. and Dean, Antony M.},
  year = {2010},
  journal = {PLoS Genetics},
  volume = {6},
  number = {10},
  pages = {1--10},
  issn = {15537390},
  doi = {10.1371/journal.pgen.1001162},
  abstract = {The functional effects of most amino acid replacements accumulated during molecular evolution are unknown, because most are not observed naturally and the possible combinations are too numerous. We created 168 single mutations in wild-type Escherichia coli isopropymalate dehydrogenase (IMDH) that match the differences found in wild-type Pseudomonas aeruginosa IMDH. 104 mutant enzymes performed similarly to E. coli wild-type IMDH, one was functionally enhanced, and 63 were functionally compromised. The transition from E. coli IMDH, or an ancestral form, to the functional wild-type P. aeruginosa IMDH requires extensive epistasis to ameliorate the combined effects of the deleterious mutations. This result stands in marked contrast with a basic assumption of molecular phylogenetics, that sites in sequences evolve independently of each other. Residues that affect function are scattered haphazardly throughout the IMDH structure. We screened for compensatory mutations at three sites, all of which lie near the active site and all of which are among the least active mutants. No compensatory mutations were found at two sites indicating that a single site may engage in compound epistatic interactions. One complete and three partial compensatory mutations of the third site are remote and lie in a different domain. This demonstrates that epistatic interactions can occur between distant ({$>$}20{\AA}) sites. Phylogenetic analysis shows that incompatible mutations were fixed in different lineages.},
  isbn = {1553-7404 (Electronic){\textbackslash}r1553-7390 (Linking)},
  pmid = {20975933},
  keywords = {nosource}
}

@article{luoConStrainsIdentifiesMicrobial2015,
  title = {{{ConStrains}} Identifies Microbial Strains in Metagenomic Datasets},
  author = {Luo, Chengwei and Knight, Rob and Siljander, Heli and Knip, Mikael and Xavier, Ramnik J and Gevers, Dirk},
  year = {2015},
  month = oct,
  journal = {Nature biotechnology},
  volume = {33},
  number = {10},
  pages = {1045--1052},
  issn = {1087-0156},
  doi = {10.1038/nbt.3319},
  urldate = {2020-10-22},
  abstract = {An important fraction of microbial diversity is harbored in strain individuality, so identification of conspecific bacterial strains is imperative for improved understanding of microbial community functions. Limitations in bioinformatics and sequencing technologies have to date precluded strain identification owing to difficulties in phasing short reads to faithfully recover the original strain-level genotypes, which have highly similar sequences. We present ConStrains, an open-source algorithm that identifies conspecific strains from metagenomic sequence data and reconstructs the phylogeny of these strains in microbial communities. The algorithm uses single-nucleotide polymorphism (SNP) patterns in a set of universal genes to infer within-species structures that represent strains. Applying ConStrains to simulated and host-derived data sets provides insights into microbial community dynamics.},
  pmcid = {PMC4676274},
  pmid = {26344404},
  file = {/home/gkonkamking/Zotero/storage/SV5IULL9/Luo et al. - 2015 - ConStrains identifies microbial strains in metagen.pdf}
}

@article{Luttik2009,
  title = {Extrapolation Factors for Small Samples of Pesticide Toxicity Data: Special Focus on {{LD50}} Values for Birds and Mammals},
  author = {Luttik, R and Aldenberg, T},
  year = {2009},
  journal = {Environmental Toxicology and {\dots}},
  volume = {16},
  number = {9},
  pages = {1785--1788},
  keywords = {duplicate-citation-key,hazard,nosource,risk assessment}
}

@article{Lysy2016,
  title = {Model Comparison and Assessment for Single Particle Tracking in Biological Fluids},
  author = {Lysy, Martin and Pillai, Natesh S. and Hill, David B. and Forest, M. Gregory and Mellnik, John W. R. and Vasquez, Paula A. and McKinley, Scott A.},
  year = {2016},
  journal = {Journal of the American Statistical Association},
  volume = {111},
  number = {516},
  pages = {1413--1426},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1158716},
  abstract = {Abstract.State-of-the-art techniques in passive particle-tracking microscopy provide high-resolution path trajectories of diverse foreign particles in biological fluids. For particles on the order of 1 {\textmu}m diameter, these paths are generally inconsistent with simple Brownian motion. Yet, despite an abundance of data confirming these findings and their wide-ranging scientific implications, stochastic modeling of the complex particle motion has received comparatively little attention. Even among posited models, there is virtually no literature on likelihood-based inference, model comparisons, and other quantitative assessments. In this article, we develop a rigorous and computationally efficient Bayesian methodology to address this gap. We analyze two of the most prevalent candidate models for 30 second paths of 1 {\textmu}m diameter tracer particles in human lung mucus: fractional Brownian motion (fBM) and a Generalized Langevin Equation (GLE) consistent with viscoelastic theory. Our model comparisons distinctly favor GLE over fBM, with the former describing the data remarkably well up to the timescales for which we have reliable information.},
  isbn = {0162-1459},
  keywords = {bayes factors,Bayes factors,bayesian,Bayesian predictive distributions,Fr,fractional brownian motion,nosource,predictive distributions}
}

@article{lyuMutationalSignatureLearning2020,
  title = {Mutational Signature Learning with Supervised Negative Binomial Non-Negative Matrix Factorization},
  author = {Lyu, Xinrui and Garret, Jean and R{\"a}tsch, Gunnar and Lehmann, Kjong-Van},
  year = {2020},
  month = jul,
  journal = {Bioinformatics},
  volume = {36},
  number = {Supplement\_1},
  pages = {i154-i160},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btaa473},
  urldate = {2021-03-31},
  abstract = {Understanding the underlying mutational processes of cancer patients has been a long-standing goal in the community and promises to provide new insights that could improve cancer diagnoses and treatments. Mutational signatures are summaries of the mutational processes, and improving the derivation of mutational signatures can yield new discoveries previously obscured by technical and biological confounders. Results from existing mutational signature extraction methods depend on the size of available patient cohort and solely focus on the analysis of mutation count data without considering the exploitation of metadata.Here we present a supervised method that utilizes cancer type as metadata to extract more distinctive signatures. More specifically, we use a negative binomial non-negative matrix factorization and add a support vector machine loss. We show that mutational signatures extracted by our proposed method have a lower reconstruction error and are designed to be more predictive of cancer type than those generated by unsupervised methods. This design reduces the need for elaborate post-processing strategies in order to recover most of the known signatures unlike the existing unsupervised signature extraction methods. Signatures extracted by a supervised model used in conjunction with cancer-type labels are also more robust, especially when using small and potentially cancer-type limited patient cohorts. Finally, we adapted our model such that molecular features can be utilized to derive an according mutational signature. We used APOBEC expression and MUTYH mutation status to demonstrate the possibilities that arise from this ability. We conclude that our method, which exploits available metadata, improves the quality of mutational signatures as well as helps derive more interpretable representations.https://github.com/ratschlab/SNBNMF-mutsig-public.Supplementary data are available at Bioinformatics online.},
  file = {/home/gkonkamking/pCloudDrive/papers/Lyu et al_2020_Mutational signature learning with supervised negative binomial non-negative.pdf}
}

@article{macarthur1957relative,
  title = {On the Relative Abundance of Bird Species},
  author = {MACARTHUR, ROBERT H},
  year = {1957},
  journal = {Proceeding of the national academy of science of USA},
  volume = {43},
  number = {3},
  pages = {293--295},
  publisher = {National Academy of Sciences},
  issn = {0027-8424},
  doi = {10.1073/pnas.43.3.293},
  keywords = {curva de componente de domin{\^a}ncia,nosource}
}

@article{Maceachern1994,
  title = {Estimating Normal Means with a Conjugate Style Dirichlet Process Prior},
  author = {Maceachern, Steven N.},
  year = {1994},
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {23},
  number = {3},
  pages = {727--741},
  issn = {0361-0918},
  doi = {10.1080/03610919408813196},
  abstract = {In this article, the Dirichlet process prior is used to provide a nonparametric Bayesian estimate of a vector of normal means. In the past there have been computational difficulties with this model. This article solves the computational difficulties by developing a "Gibbs sampler" algorithm. The estimator developed in this article is then compared to parametric empirical Bayes estimators (PEB) and nonparametric empirical Bayes estimators (NPEB) in a Monte Carlo study. The Monte Carlo study demonstrates that in some conditions the PEB is better than the NPEB and in other conditions the NPEB is better than the PEB. The Monte Carlo study also shows that the estimator developed in this article produces estimates that are about as good as the PEB when the PEB is better and produces estimates that are as good as the NPEB estimator when that method is better},
  isbn = {0361091940881},
  keywords = {nosource}
}

@article{MacEachern1998,
  title = {Estimating Mixture of Dirichlet Process Models},
  author = {MacEachern, Steven N. and M{\"u}ller, Peter},
  year = {1998},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {7},
  number = {2},
  pages = {223--238},
  issn = {1061-8600},
  doi = {10.1080/10618600.1998.10474772},
  abstract = {Current Gibbs sampling schemes in mixture of Dirichlet process (MDP) models are restricted to using "conjugate" base measures that allow analytic evaluation of the transition probabilities when resampling configurations, or alternatively need to rely on approximate numeric evaluations of some transition probabilities. Implementation of Gibbs sampling in more general MDP models is an open and important problem because most applications call for the use of nonconjugate base measures, In this article we propose a conceptual framework for computational strategies. This framework provides a perspective on current methods, facilitates comparisons between them, and leads to several new methods that expand the scope of MDP models to nonconjugate situations. We discuss one in detail. The basic strategy is based on expanding the parameter vector, and is applicable for MDP models with arbitrary base measure and likelihood. Strategies are also presented for the important class of normal-normal MDP models and for problems with fixed or few hyperparameters, The proposed algorithms are easily implemented and illustrated with an application.},
  isbn = {978-1-4612-1732-9},
  keywords = {gibbs sampling,hierarchical models,markov chain monte carlo,nosource,simu-}
}

@article{maceachern1999dependent,
  title = {Dependent Nonparametric Processes},
  author = {MacEachern, S N},
  year = {1999},
  journal = {ASA Proceedings of the Section on Bayesian Statistical Science},
  pages = {50--55},
  publisher = {Alexandria, VA: American Statistical Association},
  abstract = {DEPENDENT NONPARAM ETRIC PROCESSES Steven N. MacEachern , The Ohio State University Department of Statistics, 404 Cockins Hall, 1958 Neil Ave., Columbus, OH 43210 (snmstat.ohio-state.edu) Key Words: Dirichlet process , model adequacy, nonparametric ...},
  keywords = {nosource}
}

@article{MacEachern2000a,
  ids = {maceachern2000dependent},
  title = {Dependent Dirichlet Processes},
  author = {MacEachern, S N},
  year = {2000},
  journal = {Proceedings of the 1999 Joint Statistical Meetings},
  pages = {50--55},
  keywords = {bnp,dependent dirichlet processes,dirichlet process,nosource}
}

@article{Macedo-Rouet2009,
  title = {Students' Performance and Satisfaction with {{Web}} vs. Paper-Based Practice Quizzes and Lecture Notes},
  author = {{Macedo-Rouet}, M??nica and Ney, Muriel and Charles, Sandrine and {Lallich-Boidin}, Genevi??ve},
  year = {2009},
  journal = {Computers and Education},
  volume = {53},
  number = {2},
  pages = {375--384},
  publisher = {Elsevier},
  issn = {03601315},
  doi = {10.1016/j.compedu.2009.02.013},
  abstract = {The use of computers to deliver course-related materials is rapidly expanding in most universities. Yet the effects of computer vs. printed delivery modes on students' performance and motivation are not yet fully known. We compared the impacts of Web vs. paper to deliver practice quizzes that require information search in lecture notes. Hundred and twenty two undergraduate students used either a web site or printed documents to answer 18 mathematics questions during a tutored session. A revised Web site was designed based on ergonomic criteria, to test the hypothesis that improved usability would decrease time spent on the task, the number of pages consulted, and students' perceived cognitive load. The group working with printed documents had the highest performance. Furthermore, students perceived the paper materials as less effortful to read, and expressed preference for printing lecture notes and questions. However, students appreciated having a Web site available. No differences were found between the two sites. We conclude that Web delivery imposed higher perceived cognitive load due to the need to read lengthy documents. We suggest possible ways to improve Web-based practice materials, such as simultaneous display of questions and lecture notes. ?? 2009 Elsevier Ltd. All rights reserved.},
  isbn = {0360-1315},
  keywords = {Interactive learning environments,Media in education,Navigation,nosource,Pedagogical issues,Post-secondary education}
}

@article{macherySemanticsCrossculturalStyle2004,
  title = {Semantics, Cross-Cultural Style},
  author = {Machery, Edouard and Mallon, Ron and Nichols, Shaun and Stich, Stephen P},
  year = {2004},
  month = jul,
  journal = {Cognition},
  volume = {92},
  number = {3},
  pages = {B1-B12},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2003.10.003},
  urldate = {2020-04-16},
  abstract = {Theories of reference have been central to analytic philosophy, and two views, the descriptivist view of reference and the causal-historical view of reference, have dominated the field. In this research tradition, theories of reference are assessed by consulting one's intuitions about the reference of terms in hypothetical situations. However, recent work in cultural psychology (e.g. Nisbett, R. E., Peng, K., Choi, I., \& Norenzayan, A. (2001). Culture and systems of thought: holistic vs. analytic cognition. Psychological Review, 108, 291--310.) has shown systematic cognitive differences between East Asians and Westerners, and some work indicates that this extends to intuitions about philosophical cases (Weinberg, J., Nichols, S., \& Stich, S. (2001). Normativity and epistemic intuitions. Philosophical Topics 29(1\&2), 429--459.) In light of these findings on cultural differences, an experiment was conducted which explored intuitions about reference in Westerners and East Asians. The experiment indicated that, for certain central cases, Westerners are more likely than East Asians to report intuitions that are consistent with the causal-historical view. These results constitute prima facie evidence that semantic intuitions vary from culture to culture, and the paper argues that this fact raises questions about the nature of the philosophical enterprise of developing a theory of reference.},
  langid = {english},
  keywords = {Causal-historical theory,Cultural differences,Descriptivism,Kripke,Proper names,Reference,Semantic intuitions}
}

@article{MacKay1995,
  title = {The Development of {{South African}} Water Quality Guidelines for the Natural Aquatic Environment},
  booktitle = {Water Science and Technology},
  author = {MacKay, H.M and Roux, D.J and Ashton, P.J and {van Vliet}, H.R and Jooste, S},
  year = {1995},
  keywords = {nosource}
}

@phdthesis{malik2018,
  title = {Bias and beyond in Digital Trace Data},
  author = {Malik, Momin M.},
  year = {2018},
  month = aug,
  address = {Pittsburgh, PA},
  school = {Carnegie Mellon University}
}

@article{malsiner-walliComparingSpikeSlab2018,
  title = {Comparing {{Spike}} and {{Slab Priors}} for {{Bayesian Variable Selection}}},
  author = {{Malsiner-Walli}, Gertraud and Wagner, Helga},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.07259 [stat]},
  eprint = {1812.07259},
  primaryclass = {stat},
  urldate = {2021-03-30},
  abstract = {An important task in building regression models is to decide which regressors should be included in the final model. In a Bayesian approach, variable selection can be performed using mixture priors with a spike and a slab component for the effects subject to selection. As the spike is concentrated at zero, variable selection is based on the probability of assigning the corresponding regression effect to the slab component. These posterior inclusion probabilities can be determined by MCMC sampling. In this paper we compare the MCMC implementations for several spike and slab priors with regard to posterior inclusion probabilities and their sampling efficiency for simulated data. Further, we investigate posterior inclusion probabilities analytically for different slabs in two simple settings. Application of variable selection with spike and slab priors is illustrated on a data set of psychiatric patients where the goal is to identify covariates affecting metabolism.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {/home/gkonkamking/Zotero/storage/6NFCRSPZ/Malsiner-Walli and Wagner - 2018 - Comparing Spike and Slab Priors for Bayesian Varia.pdf}
}

@article{malsiner-walliIdentifyingMixturesMixtures2017,
  title = {Identifying {{Mixtures}} of {{Mixtures Using Bayesian Estimation}}},
  author = {{Malsiner-Walli}, Gertraud and {Fr{\"u}hwirth-Schnatter}, Sylvia and Gr{\"u}n, Bettina},
  year = {2017},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {2},
  pages = {285--295},
  publisher = {Taylor \& Francis},
  issn = {1061-8600},
  doi = {10.1080/10618600.2016.1200472},
  urldate = {2022-07-29},
  abstract = {The use of a finite mixture of normal distributions in model-based clustering allows us to capture non-Gaussian data clusters. However, identifying the clusters from the normal components is challenging and in general either achieved by imposing constraints on the model or by using post-processing procedures. Within the Bayesian framework, we propose a different approach based on sparse finite mixtures to achieve identifiability. We specify a hierarchical prior, where the hyperparameters are carefully selected such that they are reflective of the cluster structure aimed at. In addition, this prior allows us to estimate the model using standard MCMC sampling methods. In combination with a post-processing approach which resolves the label switching issue and results in an identified model, our approach allows us to simultaneously (1) determine the number of clusters, (2) flexibly approximate the cluster distributions in a semiparametric way using finite mixtures of normals and (3) identify cluster-specific parameters and classify observations. The proposed approach is illustrated in two simulation studies and on benchmark datasets. Supplementary materials for this article are available online.},
  pmid = {28626349},
  keywords = {Bayesian nonparametric mixture model,Dirichlet prior,Finite mixture model,Model-based clustering,Normal gamma prior,Number of components},
  file = {/home/gkonkamking/pCloudDrive/papers/Malsiner-Walli et al_2017_Identifying Mixtures of Mixtures Using Bayesian Estimation.pdf}
}

@article{Maltby2005,
  title = {Insecticide Species Sensitivity Distributions: Importance of Test Species Selection and Relevance to Aquatic Ecosystems.},
  author = {Maltby, Lorraine and Blake, Naomi and Brock, Theo C. M. and {van den Brink}, Paul J},
  year = {2005},
  month = feb,
  journal = {Environmental toxicology and chemistry / SETAC},
  volume = {24},
  number = {2},
  eprint = {15719998},
  eprinttype = {pubmed},
  pages = {379--388},
  issn = {0730-7268},
  doi = {10.1897/04-025R.1},
  abstract = {Single-species acute toxicity data and (micro)mesocosm data were collated for 16 insecticides. These data were used to investigate the importance of test-species selection in constructing species sensitivity distributions (SSDs) and the ability of estimated hazardous concentrations (HCs) to protect freshwater aquatic ecosystems. A log-normal model was fitted to a minimum of six data points, and the resulting distribution was used to estimate lower (95\% confidence), median (50\% confidence), and upper (5\% confidence) 5\% HC (HC5) values. Species sensitivity distributions for specific taxonomic groups (vertebrates, arthropods, nonarthropod invertebrates), habitats (saltwater, freshwater, lentic, lotic), and geographical regions (Palaearctic, Nearctic, temperate, tropical) were compared. The taxonomic composition of the species assemblage used to construct the SSD does have a significant influence on the assessment of hazard, but the habitat and geographical distribution of the species do not. Moreover, SSDs constructed using species recommended in test guidelines did not differ significantly from those constructed using nonrecommended species. Hazardous concentrations estimated using laboratory-derived acute toxicity data for freshwater arthropods (i.e., the most sensitive taxonomic group) were compared to the response of freshwater ecosystems exposed to insecticides. The sensitivity distributions of freshwater arthropods were similar for both field and laboratory exposure, and the lower HC5 (95\% protection with 95\% confidence) estimate was protective of adverse ecological effects in freshwater ecosystems. The corresponding median HC5 (95\% protection level with 50\% confidence) was generally protective of single applications of insecticide but not of continuous or multiple applications. In the latter cases, a safety factor of at least five should be applied to the median HC5.},
  isbn = {0730-7268},
  pmid = {15719998},
  keywords = {ecological risk assessment,nosource}
}

@article{maMultivariatePoissonlognormalRegression2008,
  title = {A Multivariate {{Poisson-lognormal}} Regression Model for Prediction of Crash Counts by Severity, Using {{Bayesian}} Methods},
  author = {Ma, Jianming and Kockelman, Kara M. and Damien, Paul},
  year = {2008},
  month = may,
  journal = {Accident Analysis \& Prevention},
  volume = {40},
  number = {3},
  pages = {964--975},
  issn = {0001-4575},
  doi = {10.1016/j.aap.2007.11.002},
  urldate = {2020-02-20},
  abstract = {Numerous efforts have been devoted to investigating crash occurrence as related to roadway design features, environmental factors and traffic conditions. However, most of the research has relied on univariate count models; that is, traffic crash counts at different levels of severity are estimated separately, which may neglect shared information in unobserved error terms, reduce efficiency in parameter estimates, and lead to potential biases in sample databases. This paper offers a multivariate Poisson-lognormal (MVPLN) specification that simultaneously models crash counts by injury severity. The MVPLN specification allows for a more general correlation structure as well as overdispersion. This approach addresses several questions that are difficult to answer when estimating crash counts separately. Thanks to recent advances in crash modeling and Bayesian statistics, parameter estimation is done within the Bayesian paradigm, using a Gibbs Sampler and the Metropolis--Hastings (M--H) algorithms for crashes on Washington State rural two-lane highways. Estimation results from the MVPLN approach show statistically significant correlations between crash counts at different levels of injury severity. The non-zero diagonal elements suggest overdispersion in crash counts at all levels of severity. The results lend themselves to several recommendations for highway safety treatments and design policies. For example, wide lanes and shoulders are key for reducing crash frequencies, as are longer vertical curves.},
  langid = {english},
  keywords = {Bayes' theorem,Bayesian inference,Crash severity,Gibbs sampler,Highway safety,Markov chain Monte Carlo (MCMC) simulation,Metropolis--Hastings algorithm,Multivariate Poisson-lognormal regression},
  file = {/home/gkonkamking/Zotero/storage/DZ2MN6DD/Ma et al. - 2008 - A multivariate Poisson-lognormal regression model .pdf}
}

@techreport{Manual2015,
  title = {Stan Modeling Language User ' s Guide and Reference Manual},
  author = {Manual, Reference},
  year = {2014},
  keywords = {nosource}
}

@phdthesis{margossian2022modernizing,
  title = {Modernizing Markov Chains Monte Carlo for Scientific and Bayesian Modeling},
  author = {Margossian, Charles C},
  year = {2022},
  school = {Columbia University},
  file = {/home/gkonkamking/pCloudDrive/papers/Margossian_2022_Modernizing markov chains monte carlo for scientific and bayesian modeling.pdf}
}

@misc{margossianNested$hatR$2021,
  title = {Nested \${\textbackslash}hat {{R}}\$: {{Assessing Convergence}} for {{Markov}} Chain {{Monte Carlo}} When Using Many Short Chains},
  shorttitle = {Nested \${\textbackslash}hat {{R}}\$},
  author = {Margossian, Charles C. and Hoffman, Matthew D. and Sountsov, Pavel},
  year = {2021},
  month = oct,
  number = {arXiv:2110.13017},
  eprint = {2110.13017},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.13017},
  urldate = {2022-07-07},
  abstract = {When using Markov chain Monte Carlo (MCMC) algorithms, we can increase the number of samples either by running longer chains or by running more chains. Practitioners often prefer the first approach because chains need an initial ``warmup'' phase to forget their initial states; the number of operations needed for warmup is constant with respect to chain length but increases linearly with the number of chains. However, highly parallel hardware accelerators such as GPUs may allow us to run many chains in parallel almost as quickly as a single chain. This makes it more attractive to run many chains with a short sampling phase. Unfortunately, existing diagnostics are not designed for the ``many short chains'' regime. This is notably the case for the popular \${\textbackslash}hat R\$ statistic which claims convergence only if the effective sample size {\textbackslash}textit\{per chain\} is sufficiently large. We present \${\textbackslash}mathfrak n{\textbackslash}hat R\$, a generalization of \${\textbackslash}hat R\$, which does not conflate short chains and poor mixing, and offers a useful diagnostic provided we run enough chains and meet certain initialization conditions. We define what constitutes a proper warmup in the many-chains regime and recommend a threshold for \${\textbackslash}mathfrak n {\textbackslash}hat R\$. Furthermore we use \${\textbackslash}mathfrak n {\textbackslash}hat R\$ to construct a warmup scheme with an adaptive length, allowing users to avoid running their MCMC algorithms longer than necessary.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/gkonkamking/pCloudDrive/papers/Margossian et al_2021_Nested $-hat R$.pdf}
}

@article{Marin2012,
  title = {Approximate Bayesian Computational Methods},
  author = {Marin, J M and Pudlo, P and Robert, C P and Ryder, R J},
  year = {2012},
  journal = {Statistics and Computing},
  volume = {22},
  number = {6},
  eprint = {1101.0955v1},
  pages = {1167--1180},
  issn = {0960-3174},
  doi = {10.1007/s11222-011-9288-2},
  abstract = {Approximate Bayesian Computation (ABC) methods, also known as likelihood-free techniques, have appeared in the past ten years as the most satisfactory approach to intractable likelihood problems, first in genetics then in a broader spectrum of applications. However, these methods suffer to some degree from calibration difficulties that make them rather volatile in their implementation and thus render them suspicious to the users of more traditional Monte Carlo methods. In this survey, we study the various improvements and extensions brought on the original ABC algorithm in recent years.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1101.0955v1},
  isbn = {0960-3174},
  keywords = {ABC methodology,Bayesian model choice,Bayesian statistics,CHOICE,CRITICISM,DENSITIES,DIYABC,DYNAMICAL-SYSTEMS,INFERENCE,Likelihood-free methods,LIKELIHOODS,MARGINAL,MODEL SELECTION,PARAMETER-ESTIMATION,PHYLOGEOGRAPHY,SEQUENTIAL MONTE-CARLO},
  file = {/home/gkonkamking/Zotero/storage/HYBJ4FAX/Marin et al. - 2012 - Approximate bayesian computational methods.pdf}
}

@article{Marin2014,
  title = {Relevant Statistics for {{Bayesian}} Model Choice},
  author = {Marin, Jean Michel and Pillai, Natesh S. and Robert, Christian P. and Rousseau, Judith},
  year = {2014},
  journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
  volume = {76},
  number = {5},
  eprint = {1110.4700},
  pages = {833--859},
  issn = {14679868},
  doi = {10.1111/rssb.12056},
  abstract = {The choice of the summary statistics that are used in Bayesian inference and in par-ticular in approximate Bayesian computation algorithms has bearings on the validation of the resulting inference. Those statistics are nonetheless customarily used in approximate Bayesian computation algorithms without consistency checks. We derive necessary and sufficient con-ditions on summary statistics for the corresponding Bayes factor to be convergent, namely to select the true model asymptotically. Those conditions, which amount to the expectations of the summary statistics differing asymptotically under the two models, are quite natural and can be exploited in approximate Bayesian computation settings to infer whether or not a choice of summary statistics is appropriate, via a Monte Carlo validation.},
  archiveprefix = {arXiv},
  arxivid = {1110.4700},
  keywords = {Ancillarity,Approximate Bayesian computation,Asymptotics,Bayes factor,Bayesian model choice,Gaussianity,Likelihood-free methods,nosource,Sufficiency}
}

@misc{martinez-duriveNetMob23DatasetHighresolution2023,
  title = {The {{NetMob23 Dataset}}: {{A High-resolution Multi-region Service-level Mobile Data Traffic Cartography}}},
  shorttitle = {The {{NetMob23 Dataset}}},
  author = {{Mart{\'i}nez-Durive}, Orlando E. and Mishra, Sachit and Ziemlicki, Cezary and Rubrichi, Stefania and Smoreda, Zbigniew and Fiore, Marco},
  year = {2023},
  month = jul,
  number = {arXiv:2305.06933},
  eprint = {2305.06933},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.06933},
  urldate = {2023-10-18},
  abstract = {Digital sources have been enabling unprecedented data-driven and large-scale investigations across a wide range of domains, including demography, sociology, geography, urbanism, criminology, and engineering. A major barrier to innovation is represented by the limited availability of dependable digital datasets, especially in the context of data gathered by mobile network operators or service providers, due to concerns about user privacy and industrial competition. The resulting lack of reference datasets curbs the production of new research methods and results, and prevents verifiability and reproducibility of research outcomes. The NetMob23 dataset offers a rare opportunity to the multidisciplinary research community to access rich data about the spatio-temporal consumption of mobile applications in a developed country. The generation process of the dataset sets a new quality standard, leading to information about the demands generated by 68 popular mobile services, geo-referenced at a high resolution of \$100{\textbackslash}times100\$ \$m{\textasciicircum}2\$ over 20 metropolitan areas in France, and monitored during 77 consecutive days in 2019.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Networking and Internet Architecture},
  file = {/home/gkonkamking/pCloudDrive/papers/Martínez-Durive et al_2023_The NetMob23 Dataset.pdf}
}

@article{martinoHiddenMarkovModels2020,
  title = {Hidden {{Markov Models}} for Multivariate Functional Data},
  author = {Martino, Andrea and Guatteri, Giuseppina and Paganoni, Anna Maria},
  year = {2020},
  month = dec,
  journal = {Statistics \& Probability Letters},
  volume = {167},
  pages = {108917},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2020.108917},
  urldate = {2023-03-19},
  abstract = {In this paper we extend the usual Hidden Markov Models framework, where the observed objects are univariate or multivariate data, to the case of functional data, by modeling the temporal structure of a system of multivariate curves evolving in time.},
  langid = {english},
  keywords = {Clustering,Functional data,Statistical modeling}
}

@article{maruyama2011fully,
  title = {Fully Bayes Factors with a Generalized G-Prior},
  author = {Maruyama, Yuzo and George, Edward I},
  year = {2011},
  journal = {The Annals of Statistics},
  volume = {39},
  number = {5},
  pages = {2740--2765},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/11-AOS917},
  file = {/home/gkonkamking/pCloudDrive/papers/Maruyama_George_2011_Fully bayes factors with a generalized g-prior.pdf}
}

@article{maSamplingCanBe2019,
  title = {Sampling Can Be Faster than Optimization},
  author = {Ma, Yi-An and Chen, Yuansi and Jin, Chi and Flammarion, Nicolas and Jordan, Michael I.},
  year = {2019},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {42},
  pages = {20881--20885},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1820003116},
  urldate = {2020-12-02},
  abstract = {Optimization algorithms and Monte Carlo sampling algorithms have provided the computational foundations for the rapid growth in applications of statistical machine learning in recent years. There is, however, limited theoretical understanding of the relationships between these 2 kinds of methodology, and limited understanding of relative strengths and weaknesses. Moreover, existing results have been obtained primarily in the setting of convex functions (for optimization) and log-concave functions (for sampling). In this setting, where local properties determine global properties, optimization algorithms are unsurprisingly more efficient computationally than sampling algorithms. We instead examine a class of nonconvex objective functions that arise in mixture modeling and multistable systems. In this nonconvex setting, we find that the computational complexity of sampling algorithms scales linearly with the model dimension while that of optimization algorithms scales exponentially.},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Ma et al. - 2019 - Sampling can be faster than optimization.pdf}
}

@article{mastrototaroFastNumericallyStable2022,
  title = {Fast and {{Numerically Stable Particle-Based Online Additive Smoothing}}: {{The AdaSmooth Algorithm}}},
  shorttitle = {Fast and {{Numerically Stable Particle-Based Online Additive Smoothing}}},
  author = {Mastrototaro, Alessandro and Olsson, Jimmy and Alenl{\"o}v, Johan},
  year = {2022},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {0},
  number = {0},
  pages = {1--12},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2022.2118602},
  urldate = {2022-11-16},
  abstract = {We present a novel sequential Monte Carlo approach to online smoothing of additive functionals in a very general class of path-space models. Hitherto, the solutions proposed in the literature suffer from either long-term numerical instability due to particle-path degeneracy or, in the case that degeneracy is remedied by particle approximation of the so-called backward kernel, high computational demands. In order to balance optimally computational speed against numerical stability, we propose to furnish a (fast) naive particle smoother, propagating recursively a sample of particles and associated smoothing statistics, with an adaptive backward-sampling-based updating rule which allows the number of (costly) backward samples to be kept at a minimum. This yields a new, function-specific additive smoothing algorithm, AdaSmooth, which is computationally fast, numerically stable and easy to implement. The algorithm is provided with rigorous theoretical results guaranteeing its consistency, asymptotic normality and long-term stability as well as numerical results demonstrating empirically the clear superiority of AdaSmooth to existing algorithms. Supplementary materials for this article are available online.},
  keywords = {Adaptive sequential Monte Carlo methods,Central limit theorem,Effective sample size,Particle smoothing,Particle-path degeneracy,State-space models},
  file = {/home/gkonkamking/pCloudDrive/papers/Mastrototaro et al_2022_Fast and Numerically Stable Particle-Based Online Additive Smoothing.pdf}
}

@article{Mathews2009,
  title = {A Probabilistic Assessment of the Chemical and Radiological Risks of Chronic Exposure to Uranium in Freshwater Ecosystems.},
  author = {Mathews, Teresa and {Beaugelin-Seiller}, Karine and {Garnier-Laplace}, Jacqueline and Gilbin, Rodolphe and Adam, Christelle and {Della-Vedova}, Claire},
  year = {2009},
  month = sep,
  journal = {Environmental science \& technology},
  volume = {43},
  number = {17},
  eprint = {19764235},
  eprinttype = {pubmed},
  pages = {6684--90},
  issn = {0013-936X},
  abstract = {Uranium (U) presents a unique challenge for ecological risk assessments (ERA) because it induces both chemical and radiological toxicity, and the relative importance of these two toxicities differs among the various U source terms (i.e., natural, enriched, depleted). We present a method for the conversion between chemical concentrations microg L(-1)) and radiological dose rates (microGy h(-1)) for a defined set of reference organisms, and apply this conversion method to previously derived chemical and radiological benchmarks to determine the extent to which these benchmarks ensure radiological and chemical protection, respectively, for U in freshwater ecosystems. Results show that the percentage of species radiologically protected by the chemical benchmark decreases with increasing degrees of U enrichment and with increasing periods of radioactive decay. In contrast, the freshwater ecosystem is almost never chemically protected by the radiological benchmark, regardless of the source term or decay period considered, confirming that the risks to the environment from uranium's chemical toxicity generally outweigh those of its radiological toxicity. These results are relevant to developing water quality criteria that protect freshwater ecosystems from the various risks associated with the nuclear applications of U exploitation, and highlight the need for (1) further research on the speciation, bioavailability, and toxicity of U-series radionuclides under different environmental conditions, and (2) the adoption of both chemical and radiological benchmarks for coherent ERAs to be conducted in U-contaminated freshwater ecosystems.},
  isbn = {0013-936X},
  pmid = {19764235},
  keywords = {duplicate-citation-key,Ecosystem,Environmental Exposure,Environmental Exposure: statistics \& numerical dat,Fresh Water,Fresh Water: analysis,Fresh Water: chemistry,Models,nosource,Probability,Radioactive,Radioactive: analysis,Risk Assessment,Theoretical,Time Factors,Uranium,Uranium: analysis,Water Pollutants}
}

@incollection{mayer2002time,
  title = {Time-Concentration Effect Models in Predicting Chronic Toxicity from Acute Toxicity Data},
  booktitle = {Risk Assessment with Time to Event Models.},
  author = {Mayer, Foster L and Ellersieck, Mark R and Krause, Gary F and Sun, Kai and Lee, Gunhee and Buckler, Denny R},
  year = {2002},
  pages = {39--67},
  address = {Boca Raton, FL},
  keywords = {nosource}
}

@article{mayWhatWorldWeakness2012,
  title = {What in the World Is Weakness of Will?},
  author = {May, Joshua and Holton, Richard},
  year = {2012},
  month = feb,
  journal = {Philosophical Studies},
  volume = {157},
  number = {3},
  pages = {341--360},
  issn = {1573-0883},
  doi = {10.1007/s11098-010-9651-8},
  urldate = {2020-05-04},
  abstract = {At least since the middle of the twentieth century, philosophers have tended to identify weakness of will with akrasia---i.e. acting, or having a disposition to act, contrary to one's judgments about what is best for one to do. However, there has been some recent debate about whether this captures the ordinary notion of weakness of will. Richard Holton claims that it doesn't, while Alfred Mele argues that, to a certain extent, it does. As Mele recognizes, the question about an ordinary concept here is one apt for empirical investigation. We evaluate Mele's studies and report some experiments of our own in order to investigate what in the world the ordinary concept of weakness of will is. We conclude that neither Mele nor Holton (previously) was quite right and offer a tentative proposal of our own: the ordinary notion is more like a prototype or cluster concept whose application is affected by a variety of factors.},
  langid = {english}
}

@article{mazzoli2021interplay,
  title = {Interplay between Mobility, Multi-Seeding and Lockdowns Shapes {{COVID-19}} Local Impact},
  author = {Mazzoli, Mattia and Pepe, Emanuele and Mateo, David and Cattuto, Ciro and Gauvin, Laetitia and Bajardi, Paolo and Tizzoni, Michele and Hernando, Alberto and Meloni, Sandro and Ramasco, Jos{\'e} J},
  year = {2021},
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {10},
  pages = {e1009326},
  publisher = {Public Library of Science San Francisco, CA USA},
  doi = {10.1371/journal.pcbi.1009326},
  keywords = {nosource}
}

@phdthesis{mccloskey1965model,
  title = {A Model for the Distribution of Individuals by Species in an Environment},
  author = {McCloskey, J W},
  year = {1965},
  school = {Michigan State University. Department of Statistics},
  keywords = {nosource}
}

@article{mcdonaldReviewUncertaintyQuantification2021,
  title = {A Review of Uncertainty Quantification for Density Estimation},
  author = {McDonald, Shaun and Campbell, David},
  year = {2021},
  month = jan,
  journal = {Statistics Surveys},
  volume = {15},
  number = {none},
  issn = {1935-7516},
  doi = {10.1214/21-SS130},
  urldate = {2022-04-04},
  abstract = {It is often useful to conduct inference for probability densities by constructing ``plausible'' sets in which the unknown density of given data may lie. Examples of such sets include pointwise intervals, simultaneous bands, or balls in a function space, and they may be frequentist or Bayesian in interpretation. For almost any density estimator, there are multiple approaches to inference available in the literature. Here we review such literature, providing a thorough overview of existing methods for density uncertainty quantification. The literature considered here comprises a spectrum from theoretical to practical ideas, and for some methods there is little commonality between these two extremes. After detailing some of the key concepts of nonparametric inference -- the different types of ``plausible'' sets, and their interpretation and behaviour -- we list the most prominent density estimators and the corresponding uncertainty quantification methods for each.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/UKECZ3ZD/McDonald and Campbell - 2021 - A review of uncertainty quantification for density.pdf}
}

@article{mcgill1978variations,
  title = {Variations of Box Plots},
  author = {McGill, Robert and Tukey, John W. and a. Larsen, Wayne},
  year = {1978},
  journal = {The American Statistician},
  volume = {32},
  number = {1},
  pages = {12--16},
  publisher = {Taylor \& Francis Group},
  issn = {0003-1305},
  doi = {10.2307/2683468},
  abstract = {Box plots display batches of data. Five values from a set of data are conventionally used; the extremes, the upper and lower hinges (quartiles), and the median. Such plots are becoming a widely used tool in exploratory data analysis and in preparing visual summaries for statisticians and nonstatisticians alike. Three variants of the basic display, devised by the authors, are described. The first visually incorporates a measure of group size; the second incorpo- rates an indication of rough significance of differences between medians; the third combines the features of the first two. These techniques are displayed by examples.},
  isbn = {0003-1305},
  keywords = {Box Plots,Exploratory data analysis,Graphical technique,nosource}
}

@article{mcvinishetal:09,
  title = {Bayesian Goodness of Fit Testing with Mixtures of Triangular Distributions},
  author = {McVinish, Ross and Rousseau, Judith and Mengersen, Kerrie},
  year = {2009},
  journal = {Scandinavian Journal of Statistics},
  volume = {36},
  number = {2},
  pages = {337--354},
  issn = {03036898},
  doi = {10.1111/j.1467-9469.2008.00620.x},
  keywords = {Bayesian non-parametrics,Consistency,Goodness of fit,nosource}
}

@article{meehl1967theory,
  title = {Theory-Testing in Psychology and Physics: {{A}} Methodological Paradox},
  author = {Meehl, Paul E},
  year = {1967},
  journal = {Philosophy of science},
  volume = {34},
  number = {2},
  pages = {103--115},
  publisher = {Philosophy of Science Association},
  keywords = {nosource}
}

@article{meehl1978theoretical,
  title = {Theoretical Risks and Tabular Asterisks: {{Sir Karl}}, {{Sir Ronald}}, and the Slow Progress of Soft Psychology.},
  author = {Meehl, Paul E},
  year = {1978},
  journal = {Journal of consulting and clinical Psychology},
  volume = {46},
  number = {4},
  pages = {806},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@incollection{Meeker1994,
  title = {Maximum Likelihood Methods for Fitting Parametric Statistical Models to Censored and Truncated Data},
  booktitle = {Statistical Methods for Physical Science},
  author = {Meeker, W. Q. and Escobar, L. A.},
  editor = {Stanford, J. L. and Vardeman, S. B.},
  year = {1994},
  publisher = {San Diego : Academic Press},
  chapter = {8},
  isbn = {0-12-475973-4},
  keywords = {nosource}
}

@article{mehrabi2021survey,
  title = {A Survey on Bias and Fairness in Machine Learning},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  year = {2021},
  journal = {ACM computing surveys (CSUR)},
  volume = {54},
  number = {6},
  pages = {1--35},
  publisher = {ACM New York, NY, USA},
  doi = {10.1145/3457607}
}

@inproceedings{mehtaCorrelatedMixedMembership2020,
  title = {Correlated {{Mixed Membership Modeling}} for {{Somatic Mutations}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Mehta, R. and Karaman, M.},
  year = {2020},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN48605.2020.9206748},
  abstract = {Recent studies of cancer somatic mutation profiles seek to identify mutations for targeted therapy in personalized medicine. Analysis of profiles, however, is not trivial, as each profile is heterogeneous and there are multiple confounding factors that influence the cause-and-effect relationships between cancer genes such as cancer (sub)type, biological processes, total number of mutations, and non-linear mutation interactions. Moreover, cancer is biologically redundant, i.e., distinct mutations can result in the alteration of similar biological processes, so it is important to identify all possible combinatorial sets of mutations for effective patient treatment. To model this phenomena, we propose the correlated zero-inflated negative binomial process to infer the inherent structure of somatic mutation profiles through latent representations. This stochastic process takes into account different, yet correlated, co-occurring mutations using profile-specific negative binomial dispersion parameters that are mixed with a correlated beta-Bernoulli process and a probability parameter to model profile heterogeneity. These model parameters are inferred by iterative optimization via amortized and stochastic variational inference using the Pan Cancer dataset from The Cancer Genomic Archive (TCGA). By examining the the latent space, we identify biologically relevant correlations between somatic mutations.},
  keywords = {Bayesian non-parameteric,Biological processes,Biological system modeling,Cancer,Correlation,Extraterrestrial measurements,Genetics,mixed-membership modeling,nosource,Somatic Mutations}
}

@article{meila2007comparing,
  title = {Comparing Clusterings---an Information Based Distance},
  author = {Meil{\textbackslash}ua, Marina},
  year = {2007},
  journal = {Journal of Multivariate Analysis},
  volume = {98},
  number = {5},
  pages = {873--895},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{Meinhardt1982,
  title = {Models of Biological Pattern Formation},
  author = {Meinhardt, Hans},
  year = {1982},
  pages = {1--211},
  issn = {0070-2153},
  doi = {10.1016/S0070-2153(07)81001-5},
  abstract = {The development of a higher organism is controlled by a complex network of biochemical reactions that are under genetic control. In the following a short overview is given for some of the models we have proposed to describe essential steps in this process. Many of these models have found meanwhile direct support by molecular-genetic experiments. By computer simulations it has been shown that the models describe many of the observed phenomena},
  isbn = {0124886205; National Library: 8303668 LCCN: 82-246725},
  pmid = {18023723},
  keywords = {nosource}
}

@article{Mena2009,
  title = {Working Paper Series on a Construction of Markov Models In},
  author = {Mena, R H and Walker, S G},
  year = {2009},
  volume = {LXVII},
  number = {25},
  pages = {303--323},
  keywords = {gibbs sampler,markov process,nosource,stationary process}
}

@article{mena2011geometric,
  title = {Geometric Stick-Breaking Processes for Continuous-Time {{Bayesian}} Nonparametric Modeling},
  author = {Mena, Rams{\'e}s H and Ruggiero, Matteo and Walker, Stephen G},
  year = {2011},
  journal = {Journal of Statistical Planning and Inference},
  volume = {141},
  number = {9},
  pages = {3217--3230},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{Mena2016,
  title = {Dynamic Density Estimation with Diffusive {{Dirichlet}} Mixtures},
  author = {Mena, Rams{\'e}s H. and Ruggiero, Matteo},
  year = {2016},
  journal = {Bernoulli},
  volume = {22},
  number = {2},
  eprint = {1410.2477v2},
  pages = {901--926},
  issn = {1350-7265},
  doi = {10.3150/14-BEJ681},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1410.2477v2},
  keywords = {density estimation,dirichlet process,Dirichlet process,fisher diffusion,hidden Mark,hidden markov model,nonparametric regression,nosource,pitman,wright,yor process}
}

@article{menegauxContinuousEmbeddingsDNA2019,
  title = {Continuous {{Embeddings}} of {{DNA Sequencing Reads}} and {{Application}} to {{Metagenomics}}},
  author = {Menegaux, Romain and Vert, Jean-Philippe},
  year = {2019},
  month = jun,
  journal = {Journal of Computational Biology},
  volume = {26},
  number = {6},
  pages = {509--518},
  issn = {1557-8666},
  doi = {10.1089/cmb.2018.0174},
  urldate = {2021-05-12},
  abstract = {We propose a new model for fast classification of DNA sequences output by next-generation sequencing machines. The model, which we call fastDNA, embeds DNA sequences in a vector space by learning continuous low-dimensional representations of the k-mers it contains. We show on metagenomics benchmarks that it outperforms the state-of-the-art methods in terms of accuracy and scalability.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/75EDB8AL/Menegaux and Vert - 2019 - Continuous Embeddings of DNA Sequencing Reads and .pdf}
}

@article{mengSeekingEfficientData1999,
  title = {Seeking {{Efficient Data Augmentation Schemes}} via {{Conditional}} and {{Marginal Augmentation}}},
  author = {Meng, Xiao-Li and Van Dyk, David A.},
  year = {1999},
  journal = {Biometrika},
  volume = {86},
  number = {2},
  eprint = {2673513},
  eprinttype = {jstor},
  pages = {301--320},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.1093/biomet/86.2.301},
  urldate = {2022-04-08},
  abstract = {Data augmentation, sometimes known as the method of auxiliary variables, is a powerful tool for constructing optimisation and simulation algorithms. In the context of optimisation, Meng \& van Dyk (1997, 1998) reported several successes of the `working parameter' approach for constructing efficient data-augmentation schemes for fast and simple EM-type algorithms. This paper investigates the use of working parameters in the context of Markov chain Monte Carlo, in particular in the context of Tanner \& Wong's (1987) data augmentation algorithm, via a theoretical study of two working-parameter approaches, the conditional augmentation approach and the marginal augmentation approach. Posterior sampling under the univariate t model is used as a running example, which particularly illustrates how the marginal augmentation approach obtains a fast-mixing positive recurrent Markov chain by first constructing a nonpositive recurrent Markov chain in a larger space.},
  file = {/home/gkonkamking/Zotero/storage/32HC4PP5/Meng and Van Dyk - 1999 - Seeking Efficient Data Augmentation Schemes via Co.pdf}
}

@article{menichelliIdentificationLongRegulatory2021,
  title = {Identification of Long Regulatory Elements in the Genome of {{Plasmodium}} Falciparum and Other Eukaryotes},
  author = {Menichelli, Christophe and Guitard, Vincent and Martins, Rafael M. and L{\`e}bre, Sophie and {Lopez-Rubio}, Jose-Juan and Lecellier, Charles-Henri and Br{\'e}h{\'e}lin, Laurent},
  year = {2021},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {4},
  pages = {e1008909},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008909},
  urldate = {2024-01-09},
  abstract = {Long regulatory elements (LREs), such as CpG islands, polydA:dT tracts or AU-rich elements, are thought to play key roles in gene regulation but, as opposed to conventional binding sites of transcription factors, few methods have been proposed to formally and automatically characterize them. We present here a computational approach named DExTER (Domain Exploration To Explain gene Regulation) dedicated to the identification of candidate LREs (cLREs) and apply it to the analysis of the genomes of P. falciparum and other eukaryotes. Our analyses show that all tested genomes contain several cLREs that are somewhat conserved along evolution, and that gene expression can be predicted with surprising accuracy on the basis of these long regions only. Regulation by cLREs exhibits very different behaviours depending on species and conditions. In P. falciparum and other Apicomplexan organisms as well as in Dictyostelium discoideum, the process appears highly dynamic, with different cLREs involved at different phases of the life cycle. For multicellular organisms, the same cLREs are involved in all tissues, but a dynamic behavior is observed along embryonic development stages. In P. falciparum, whose genome is known to be strongly depleted of transcription factors, cLREs are predictive of expression with an accuracy above 70\%, and our analyses show that they are associated with both transcriptional and post-transcriptional regulation signals. Moreover, we assessed the biological relevance of one LRE discovered by DExTER in P. falciparum using an in vivo reporter assay. The source code (python) of DExTER is available at https://gite.lirmm.fr/menichelli/DExTER.},
  langid = {english},
  keywords = {Drosophila melanogaster,Gene expression,Gene prediction,Gene regulation,Genomics,Plasmodium,Post-transcriptional gene regulation,Sequence motif analysis},
  file = {/home/gkonkamking/pCloudDrive/papers/Menichelli et al_2021_Identification of long regulatory elements in the genome of Plasmodium.pdf}
}

@article{Meyer2005,
  title = {Opinion of the {{Scientific Panel}} on {{Plant}} Health, {{Plant}} Protection Products and Their {{Residues}} on a Request from {{EFSA}} Related to the Assessment of the Acute and Chronic Risk to Aquatic Organisms with Regard to the Possibility of Lowering the Uncertainty Fa},
  author = {{PPR Panel}},
  year = {2005},
  journal = {EFSA Journal},
  number = {301},
  pages = {45},
  abstract = {The Scientific Panel on Plant Health, Plant Protection Products and their Residues (PPR Panel) was asked by EFSA for an opinion on the possibility of refining the acute and chronic aquatic risk assessment of pesticides by lowering the assessment factor if additional species were tested. In particular, the PPR Panel was asked how these values could be reduced when additional single- species studies are available whilst still maintaining the same level of protection as foreseen in the Directive 91/414/EEC. The current approach for acute and chronic risk assessment to protect the ecosystem against adverse effects of pesticides uses the lowest available toxicity value from laboratory standard toxicity tests, i.e. the most sensitive tested species, and divides this value by a fixed assessment factor. This results in an increase of conservatism when more species are tested and does not reflect the increased certainty that more data provide. To answer this question the PPR Panel reviewed existing literature, guidance documents, and data. Statistical calculations based on species sensitivity distributions were used to develop a range of options for adjusting the risk assessment when more species are tested. The PPR Panel assessed the current level of protection and found that it is not equal for different taxonomic groups and for different substances. On average, the level of protection provided by the current approach is, for example, markedly higher for fish than for crustaceans and insects. The PPR Panel identified a range of possible methods either to maintain at least the current unspecified level of protection, or to achieve any specified level of protection. For taxonomic groups where the legislation requires only one species (e.g. crustaceans), this effectively sets the level of protection in the effects assessment. When additional species are tested, the same average level of protection can be maintained by taking the geometric mean (rather than the lowest value) and dividing by the current assessment factor. For fish, where the legislation requires that at least two species are tested, this implies a higher level of protection in the effects assessment. In this case, a different procedure is required when additional species are tested. The minimum is then replaced by the second or third lowest toxicity value depending on the sample size available, and divided by the current assessment factor. The Panel described three further approaches that allow a particular level of protection to be achieved, provided such a level is specified. These methods involve using a modified assessment factor that incorporates an estimate of the variation between species, which can either be specific to the substance under consideration or derived from existing information on related substances. These three methods relate only to uncertainty due to variation between species. Any other uncertainties that are relevant to the assessment would need to be accounted for separately.},
  keywords = {nosource}
}

@article{Meyer2013,
  title = {Integrating Sequence Variation and Protein Structure to Identify Sites under Selection},
  author = {Meyer, Austin G. and Wilke, Claus O.},
  year = {2013},
  journal = {Molecular Biology and Evolution},
  volume = {30},
  number = {1},
  pages = {36--44},
  issn = {07374038},
  doi = {10.1093/molbev/mss217},
  abstract = {We present a novel method to identify sites under selection in protein-coding genes. Our method combines the traditional Goldman-Yang model of coding-sequence evolution with the information obtained from the 3D structure of the evolving protein, specifically the relative solvent accessibility (RSA) of individual residues. We develop a random-effects likelihood sites model in which rate classes are RSA dependent. The RSA dependence is modeled with linear functions. We demonstrate that our RSA-dependent model provides a significantly better fit to molecular sequence data than does a traditional, RSA-independent model. We further show that our model provides a natural, RSA-dependent neutral baseline for the evolutionary rate ratio {$\omega$} = dN/dS Sites that deviate from this neutral baseline likely experience selection pressure for function. We apply our method to the influenza proteins hemagglutinin and neuraminidase. For hemagglutinin, our method recovers positively selected sites near the sialic acid-binding site and negatively selected sites that may be important for trimerization. For neuraminidase, our method recovers the oseltamivir resistance site and otherwise suggests that few sites deviate from the neutral baseline. Our method is broadly applicable to any protein sequences for which structural data are available or can be obtained via homology modeling or threading.},
  pmid = {22977116},
  keywords = {influenza,nosource,positive selection,protein evolution,relative solvent accessibility}
}

@article{MH15,
  title = {{{CauseMap}}: {{Fast}} Inference of Causality from Complex Time Series},
  author = {Maher, M Cyrus and Hernandez, Ryan D},
  year = {2015},
  doi = {10.7287/peerj.preprints.583v2},
  abstract = {Background: Establishing health-related causal relationships is a central pursuit in biomedical research. Yet, the interdependent non-linearity of biological systems renders causal dynamics laborious and at times impractical to disentangle. This pursuit is further impeded by the dearth of time series that are sufficiently long to observe and understand recurrent patterns of flux. However, as data generation costs plummet and technologies like wearable devices democratize data collection, we anticipate a coming surge in the availability of biomedically-relevant time series data. Given the life-saving potential of these burgeoning resources, it is critical to invest in the development of open source software tools that are capable of drawing meaningful insight from vast amounts of time series data. Results: Here we present CauseMap, the first open source implementation of convergent cross mapping (CCM), a method for establishing causality from long time series data ({$>$}{\~ }25 observations). Compared to existing time series methods, CCM has the advantage of being model-free and robust to unmeasured confounding that could otherwise induce spurious associations. CCM builds on Takens' Theorem, a well-established result from dynamical systems theory that requires only mild assumptions. This theorem allows us to reconstruct high dimensional system dynamics using a time series of only a single variable. These reconstructions can be thought of as shadows of the true causal system. If the reconstructed shadows can predict points from the opposing time series, we can infer that the corresponding variables are providing views of the same causal system, and so are causally related. Unlike traditional metrics, this test can establish the directionality of causation, even in the presence of feedback loops. Furthermore, since CCM can extract causal relationships from times series of, e.g. a single individual, it may be a valuable tool to personalized medicine. We implement CCM in Julia, a high-performance programming language designed for facile technical computing. Our software package, CauseMap, is platform-independent and freely available as an official Julia package. Conclusions: CauseMap is an efficient implementation of a state-of-the-art algorithm for detecting causality from time series data. We believe this tool will be a valuable resource for biomedical research and personalized medicine.},
  keywords = {nosource}
}

@incollection{michael1986analysis,
  title = {Analysis of Data from Censored Samples},
  booktitle = {Goodness-of-Fit Techniques},
  author = {Michael, John R and Schucany, William R},
  editor = {D'Agostino, Ralph B and Stephens, Michael A},
  year = {1986},
  pages = {461--496},
  publisher = {Marcel Dekker},
  address = {New York},
  keywords = {nosource}
}

@book{MichaelBatty1994a,
  title = {Fractal Cities},
  author = {Batty, Michael and Longley, Paul},
  year = {1994},
  publisher = {Academic Press},
  address = {San Diego, CA and London},
  isbn = {0-12-455570-5},
  keywords = {nosource}
}

@article{michaelGeneratingRandomVariates1976,
  title = {Generating {{Random Variates Using Transformations}} with {{Multiple Roots}}},
  author = {Michael, John R. and Schucany, William R. and Haas, Roy W.},
  year = {1976},
  journal = {The American Statistician},
  volume = {30},
  number = {2},
  eprint = {2683801},
  eprinttype = {jstor},
  pages = {88--90},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0003-1305},
  doi = {10.2307/2683801},
  urldate = {2022-04-06},
  abstract = {The general approach to generating random variates through transformations with multiple roots is discussed. Multinomial probabilities are determined for the selection of the different roots. An application of the general result yields a new and simple technique for the generation of variates from the inverse Gaussian distribution.},
  file = {/home/gkonkamking/Zotero/storage/VC2EI7GN/Michael et al. - 1976 - Generating Random Variates Using Transformations w.pdf}
}

@article{michelForwardEventChainMonte2020,
  title = {Forward {{Event-Chain Monte Carlo}}: {{Fast Sampling}} by {{Randomness Control}} in {{Irreversible Markov Chains}}},
  shorttitle = {Forward {{Event-Chain Monte Carlo}}},
  author = {Michel, Manon and Durmus, Alain and S{\'e}n{\'e}cal, St{\'e}phane},
  year = {2020},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {29},
  number = {4},
  pages = {689--702},
  publisher = {Taylor \& Francis},
  issn = {1061-8600},
  doi = {10.1080/10618600.2020.1750417},
  urldate = {2021-05-04},
  abstract = {Irreversible and rejection-free Monte Carlo methods, recently developed in physics under the name event-chain and known in statistics as piecewise deterministic Monte Carlo (PDMC), have proven to produce clear acceleration over standard Monte Carlo methods, thanks to the reduction of their random-walk behavior. However, while applying such schemes to standard statistical models, one generally needs to introduce an additional randomization for sake of correctness. We propose here a new class of event-chain Monte Carlo methods that reduces this extra-randomization to a bare minimum. We compare the efficiency of this new methodology to standard PDMC and Monte Carlo methods. Accelerations up to several magnitudes and reduced dimensional scalings are exhibited. Supplementary materials for this article are available online.},
  file = {/home/gkonkamking/Zotero/storage/3IL2UGSS/Michel et al. - 2020 - Forward Event-Chain Monte Carlo Fast Sampling by .pdf}
}

@inproceedings{miller2013simple,
  title = {A Simple Example of {{Dirichlet}} Process Mixture Inconsistency for the Number of Components},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Miller, Jeffrey W and Harrison, Matthew T},
  year = {2013},
  pages = {199--206},
  file = {/home/gkonkamking/pCloudDrive/papers/Miller and Harrison - 2013 - A simple example of Dirichlet process mixture inconsistency for the number of components.pdf}
}

@article{millerInconsistencyPitmanYor2014,
  title = {Inconsistency of {{Pitman}}--{{Yor Process Mixtures}} for the {{Number}} of {{Components}}},
  author = {Miller, Jeffrey W and Harrison, Matthew T},
  year = {2014},
  journal = {The Journal of Machine Learning Research},
  volume = {15},
  number = {1},
  pages = {3333--3370},
  abstract = {In many applications, a finite mixture is a natural model, but it can be difficult to choose an appropriate number of components. To circumvent this choice, investigators are increasingly turning to Dirichlet process mixtures (DPMs), and Pitman--Yor process mixtures (PYMs), more generally. While these models may be well-suited for Bayesian density estimation, many investigators are using them for inferences about the number of components, by considering the posterior on the number of components represented in the observed data. We show that this posterior is not consistent---that is, on data from a finite mixture, it does not concentrate at the true number of components. This result applies to a large class of nonparametric mixtures, including DPMs and PYMs, over a wide variety of families of component distributions, including essentially all discrete families, as well as continuous exponential families satisfying mild regularity conditions (such as multivariate Gaussians).},
  langid = {english},
  keywords = {\_tablet},
  file = {/home/gkonkamking/pCloudDrive/papers/Miller_Harrison_Inconsistency of Pitman–Yor Process Mixtures for the Number of Components.pdf}
}

@article{millerMixtureModelsPrior2018,
  title = {Mixture {{Models With}} a {{Prior}} on the {{Number}} of {{Components}}},
  author = {Miller, Jeffrey W. and Harrison, Matthew T.},
  year = {2018},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {521},
  pages = {340--356},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1255636},
  urldate = {2021-11-08},
  abstract = {A natural Bayesian approach for mixture models with an unknown number of components is to take the usual finite mixture model with symmetric Dirichlet weights, and put a prior on the number of components---that is, to use a mixture of finite mixtures (MFM). The most commonly used method of inference for MFMs is reversible jump Markov chain Monte Carlo, but it can be nontrivial to design good reversible jump moves, especially in high-dimensional spaces. Meanwhile, there are samplers for Dirichlet process mixture (DPM) models that are relatively simple and are easily adapted to new applications. It turns out that, in fact, many of the essential properties of DPMs are also exhibited by MFMs---an exchangeable partition distribution, restaurant process, random measure representation, and stick-breaking representation---and crucially, the MFM analogues are simple enough that they can be used much like the corresponding DPM properties. Consequently, many of the powerful methods developed for inference in DPMs can be directly applied to MFMs as well; this simplifies the implementation of MFMs and can substantially improve mixing. We illustrate with real and simulated data, including high-dimensional gene expression data used to discriminate cancer subtypes. Supplementary materials for this article are available online.},
  pmid = {29983475},
  keywords = {Bayesian,Clustering,Density estimation,Model selection,Nonparametric},
  file = {/home/gkonkamking/Zotero/storage/TDUDPSET/Miller and Harrison - 2018 - Mixture Models With a Prior on the Number of Compo.pdf}
}

@article{minitab2000minitab,
  title = {{{MINITAB}} Statistical Software},
  year = {2000},
  publisher = {Minitab Inc.},
  keywords = {nosource}
}

@article{Mirauta2014a,
  title = {Parseq: {{Reconstruction}} of Microbial Transcription Landscape from {{RNA-Seq}} Read Counts Using State-Space Models},
  author = {Mirauta, Bogdan and Nicolas, Pierre and Richard, Hugues},
  year = {2014},
  journal = {Bioinformatics},
  volume = {30},
  number = {10},
  pages = {1409--1416},
  issn = {14602059},
  doi = {10.1093/bioinformatics/btu042},
  abstract = {MOTIVATION: The most common RNA-Seq strategy consists of random shearing, amplification and high-throughput sequencing of the RNA fraction. Methods to analyze transcription level variations along the genome from the read count profiles generated by the RNA-Seq protocol are needed.{\textbackslash}n{\textbackslash}nRESULTS: We developed a statistical approach to estimate the local transcription levels and to identify transcript borders. This transcriptional landscape reconstruction relies on a state-space model to describe transcription level variations in terms of abrupt shifts and more progressive drifts. A new emission model is introduced to capture not only the read count variance inside a transcript but also its short-range autocorrelation and the fraction of positions with zero counts. The estimation relies on a particle Gibbs algorithm whose running time makes it more suited to microbial genomes. The approach outperformed read-overlapping strategies on synthetic and real microbial datasets.{\textbackslash}n{\textbackslash}nAVAILABILITY: A program named Parseq is available at: http://www.lgm.upmc.fr/parseq/.{\textbackslash}n{\textbackslash}nCONTACT: bodgan.mirauta@upmc.fr{\textbackslash}n{\textbackslash}nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
  keywords = {nosource}
}

@article{mirmanAttractorDynamicsSemantic2008,
  ids = {mirmanAttractorDynamicsSemantic2008a},
  title = {Attractor Dynamics and Semantic Neighborhood Density: {{Processing}} Is Slowed by near Neighbors and Speeded by Distant Neighbors},
  shorttitle = {Attractor Dynamics and Semantic Neighborhood Density},
  author = {Mirman, Daniel and Magnuson, James S.},
  year = {2008},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {34},
  number = {1},
  pages = {65--79},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1285(Electronic),0278-7393(Print)},
  doi = {10.1037/0278-7393.34.1.65},
  abstract = {The authors investigated semantic neighborhood density effects on visual word processing to examine the dynamics of activation and competition among semantic representations. Experiment 1 validated feature-based semantic representations as a basis for computing semantic neighborhood density and suggested that near and distant neighbors have opposite effects on word processing. Experiment 2 confirmed these results: Word processing was slower for dense near neighborhoods and faster for dense distant neighborhoods. Analysis of a computational model showed that attractor dynamics can produce this pattern of neighborhood effects. The authors argue for reconsideration of traditional models of neighborhood effects in terms of attractor dynamics, which allow both inhibitory and facilitative effects to emerge. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Models,nosource,Semantics,Visual Discrimination,Word Associations,Word Recognition}
}

@article{miyamotoCulturalVariationCorrespondence2002,
  title = {Cultural Variation in Correspondence Bias: {{The}} Critical Role of Attitude Diagnosticity of Socially Constrained Behavior},
  shorttitle = {Cultural Variation in Correspondence Bias},
  author = {Miyamoto, Yuri and Kitayama, Shinobu},
  year = {2002},
  journal = {Journal of Personality and Social Psychology},
  volume = {83},
  number = {5},
  pages = {1239--1248},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1315(Electronic),0022-3514(Print)},
  doi = {10.1037/0022-3514.83.5.1239},
  abstract = {Upon observing another's socially constrained behavior, people often ascribe to the person an attitude that corresponds to the behavior (called the correspondence bias [CB]). The authors found that when a socially constrained behavior is still diagnostic of the actor's attitude, both Americans and Japanese show an equally strong CB. A major cultural difference occurred when the behavior was minimally diagnostic. Demonstrating their persistent bias toward dispositional attribution, Americans showed a strong CB. But Japanese did not show any CB (Study 1). Furthermore, a mediational analysis revealed that this cross-cultural difference was due in part to the nature of explicit inferences generated online during attitudinal judgment (Study 2). Implications for the cultural grounding of social perception are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Attitudes,Behavior,Cross Cultural Differences,Internal External Locus of Control,nosource,Social Perception}
}

@article{Mochida2012,
  title = {Species Sensitivity Distribution Approach to Primary Risk Analysis of the Metal Pyrithione Photodegradation Product, 2,2'-Dipyridyldisulfide in the {{Inland Sea}} and Induction of Notochord Undulation in Fish Embryos.},
  author = {Mochida, Kazuhiko and Amano, Haruna and Ito, Katsutoshi and Ito, Mana and Onduka, Toshimitsu and Ichihashi, Hideki and Kakuno, Akira and Harino, Hiroya and Fujii, Kazunori},
  year = {2012},
  month = aug,
  journal = {Aquatic toxicology (Amsterdam, Netherlands)},
  volume = {118--119},
  eprint = {22561701},
  eprinttype = {pubmed},
  pages = {152--63},
  publisher = {Elsevier B.V.},
  issn = {1879-1514},
  doi = {10.1016/j.aquatox.2012.04.002},
  abstract = {To carry out a primary risk assessment in the Inland Sea of Japan for 2,2'-dipyridyldisulfide [(PS)(2)], a metal pyrithione photodegradation product, we used a methodology based on the species sensitivity distribution (SSD) estimated with a Bayesian statistical model. We first conducted growth inhibition tests with three marine phytoplankton species, Tetraselmis tetrathele, Chaetoceros calcitrans, and Dunaliella tertiolecta. We also performed acute and early life stage toxicity (ELS) tests with a teleost fish, the mummichog (Fundulus heteroclitus). The algal growth inhibition tests revealed that the 72-h EC(50) ranged from 62 to 1100 {$\mu$}g/L. Acute toxicity tests with larval mummichogs revealed that the 96-h LC(50) was approximately 500 {$\mu$}g/L based on the actual toxicant concentrations. ELS testing of (PS)(2) under continuous flow-through conditions for 50 days revealed that growth was the most sensitive endpoint, and both total length and body weight were significantly lower in the groups exposed to 27 {$\mu$}g/L (PS)(2) compared to the solvent control group. We determined a lowest observed effect concentration of 17 {$\mu$}g/L and a NOEC of 5.9 {$\mu$}g/L based on the actual toxicant concentrations. By using the ecotoxicity data (LC(50) and EC(50)) from this study and previous work, we calculated a hazardous concentration that should protect 95\% and 99\% of species (HC(5) and HC(1)) based on the SSD derived with a Bayesian statistical model. The medians with 90\% confidence intervals (parentheses) of the HC(5) and HC(1) were 31.0 (3.2, 101.8) {$\mu$}g/L and 10.1 (0.5, 44.2) {$\mu$}g/L, respectively. In the ELS test, about 80\% of hatched larvae exposed to 243-{$\mu$}g/L (PS)(2) displayed a notochord undulation. To elucidate the cause of the notochord undulation, we carried out embryo toxicity tests by exposing embryos at various developmental stages to (PS)(2). Exposure to (PS)(2) through the entire gastrulae stage was important to induction of the morphological abnormality. Lysyl oxidase activity was significantly decreased in these embryos compared to the control group, a suggestion that lysyl oxidase-mediated collagen fiber organization, which is essential for notochord formation, is disrupted because of (PS)(2) toxicity. We also investigated the occurrence of (PS)(2) in water from several coastal sites of the Inland Sea and detected (PS)(2) at concentrations of {$<$}0.1-0.4 ng/L. Comparison of environmental concentrations to the HC values suggests that the current ecological risk posed by (PS)(2) in the Inland Sea is low. This is the first report of the detection of a metal pyrithione degradation product in the natural marine environment.},
  isbn = {0166-445X},
  pmid = {22561701},
  keywords = {Antifouling biocide,Bayesian inference,duplicate-citation-key,Hazardous concentration,Lysyl oxidase,nosource,Notochord undulation}
}

@article{Mochida2012a,
  title = {Use of Species Sensitivity Distributions to Predict No-Effect Concentrations of an Antifouling Biocide, Pyridine Triphenylborane, for Marine Organisms.},
  author = {Mochida, Kazuhiko and Onduka, Toshimitsu and Amano, Haruna and Ito, Mana and Ito, Katsutoshi and Tanaka, Hiroyuki and Fujii, Kazunori},
  year = {2012},
  month = oct,
  journal = {Marine pollution bulletin},
  eprint = {23044030},
  eprinttype = {pubmed},
  publisher = {Elsevier Ltd},
  issn = {1879-3363},
  doi = {10.1016/j.marpolbul.2012.09.007},
  abstract = {We used species sensitivity distributions (SSDs) and a Bayesian statistical model to carry out a primary risk assessment for pyridine triphenylborane (PTPB) in Hiroshima Bay, Japan. We used SSDs derived from toxicity values, such as EC(50) and LC(50), obtained from this study and previous work to calculate hazardous concentrations that should protect 95\% and 99\% of species (HC(5) and HC(1)) and demonstrated that the medians of the HC(5) and HC(1) were 0.78 and 0.17{$\mu$}g/L, respectively. We also used liquid chromatography/mass spectrometry to investigate the occurrence of PTPB in seawater from several coastal sites of Hiroshima Bay and detected PTPB at concentrations of 4.8-21pg/L. Comparison of environmental concentrations to the HC values suggests that the current ecological risk posed by PTPB in Hiroshima Bay is low. This is the first report of the detection of PTPB in the natural marine environment.},
  pmid = {23044030},
  keywords = {algal growth inhibition test,Algal growth inhibition test,Bayesian inference,Crustacean immobilization test,duplicate-citation-key,Fish acute toxicity,nosource,Primary risk analysis}
}

@article{mollerStructuredSpatioTemporalShotNoise2010,
  title = {Structured {{Spatio-Temporal Shot-Noise Cox Point Process Models}}, with a {{View}} to {{Modelling Forest Fires}}},
  author = {M{\o}ller, Jesper and {D{\'i}az-Avalos}, Carlos},
  year = {2010},
  journal = {Scandinavian Journal of Statistics},
  volume = {37},
  number = {1},
  pages = {2--25},
  issn = {1467-9469},
  doi = {10.1111/j.1467-9469.2009.00670.x},
  urldate = {2023-10-29},
  abstract = {Abstract. Spatio-temporal Cox point process models with a multiplicative structure for the driving random intensity, incorporating covariate information into temporal and spatial components, and with a residual term modelled by a shot-noise process, are considered. Such models are flexible and tractable for statistical analysis, using spatio-temporal versions of intensity and inhomogeneous K-functions, quick estimation procedures based on composite likelihoods and minimum contrast estimation, and easy simulation techniques. These advantages are demonstrated in connection with the analysis of a relatively large data set consisting of 2796 days and 5834 spatial locations of fires. The model is compared with a spatio-temporal log-Gaussian Cox point process model, and likelihood-based methods are discussed to some extent.},
  copyright = {{\copyright} 2010 Board of the Foundation of the Scandinavian Journal of Statistics},
  langid = {english},
  keywords = {composite likelihood,Cox process,forest fires,inhomogeneous K-function,intensity,log-Gaussian process,minimum contrast estimation,multiplicative model,pair correlation function,Poisson process,shot-noise process,simulation,spatio-temporal point process},
  file = {/home/gkonkamking/pCloudDrive/papers/Møller_Díaz-Avalos_2010_Structured Spatio-Temporal Shot-Noise Cox Point Process Models, with a View to.pdf}
}

@article{moninMoralCredentialsExpression2001,
  title = {Moral Credentials and the Expression of Prejudice},
  author = {Monin, Beno{\^i}t and Miller, Dale T.},
  year = {2001},
  journal = {Journal of Personality and Social Psychology},
  volume = {81},
  number = {1},
  pages = {33--43},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1315(Electronic),0022-3514(Print)},
  doi = {10.1037/0022-3514.81.1.33},
  abstract = {Three experiments supported the hypothesis that people are more willing to express attitudes that could be viewed as prejudiced when their past behavior has established their credentials as nonprejudiced persons. In Study 1, participants given the opportunity to disagree with blatantly sexist statements were later more willing to favor a man for a stereotypically male job. In Study 2, participants who first had the opportunity to select a member of a stereotyped group (a woman or an African American) for a category-neutral job were more likely to reject a member of that group for a job stereotypically suited for majority members. In Study 3, participants who had established credentials as nonprejudiced persons revealed a greater willingness to express a politically incorrect opinion even when the audience was unaware of their credentials. The general conditions under which people feel licensed to act on illicit motives are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Attitudes,Behavior,Morality,nosource,Prejudice,Social Perception}
}

@article{Montet2009,
  title = {Fate of Acid-Resistant and Non-Acid Resistant {{Shiga}} Toxin-Producing {{Escherichia}} Coli Strains in Experimentally Contaminated {{French}} Fermented Raw Meat Sausages},
  author = {Montet, M. P. and Christieans, S. and Thevenot, D. and Coppet, V. and Ganet, S. and {Delignette-Muller}, Marie Laure and Duni??re, L. and Miszczycha, S. and {Vernozy-Rozand}, C.},
  year = {2009},
  month = feb,
  journal = {International Journal of Food Microbiology},
  volume = {129},
  number = {3},
  eprint = {19157612},
  eprinttype = {pubmed},
  pages = {264--270},
  publisher = {Elsevier B.V.},
  issn = {01681605},
  doi = {10.1016/j.ijfoodmicro.2008.12.002},
  abstract = {Both pathogenic and nonpathogenic E. coli exhibit a stress response to sublethal environmental stresses. Several studies have reported acid tolerance and survival characteristics of E. coli O157:H7 in foodstuffs, but there are few reports about the tolerance of non-O157 serogroups (STEC) to organic acids in foods. The purpose of this study was to examine the effect of the manufacturing process of French fermented raw meat sausages on the growth and survival of acid-resistant (AR) and non-acid resistant (NAR) STEC strains. The six strains, 3 AR and 3 NAR, were inoculated separately into raw sausage mixture at a level of 104-105??CFU/g. A total of 19 batches of sausages were manufactured. A rapid and similar decrease in the number of both AR and NAR STEC strains, from less than 1 to 1.5 log10 CFU/g, was observed during the first 5??days of fermentation at 20-24????C. This rapid decrease was followed by a more gradual but continuous decrease in STEC counts after drying at 13-14????C, up to day 35. The STEC counts were {$<$} 10??CFU/g after 35??days for the NAR strains and the same concentration for the AR strains on the best before date (day 60). It was not possible to detect any NAR STEC after 60??days. The present study shows that the process used in the manufacture of French sausages results in a complete destruction of NAR STEC strains after 60??days, but it does not have the same effect on the AR STEC strains. ?? 2008 Elsevier B.V. All rights reserved.},
  isbn = {1879-3460 (Electronic){\textbackslash}r0168-1605 (Linking)},
  pmid = {19157612},
  keywords = {Acid resistance,French sausages,nosource,Shiga-toxin Escherichia coli,Survival}
}

@article{montgomeryBayesianModelAveraging2010,
  title = {Bayesian {{Model Averaging}}: {{Theoretical Developments}} and {{Practical Applications}}},
  shorttitle = {Bayesian {{Model Averaging}}},
  author = {Montgomery, Jacob M. and Nyhan, Brendan},
  year = {2010},
  journal = {Political Analysis},
  volume = {18},
  number = {2},
  pages = {245--270},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpq001},
  urldate = {2024-11-25},
  abstract = {Political science researchers typically conduct an idiosyncratic search of possible model configurations and then present a single specification to readers. This approach systematically understates the uncertainty of our results, generates fragile model specifications, and leads to the estimation of bloated models with too many control variables. Bayesian model averaging (BMA) offers a systematic method for analyzing specification uncertainty and checking the robustness of one's results to alternative model specifications, but it has not come into wide usage within the discipline. In this paper, we introduce important recent developments in BMA and show how they enable a different approach to using the technique in applied social science research. We illustrate the methodology by reanalyzing data from three recent studies using BMA software we have modified to respect statistical conventions within political science.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Montgomery and Nyhan - 2010 - Bayesian Model Averaging Theoretical Developments and Practical Applications.pdf}
}

@incollection{Moore2010,
  title = {Uncertainty Analysis Using Classical and Bayesian Hierarchical Models},
  booktitle = {Application of Uncertainty Analysis to Ecological Risk of Pesticides},
  author = {Moore, {\relax DRJ} and {Warren-Hicks}, William and Qian, S and Fairbrother, Anne and Aldenberg, Tom and Barry, T and Luttik, Robert and Ratte, {\relax HT}},
  editor = {{Warren-Hicks}, {\relax WJ} and Hart, Andy},
  year = {2010},
  pages = {134--141},
  publisher = {CRC press},
  address = {Pensacola (Florida), US},
  keywords = {nosource}
}

@article{moranVariancePriorForms2019,
  title = {Variance {{Prior Forms}} for {{High-Dimensional Bayesian Variable Selection}}},
  author = {Moran, Gemma E. and Ro{\v c}kov{\'a}, Veronika and George, Edward I.},
  year = {2019},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {14},
  number = {4},
  pages = {1091--1119},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/19-BA1149},
  urldate = {2020-04-10},
  abstract = {Consider the problem of high dimensional variable selection for the Gaussian linear model when the unknown error variance is also of interest. In this paper, we show that the use of conjugate shrinkage priors for Bayesian variable selection can have detrimental consequences for such variance estimation. Such priors are often motivated by the invariance argument of Jeffreys (1961). Revisiting this work, however, we highlight a caveat that Jeffreys himself noticed; namely that biased estimators can result from inducing dependence between parameters a priori. In a similar way, we show that conjugate priors for linear regression, which induce prior dependence, can lead to such underestimation in the Bayesian high-dimensional regression setting. Following Jeffreys, we recommend as a remedy to treat regression coefficients and the error variance as independent a priori. Using such an independence prior framework, we extend the Spike-and-Slab Lasso of Ro{\v c}kov{\'a} and George (2018) to the unknown variance case. This extended procedure outperforms both the fixed variance approach and alternative penalized likelihood methods on simulated data. On the protein activity dataset of Clyde and Parmigiani (1998), the Spike-and-Slab Lasso with unknown variance achieves lower cross-validation error than alternative penalized likelihood methods, demonstrating the gains in predictive accuracy afforded by simultaneous error variance estimation. The unknown variance implementation of the Spike-and-Slab Lasso is provided in the publicly available R package SSLASSO (Ro{\v c}kov{\'a} and Moran, 2017).},
  langid = {english},
  mrnumber = {MR4044847},
  zmnumber = {07159869},
  keywords = {Bayesian shrinkage,Bayesian variable selection,Jeffreys' priors,penalized likelihood,Spike-and-Slab Lasso},
  file = {/home/gkonkamking/Zotero/storage/K2W47UZU/Moran et al. - 2019 - Variance Prior Forms for High-Dimensional Bayesian.pdf}
}

@article{morcos2011direct,
  title = {Direct-Coupling Analysis of Residue Coevolution Captures Native Contacts across Many Protein Families.},
  author = {Morcos, Faruck and Pagnani, Andrea and Lunt, Bryan and Bertolino, Arianna and Marks, Debora S and Sander, Chris and Zecchina, Riccardo and Onuchic, Jos{\'e} N and Hwa, Terence and Weigt, Martin},
  year = {2011},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {108},
  number = {49},
  eprint = {1110.5223},
  pages = {E1293-301},
  publisher = {National Acad Sciences},
  issn = {1091-6490},
  doi = {10.1073/pnas.1111471108},
  abstract = {The similarity in the three-dimensional structures of homologous proteins imposes strong constraints on their sequence variability. It has long been suggested that the resulting correlations among amino acid compositions at different sequence positions can be exploited to infer spatial contacts within the tertiary protein structure. Crucial to this inference is the ability to disentangle direct and indirect correlations, as accomplished by the recently introduced direct-coupling analysis (DCA). Here we develop a computationally efficient implementation of DCA, which allows us to evaluate the accuracy of contact prediction by DCA for a large number of protein domains, based purely on sequence information. DCA is shown to yield a large number of correctly predicted contacts, recapitulating the global structure of the contact map for the majority of the protein domains examined. Furthermore, our analysis captures clear signals beyond intradomain residue contacts, arising, e.g., from alternative protein conformations, ligand-mediated residue couplings, and interdomain interactions in protein oligomers. Our findings suggest that contacts predicted by DCA can be used as a reliable guide to facilitate computational predictions of alternative protein conformations, protein complex formation, and even the de novo prediction of protein domain structures, contingent on the existence of a large number of homologous sequences which are being rapidly made available due to advances in genome sequencing.},
  archiveprefix = {arXiv},
  arxivid = {1110.5223},
  isbn = {1091-6490 (Electronic){\textbackslash}r0027-8424 (Linking)},
  pmid = {22106262},
  keywords = {Algorithms,Amino Acids,Amino Acids: chemistry,Amino Acids: genetics,Amino Acids: metabolism,Binding Sites,Binding Sites: genetics,Computational Biology,Computational Biology: methods,Models,Molecular,nosource,Protein Binding,Protein Conformation,Protein Interaction Mapping,Protein Interaction Mapping: methods,Protein Multimerization,Proteins,Proteins: chemistry,Proteins: genetics,Proteins: metabolism,Reproducibility of Results}
}

@article{morisiHowCOVID19Affects2024,
  title = {How {{COVID-19}} Affects Voting for Incumbents: {{Evidence}} from Local Elections in {{France}}},
  shorttitle = {How {{COVID-19}} Affects Voting for Incumbents},
  author = {Morisi, Davide and Clol{\'e}ry, H{\'e}lo{\"i}se and King, Guillaume Kon Kam and Schaub, Max},
  year = {2024},
  month = mar,
  journal = {PLOS ONE},
  volume = {19},
  number = {3},
  pages = {e0297432},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0297432},
  urldate = {2024-11-27},
  abstract = {How do voters react to an ongoing natural threat? Do voters sanction or reward incumbents even when incumbents cannot be held accountable because an unforeseeable natural disaster is unfolding? We address this question by investigating voters' reactions to the early spread of COVID-19 in the 2020 French municipal elections. Using a novel, fine-grained measure of the circulation of the virus based on excess-mortality data, we find that support for incumbents increased in areas that were particularly hard hit by the virus. Incumbents from both left and right gained votes in areas more strongly affected by COVID-19. We provide suggestive evidence for two mechanisms that can explain our findings: an emotional channel related to feelings of fear and anxiety, and a prospective-voting channel, related to the ability of incumbents to act more swiftly against the diffusion of the virus than challengers.},
  langid = {english},
  keywords = {Anxiety,COVID 19,Death rates,Elections,Fear,Natural disasters,Pandemics,Virus testing},
  file = {/home/gkonkamking/pCloudDrive/papers/Morisi et al. - 2024 - How COVID-19 affects voting for incumbents Evidence from local elections in France 1.pdf}
}

@article{morrisonDistinguishingSilentVocal2008,
  title = {Distinguishing between Silent and Vocal Minorities: {{Not}} All Deviants Feel Marginal},
  shorttitle = {Distinguishing between Silent and Vocal Minorities},
  author = {Morrison, Kimberly Rios and Miller, Dale T.},
  year = {2008},
  journal = {Journal of Personality and Social Psychology},
  volume = {94},
  number = {5},
  pages = {871--882},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1315(Electronic),0022-3514(Print)},
  doi = {10.1037/0022-3514.94.5.871},
  abstract = {People's opinions can deviate from that of the average group member in two ways. Descriptive deviants diverge from the average group attitude in a direction consistent with the desirable group attitude; prescriptive deviants diverge from the average group attitude in a direction inconsistent with the desirable group attitude. Three studies tested the hypothesis that descriptive deviants are more willing to express their opinions than either nondeviants or prescriptive deviants. Study 1 found that college students reported more comfort in expressing descriptive deviant opinions because descriptive deviance induced feelings of superior conformity (i.e., being "different but good"). Study 2 found that descriptive deviants reported more pride after expressing their opinions, were rated as more proud by an observer, and were more willing to publicize their opinions. Study 3 showed that political bumper stickers with descriptive deviant messages were displayed disproportionately more frequently than were those with prescriptive deviant messages. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Antisocial Behavior,Attitudes,Messages,Minority Groups,nosource,Social Norms}
}

@article{mouldingAssessingRelativeToxicity2022,
  title = {Assessing the {{Relative Toxicity}} of {{Different Road Salts}} and {{Effect}} of {{Temperature}} on {{Salinity Toxicity}}: {{LCx Values}} versus {{No-Effect Concentration}} ({{NEC}}) {{Values}}},
  author = {Moulding, Benjamin JG and Kon Kam King, Guillaume and Shenton, Mark and Bray, Jon P and Nichols, Susan J and Kefford, Ben J},
  year = {2022},
  journal = {Archives of Environmental Contamination and Toxicology},
  volume = {82},
  number = {2},
  pages = {281--293},
  publisher = {Springer},
  doi = {10.1007/s00244-021-00908-1},
  keywords = {nosource},
  file = {/home/gkonkamking/pCloudDrive/papers/Moulding et al_2022_Assessing the Relative Toxicity of Different Road Salts and Effect of.pdf}
}

@article{Mramba2013,
  title = {Standardizing Visual Control Devices for Tsetse Flies: {{East}} African Species Glossina Swynnertoni},
  author = {Mramba, Furaha and Oloo, Francis and Byamungu, Mechtilda and Kr??ber, Thomas and McMullin, Andrew and Mihok, Steve and Guerin, Patrick M.},
  editor = {Masiga, Daniel K.},
  year = {2013},
  month = feb,
  journal = {PLoS Neglected Tropical Diseases},
  volume = {7},
  number = {2},
  pages = {e2063},
  publisher = {Public Library of Science},
  issn = {19352727},
  doi = {10.1371/journal.pntd.0002063},
  abstract = {Here we set out to standardize long-lasting, visually-attractive devices for , a vector of both human and animal trypanosomiasis in open savannah in Tanzania and Kenya, and in neighbouring conservation areas used by pastoralists. The goal was to determine the most practical device/material that would induce the strongest landing response in for use in area-wide population suppression of this fly with insecticide-impregnated devices.},
  pmid = {23469299},
  keywords = {Agriculture,Biology,Infectious Diseases,Medicine,nosource,Pest control,Research Article,Veterinary diseases,Veterinary science,Zoology}
}

@article{MS14,
  title = {Bayesian Estimation of Discretely Observed Multi-Dimensional Diffusion Processes Using Guided Proposals},
  author = {{van der Meulen}, Frank and Schauer, Moritz},
  year = {2014},
  abstract = {Bayesian estimation of parameters of a diffusion based on discrete time observations poses a difficult problem due to the lack of a closed form expression for the likelihood. Data-augmentation has been proposed for obtaining draws from the posterior distribution of the parameters. Within this approach, the discrete time observations are augmented with diffusion bridges connecting these observations. This poses two challenges: (i) efficiently generating diffusion bridges; (ii) if unknown parameters appear in the diffusion coefficient, then direct implementation of data-augmentation results in an induced Markov chain which is reducible. In this paper we show how both challenges can be addressed in continuous time (before discretisation) by using guided proposals. These are Markov processes with dynamics described by the stochastic differential equation of the diffusion process with an additional term added to the drift coefficient to guide the process to hit the right end point of the bridge. The form of these proposals naturally provides a mapping that decouples the dependence between the diffusion coefficient and diffusion bridge using the driving Brownian motion of the proposals. As the guiding term has a singularity at the right end point, care is needed when discretisation is applied for implementation purposes. We show that this problem can be dealt with by appropriately time changing and scaling of the guided proposal process. In two examples we illustrate the performance of the algorithms we propose. The second of these concerns a diffusion approximation of a chemical reaction network with a four-dimensional diffusion driven by an eight-dimensional Brownian motion.},
  keywords = {nosource}
}

@article{mueller2018nonparametric,
  title = {Nonparametric {{Bayesian}} Inference in Applications},
  author = {M{\"u}eller, Peter and Quintana, Fernando A and Page, Garritt},
  year = {2018},
  journal = {Statistical Methods \& Applications},
  volume = {27},
  number = {2},
  pages = {175--206},
  publisher = {Springer},
  keywords = {nosource}
}

@article{mukerjee2002multi,
  title = {Multi--Objective Evolutionary Algorithms for the Risk--Return Trade--off in Bank Loan Management},
  author = {Mukerjee, Amitabha and Biswas, Rita and Deb, Kalyanmoy and Mathur, Amrit P},
  year = {2002},
  journal = {International Transactions in operational research},
  volume = {9},
  number = {5},
  pages = {583--597},
  publisher = {Wiley Online Library},
  doi = {10.1111/1475-3995.00375}
}

@article{muliere1993bayesian,
  title = {A {{Bayesian}} Predictive Approach to Sequential Search for an Optimal Dose: Parametric and Nonparametric Models.},
  author = {Muliere, P and Petrone, S},
  year = {1993},
  journal = {Journal of the Italian Statistical Society},
  volume = {2},
  number = {3},
  pages = {349--364},
  publisher = {Springer},
  issn = {1618-2510},
  keywords = {nosource}
}

@article{muller1996bayesian,
  title = {Bayesian Curve Fitting Using Multivariate Normal Mixtures},
  author = {M{\"u}ller, Peter and Erkanli, Alaattin and West, Mike},
  year = {1996},
  journal = {Biometrika},
  volume = {83},
  number = {1},
  pages = {67--79},
  publisher = {Biometrika Trust},
  issn = {00063444},
  doi = {10.1093/biomet/83.1.67},
  abstract = {Problems of regression smoothing and curve fitting are addressed via predictive inference in a flexible class of mixture models. Multidimensional density estimation using Dirichlet mixture models provides the theoretical basis for semi-parametric regression methods in which fitted regression functions may be deduced as means of conditional predictive distributions. These Bayesian regression functions have features similar to generalised kernel regression estimates, but the formal analysis addresses problems of multivariate smoothing, parameter estimation, and the assessment of uncertainties about regression functions naturally. Computations are based on multidimensional versions of existing Markov chain simulation analysis of univariate Dirichlet mixture models.},
  keywords = {nosource}
}

@article{muller2004nonparametric,
  title = {Nonparametric Bayesian Data Analysis},
  author = {M{\"u}ller, Peter and Quintana, Fernando A and Muller, Peter},
  year = {2004},
  journal = {Statistical science},
  volume = {19},
  number = {1},
  pages = {95--110},
  publisher = {JSTOR},
  issn = {0883-4237},
  doi = {10.1214/088342304000000017},
  abstract = {We review the current state of nonparametric Bayesian inference. The discussion follows a list of important statistical inference problems, including density estimation, regression, survival analysis, hierarchical models and model validation. For each in- ference problem we review relevant nonparametric Bayesian models and approaches including Dirichlet process (DP) models and variations, Polya trees, wavelet based models, neural network models, spline regression, CART, dependent DP models, and model validation with DP and Polya tree extensions of parametric models.},
  keywords = {and phrases,density estimation,dirichlet process,nosource,p{\'o}lya tree,random probability model,regression,rpm,survival analysis}
}

@article{muller2011product,
  title = {A Product Partition Model with Regression on Covariates},
  author = {M{\"u}ller, Peter and Quintana, Fernando A and Rosner, Gary L},
  year = {2011},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {20},
  number = {1},
  keywords = {nosource}
}

@article{Muller2016,
  title = {A Short Tutorial on {{Bayesian}} Nonparametrics},
  author = {M{\"u}ller, Peter and Xu, Yanxun and Jara, Alejandro},
  year = {2016},
  journal = {Journal of Statistical Research},
  volume = {48},
  number = {2},
  pages = {1--19},
  abstract = {Bayesian nonparametric (BNP) models are prior models for infinite-dimensional parameters , such as an unknown probability measure F or an unknown regression mean function f. We review some of the most widely used BNP priors, including the Dirichlet process (DP), DP mixture, the Polya tree (PT), and Gaussian process (GP) priors. We discuss how these models are used in typical inference problems. The examples include R code using available packages for inference under BNP priors.},
  keywords = {62F15,62G07,62G08,and phrases: Bayes,nonparametrics,nosource,stochastic processes AMS Classification: 62G05}
}

@article{mullerbayesian,
  title = {Bayesian Nonparametric {{Inference}}--{{Why}} and How},
  author = {M{\"u}ller, Peter and Mitra, Riten},
  year = {2013},
  journal = {Bayesian Analysis},
  volume = {8},
  number = {2},
  pages = {323--356},
  publisher = {Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/13-BA811},
  abstract = {We review inference under models with nonparametric Bayesian (BNP) priors. The discussion follows a set of examples for some common inference problems. The examples are chosen to highlight problems that are challenging for standard parametric inference. We discuss inference for density estimation, clustering, regression and for mixed effects models with random effects distributions. While we focus on arguing for the need for the flexibility of BNP models, we also review some of the more commonly used BNP models, thus hopefully answering a bit of both questions, why and how to use BNP. This review was sponsored by the Bayesian Nonparametrics Section of ISBA (ISBA/BNP). The authors thank the section officers for the support and encouragement.},
  keywords = {nosource}
}

@article{mullerInterpretationCompositionalRegression2018,
  title = {Interpretation of {{Compositional Regression}} with {{Application}} to {{Time Budget Analysis}}},
  author = {Muller, Ivo and Hron, Karel and Fiserova, Eva and Smahaj, Jan and Cakirpaloglu, Panajotis and Vancakova, Jana},
  year = {2018},
  month = feb,
  journal = {Austrian Journal of Statistics},
  volume = {47},
  number = {2},
  pages = {3--19},
  issn = {1026-597X},
  doi = {10.17713/ajs.v47i2.652},
  urldate = {2024-07-15},
  abstract = {Regression with compositional response or covariates, or even regression between parts of a composition, is frequently employed in social sciences. Among other possible applications, it may help to reveal interesting features in time allocation analysis. As individual activities represent relative contributions to the total amount of time, statistical processing of raw data (frequently represented directly as proportions or percentages) using standard methods may lead to biased results. Specific geometrical features of time budget variables are captured by the logratio methodology of compositional data, whose aim is to build (preferably orthonormal) coordinates to be applied with popular statistical methods. The aim of this paper is to present recent tools of regression analysis within the logratio methodology and apply them to reveal potential relationships among psychometric indicators in a real-world data set. In particular, orthogonal logratio coordinates have been introduced to enhance the interpretability of coefficients in regression models.},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Muller et al_2018_Interpretation of Compositional Regression with Application to Time Budget.pdf}
}

@article{Murray1981,
  title = {A {{Pre-pattern}} Formation Mechanism for Animal Coat Markings},
  author = {Murray, J. D.},
  year = {1981},
  journal = {Journal of Theoretical Biology},
  volume = {88},
  number = {1},
  pages = {161--199},
  issn = {10958541},
  doi = {10.1016/0022-5193(81)90334-9},
  abstract = {It is generally accepted that colour patterns in animals are genetically determined but the mechanism is not known. We suggest that a single mechanism which can exhibit an infinite variety of patterns is a candidate for it. We thus propose that a reaction-diffusion system which can be diffusively driven unstable could be responsible for the laying down of the pre-pattern for animal coat markings. For illustrative purposes only we consider a specific practical substrate inhibition reaction mechanism in detail and show that the geometry and scale of the domain, the relevant part of the integument, during the time of laying down plays a crucial role in the structural patterns which result. Patterns which exhibit a limited randomness are obtained for a selection of geometries and are specifically related to the coat colour distribution in the spotted felidae, zebra and other animals. ?? 1981.},
  isbn = {0022-5193},
  keywords = {nosource}
}

@article{murray1988leopard,
  title = {How the Leopard Gets Its Spots},
  author = {Murray, James D},
  year = {1988},
  journal = {Scientific American},
  volume = {258},
  number = {3},
  pages = {80--87},
  keywords = {nosource}
}

@book{murray2001mathematical,
  title = {Mathematical Biology. {{II}} Spatial Models and Biomedical Applications},
  author = {Murray, James D},
  year = {2001},
  edition = {Interdisci},
  publisher = {Springer-Verlag New York Incorporated},
  keywords = {nosource}
}

@article{murrayExplainingAwayIncompatibilist2014,
  title = {Explaining {{Away Incompatibilist Intuitions}}},
  author = {Murray, Dylan and Nahmias, Eddy},
  year = {2014},
  journal = {Philosophy and Phenomenological Research},
  volume = {88},
  number = {2},
  pages = {434--467},
  issn = {1933-1592},
  doi = {10.1111/j.1933-1592.2012.00609.x},
  urldate = {2020-05-11},
  abstract = {The debate between compatibilists and incompatibilists depends in large part on what ordinary people mean by `free will', a matter on which previous experimental philosophy studies have yielded conflicting results. In Nahmias, Morris, Nadelhoffer, and Turner (2005, 2006), most participants judged that agents in deterministic scenarios could have free will and be morally responsible. Nichols and Knobe (2007), though, suggest that these apparent compatibilist responses are performance errors produced by using concrete scenarios, and that their abstract scenarios reveal the folk theory of free will for what it actually is---incompatibilist. Here, we argue that the results of two new studies suggest just the opposite. Most participants only give apparent incompatibilist judgments when they mistakenly interpret determinism to imply that agents' mental states are bypassed in the causal chains that lead to their behavior. Determinism does not entail bypassing, so these responses do not reflect genuine incompatibilist intuitions. When participants understand what determinism does mean, the vast majority take it to be compatible with free will. Further results indicate that most people's concepts of choice and the ability to do otherwise do not commit them to incompatibilism, either, putting pressure on incompatibilist arguments that rely on transfer principles, such as the Consequence Argument. We discuss the implications of these findings for philosophical debates about free will, and suggest that incompatibilism appears to be either false, or else a thesis about something other than what most people mean by `free will'.},
  copyright = {{\copyright} 2012 Philosophy and Phenomenological Research, LLC},
  langid = {english},
  keywords = {nosource}
}

@article{Muse1994,
  title = {A Likelihood Approach for Comparing Synonymous and Nonsynonymous Nucleotide Substitution Rates, with Application to the Chloroplast Genome.},
  author = {Muse, S V and Gaut, B S},
  year = {1994},
  journal = {Molecular Biology and Evolution},
  volume = {11},
  number = {5},
  pages = {715--724},
  issn = {0737-4038},
  abstract = {A model of DNA sequence evolution applicable to coding regions is presented. This represents the first evolutionary model that accounts for dependencies among nucleotides within a codon. The model uses the codon, as opposed to the nucleotide, as the unit of evolution, and is parameterized in terms of synonymous and nonsynonymous nucleotide substitution rates. One of the model's advantages over those used in methods for estimating synonymous and nonsynonymous substitution rates is that it completely corrects for multiple hits at a codon, rather than taking a parsimony approach and considering only pathways of minimum change between homologous codons. Likelihood-ratio versions of the relative-rate test are constructed and applied to data from the complete chloroplast DNA sequences of Oryza sativa, Nicotiana tabacum, and Marchantia polymorpha. Results of these tests confirm previous findings that substitution rates in the chloroplast genome are subject to both lineage-specific and locus-specific effects. Additionally, the new tests suggest tha the rate heterogeneity is due primarily to differences in nonsynonymous substitution rates. Simulations help confirm previous suggestions that silent sites are saturated, leaving no evidence of heterogeneity in synonymous substitution rates.},
  isbn = {0737-4038},
  pmid = {7968485},
  keywords = {1,codon evolution,cpdna,evolutionary model,institute,likelihood ratio,muse,nosource,present address and,relative-rate test,spencer v}
}

@article{Mykland1996,
  title = {Algorithms for Computing Self-Consistent and Maximum Likelihood Estimators with Doubly Censored Data},
  author = {Mykland, Per A. and Ren, Jian Jian},
  year = {1996},
  month = aug,
  journal = {Annals of Statistics},
  volume = {24},
  number = {4},
  pages = {1740--1764},
  issn = {00905364},
  doi = {10.1214/aos/1032298293},
  abstract = {Abstract The paper investigates the structure of the self - consistent estimators (SCE) and the nonparametric maximum likelihood estimator (NPMLE) for doubly censored data . An explicit sufficient and necessary condition for an SCE to be the NPMLE is given. Based on this, ... {\textbackslash}n},
  keywords = {EM algorithm,Fixed point problem,nosource,Survival function}
}

@article{nadelhoffer2004blame,
  title = {Blame, Badness, and Intentional Action: A Reply to Knobe and Mendlow.},
  author = {Nadelhoffer, Thomas},
  year = {2004},
  publisher = {Division 24 of the American Psychological Association},
  keywords = {nosource}
}

@article{nadelhofferActorObserverBias2008,
  title = {The {{Actor}}--{{Observer Bias}} and {{Moral Intuitions}}: {{Adding Fuel}} to {{Sinnott-Armstrong}}'s {{Fire}}},
  shorttitle = {The {{Actor}}--{{Observer Bias}} and {{Moral Intuitions}}},
  author = {Nadelhoffer, Thomas and Feltz, Adam},
  year = {2008},
  month = jul,
  journal = {Neuroethics},
  volume = {1},
  number = {2},
  pages = {133--144},
  issn = {1874-5504},
  doi = {10.1007/s12152-008-9015-7},
  urldate = {2020-04-16},
  abstract = {In a series of recent papers, Walter Sinnott-Armstrong has used findings in social psychology to put pressure on the claim that our moral beliefs can be non-inferentially justified. More specifically, he has suggested that insofar as our moral intuitions are subject to what psychologists call framing effects, this poses a real problem for moral intuitionism. In this paper, we are going to try to add more fuel to the empirical fire that Sinnott-Armstrong has placed under the feet of the intuitionist. Along the way, we first provide an overview of what Sinnott-Armstrong calls the Master Argument against intuitionism. Then we examine some of the literature on framing effects---especially as it pertains to moral philosophy. Finally, we present the results of a new study which create yet another hurdle intuitionists must clear if they want to motivate their view. It appears that in addition to being influenced by framing effects, our moral intuitions are also influenced by an actor--observer bias as well---a bias whereby we hold other people to different moral standards than we would hold ourselves even if we were in the same situation. If we're right, the burden is on the moral intuitionist to explain why we should have faith in our moral intuitions despite the gathering evidence concerning their seeming unreliability. And by our lights, this is something that simply cannot be done from the armchair.},
  langid = {english},
  keywords = {nosource}
}

@article{nadelhofferBadActsBlameworthy2006,
  title = {Bad Acts, Blameworthy Agents, and Intentional Actions: {{Some}} Problems for Juror Impartiality},
  shorttitle = {Bad Acts, Blameworthy Agents, and Intentional Actions},
  author = {Nadelhoffer, Thomas},
  year = {2006},
  month = jun,
  journal = {Philosophical Explorations},
  volume = {9},
  number = {2},
  pages = {203--219},
  publisher = {Routledge},
  issn = {1386-9795},
  doi = {10.1080/13869790600641905},
  urldate = {2020-04-16},
  abstract = {In this paper, I first review some of the recent empirical work on the biasing effect that moral considerations have on folk ascriptions of intentional action. Then, I use Mark Alicke's affective model of blame attribution to explain this biasing effect. Finally, I discuss the relevance of this research---both philosophical and psychological---to the problem of the partiality of jury deliberation. After all, if the immorality of an action does affect folk ascriptions of intentionality, and all serious criminal offenses---e.g., murder and rape---are immoral in addition to being illegal, then a juror's ability to determine the relevant mens rea (i.e., guilty mind) of a defendant in an unbiased way may be seriously undermined.},
  keywords = {nosource}
}

@article{nadelhofferTemperamentIntuitionCommentary2009,
  title = {Temperament and Intuition: {{A}} Commentary on {{Feltz}} and {{Cokely}}},
  shorttitle = {Temperament and Intuition},
  author = {Nadelhoffer, Thomas and Kvaran, Trevor and Nahmias, Eddy},
  year = {2009},
  month = mar,
  journal = {Consciousness and Cognition},
  volume = {18},
  number = {1},
  pages = {351--355},
  issn = {1053-8100},
  doi = {10.1016/j.concog.2008.11.006},
  urldate = {2020-04-16},
  abstract = {In this paper, we examine Adam Feltz and Edward Cokely's recent claim that ``the personality trait extraversion predicts people's intuitions about the relationship of determinism to free will and moral responsibility'' (INSERT REFERENCE). We will first present some criticisms of their work before briefly examining the results of a recent study of our own. We argue that while Feltz and Cokely have their finger on the pulse of an interesting and important issue, they have not established a robust and stable connection between extraversion and compatibilist-friendly intuitions.},
  langid = {english},
  keywords = {Extraversion,Folk intuitions,Free will,Moral responsibility,Personality}
}

@article{nagelLayDenialKnowledge2013,
  title = {Lay Denial of Knowledge for Justified True Beliefs},
  author = {Nagel, Jennifer and San Juan, Valerie and Mar, Raymond A.},
  year = {2013},
  month = dec,
  journal = {Cognition},
  volume = {129},
  number = {3},
  pages = {652--661},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2013.02.008},
  urldate = {2020-05-04},
  abstract = {Intuitively, there is a difference between knowledge and mere belief. Contemporary philosophical work on the nature of this difference has focused on scenarios known as ``Gettier cases.'' Designed as counterexamples to the classical theory that knowledge is justified true belief, these cases feature agents who arrive at true beliefs in ways which seem reasonable or justified, while nevertheless seeming to lack knowledge. Prior empirical investigation of these cases has raised questions about whether lay people generally share philosophers' intuitions about these cases, or whether lay intuitions vary depending on individual factors (e.g. ethnicity) or factors related to specific types of Gettier cases (e.g. cases that include apparent evidence). We report an experiment on lay attributions of knowledge and justification for a wide range of Gettier Cases and for a related class of controversial cases known as Skeptical Pressure cases, which are also thought by philosophers to elicit intuitive denials of knowledge. Although participants rated true beliefs in Gettier and Skeptical Pressure cases as being justified, they were significantly less likely to attribute knowledge for these cases than for matched True Belief cases. This pattern of response was consistent across different variations of Gettier cases and did not vary by ethnicity or gender, although attributions of justification were found to be positively related to measures of empathy. These findings therefore suggest that across demographic groups, laypeople share similar epistemic concepts with philosophers, recognizing a difference between knowledge and justified true belief.},
  langid = {english},
  keywords = {Gettier cases,Individual differences,Justified true belief,Knowledge ascription,Lay beliefs,Mental state inference,nosource}
}

@article{nahmiasFreeWillMoral2007,
  title = {Free {{Will}}, {{Moral Responsibility}}, and {{Mechanism}}: {{Experiments}} on {{Folk Intuitions}}},
  shorttitle = {Free {{Will}}, {{Moral Responsibility}}, and {{Mechanism}}},
  author = {Nahmias, Eddy and Coates, D. Justin and Kvaran, Trevor},
  year = {2007},
  journal = {Midwest Studies In Philosophy},
  volume = {31},
  number = {1},
  pages = {214--242},
  issn = {1475-4975},
  doi = {10.1111/j.1475-4975.2007.00158.x},
  urldate = {2020-04-16},
  langid = {english},
  keywords = {nosource}
}

@article{nahmiasIncompatibilismIntuitive2006,
  ids = {nahmiasIncompatibilismIntuitive2006a},
  title = {Is {{Incompatibilism Intuitive}}?},
  author = {Nahmias, Eddy and Morris, Stephen G. and Nadelhoffer, Thomas and Turner, Jason},
  year = {2006},
  journal = {Philosophy and Phenomenological Research},
  volume = {73},
  number = {1},
  pages = {28--53},
  issn = {1933-1592},
  doi = {10.1111/j.1933-1592.2006.tb00603.x},
  urldate = {2020-04-16},
  abstract = {Incompatibilists believe free will is impossible if determinism is true, and they often claim that this view is supported by ordinary intuitions. We challenge the claim that incompatibilism is intuitive to most laypersons and discuss the significance of this challenge to the free will debate. After explaining why incompatibilists should want their view to accord with pretheoretical intuitions, we suggest that determining whether incompatibilism is in fact intuitive calls for empirical testing. We then present the results of our studies, which put significant pressure on the claim that incompatibilism is intuitive. Finally, we consider and respond to several potential objections to our approach.},
  langid = {english},
  keywords = {nosource}
}

@article{nairneAdaptiveMemoryComparative2008,
  title = {Adaptive {{Memory}}: {{The Comparative Value}} of {{Survival Processing}}},
  shorttitle = {Adaptive {{Memory}}},
  author = {Nairne, James S. and Pandeirada, Josefa N. S. and Thompson, Sarah R.},
  year = {2008},
  month = feb,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {1467-9280},
  urldate = {2020-07-08},
  abstract = {We recently proposed that human memory systems are ``tuned'' to remember information that is processed for survival, perhaps as a result of fitness advantages acc...},
  langid = {english},
  keywords = {nosource}
}

@article{narisettyBayesianVariableSelection2014,
  title = {Bayesian Variable Selection with Shrinking and Diffusing Priors},
  author = {Narisetty, Naveen Naidu and He, Xuming},
  year = {2014},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {42},
  number = {2},
  issn = {0090-5364},
  doi = {10.1214/14-AOS1207},
  urldate = {2022-03-18},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Narisetty_He_2014_Bayesian variable selection with shrinking and diffusing priors.pdf}
}

@article{naveauBayesianHighdimensionalCovariate2023,
  title = {Bayesian High-Dimensional Covariate Selection in Non-Linear Mixed-Effects Models Using the {{SAEM}} Algorithm},
  author = {Naveau, Marion and Kon Kam King, Guillaume and Rincent, Renaud and Sansonnet, Laure and Delattre, Maud},
  year = {2023},
  month = dec,
  journal = {Statistics and Computing},
  volume = {34},
  number = {1},
  pages = {53},
  issn = {1573-1375},
  doi = {10.1007/s11222-023-10367-4},
  urldate = {2024-06-26},
  abstract = {High-dimensional variable selection, with many more covariates than observations, is widely documented in standard regression models, but there are still few tools to address it in non-linear mixed-effects models where data are collected repeatedly on several individuals. In this work, variable selection is approached from a Bayesian perspective and a selection procedure is proposed, combining the use of a spike-and-slab prior and the Stochastic Approximation version of the Expectation Maximisation (SAEM) algorithm. Similarly to Lasso regression, the set of relevant covariates is selected by exploring a grid of values for the penalisation parameter. The SAEM approach is much faster than a classical Markov chain Monte Carlo algorithm and our method shows very good selection performances on simulated data. Its flexibility is demonstrated by implementing it for a variety of nonlinear mixed effects models. The usefulness of the proposed method is illustrated on a problem of genetic markers identification, relevant for genomic-assisted selection in plant breeding.},
  langid = {english},
  keywords = {High-dimension,Non-linear mixed-effects models,SAEM algorithm,Spike-and-slab prior,Variable selection},
  file = {/home/gkonkamking/pCloudDrive/papers/Naveau et al_2023_Bayesian high-dimensional covariate selection in non-linear mixed-effects.pdf}
}

@misc{naveauPosteriorContractionRates2024,
  title = {Posterior Contraction Rates in a Sparse Non-Linear Mixed-Effects Model},
  author = {Naveau, Marion and Delattre, Maud and Sansonnet, Laure},
  year = {2024},
  month = may,
  number = {arXiv:2405.01206},
  eprint = {2405.01206},
  primaryclass = {math, stat},
  publisher = {arXiv},
  urldate = {2024-06-27},
  abstract = {Recent works have shown an interest in investigating the frequentist asymptotic properties of Bayesian procedures for high-dimensional linear models under sparsity constraints. However, there exists a gap in the literature regarding analogous theoretical findings for non-linear models within the high-dimensional setting. The current study provides a novel contribution, focusing specifically on a non-linear mixed-effects model. In this model, the residual variance is assumed to be known, while the covariance matrix of the random effects and the regression vector are unknown and must be estimated. The prior distribution for the sparse regression coefficients consists of a mixture of a point mass at zero and a Laplace distribution, while an Inverse-Wishart prior is employed for the covariance parameter of the random effects. First, the effective dimension of this model is bounded with high posterior probabilities. Subsequently, we derive posterior contraction rates for both the covariance parameter and the prediction term of the response vector. Finally, under additional assumptions, the posterior distribution is shown to contract for recovery of the unknown sparse regression vector at the same rate as observed in the linear case.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/gkonkamking/Zotero/storage/JQYKE48W/Naveau et al. - 2024 - Posterior contraction rates in a sparse non-linear.pdf}
}

@article{neal2000markov,
  title = {Markov Chain Sampling Methods for {{Dirichlet}} Process Mixture Models},
  author = {Neal, Radford M},
  year = {2000},
  journal = {Journal of computational and graphical statistics},
  volume = {9},
  number = {2},
  pages = {249--265},
  publisher = {Taylor \& Francis Group},
  issn = {10618600},
  doi = {10.2307/1390653},
  abstract = {This article reviews Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model and presents two new classes of methods. One new approach is to make Metropolis-Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling. The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters. These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors.},
  isbn = {10618600},
  keywords = {auxiliary variable methods,carlo,density estimation,latent class models,metropolis-hasting algorithm,monte},
  file = {/home/gkonkamking/pCloudDrive/papers/Neal_2000_Markov chain sampling methods for Dirichlet process mixture models.pdf}
}

@article{nealSliceSampling2003,
  title = {Slice Sampling},
  author = {Neal, Radford M.},
  year = {2003},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {31},
  number = {3},
  pages = {705--767},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1056562461},
  urldate = {2024-11-29},
  abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
  keywords = {65C05,65C60,Adaptive methods,auxiliary variables,dynamical methods,Gibbs sampling,Markov chain Monte Carlo,Metropolis algorithm,overrelaxation},
  file = {/home/gkonkamking/pCloudDrive/papers/Neal - 2003 - Slice sampling.pdf}
}

@article{neiESTIMATIONAVERAGEHETEROZYGOSITY1978,
  title = {{{ESTIMATION OF AVERAGE HETEROZYGOSITY AND GENETIC DISTANCE FROM A SMALL NUMBER OF INDIVIDUALS}}},
  author = {Nei, Masatoshi},
  year = {1978},
  month = jul,
  journal = {Genetics},
  volume = {89},
  number = {3},
  pages = {583--590},
  issn = {1943-2631},
  doi = {10.1093/genetics/89.3.583},
  urldate = {2022-12-19},
  abstract = {The magnitudes of the systematic biases involved in sample heterozygosity and sample genetic distances are evaluated, and formulae for obtaining unbiased estimates of average heterozygosity and genetic distance are developed. It is also shown that the number of individuals to be used for estimating average heterozygosity can be very small if a large number of loci are studied and the average heterozygosity is low. The number of individuals to be used for estimating genetic distance can also be very small if the genetic distance is large and the average heterozygosity of the two species compared is low.},
  file = {/home/gkonkamking/pCloudDrive/papers/Nei_1978_ESTIMATION OF AVERAGE HETEROZYGOSITY AND GENETIC DISTANCE FROM A SMALL NUMBER.pdf}
}

@article{neklyudovInvolutiveMCMCUnifying,
  title = {Involutive {{MCMC}}: A {{Unifying Framework}}},
  author = {Neklyudov, Kirill and Welling, Max and Egorov, Evgenii and Vetrov, Dmitry},
  abstract = {Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental problems such as inference, integration, optimization, and simulation. The field has developed a broad spectrum of algorithms, varying in the way they are motivated, the way they are applied and how efficiently they sample. Despite all the differences, many of them share the same core principle, which we unify as the Involutive MCMC (iMCMC) framework. Building upon this, we describe a wide range of MCMC algorithms in terms of iMCMC, and formulate a number of ``tricks'' which one can use as design principles for developing new MCMC algorithms. Thus, iMCMC provides a unified view of many known MCMC algorithms, which facilitates the derivation of powerful extensions. We demonstrate the latter with two examples where we transform known reversible MCMC algorithms into more efficient irreversible ones.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/Zotero/storage/T3REY4Y2/Neklyudov et al. - Involutive MCMC a Unifying Framework.pdf}
}

@inproceedings{nemo,
  title = {Nemo/{{Hecke}}: {{Computer}} Algebra and Number Theory Packages for the Julia Programming Language},
  booktitle = {Proceedings of the 2017 {{ACM}} on International Symposium on Symbolic and Algebraic Computation},
  author = {Fieker, Claus and Hart, William and Hofmann, Tommy and Johansson, Fredrik},
  year = {2017},
  series = {{{ISSAC}} '17},
  pages = {157--164},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/3087604.3087611},
  keywords = {nosource}
}

@article{nesrstovaSimpleEnoughNot2023,
  title = {Simple Enough, but Not Simpler: Reconsidering Additive Logratio Coordinates in Compositional Analysis},
  shorttitle = {Simple Enough, but Not Simpler},
  author = {Nesrstov{\'a}, Viktorie and Ja{\v s}kov{\'a}, Paul{\'i}na and Pavl{\r u}, Ivana and Hron, Karel and {Palarea-Albaladejo}, Javier and G{\'a}ba, Ale{\v s} and Pelclov{\'a}, Jana and Fa{\v c}evicov{\'a}, Kamila},
  year = {2023},
  month = nov,
  journal = {SORT-Statistics and Operations Research Transactions},
  pages = {269--294},
  issn = {2013-8830},
  doi = {10.57645/20.8080.02.9},
  urldate = {2024-07-16},
  abstract = {Compositional data, multivariate observations carrying relative information, are popularly expressed in additive logratio coordinates which are easily interpretable as they use one of the components as ratioing part to produce pairwise logratios. These coordinates are however oblique and they lead to issues when applying multivariate methods on them, including widely-used techniques such as principal component analysis and linear regression. In this paper we propose a way to redefine alr coordinates with respect to an orthonormal system and we also extend the idea to the case of compositional tables. The new approach is demonstrated in an application to movement behavior data.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {compositional data,compositional tables,principal component analysis,regression},
  file = {/home/gkonkamking/pCloudDrive/papers/Nesrstová et al_2023_Simple enough, but not simpler.pdf}
}

@book{newman1994quantitative,
  title = {Quantitative Methods in Aquatic Ecotoxicology},
  author = {Newman, Michael C.},
  year = {1994},
  publisher = {CRC press},
  keywords = {nosource}
}

@article{newman1996ecologically,
  title = {Ecologically Meaningful Estimates of Lethal Effect in Individuals},
  author = {Newman, Michael C and Dixon, Philip M},
  year = {1996},
  journal = {Ecotoxicology: A Hierarchical Treatment. CRC/Lewis Publishers, Inc., Boca Raton, FL},
  pages = {225--253},
  keywords = {nosource}
}

@article{Newman2000,
  title = {Applying Species-Sensitivity Distributions in Ecological Risk Assessment: {{Assumptions}} of Distribution Type and Sufficient Numbers of Species},
  author = {Newman, Michael C. and Ownby, David R. and M{\'e}zin, Laurent C. A. and Powell, David C. and Christensen, Tyler R. L. and Lerberg, Scott B. and Anderson, Britt-Anne and Hristensen, T Yler R L C and Erberg, S Cott B L and Nderson, B Ritt N N E A},
  year = {2000},
  month = feb,
  journal = {Environmental Toxicology and Chemistry},
  volume = {19},
  number = {2},
  pages = {508--515},
  issn = {07307268},
  doi = {10.1002/etc.5620190233},
  keywords = {nosource,risk assessment}
}

@article{Newman2000b,
  title = {Applying Species-Sensitivity Distributions in Ecological Risk Assessment: {{Assumptions}} of Distribution Type and Sufficient Numbers of Species},
  author = {Newman, Michael C. and Ownby, D R and Mezin, L C A and Powell, D C and Christensen, T R L and Lerberg, S B and Anderson, B A},
  year = {2000},
  month = feb,
  journal = {Environmental Toxicology and Chemistry},
  volume = {19},
  number = {2},
  pages = {508--515},
  publisher = {Wiley Periodicals, Inc.},
  issn = {07307268},
  doi = {10.1897/1551-5028(2000)019<0508:assdie>2.3.co;2},
  abstract = {Species-sensitivity distribution methods assemble single-species toxicity data to predict hazardous concentrations (HCps) affecting a certain percentage (p) of species in a community. The fit of the lognormal model and required number of individual species values were evaluated with 30 published data sets. The increasingly common assumption that a lognormal model best fits these data was not supported. Fifteen data sets failed a formal test of conformity to a lognormal distribution; other distributions often provided better fit to the data than the lognormal distribution. An alternate bootstrap method provided accurate estimates of HCp without the assumption of a specific distribution. Approximate sample sizes producing HC5 estimates with minimal variance ranged from 15 to 55, and had a median of 30 species-sensitivity values. These sample sizes are higher than those suggested in recent regulatory documents. A bootstrap method is recommended that predicts with 95\% confidence the concentration affecting 5\% or fewer species.},
  isbn = {0730-7268},
  keywords = {biodiversity,bootstrap,ecosystems,metals,mysidopsis-bahia,noec toxicity data,nosource,protection levels,risk assessment,sample size,soil,species sensitivity,stability,statistics,surface waters,tests}
}

@article{newman2005power,
  title = {Power Laws, Pareto Distributions and Zipf's Law},
  author = {Newman, Mark E J},
  year = {2005},
  journal = {Contemporary physics},
  volume = {46},
  number = {5},
  pages = {323--351},
  publisher = {Taylor \& Francis},
  keywords = {nosource}
}

@book{newman2012quantitative,
  title = {Quantitative Ecotoxicology},
  author = {Newman, Michael C.},
  year = {2012},
  publisher = {CRC Press},
  abstract = {"This book provides a quantitative treatment of the science of ecotoxicology. The first chapters consider fundamental concepts and definitions essential to understanding the fate and effects of toxicants at various levels of ecological organization as covered in the remaining chapters. Scientific ecotoxicology and associated topics are defined. The historical perspective, rationale, and characteristics are outlined for the strong inferential and quantitative approach advocated in this book. The general measurement process is discussed, and methodologies for defining and controlling variance, which could otherwise exclude valid conclusions regarding ecotoxicological endeavors, are considered.Ecotoxicological concepts at increasing levels of ecological organization are discussed in the second part of the book. Quantitative methods used to measure toxicant effects are outlined in this section. The final chapter summarizes the book with a brief discussion of ecotoxicological assessment. Numerous figures and tables accompany text, with many statistical tables found in the appendix for quick reference. Although the book primarily focuses on aquatic systems, with appropriate modification the concepts and methods can be applied to terrestrial systems"--},
  isbn = {978-1-4398-3565-4},
  keywords = {nosource}
}

@article{Newton2010,
  title = {Gamma-Based Clustering via Ordered Means with Application to Gene-Expression Analysis},
  author = {Newton, Michael A. and Chung, Lisa M.},
  year = {2010},
  journal = {Annals of Statistics},
  volume = {38},
  number = {6},
  eprint = {0907.3837v3},
  pages = {3217--3244},
  issn = {00905364},
  doi = {10.1214/10-AOS805},
  abstract = {Discrete mixture models provide a well-known basis for effec- tive clustering algorithms, although technical challenges have lim- ited their scope. In the context of gene-expression data analysis, a model is presented that mixes over a finite catalog of structures, each one representing equality and inequality constraints among latent expected values. Computations depend on the probability that in- dependent gamma-distributed variables attain each of their possible orderings. Each ordering event is equivalent to an event in indepen- dent negative-binomial random variables, and this finding guides a dynamic-programming calculation. The structuring of mixture-model components according to constraints among latent means leads to strict concavity of the mixture log likelihood. In addition to its ben- eficial numerical properties, the clustering method shows promising results in an empirical study.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:0907.3837v3},
  keywords = {Gamma ranking,Mixture model,Next generation sequencing,nosource,Poisson embedding,Rank probability}
}

@article{nguyenBorrowingStrenghHierarchical2016,
  title = {Borrowing Strengh in Hierarchical {{Bayes}}: {{Posterior}} Concentration of the {{Dirichlet}} Base Measure},
  shorttitle = {Borrowing Strengh in Hierarchical {{Bayes}}},
  author = {Nguyen, XuanLong},
  year = {2016},
  month = aug,
  journal = {Bernoulli},
  volume = {22},
  number = {3},
  pages = {1535--1571},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  doi = {10.3150/15-BEJ703},
  urldate = {2022-06-07},
  abstract = {This paper studies posterior concentration behavior of the base probability measure of a Dirichlet measure, given observations associated with the sampled Dirichlet processes, as the number of observations tends to infinity. The base measure itself is endowed with another Dirichlet prior, a construction known as the hierarchical Dirichlet processes (Teh et al. [J. Amer. Statist. Assoc. 101 (2006) 1566--1581]). Convergence rates are established in transportation distances (i.e., Wasserstein metrics) under various conditions on the geometry of the support of the true base measure. As a consequence of the theory, we demonstrate the benefit of ``borrowing strength'' in the inference of multiple groups of data -- a powerful insight often invoked to motivate hierarchical modeling. In certain settings, the gain in efficiency due to the latent hierarchy can be dramatic, improving from a standard nonparametric rate to a parametric rate of convergence. Tools developed include transportation distances for nonparametric Bayesian hierarchies of random measures, the existence of tests for Dirichlet measures, and geometric properties of the support of Dirichlet measures.},
  keywords = {Bayesian asymptotics,Dirichlet processes,geometry of support,posterior concentration,Random measures,transportation distances,Wasserstein metrics},
  file = {/home/gkonkamking/Zotero/storage/6CSL68DQ/Nguyen - 2016 - Borrowing strengh in hierarchical Bayes Posterior.pdf}
}

@article{nguyenConvergenceLatentMixing2013,
  title = {Convergence of Latent Mixing Measures in Finite and Infinite Mixture Models},
  author = {Nguyen, XuanLong},
  year = {2013},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {41},
  number = {1},
  pages = {370--400},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/12-AOS1065},
  urldate = {2021-11-08},
  abstract = {This paper studies convergence behavior of latent mixing measures that arise in finite and infinite mixture models, using transportation distances (i.e., Wasserstein metrics). The relationship between Wasserstein distances on the space of mixing measures and \$f\$-divergence functionals such as Hellinger and Kullback--Leibler distances on the space of mixture distributions is investigated in detail using various identifiability conditions. Convergence in Wasserstein metrics for discrete measures implies convergence of individual atoms that provide support for the measures, thereby providing a natural interpretation of convergence of clusters in clustering applications where mixture models are typically employed. Convergence rates of posterior distributions for latent mixing measures are established, for both finite mixtures of multivariate distributions and infinite mixtures based on the Dirichlet process.},
  keywords = {\$f\$-divergence,62F15,62G05,62G20,Bayesian nonparametrics,Dirichlet processes,hierarchical models,mixture distributions,rates of convergence,transportation distances,Wasserstein metric},
  file = {/home/gkonkamking/pCloudDrive/papers/Nguyen_2013_Convergence of latent mixing measures in finite and infinite mixture models.pdf}
}

@misc{nguyenHyenaDNALongRangeGenomic2023,
  title = {{{HyenaDNA}}: {{Long-Range Genomic Sequence Modeling}} at {{Single Nucleotide Resolution}}},
  shorttitle = {{{HyenaDNA}}},
  author = {Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and {Birch-Sykes}, Callum and Wornow, Michael and Patel, Aman and Rabideau, Clayton and Massaroli, Stefano and Bengio, Yoshua and Ermon, Stefano and Baccus, Stephen A. and R{\'e}, Chris},
  year = {2023},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-10-10},
  abstract = {Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context ({$<$}0.001\% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.},
  howpublished = {https://arxiv.org/abs/2306.15794v2},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/Q8HIEP36/Nguyen et al. - 2023 - HyenaDNA Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.pdf}
}

@misc{nguyenSequenceModelingDesign2024,
  title = {Sequence Modeling and Design from Molecular to Genome Scale with {{Evo}}},
  author = {Nguyen, Eric and Poli, Michael and Durrant, Matthew G and Thomas, Armin W and Kang, Brian and Sullivan, Jeremy and Ng, Madelena Y and Lewis, Ashley and Patel, Aman and Lou, Aaron and Ermon, Stefano and Baccus, Stephen A and {Hernandez-Boussard}, Tina and Re, Christopher and Hsu, Patrick D and Hie, Brian L},
  year = {2024},
  month = feb,
  publisher = {Synthetic Biology},
  doi = {10.1101/2024.02.27.582234},
  urldate = {2024-10-10},
  abstract = {The genome is a sequence that completely encodes the DNA, RNA, and proteins that orchestrate the function of a whole organism. Advances in machine learning combined with massive datasets of whole genomes could enable a biological foundation model that accelerates the mechanistic understanding and generative design of complex molecular interactions. We report Evo, a genomic foundation model that enables prediction and generation tasks from the molecular to genome scale. Using an architecture based on advances in deep signal processing, we scale Evo to 7 billion parameters with a context length of 131 kilobases (kb) at singlenucleotide, byte resolution. Trained on whole prokaryotic genomes, Evo can generalize across the three fundamental modalities of the central dogma of molecular biology to perform zero-shot function prediction that is competitive with, or outperforms, leading domain-specific language models. Evo also excels at multielement generation tasks, which we demonstrate by generating synthetic CRISPR-Cas molecular complexes and entire transposable systems for the first time. Using information learned over whole genomes, Evo can also predict gene essentiality at nucleotide resolution and can generate coding-rich sequences up to 650 kb in length, orders of magnitude longer than previous methods. Advances in multi-modal and multi-scale learning with Evo provides a promising path toward improving our understanding and control of biology across multiple levels of complexity.},
  archiveprefix = {Synthetic Biology},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/GQCDZYCL/Nguyen et al. - 2024 - Sequence modeling and design from molecular to genome scale with Evo.pdf}
}

@article{nichollsRecoveryGeneHaplotypes2019,
  title = {Recovery of Gene Haplotypes from a Metagenome},
  author = {Nicholls, Samuel M. and Aubrey, Wayne and Edwards, Arwyn and de Grave, Kurt and Huws, Sharon and Schietgat, Leander and Soares, Andr{\'e} and Creevey, Christopher J. and Clare, Amanda},
  year = {2019},
  month = oct,
  journal = {bioRxiv},
  pages = {223404},
  publisher = {Cold Spring Harbor Laboratory},
  doi = {10.1101/223404},
  urldate = {2021-04-30},
  abstract = {Elucidation of population-level diversity of microbiomes is a significant step towards a complete understanding of the evolutionary, ecological and functional importance of microbial communities. Characterizing this diversity requires the recovery of the exact DNA sequence (haplotype) of each gene isoform from every individual present in the community. To address this, we present Hansel and Gretel: a freely-available data structure and algorithm, providing a software package that reconstructs the most likely haplotypes from metagenomes. We demonstrate recovery of haplotypes from short-read Illumina data for a bovine rumen microbiome, and verify our predictions are 100\% accurate with long-read PacBio CCS sequencing. We show that Gretel's haplotypes can be analyzed to determine a significant difference in mutation rates between core and accessory gene families in an ovine rumen microbiome. All tools, documentation and data for evaluation are open source and available via our repository: https://github.com/samstudio8/gretel{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {{\copyright} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/33BXNLUU/Nicholls et al. - 2019 - Recovery of gene haplotypes from a metagenome.pdf}
}

@article{nicholsFolkIntuitionsFree2006,
  title = {Folk {{Intuitions}} on {{Free Will}}},
  author = {Nichols, Shaun},
  year = {2006},
  month = jan,
  journal = {Journal of Cognition and Culture},
  volume = {6},
  number = {1-2},
  pages = {57--86},
  publisher = {Brill},
  issn = {1568-5373, 1567-7095},
  doi = {10.1163/156853706776931385},
  urldate = {2020-04-16},
  abstract = {{$<$}section class="abstract"{$><$}div id="" class="section"{$><$}h3 class="abstractTitle text-title my-1" id="d660e3"{$>$}Abstract{$<$}/h3{$><$}p{$>$}This paper relies on experimental methods to explore the psychological underpinnings of folk intuitions about free will and responsibility. In different conditions, people give conflicting responses about agency and responsibility. In some contexts, people treat agency as indeterminist; in other contexts, they treat agency as determinist. Furthermore, in some contexts people treat responsibility as incompatible with determinism, and in other contexts people treat responsibility as compatible with determinism. The paper considers possible accounts of the psychological mechanisms that underlie these conflicting responses.{$<$}/p{$><$}/div{$><$}/section{$>$}},
  chapter = {Journal of Cognition and Culture},
  langid = {english},
  keywords = {nosource}
}

@article{nicholsMoralResponsibilityDeterminism2007,
  title = {Moral {{Responsibility}} and {{Determinism}}: {{The Cognitive Science}} of {{Folk Intuitions}}},
  shorttitle = {Moral {{Responsibility}} and {{Determinism}}},
  author = {Nichols, Shaun and Knobe, Joshua},
  year = {2007},
  journal = {No{\^u}s},
  volume = {41},
  number = {4},
  pages = {663--685},
  issn = {1468-0068},
  doi = {10.1111/j.1468-0068.2007.00666.x},
  urldate = {2020-04-16},
  langid = {english},
  keywords = {nosource}
}

@article{niConsensusMonteCarlo2020,
  title = {Consensus {{Monte Carlo}} for {{Random Subsets Using Shared Anchors}}},
  author = {Ni, Yang and Ji, Yuan and M{\"u}ller, Peter},
  year = {2020},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {29},
  number = {4},
  pages = {703--714},
  publisher = {Taylor \& Francis},
  issn = {1061-8600},
  doi = {10.1080/10618600.2020.1737085},
  urldate = {2021-05-04},
  abstract = {We present a consensus Monte Carlo algorithm that scales existing Bayesian nonparametric models for clustering and feature allocation to big data. The algorithm is valid for any prior on random subsets such as partitions and latent feature allocation, under essentially any sampling model. Motivated by three case studies, we focus on clustering induced by a Dirichlet process mixture sampling model, inference under an Indian buffet process prior with a binomial sampling model, and with a categorical sampling model. We assess the proposed algorithm with simulation studies and show results for inference with three datasets: an MNIST image dataset, a dataset of pancreatic cancer mutations, and a large set of electronic health records. Supplementary materials for this article are available online.},
  file = {/home/gkonkamking/Zotero/storage/2IHCWMQ7/Ni et al. - 2020 - Consensus Monte Carlo for Random Subsets Using Sha.pdf}
}

@article{Nielsen2003,
  title = {Effects of Increasing Salinity on Freshwater Ecosystems in {{Australia}}},
  author = {Nielsen, D. L. and Brock, M. A. and Rees, G. N. and Baldwin, D. S.},
  year = {2003},
  journal = {Australian Journal of Botany},
  volume = {51},
  number = {6},
  pages = {655--665},
  issn = {00671924},
  doi = {10.1071/BT02115},
  abstract = {Salt is a natural component of the Australian landscape to which a number of biota inhabiting rivers and wetlands are adapted. Under natural flow conditions periods of low flow have resulted in the concentration of salts in wetlands and riverine pools. The organisms of these systems survive these salinities by tolerance or avoidance. Freshwater ecosystems in Australia are now becoming increasingly threatened by salinity because of rising saline groundwater and modification of the water regime reducing the frequency of high-flow (flushing) events, resulting in an accumulation of salt. Available data suggest that aquatic biota will be adversely affected as salinity exceeds 1000 mg L\textsuperscript{\&\#8211;1} (1500 EC) but there is limited information on how increasing salinity will affect the various life stages of the biota. Salinisation can lead to changes in the physical environment that will affect ecosystem processes. However, we know little about how salinity interacts with the way nutrients and carbon are processed within an ecosystem. This paper updates the knowledge base on how salinity affects the physical and biotic components of aquatic ecosystems and explores the needs for information on how structure and function of aquatic ecosystems change with increasing salinity.},
  isbn = {00671924 (ISSN)},
  keywords = {nosource}
}

@article{Nieto-Barajas2002markov,
  title = {Markov Beta and Gamma Processes for Modelling Hazard Rates},
  author = {{Nieto-Barajas}, Luis E. and Walker, Stephen G.},
  year = {2002},
  journal = {Scandinavian Journal of Statistics},
  volume = {29},
  number = {3},
  pages = {413--424},
  publisher = {Wiley on behalf of Board of the Foundation of the Scandinavian Journal of Statistics},
  issn = {0303-6898},
  doi = {10.1111/1467-9469.00298},
  abstract = {This paper generalizes the discrete time independent increment beta process of Hjort (1990), for modelling discrete failure times, and also generalizes the independent gamma process for modelling piecewise constant hazard rates (Walker and Mallick, 1997). The generalizations are from independent increment to Markov increment prior processes allowing the modelling of smoothness. We derive posterior distributions and undertake a full Bayesian analysis},
  keywords = {bayes non-parametrics,beta process,gamma process,markov process,nosource,process,stationary,survival analysis}
}

@article{Nieto-Barajas2004bayesian,
  title = {Bayesian Nonparametric Survival Analysis via {{L{\'e}vy}} Driven {{Markov}} Processes},
  author = {{Nieto-Barajas}, Luis E. and Walker, Stephen G},
  year = {2004},
  journal = {Statistica Sinica},
  volume = {14},
  number = {4},
  pages = {1127--1146},
  keywords = {nosource}
}

@article{Nieto-Barajas2014,
  title = {A {{Bayesian}} Nonparametric Approach for Time Series Clustering},
  author = {{Nieto-Barajas}, Luis E. and {Contreras-Crist??n}, Alberto},
  year = {2014},
  journal = {Bayesian Analysis},
  volume = {9},
  number = {1},
  pages = {147--170},
  issn = {19316690},
  doi = {10.1214/13-BA852},
  keywords = {Bayes nonparametrics,Dynamic linear model,Model-based clustering,nosource,Pitman-Yor process,Time series analysis}
}

@article{Nieto-Barajas2014bayesian,
  title = {Bayesian Semiparametric Analysis of Short- and Long-Term Hazard Ratios with Covariates},
  author = {{Nieto-Barajas}, Luis E.},
  year = {2014},
  journal = {Computational Statistics and Data Analysis},
  volume = {71},
  number = {0},
  pages = {477--490},
  issn = {01679473},
  doi = {10.1016/j.csda.2013.03.012},
  abstract = {A full Bayesian analysis is developed for an extension to the short-term and long-term hazard ratios model that has been previously introduced. This model is specified by two parameters, short- and long-term hazard ratios respectively, and an unspecified baseline function. Furthermore, the model also allows for crossing hazards in two groups and includes the proportional hazards, and the proportional odds models as particular cases. The model is extended to include covariates in both, the short- and long-term parameters, and uses a Bayesian nonparametric prior, based on increasing additive processes mixtures, to model the baseline function. Posterior distributions are characterized via their full conditionals. Latent variables are introduced wherever needed to simplify computations. The algorithm is tested with a simulation study and posterior inference is illustrated with a survival study of ovarian cancer patients who have undergone a treatment with erythropoietin stimulating agents. ?? 2013 Elsevier B.V. All rights reserved.},
  keywords = {Bayesian nonparametrics,Crossing hazards,Increasing additive process,L??vy-driven process,Latent variables,nosource,Survival analysis}
}

@article{nieto2004normalized,
  title = {Normalized Random Measures Driven by Increasing Additive Processes},
  author = {{Nieto-Barajas}, Luis E. and Pr{\"u}nster, Igor and Walker, Stephen G.},
  year = {2004},
  journal = {Annals of Statistics},
  volume = {32},
  number = {6},
  pages = {2343--2360},
  publisher = {Institute of Mathematical Statistics},
  issn = {00905364},
  doi = {10.1214/009053604000000625},
  abstract = {This paper introduces and studies a new class of nonparametric prior distributions. Random probability distribution functions are constructed via normalization of random measures driven by increasing additive processes. In particular, we present results for the distribution of means under both prior and posterior conditions and, via the use of strategic latent variables, undertake a full Bayesian analysis. Our class of priors includes the well-known and widely used mixture of a Dirichlet process.},
  arxiv = {0508592 [math]},
  arxivid = {math/0508592},
  keywords = {Bayesian nonparametric inference,Distribution of means of random probability measur,Increasing additive process,Levy measure,Mixtures of Dirichlet process,nosource}
}

@article{nieto2009sensitivity,
  title = {A Sensitivity Analysis for Bayesian Nonparametric Density Estimators},
  author = {{Nieto-barajas}, Luis E and Pr{\"u}nster, Igor},
  year = {2009},
  journal = {Statistica Sinicain.},
  volume = {19},
  number = {2},
  pages = {685--705},
  keywords = {and phrases,bayesian nonparametric inference,creasing additive process,density estimation,evy process,in-,l,latent variables,mixture model,nosource,sensitivity}
}

@article{nik-zainalLifeHistory212012,
  title = {The {{Life History}} of 21 {{Breast Cancers}}},
  author = {{Nik-Zainal}, Serena and Van~Loo, Peter and Wedge, David~C. and Alexandrov, Ludmil~B. and Greenman, Christopher~D. and Lau, King~Wai and Raine, Keiran and Jones, David and Marshall, John and Ramakrishna, Manasa and Shlien, Adam and Cooke, Susanna~L. and Hinton, Jonathan and Menzies, Andrew and Stebbings, Lucy~A. and Leroy, Catherine and Jia, Mingming and Rance, Richard and Mudie, Laura~J. and Gamble, Stephen~J. and Stephens, Philip~J. and McLaren, Stuart and Tarpey, Patrick~S. and Papaemmanuil, Elli and Davies, Helen~R. and Varela, Ignacio and McBride, David~J. and Bignell, Graham~R. and Leung, Kenric and Butler, Adam~P. and Teague, Jon~W. and Martin, Sancha and J{\"o}nsson, Goran and Mariani, Odette and Boyault, Sandrine and Miron, Penelope and Fatima, Aquila and Langer{\o}d, Anita and Aparicio, Samuel~A. J. R. and Tutt, Andrew and Sieuwerts, Anieta~M. and Borg, {\AA}ke and Thomas, Gilles and Salomon, Anne~Vincent and Richardson, Andrea~L. and {B{\o}rresen-Dale}, Anne-Lise and Futreal, P. Andrew and Stratton, Michael~R. and Campbell, Peter~J.},
  year = {2012},
  month = may,
  journal = {Cell},
  volume = {149},
  number = {5},
  pages = {994--1007},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2012.04.023},
  urldate = {2021-06-03},
  abstract = {Cancer evolves dynamically as clonal expansions supersede one another driven by shifting selective pressures, mutational processes, and disrupted cancer genes. These processes mark the genome, such that a cancer's life history is encrypted in the somatic mutations present. We developed algorithms to decipher this narrative and applied them to 21 breast cancers. Mutational processes evolve across a cancer's lifespan, with many emerging late but contributing extensive genetic variation. Subclonal diversification is prominent, and most mutations are found in just a fraction of tumor cells. Every tumor has a dominant subclonal lineage, representing more than 50\% of tumor cells. Minimal expansion of these subclones occurs until many hundreds to thousands of mutations have accumulated, implying the existence of long-lived, quiescent cell lineages capable of substantial proliferation upon acquisition of enabling genomic changes. Expansion of the dominant subclone to an appreciable mass may therefore represent the final rate-limiting step in a breast cancer's development, triggering diagnosis. PaperClip},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/K8ILJXKM/Nik-Zainal et al. - 2012 - The Life History of 21 Breast Cancers.pdf}
}

@article{niScalableBayesianNonparametric2020,
  title = {Scalable {{Bayesian Nonparametric Clustering}} and {{Classification}}},
  author = {Ni, Yang and M{\"u}ller, Peter and Diesendruck, Maurice and Williamson, Sinead and Zhu, Yitan and Ji, Yuan},
  year = {2020},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {29},
  number = {1},
  pages = {53--65},
  publisher = {Taylor \& Francis},
  issn = {1061-8600},
  doi = {10.1080/10618600.2019.1624366},
  urldate = {2021-05-04},
  abstract = {We develop a scalable multistep Monte Carlo algorithm for inference under a large class of nonparametric Bayesian models for clustering and classification. Each step is ``embarrassingly parallel'' and can be implemented using the same Markov chain Monte Carlo sampler. The simplicity and generality of our approach make inference for a wide range of Bayesian nonparametric mixture models applicable to large datasets. Specifically, we apply the approach to inference under a product partition model with regression on covariates. We show results for inference with two motivating datasets: a large set of electronic health records and a bank telemarketing dataset. We find interesting clusters and competitive classification performance relative to other widely used competing classifiers. Supplementary materials for this article are available online.},
  file = {/home/gkonkamking/Zotero/storage/VJ5MBRIC/Ni et al. - 2020 - Scalable Bayesian Nonparametric Clustering and Cla.pdf}
}

@article{nishiInequalityVisibilityWealth2015,
  title = {Inequality and Visibility of Wealth in Experimental Social Networks},
  author = {Nishi, Akihiro and Shirado, Hirokazu and Rand, David G. and Christakis, Nicholas A.},
  year = {2015},
  month = oct,
  journal = {Nature},
  volume = {526},
  number = {7573},
  pages = {426--429},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature15392},
  urldate = {2020-04-16},
  abstract = {Wealth inequality and wealth visibility can potentially affect overall levels of cooperation and economic success, and an online experiment was used to test how these factors interact; wealth inequality by itself did not substantially damage overall cooperation or overall wealth, but making wealth levels visible had a detrimental effect on social welfare.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {nosource}
}

@article{Nishimura2017,
  title = {Discontinuous Hamiltonian Monte Carlo for Sampling Discrete Parameters},
  author = {Nishimura, Akihiko and Dunson, David and Lu, Jianfeng},
  year = {2017},
  eprint = {1705.08510},
  abstract = {Hamiltonian Monte Carlo (HMC) is a powerful sampling algorithm employed by several probabilistic programming languages. Its fully automatic implementations have made HMC a standard tool for applied Bayesian modeling. While its performance is often superior to alternatives under a wide range of models, one weakness of HMC is its inability to handle discrete parameters. In this article, we present discontinuous HMC, an extension that can efficiently explore discrete parameter spaces as well as continuous ones. The proposed algorithm is based on two key ideas: embedding of discrete parameters into a continuous space and simulation of Hamiltonian dynamics on a piecewise smooth density function. The latter idea has been explored under special cases in the literature, but the extensions introduced here are critical in turning the idea into a general and practical sampling algorithm. Discontinuous HMC is guaranteed to outperform a Metropolis-within-Gibbs algorithm as the two algorithms coincide under a specific (and sub-optimal) implementation of discontinuous HMC. It is additionally shown that the dynamics underlying discontinuous HMC have a remarkable similarity to a zig-zag process, a continuous-time Markov process behind a state-of-the-art non-reversible rejection-free sampler. We apply our algorithm to challenging posterior inference problems to demonstrate its wide applicability and superior performance.},
  archiveprefix = {arXiv},
  arxivid = {1705.08510},
  keywords = {bayesian inference,event-driven monte carlo,hybrid monte carlo,integrator,markov chain monte carlo,nosource,rejection-free}
}

@article{Nolan2016,
  title = {{{SimplicialCubature}}},
  author = {Nolan, John P.},
  year = {2016},
  keywords = {nosource}
}

@article{norenzayanCulturalPreferencesFormal2002,
  title = {Cultural Preferences for Formal versus Intuitive Reasoning},
  author = {Norenzayan, Ara and Smith, Edward E. and Kim, Beom Jun and Nisbett, Richard E.},
  year = {2002},
  journal = {Cognitive Science},
  volume = {26},
  number = {5},
  pages = {653--684},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog2605_4},
  urldate = {2020-06-19},
  abstract = {The authors examined cultural preferences for formal versus intuitive reasoning among East Asian (Chinese and Korean), Asian American, and European American university students. We investigated categorization (Studies 1 and 2), conceptual structure (Study 3), and deductive reasoning (Studies 3 and 4). In each study a cognitive conflict was activated between formal and intuitive strategies of reasoning. European Americans, more than Chinese and Koreans, set aside intuition in favor of formal reasoning. Conversely, Chinese and Koreans relied on intuitive strategies more than European Americans. Asian Americans' reasoning was either identical to that of European Americans, or intermediate. Differences emerged against a background of similar reasoning tendencies across cultures in the absence of conflict between formal and intuitive strategies.},
  copyright = {{\copyright} 2002 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Concepts,Cross-cultural analysis,Epistemology,nosource,Reasoning}
}

@techreport{norets2011posterior,
  title = {Posterior Consistency in Conditional Density Estimation by Covariate Dependent Mixtures},
  author = {Norets, Andriy and Pelenis, Justinas},
  year = {2011},
  institution = {Economics Series, Institute for Advanced Studies},
  keywords = {nosource}
}

@article{nosekHarvestingImplicitGroup2002,
  title = {Harvesting Implicit Group Attitudes and Beliefs from a Demonstration Web Site.},
  author = {Nosek, Brian A. and Banaji, Mahzarin R. and Greenwald, Anthony G.},
  year = {2002},
  journal = {Group Dynamics: Theory, Research, and Practice},
  volume = {6},
  number = {1},
  pages = {101--115},
  doi = {10.1037/1089-2699.6.1.101},
  abstract = {Respondents at an Internet site completed over 600,000 tasks between October 1998 and April 2000 measuring attitudes toward and stereotypes of social groups. Their responses demonstrated, on average, implicit preference for White over Black and young over old and stereotypic: associations linking male terms with science and career and female terms with liberal arts and family. The main purpose was to provide a demonstration site at which respondents could experience their implicit attitudes and stereotypes toward social groups. Nevertheless, the data collected are rich in information regarding the operation of attitudes and stereotypes, most notably the strength of implicit attitudes, the association and dissociation between implicit and explicit attitudes, and the effects of group membership on attitudes and stereotypes.},
  keywords = {nosource}
}

@article{nouryHowDoesCOVID192021,
  title = {How Does {{COVID-19}} Affect Electoral Participation? Evidence from the {{French}} Municipal Elections},
  shorttitle = {How Does {{COVID-19}} Affect Electoral Participation?},
  author = {Noury, Abdul and Fran{\c c}ois, Abel and Gergaud, Olivier and Garel, Alexandre},
  editor = {Ha, Shang E.},
  year = {2021},
  month = feb,
  journal = {PLOS ONE},
  volume = {16},
  number = {2},
  pages = {e0247026},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0247026},
  urldate = {2021-11-17},
  abstract = {This article investigates the effects of the COVID-19 outbreak on electoral participation. We study the French municipal elections that took place at the very beginning of the ongoing pandemic and held in over 9,000 municipalities on March 15, 2020. In addition to the simple note that turnout rates decreased to a historically low level, we establish a robust relationship between the depressed turnout rate and the disease. Using various estimation strategies and employing a large number of potential confounding factors, we find that the participation rate decreases with city proximity to COVID-19 clusters. Furthermore, the proximity has conditioned impacts according to the proportion of elderly --who are the most threatened-- within the city. Cities with higher population density, where the risk of infection is higher, and cities where only one list ran at the election, which dramatically reduces competitiveness, experienced differentiated effects of distance.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/9QP9NMDJ/Noury et al. - 2021 - How does COVID-19 affect electoral participation .pdf}
}

@article{novembrePritchardStephensDonnelly2016,
  title = {Pritchard, {{Stephens}}, and {{Donnelly}} on {{Population Structure}}},
  author = {Novembre, J.},
  year = {2016},
  month = oct,
  journal = {Genetics},
  volume = {204},
  number = {2},
  pages = {391--393},
  issn = {0016-6731},
  doi = {10.1534/genetics.116.195164},
  urldate = {2020-10-12},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/QFMEB8WX/Novembre - 2016 - Pritchard, Stephens, and Donnelly on Population St.pdf}
}

@book{nowak2006evolutionary,
  title = {Evolutionary Dynamics},
  author = {Nowak, Martin A},
  year = {2006},
  publisher = {Harvard University Press},
  keywords = {nosource}
}

@incollection{nugegoda2013water,
  title = {Water Quality Guidelines for the Protection of Aquatic Ecosystems},
  booktitle = {Encyclopedia of Aquatic Ecotoxicology},
  author = {Nugegoda, Dayanthi and Kibria, Golam},
  year = {2013},
  pages = {521--538},
  publisher = {Springer},
  doi = {10.1007/978-94-007-5704-2},
  isbn = {978-94-007-5704-2},
  keywords = {nosource}
}

@article{obrienBayesianApproachInferring2014,
  title = {A {{Bayesian Approach}} to {{Inferring}} the {{Phylogenetic Structure}} of {{Communities}} from {{Metagenomic Data}}},
  author = {O'Brien, John D. and Didelot, Xavier and Iqbal, Zamin and {Amenga-Etego}, Lucas and Ahiska, Bartu and Falush, Daniel},
  year = {2014},
  month = jul,
  journal = {Genetics},
  volume = {197},
  number = {3},
  pages = {925--937},
  issn = {0016-6731, 1943-2631},
  doi = {10.1534/genetics.114.161299},
  urldate = {2020-10-08},
  abstract = {Metagenomics provides a powerful new tool set for investigating evolutionary interactions with the environment. However, an absence of model-based statistical methods means that researchers are often not able to make full use of this complex information. We present a Bayesian method for inferring the phylogenetic relationship among related organisms found within metagenomic samples. Our approach exploits variation in the frequency of taxa among samples to simultaneously infer each lineage haplotype, the phylogenetic tree connecting them, and their frequency within each sample. Applications of the algorithm to simulated data show that our method can recover a substantial fraction of the phylogenetic structure even in the presence of high rates of migration among sample sites. We provide examples of the method applied to data from green sulfur bacteria recovered from an Antarctic lake, plastids from mixed Plasmodium falciparum infections, and virulent Neisseria meningitidis samples.},
  langid = {english},
  keywords = {nosource}
}

@article{odoneDoubledMortalityRate2021,
  title = {Doubled Mortality Rate during the {{COVID-19}} Pandemic in {{Italy}}: Quantifying What Is Not Captured by Surveillance},
  shorttitle = {Doubled Mortality Rate during the {{COVID-19}} Pandemic in {{Italy}}},
  author = {Odone, A. and Delmonte, D. and Gaetti, G. and Signorelli, C.},
  year = {2021},
  month = jan,
  journal = {Public Health},
  volume = {190},
  pages = {108--115},
  issn = {0033-3506},
  doi = {10.1016/j.puhe.2020.11.016},
  urldate = {2021-05-07},
  abstract = {Objectives It is important to quantify the true burden of coronavirus disease 2019 (COVID-19) in different countries, to enable informed decisions about imposing and relaxing control measures. COVID-19 surveillance data fails in this respect, as it is influenced by different definitions, control policies and capacities. This article aims to quantify excess mortality and estimate the distribution between COVID-19 and non-COVID-19 causes of death. Study design Observational study and mathematical modelling. Methods Publicly available data from multiple institutional sources were used and an in-depth analysis was carried out of deaths from all causes between 2015 and 2020 in Italy at the national, regional and local level. Excess mortality over time and space was first explored, followed by an assessment of how this related to COVID-19 surveillance and, ultimately, assuming a fixed male:female ratio, a model was developed and applied to estimate the proportions of COVID-19 and non-COVID-19 excess mortality in 2020. Results In Italy, the mortality rate doubled in March and April 2020 compared with data from 2015 to 2019 (+109\%, when considering municipalites with {$>$}10.000 inhabitants), with excess mortality reaching {$>$}600\% in large municipalities in northern areas. Notified COVID-19 deaths accounted for only 43.5\% (regional range: 43--62\%) of excess mortality. It is estimated that more than two-thirds of excess deaths that were not captured by surveillance are non-COVID-19 deaths, which could be a result of the excess burden on the health systems, in addition to reduced demand and supply of other non-COVID healthcare services. Conclusions The impact of COVID-19 during the early stages of the pandemic is much larger than official figures have reported. Monitoring excess mortality helps to capture the full effect of the COVID-19 pandemic, which differs between regions in Italy and which might have resulted in significant indirect effects on the well-being of the population. In addition, the COVID-19 pandemic has also resulted in significant indirect effects on the well-being of the population.},
  langid = {english},
  keywords = {COVID-19,Excess mortality,Healthcare service response,Mathematical modelling},
  file = {/home/gkonkamking/Zotero/storage/3RBES6ML/Odone et al. - 2021 - Doubled mortality rate during the COVID-19 pandemi.pdf}
}

@article{odriscollAgespecificMortalityImmunity2021,
  title = {Age-Specific Mortality and Immunity Patterns of {{SARS-CoV-2}}},
  author = {O'Driscoll, Megan and Ribeiro Dos Santos, Gabriel and Wang, Lin and Cummings, Derek A. T. and Azman, Andrew S. and Paireau, Juliette and Fontanet, Arnaud and Cauchemez, Simon and Salje, Henrik},
  year = {2021},
  month = feb,
  journal = {Nature},
  volume = {590},
  number = {7844},
  pages = {140--145},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-2918-0},
  urldate = {2021-03-05},
  langid = {english},
  keywords = {nosource}
}

@article{oesperTHetAInferringIntratumor2013,
  title = {{{THetA}}: Inferring Intra-Tumor Heterogeneity from High-Throughput {{DNA}} Sequencing Data},
  shorttitle = {{{THetA}}},
  author = {Oesper, Layla and Mahmoody, Ahmad and Raphael, Benjamin J.},
  year = {2013},
  month = jul,
  journal = {Genome Biology},
  volume = {14},
  number = {7},
  pages = {R80},
  issn = {1474-760X},
  doi = {10.1186/gb-2013-14-7-r80},
  urldate = {2021-05-02},
  abstract = {Tumor samples are typically heterogeneous, containing admixture by normal, non-cancerous cells and one or more subpopulations of cancerous cells. Whole-genome sequencing of a tumor sample yields reads from this mixture, but does not directly reveal the cell of origin for each read. We introduce THetA (Tumor Heterogeneity Analysis), an algorithm that infers the most likely collection of genomes and their proportions in a sample, for the case where copy number aberrations distinguish subpopulations. THetA successfully estimates normal admixture and recovers clonal and subclonal copy number aberrations in real and simulated sequencing data. THetA is available at http://compbio.cs.brown.edu/software/},
  langid = {english}
}

@article{ogundijoCharacterizationTumorHeterogeneity2018,
  title = {Characterization of Tumor Heterogeneity by Latent Haplotypes: A Sequential {{Monte Carlo}} Approach},
  shorttitle = {Characterization of Tumor Heterogeneity by Latent Haplotypes},
  author = {Ogundijo, Oyetunji E. and Wang, Xiaodong},
  year = {2018},
  month = may,
  journal = {PeerJ},
  volume = {6},
  pages = {e4838},
  publisher = {PeerJ Inc.},
  issn = {2167-8359},
  doi = {10.7717/peerj.4838},
  urldate = {2021-05-04},
  abstract = {Tumor samples obtained from a single cancer patient spatially or temporally often consist of varying cell populations, each harboring distinct mutations that uniquely characterize its genome. Thus, in any given samples of a tumor having more than two haplotypes, defined as a scaffold of single nucleotide variants (SNVs) on the same homologous genome, is evidence of heterogeneity because humans are diploid and we would therefore only observe up to two haplotypes if all cells in a tumor sample were genetically homogeneous. We characterize tumor heterogeneity by latent haplotypes and present state-space formulation of the feature allocation model for estimating the haplotypes and their proportions in the tumor samples. We develop an efficient sequential Monte Carlo (SMC) algorithm that estimates the states and the parameters of our proposed state-space model, which are equivalently the haplotypes and their proportions in the tumor samples. The sequential algorithm produces more accurate estimates of the model parameters when compared with existing methods. Also, because our algorithm processes the variant allele frequency (VAF) of a locus as the observation at a single time-step, VAF from newly sequenced candidate SNVs from next-generation sequencing (NGS) can be analyzed to improve existing estimates without re-analyzing the previous datasets, a feature that existing solutions do not possess.},
  langid = {english},
  keywords = {nosource}
}

@article{ohaganEstimatingSpeciesSensitivity2005,
  title = {Estimating Species Sensitivity Distributions with the Aid of Expert Judgements},
  author = {O'Hagan, Anthony and Crane, Mark and Grist, Eric},
  year = {2005},
  pages = {1--12},
  isbn = {Research Report No 556/05},
  keywords = {bayesian inference,censored data,chlorpyrifos,duplicate-citation-key,environ-,hc5,hierarchical model,lc50,mental standards,nosource,ssd,toxicology}
}

@misc{ohaganTreeBanditsGenerative2024,
  title = {Tree {{Bandits}} for {{Generative Bayes}}},
  author = {O'Hagan, Sean and Kim, Jungeum and Rockova, Veronika},
  year = {2024},
  month = apr,
  number = {arXiv:2404.10436},
  eprint = {2404.10436},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.10436},
  urldate = {2024-07-03},
  abstract = {In generative models with obscured likelihood, Approximate Bayesian Computation (ABC) is often the tool of last resort for inference. However, ABC demands many prior parameter trials to keep only a small fraction that passes an acceptance test. To accelerate ABC rejection sampling, this paper develops a self-aware framework that learns from past trials and errors. We apply recursive partitioning classifiers on the ABC lookup table to sequentially refine high-likelihood regions into boxes. Each box is regarded as an arm in a binary bandit problem treating ABC acceptance as a reward. Each arm has a proclivity for being chosen for the next ABC evaluation, depending on the prior distribution and past rejections. The method places more splits in those areas where the likelihood resides, shying away from low-probability regions destined for ABC rejections. We provide two versions: (1) ABC-Tree for posterior sampling, and (2) ABC-MAP for maximum a posteriori estimation. We demonstrate accurate ABC approximability at much lower simulation cost. We justify the use of our tree-based bandit algorithms with nearly optimal regret bounds. Finally, we successfully apply our approach to the problem of masked image classification using deep generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Methodology},
  file = {/home/gkonkamking/pCloudDrive/papers/O'Hagan et al_2024_Tree Bandits for Generative Bayes.pdf;/home/gkonkamking/Zotero/storage/CLMDA5S7/O'Hagan et al. - 2024 - Tree Bandits for Generative Bayes.pdf}
}

@article{oharaReviewBayesianVariable2009,
  title = {A Review of {{Bayesian}} Variable Selection Methods: What, How and Which},
  shorttitle = {A Review of {{Bayesian}} Variable Selection Methods},
  author = {O'Hara, R. B. and Sillanp{\"a}{\"a}, M. J.},
  year = {2009},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {4},
  number = {1},
  pages = {85--117},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/09-BA403},
  urldate = {2020-04-10},
  abstract = {The selection of variables in regression problems has occupied the minds of many statisticians. Several Bayesian variable selection methods have been developed, and we concentrate on the following methods: Kuo \& Mallick, Gibbs Variable Selection (GVS), Stochastic Search Variable Selection (SSVS), adaptive shrinkage with Jeffreys' prior or a Laplacian prior, and reversible jump MCMC. We review these methods, in the context of their different properties. We then implement the methods in BUGS, using both real and simulated data as examples, and investigate how the different methods perform in practice. Our results suggest that SSVS, reversible jump MCMC and adaptive shrinkage methods can all work well, but the choice of which method is better will depend on the priors that are used, and also on how they are implemented.},
  langid = {english},
  mrnumber = {MR2486240},
  zmnumber = {1330.62291},
  keywords = {BUGS,MCMC,Variable Selection},
  file = {/home/gkonkamking/Zotero/storage/9W2955HT/O'Hara and Sillanpää - 2009 - A review of Bayesian variable selection methods w.pdf}
}

@article{ohModelBasedClusteringDissimilarities2007,
  title = {Model-{{Based Clustering With Dissimilarities}}: {{A Bayesian Approach}}},
  shorttitle = {Model-{{Based Clustering With Dissimilarities}}},
  author = {Oh, Man-Suk and Raftery, Adrian E},
  year = {2007},
  month = sep,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {16},
  number = {3},
  pages = {559--585},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/106186007X236127},
  urldate = {2021-02-22},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/KD622EKG/Oh and Raftery - 2007 - Model-Based Clustering With Dissimilarities A Bay.pdf}
}

@misc{ohnOptimalBayesianEstimation2022,
  title = {Optimal {{Bayesian}} Estimation of {{Gaussian}} Mixtures with Growing Number of Components},
  author = {Ohn, Ilsang and Lin, Lizhen},
  year = {2022},
  month = mar,
  number = {arXiv:2007.09284},
  eprint = {2007.09284},
  primaryclass = {math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.09284},
  urldate = {2022-11-07},
  abstract = {We study Bayesian estimation of finite mixture models in a general setup where the number of components is unknown and allowed to grow with the sample size. An assumption on growing number of components is a natural one as the degree of heterogeneity present in the sample can grow and new components can arise as sample size increases, allowing full flexibility in modeling the complexity of data. This however will lead to a high-dimensional model which poses great challenges for estimation. We novelly employ the idea of a sample size dependent prior in a Bayesian model and establish a number of important theoretical results. We first show that under mild conditions on the prior, the posterior distribution concentrates around the true mixing distribution at a near optimal rate with respect to the Wasserstein distance. Under a separation condition on the true mixing distribution, we further show that a better and adaptive convergence rate can be achieved, and the number of components can be consistently estimated. Furthermore, we derive optimal convergence rates for the higher-order mixture models where the number of components diverges arbitrarily fast. In addition, we suggest a simple recipe for using Dirichlet process (DP) mixture prior for estimating the finite mixture models and provide theoretical guarantees. In particular, we provide a novel solution for adopting the number of clusters in a DP mixture model as an estimate of the number of components in a finite mixture model. Simulation study and real data applications are carried out demonstrating the utilities of our method.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/gkonkamking/pCloudDrive/papers/Ohn_Lin_2022_Optimal Bayesian estimation of Gaussian mixtures with growing number of.pdf}
}

@article{Okamura2009,
  title = {Ecotoxicity of the Degradation Products of Triphenylborane Pyridine ({{TPBP}}) Antifouling Agent.},
  author = {Okamura, H and Kitano, S and Toyota, S and Harino, H and Thomas, K V},
  year = {2009},
  month = mar,
  journal = {Chemosphere},
  volume = {74},
  number = {9},
  eprint = {19095285},
  eprinttype = {pubmed},
  pages = {1275--8},
  publisher = {Elsevier Ltd},
  issn = {1879-1298},
  doi = {10.1016/j.chemosphere.2008.11.014},
  abstract = {Triphenylborane pyridine (TPBP) is an alternative to organotin antifouling compounds. This work aimed to identify the unknown Peak \#1, and to evaluate the ecotoxicity of TPBP and its degradation products. Peak \#1 was produced from TPBP dissolved in acetonitrile under UV-A photolysis using a high-pressure mercury lamp. The Peak \#1 fraction was purified using two-step column chromatography from a TPBP-acetonitrile solution. The major compound of the fraction was identified as being biphenyl from the 1H NMR and 13C NMR spectra. The ecotoxicity of four degradation products (diphenylborane hydroxide, phenylborane dihydroxide, phenol, and biphenyl) and TPBP towards two marine planktons were assessed. The 48 h LC(50) values of the crustacean, Artemia salina, were 0.13 mg L(-1) for TPBP, 14 mg L(-1) for biphenyl, 17 mg L(-1) for phenol, and {$>$} 50 mg L(-1) for the other degradation products. The 72 h EC(50) values of the diatom, Skeletonema costatum, were 0.0022 mg L(-1) for TPBP, 1.2 mg L(-1) for biphenyl, and {$>$} 2 mg L(-1) for the other degradation products. Thus, the ecotoxicity of biphenyl and the other degradation products were not high compared to the parent compound, TPBP.},
  isbn = {0045-6535},
  pmid = {19095285},
  keywords = {Animals,Artemia,Artemia: drug effects,Bioassay,Biocide,Biphenyl,Boranes,Boranes: chemistry,Boranes: toxicity,Chemical,Chemical: chemistry,Chemical: toxicity,Chromatography,Diatoms,Diatoms: drug effects,Disinfectants,Disinfectants: chemistry,Disinfectants: toxicity,duplicate-citation-key,Ecotoxicology,High Pressure Liquid,Lethal Dose 50,Magnetic Resonance Spectroscopy,Marina,nosource,Pyridines,Pyridines: chemistry,Pyridines: toxicity,Skeletonema,Water Pollutants}
}

@article{oldenkamp2015hierarchical,
  title = {Hierarchical Bayesian Approach to Reduce Uncertainty in the Aquatic Effect Assessment of Realistic Chemical Mixtures},
  author = {Oldenkamp, Rik and Hendriks, Harrie W M and {van de Meent}, Dik and Ragas, Ad M J},
  year = {2015},
  journal = {Environmental science \& technology},
  volume = {49},
  number = {17},
  pages = {10457--10465},
  publisher = {ACS Publications},
  keywords = {nosource}
}

@article{olmInStrainProfilesPopulation2021,
  title = {{{inStrain}} Profiles Population Microdiversity from Metagenomic Data and Sensitively Detects Shared Microbial Strains},
  author = {Olm, Matthew R. and {Crits-Christoph}, Alexander and {Bouma-Gregson}, Keith and Firek, Brian A. and Morowitz, Michael J. and Banfield, Jillian F.},
  year = {2021},
  month = jan,
  journal = {Nature Biotechnology},
  pages = {1--10},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/s41587-020-00797-0},
  urldate = {2021-05-02},
  abstract = {Coexisting microbial cells of the same species often exhibit genetic variation that can affect phenotypes ranging from nutrient preference to pathogenicity. Here we present inStrain, a program that uses metagenomic paired reads to profile intra-population genetic diversity (microdiversity) across whole genomes and compares microbial populations in a microdiversity-aware manner, greatly increasing the accuracy of genomic comparisons when benchmarked against existing methods. We use inStrain to profile {$>$}1,000 fecal metagenomes from newborn premature infants and find that siblings share significantly more strains than unrelated infants, although identical twins share no more strains than fraternal siblings. Infants born by cesarean section harbor Klebsiella with significantly higher nucleotide diversity than infants delivered vaginally, potentially reflecting acquisition from hospital rather than maternal microbiomes. Genomic loci that show diversity in individual infants include variants found between other infants, possibly reflecting inoculation from diverse hospital-associated sources. inStrain can be applied to any metagenomic dataset for microdiversity analysis and rigorous strain comparison.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {nosource},
  file = {/home/gkonkamking/pCloudDrive/papers/Olm et al_2021_inStrain profiles population microdiversity from metagenomic data and.pdf}
}

@article{olsen2011arctic,
  title = {Arctic versus Temperate Comparison of Risk Assessment Metrics for 2-Methyl-Naphthalene},
  author = {Olsen, Gro Harlaug and Smit, Mathijs G D and Carroll, JoLynn and J{\ae}ger, Iris and Smith, Tim and Camus, Lionel},
  year = {2011},
  journal = {Marine environmental research},
  volume = {72},
  number = {4},
  pages = {179--187},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{omalleyReadingAloudSpellingsound2008,
  title = {Reading Aloud: {{Spelling-sound}} Translation Uses Central Attention},
  shorttitle = {Reading Aloud},
  author = {O'Malley, Shannon and Reynolds, Michael G. and Stolz, Jennifer A. and Besner, Derek},
  year = {2008},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {34},
  number = {2},
  pages = {422--429},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1285(Electronic),0278-7393(Print)},
  doi = {10.1037/0278-7393.34.2.422},
  abstract = {Contrary to the received view that reading aloud reflects processes that are "automatic," recent evidence suggests that some of these processes require a form of attention. This issue was investigated further by examining the effect of a prior presentation of exception words (words whose spelling-sound translation are atypical, such as pint as compared with mint, hint, or lint) and pseudohomophones (nonwords that sound identical to words, such as brane from brain) on reading aloud in the context of the psychological refractory period paradigm. For exception words, the joint effects of repetition and stimulus onset asynchrony (SOA) yielded an underadditive interaction on the time to read aloud, replicating previous work--a short SOA between Task 1 and Task 2 increased reaction time (RT) and reduced the magnitude of the repetition effect relative to the long SOA. For pseudohomophones, in contrast, the joint effects of repetition and SOA were additive on RT. These results provide converging evidence for the conclusion that (a) processing up to and including the orthographic input lexicon does not require central attention when reading aloud, whereas (b) translating lexical and sublexical spelling to sound requires the use of central attention. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Attention,nosource,Oral Reading,Phonology,Spelling,Word Recognition,Words (Phonetic Units)}
}

@article{ONT14,
  title = {Sampling Unitary Invariant Ensembles},
  author = {Olver, Sheehan and Nadakuditi, Raj Rao and Trogdon, Thomas},
  year = {2014},
  abstract = {We develop an algorithm for sampling from the unitary invariant random matrix ensembles. The algorithm is based on the representation of their eigenvalues as a determinantal point process whose kernel is given in terms of orthogonal polynomials. Using this algorithm, statistics beyond those known through analysis are calculable through Monte Carlo simulation. Unexpected phenomena are observed in the simulations.},
  keywords = {nosource}
}

@article{oppenheimer2009retrospective,
  title = {The Retrospective Gambler's Fallacy: {{Unlikely}} Events, Constructing the Past, and Multiple Universes},
  author = {Oppenheimer, Daniel M and Monin, Benot},
  year = {2009},
  journal = {Judgment and Decision Making},
  volume = {4},
  number = {5},
  pages = {326},
  publisher = {Society for Judgment \& Decision Making},
  keywords = {nosource}
}

@article{oppenheimerInstructionalManipulationChecks2009,
  title = {Instructional Manipulation Checks: {{Detecting}} Satisficing to Increase Statistical Power},
  shorttitle = {Instructional Manipulation Checks},
  author = {Oppenheimer, Daniel M. and Meyvis, Tom and Davidenko, Nicolas},
  year = {2009},
  month = jul,
  journal = {Journal of Experimental Social Psychology},
  volume = {45},
  number = {4},
  pages = {867--872},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2009.03.009},
  urldate = {2020-05-15},
  abstract = {Participants are not always as diligent in reading and following instructions as experimenters would like them to be. When participants fail to follow instructions, this increases noise and decreases the validity of their data. This paper presents and validates a new tool for detecting participants who are not following instructions -- the Instructional manipulation check (IMC). We demonstrate how the inclusion of an IMC can increase statistical power and reliability of a dataset.},
  langid = {english},
  keywords = {Instructions,Manipulation checks,nosource,Satisficing,Screening}
}

@article{OPV16,
  title = {Introduction to the Equilibrium {{Green}}'s Functions: Condensed Matter Examples with Numerical Implementations},
  author = {{Mariana M. Odashima Beatriz G. Prado} and Vernek, E},
  year = {2016},
  pages = {1--20},
  abstract = {The Green's function method has applications in several fields in Physics, from classical differential equations to quantum many-body problems. In the quantum context, Green's functions are correlation functions, from which it is possible to extract information from the system under study, such as the density of states, relaxation times and response functions. Despite its power and versatility, it is known as a laborious and sometimes cumbersome method. Here we introduce the equilibrium Green's functions and the equation-of-motion technique, exemplifying the method in discrete lattices of non-interacting electrons. We start with simple models, such as the two-site molecule, the infinite and semi-infinite one-dimensional chains, and the two-dimensional ladder. Numerical implementations are developed via the recursive Green's function, implemented in Julia, an open-source, efficient and easy-to-learn scientific language. We also present a new variation of the surface recursive Green's function method, which can be of interest when simulating simultaneously the properties of surface and bulk.},
  keywords = {nosource}
}

@article{orbanz2011projective,
  title = {Projective Limit Random Probabilities on {{Polish}} Spaces},
  author = {Orbanz, Peter},
  year = {2011},
  journal = {Electronic Journal of Statistics},
  volume = {5},
  eprint = {1101.4657},
  pages = {1354--1373},
  publisher = {Institute of Mathematical Statistics},
  issn = {19357524},
  doi = {10.1214/11-EJS641},
  abstract = {A pivotal problem in Bayesian nonparametrics is the construction of prior distributions on the space M(V) of probability measures on a given domain V. In principle, such distributions on the infinite-dimensional space M(V) can be constructed from their finite-dimensional marginals -- the most prominent example being the construction of the Dirichlet process from finite-dimensional Dirichlet distributions. This approach is both intuitive and applicable to the construction of arbitrary distributions on M(V), but also hamstrung by a number of technical difficulties. We show how these difficulties can be resolved if the domain V is a Polish topological space, and give a representation theorem directly applicable to the construction of any probability distribution on M(V). The proof draws on a projective limit theorem of Bochner, and on an inner regularity criterion to establish countable additivity of the resulting random probabilities.},
  archiveprefix = {arXiv},
  arxivid = {1101.4657},
  keywords = {Bayesian nonparametrics,Dirichlet processes,nosource,Random probability measures}
}

@article{orbanz2011unit,
  title = {Unit--Rate {{Poisson}} Representations of Completely Random Measures},
  author = {Orbanz, Peter and Williamson, Sinead},
  year = {2011},
  journal = {Electronic Journal of Statistics},
  volume = {5},
  pages = {1--12},
  keywords = {nosource}
}

@inproceedings{OT14,
  title = {A Practical Framework for Infinite-Dimensional Linear Algebra},
  booktitle = {{{HPTCDL}}'14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages},
  author = {Olver, Sheehan and Townsend, Alex},
  year = {2014},
  pages = {57--62},
  publisher = {ACM},
  address = {New York},
  doi = {10.1109/HPTCDL.2014.10},
  abstract = {We describe a framework for solving a broad class of infinite-dimensional linear equations, consisting of almost banded operators, which can be used to represent linear ordinary differential equations with general boundary conditions. The framework contains a data structure on which row operations can be performed, allowing for the solution of linear equations by the adaptive QR approach. The algorithm achieves O(nopt) complexity, where nopt is the number of degrees of freedom required to achieve a desired accuracy, which is determined adaptively. In addition, special tensor product equations, such as partial differential equations on rectangles, can be solved by truncating the operator in the y-direction with n\_y degrees of freedom and using a generalized Schur decomposition to upper triangularize, before applying the adaptive QR approach to the x-direction, requiring O(n{\textasciicircum}2\_y n{\textasciicircum}\{opt\}\_x) operations. The framework is implemented in the ApproxFun package written in the Julia programming language, which achieves highly competitive computational costs by exploiting unique features of Julia.},
  keywords = {nosource}
}

@article{Overbeck1997,
  title = {Estimation in the Cox-Ingersoll-Ross Model},
  author = {Overbeck, Ludger and Ryd{\'e}n, Tobias},
  year = {1997},
  journal = {Econometric Theory},
  volume = {13},
  number = {3},
  pages = {430--461},
  issn = {0266-4666},
  doi = {10.1017/s0266466600005880},
  abstract = {The Cox-Ingersoll-Ross model is a diffusion process suitable for modeling the term structure of interest rates. In this paper, we consider estimation of the parameters of this process from observations at equidistant time points. We study two estimators based on conditional least squares as well as a one-step improvement of these, two weighted conditional least-squares estimators, and the maximum likelihood estimator. Asymptotic properties of the various estimators are discussed, and we also compare their performance in a simulation study.},
  keywords = {nosource}
}

@article{ozella2019wearable,
  title = {Wearable Proximity Sensors for Monitoring a Mass Casualty Incident Exercise: {{Feasibility}} Study},
  author = {Ozella, Laura and Gauvin, Laetitia and Carenzo, Luca and Quaggiotto, Marco and Ingrassia, Pier Luigi and Tizzoni, Michele and Panisson, Andr{\'e} and Colombo, Davide and Sapienza, Anna and Kalimeri, Kyriaki and others},
  year = {2019},
  journal = {Journal of Medical Internet Research},
  volume = {21},
  number = {4},
  pages = {e12251},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/12251},
  keywords = {nosource}
}

@article{ozella2020effect,
  title = {The Effect of Age, Environment and Management on Social Contact Patterns in Sheep},
  author = {Ozella, Laura and Langford, Joss and Gauvin, Laetitia and Price, Emily and Cattuto, Ciro and Croft, Darren P},
  year = {2020},
  journal = {Applied Animal Behaviour Science},
  volume = {225},
  pages = {104964},
  publisher = {Elsevier},
  doi = {10.1016/j.applanim.2020.104964},
  keywords = {nosource}
}

@article{Package2012,
  title = {Package `interval'},
  author = {Package, Type and Weighted, Title and Tests, Logrank and Fay, Michael P},
  year = {2012},
  keywords = {duplicate-citation-key,nosource}
}

@article{Package2013,
  title = {Package ` Pesticides '},
  author = {Package, Type and Diez, Author David M},
  year = {2013},
  keywords = {nosource}
}

@article{padgettNonparametricDensityEstimation1984,
  title = {Nonparametric Density Estimation from Censored Data},
  author = {Padgett, W.J. and McNichols, Diane T.},
  year = {1984},
  month = jan,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {13},
  number = {13},
  pages = {1581--1611},
  issn = {0361-0926, 1532-415X},
  doi = {10.1080/03610928408828780},
  urldate = {2020-07-17},
  langid = {english},
  file = {/home/gkonkamking/Downloads/padgett1984.pdf}
}

@article{pagnottoni2018price,
  title = {Price Discovery on Bitcoin Markets},
  author = {Pagnottoni, Paolo and Dimpfl, Thomas},
  year = {2018},
  journal = {Working paper},
  series = {{{SSRN}} 3280261},
  keywords = {nosource}
}

@article{palla2013dependent,
  title = {A Dependent Partition-Valued Process for Multitask Clustering and Time Evolving Network Modelling},
  author = {Palla, Konstantina and Knowles, {\relax DA} and Ghahramani, Zoubin},
  year = {2013},
  journal = {arXiv preprint arXiv:1303.3265},
  eprint = {1303.3265},
  pages = {9},
  abstract = {The fundamental aim of clustering algorithms is to partition data points. We consider tasks where the discovered partition is allowed to vary with some covariate such as space or time. One approach would be to use fragmentation-coagulation processes, but these, being Markov processes, are restricted to linear or tree structured covariate spaces. We define a partition-valued process on an arbitrary covariate space using Gaussian processes. We use the process to construct a multitask clustering model which partitions datapoints in a similar way across multiple data sources, and a time series model of network data which allows cluster assignments to vary over time. We describe sampling algorithms for inference and apply our method to defining cancer subtypes based on different types of cellular characteristics, finding regulatory modules from gene expression data from multiple human populations, and discovering time varying community structure in a social network.},
  archiveprefix = {arXiv},
  arxivid = {1303.3265},
  keywords = {nosource}
}

@article{pangSpikeSlabPrior,
  title = {Spike and {{Slab Prior Distributions}} for {{Simultaneous Bayesian Hypothesis Testing}}, {{Model Selection}}, and {{Prediction}}, of {{Nonlinear Outcomes}}},
  author = {Pang, Xun and Gill, Jeff},
  pages = {52},
  abstract = {A small body of literature has used the spike and slab prior specification for model selection with strictly linear outcomes. In this setup a two-component mixture distribution is stipulated for coefficients of interest with one part centered at zero with very high precision (the spike) and the other as a distribution diffusely centered at the research hypothesis (the slab). With the selective shrinkage, this setup incorporates the zero coefficient contingency directly into the modeling process to produce posterior probabilities for hypothesized outcomes. We extend the model to qualitative responses by designing a hierarchy of forms over both the parameter and model spaces to achieve variable selection, model averaging, and individual coefficient hypothesis testing. To overcome the technical challenges in estimating the marginal posterior distributions possibly with a dramatic ratio of density heights of the spike to the slab, we develop a hybrid Gibbs sampling algorithm using an adaptive rejection approach for various discrete outcome models, including dichotomous, polychotomous, and count responses. The performance of the models and methods are assessed with both Monte Carlo experiments and empirical applications in political science.},
  langid = {english}
}

@inproceedings{panisson2013fingerprinting,
  title = {Fingerprinting Temporal Networks of Close-Range Human Proximity},
  booktitle = {2013 {{IEEE}} International Conference on Pervasive Computing and Communications Workshops ({{PERCOM}} Workshops)},
  author = {Panisson, Andr{\'e} and Gauvin, Laetitia and Barrat, Alain and Cattuto, Ciro},
  year = {2013},
  pages = {261--266},
  doi = {10.1109/PerComW.2013.6529492},
  organization = {IEEE},
  keywords = {nosource}
}

@inproceedings{panisson2014mining,
  title = {Mining Concurrent Topical Activity in Microblog Streams},
  booktitle = {Proceedings of the the 4th {{Workshop}} on {{Making Sense}} of {{Microposts}} Co-Located with the 23rd {{International World Wide Web Conference}} ({{WWW}} 2014)},
  author = {Panisson, A and Gauvin, L and Quaggiotto, M and Cattuto, C},
  year = {2014},
  keywords = {⛔ No DOI found,nosource}
}

@misc{papamarkouPositionPaperBayesian2024,
  title = {Position {{Paper}}: {{Bayesian Deep Learning}} in the {{Age}} of {{Large-Scale AI}}},
  shorttitle = {Position {{Paper}}},
  author = {Papamarkou, Theodore and Skoularidou, Maria and Palla, Konstantina and Aitchison, Laurence and Arbel, Julyan and Dunson, David and Filippone, Maurizio and Fortuin, Vincent and Hennig, Philipp and Lobato, Jose Miguel Hernandez and Hubin, Aliaksandr and Immer, Alexander and Karaletsos, Theofanis and Khan, Mohammad Emtiyaz and Kristiadi, Agustinus and Li, Yingzhen and Mandt, Stephan and Nemeth, Christopher and Osborne, Michael A. and Rudner, Tim G. J. and R{\"u}gamer, David and Teh, Yee Whye and Welling, Max and Wilson, Andrew Gordon and Zhang, Ruqi},
  year = {2024},
  month = feb,
  number = {arXiv:2402.00809},
  eprint = {2402.00809},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/gkonkamking/Zotero/storage/PJXIY5FG/Papamarkou et al. - 2024 - Position Paper Bayesian Deep Learning in the Age .pdf}
}

@article{Papaspiliopoulos2007,
  title = {A General Framework for the Parametrization of Hierarchical Models},
  author = {Papaspiliopoulos, Omiros and Roberts, Gareth O. and Sk{\"o}ld, Martin},
  year = {2007},
  journal = {Statistical Science},
  volume = {22},
  number = {1},
  eprint = {0708.3797},
  pages = {59--73},
  issn = {0883-4237},
  doi = {10.1214/088342307000000014},
  abstract = {In this paper, we describe centering and noncentering methodology as complementary techniques for use in parametrization of broad classes of hierarchical models, with a view to the construction of effective MCMC algorithms for exploring posterior distributions from these models. We give a clear qualitative understanding as to when centering and noncentering work well, and introduce theory concerning the convergence time complexity of Gibbs samplers using centered and noncentered parametrizations. We give general recipes for the construction of noncentered parametrizations, including an auxiliary variable technique called the state-space expansion technique. We also describe partially noncentered methods, and demonstrate their use in constructing robust Gibbs sampler algorithms whose convergence properties are not overly sensitive to the data.},
  archiveprefix = {arXiv},
  arxivid = {0708.3797},
  keywords = {QA Mathematics},
  file = {/home/gkonkamking/Zotero/storage/M573LT9P/Papaspiliopoulos et al. - 2007 - A general framework for the parametrization of hie.pdf}
}

@article{papaspiliopoulos2008retrospective,
  title = {Retrospective {{Markov}} Chain {{Monte Carlo}} Methods for {{Dirichlet}} Process Hierarchical Models},
  author = {Papaspiliopoulos, Omiros and Roberts, Gareth O.},
  year = {2008},
  journal = {Biometrika},
  volume = {95},
  number = {1},
  eprint = {0710.4228},
  pages = {169--186},
  publisher = {Biometrika Trust},
  issn = {00063444},
  doi = {10.1093/biomet/asm086},
  abstract = {Inference for Dirichlet process hierarchical models is typically performed using Markov chain Monte Carlo methods, which can be roughly categorized into marginal and conditional methods. The former integrate out analytically the infinite-dimensional component of the hierarchical model and sample from the marginal distribution of the remaining variables using the Gibbs sampler. Conditional methods impute the Dirichlet process and update it as a component of the Gibbs sampler. Since this requires imputation of an infinite-dimensional process, implementation of the conditional method has relied on finite approximations. In this paper, we show how to avoid such approximations by designing two novel Markov chain Monte Carlo algorithms which sample from the exact posterior distribution of quantities of interest. The approximations are avoided by the new technique of retrospective sampling. We also show how the algorithms can obtain samples from functionals of the Dirichlet process. The marginal and the conditional methods are compared and a careful simulation study is included, which involves a non-conjugate model, different datasets and prior specifications.},
  archiveprefix = {arXiv},
  arxivid = {0710.4228},
  keywords = {Exact simulation,Label switching,Mixture model,Retrospective sampling,Stick-breaking prior},
  file = {/home/gkonkamking/pCloudDrive/papers/Papaspiliopoulos_Roberts_2008_Retrospective Markov chain Monte Carlo methods for Dirichlet process.pdf}
}

@article{Papaspiliopoulos2012,
  title = {Nonparametric Estimation of Diffusions: {{A}} Differential Equations Approach},
  author = {Papaspiliopoulos, Omiros and Pokern, Yvo and Roberts, Gareth O. and Stuart, Andrew M.},
  year = {2012},
  journal = {Biometrika},
  volume = {99},
  number = {3},
  pages = {511--531},
  issn = {00063444},
  doi = {10.1093/biomet/ass034},
  abstract = {We consider estimation of scalar functions that determine the dynamics of diffusion processes. It has been recently shown that nonparametric maximum likelihood estimation is ill-posed in this context. We adopt a probabilistic approach to regularize the problem by the adoption of a prior distribution for the unknown functional. A Gaussian prior measure is chosen in the function space by specifying its precision operator as an appropriate differential operator. We establish that a Bayesian--Gaussian conjugate analysis for the drift of one-dimensional nonlinear diffusions is feasible using high-frequency data, by expressing the loglikelihood as a quadratic function of the drift, with sufficient statistics given by the local time process and the end points of the observed path. Computationally efficient posterior inference is carried out using a finite element method. We embed this technology in partially observed situations and adopt a data augmentation approach whereby we iteratively generate missing data paths and draws from the unknown functional. Our methodology is applied to estimate the drift of models used in molecular dynamics and financial econometrics using high- and low-frequency observations. We discuss extensions to other partially observed schemes and connections to other types of nonparametric inference.},
  isbn = {0006-3444},
  keywords = {Finite element method,Gaussian measure,Inverse problem,Local time,Markov chain Monte Carlo,Markov process},
  file = {/home/gkonkamking/Zotero/storage/U87B473T/Papaspiliopoulos et al. - 2012 - Nonparametric estimation of diffusions A differen.pdf}
}

@article{Papaspiliopoulos2014a,
  title = {Optimal Filtering and the Dual Process},
  author = {Papaspiliopoulos, Omiros and Ruggiero, Matteo},
  year = {2014},
  journal = {Bernoulli},
  volume = {20},
  number = {4},
  eprint = {1305.4571v4},
  pages = {1999--2019},
  issn = {1350-7265},
  doi = {10.3150/13-BEJ548},
  abstract = {We link optimal filtering for hidden Markov models to the notion of duality for Markov processes. We show that when the signal is dual to a process that has two components, one deterministic and one a pure death process, and with respect to functions that define changes of measure conjugate to the emission density, the filtering distributions evolve in the family of finite mixtures of such measures and the filter can be computed at a cost that is polynomial in the number of observations. Special cases of our framework include the Kalman filter, and computable filters for the Cox--Ingersoll--Ross process and the one-dimensional Wright--Fisher process, which have been investigated before. The dual we obtain for the Cox--Ingersoll--Ross process appears to be new in the literature.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1305.4571v4},
  keywords = {bayesian conjugacy,Bayesian conjugacy,cox,Cox--Ingersoll--Ross process,Cox-Ingersoll-Ross process,finite mixture models,Finite mixture models,hidden,hidden Markov model,Hidden Markov model,ingersoll,kalman filter,Kalman filter,markov model,ross process},
  file = {/home/gkonkamking/pCloudDrive/papers/Papaspiliopoulos_Ruggiero_2014_Optimal filtering and the dual process.pdf}
}

@article{Papaspiliopoulos2016,
  title = {Conjugacy Properties of Time-Evolving {{Dirichlet}} and Gamma Random Measures},
  author = {Papaspiliopoulos, Omiros and Ruggiero, Matteo and Spano, Dario},
  year = {2016},
  journal = {Electronic Journal of Statistics},
  volume = {10},
  number = {2},
  eprint = {1607.02896},
  pages = {3452--3489},
  issn = {19357524},
  doi = {10.1214/16-EJS1194},
  abstract = {We extend classic characterisations of posterior distributions under Dirichlet process and gamma random measures priors to a dynamic framework. We consider the problem of learning, from indirect observations, two families of time-dependent processes of interest in Bayesian nonparametrics: the first is a dependent Dirichlet process driven by a Fleming-Viot model, and the data are random samples from the process state at discrete times; the second is a collection of dependent gamma random measures driven by a Dawson-Watanabe model, and the data are collected according to a Poisson point process with intensity given by the process state at discrete times. Both driving processes are diffusions taking values in the space of discrete measures whose support varies with time, and are stationary and reversible with respect to Dirichlet and gamma priors respectively. A common methodology is developed to obtain in closed form the time-marginal posteriors given past and present data. These are shown to belong to classes of finite mixtures of Dirichlet processes and gamma random measures for the two models respectively, yielding conjugacy of these classes to the type of data we consider. We provide explicit results on the parameters of the mixture components and on the mixing weights, which are time-varying and drive the mixtures towards the respective priors in absence of further data. Explicit algorithms are provided to recursively compute the parameters of the mixtures. Our results are based on the projective properties of the signals and on certain duality properties of their projections.},
  archiveprefix = {arXiv},
  arxivid = {1607.02896},
  keywords = {Bayesian nonparametrics,Dawson-Watanabe process,Dirichlet process,Duality,Fleming-Viot process,Gamma random measure},
  file = {/home/gkonkamking/Zotero/storage/U6RTRPP9/Papaspiliopoulos et al. - 2016 - Conjugacy properties of time-evolving Dirichlet an.pdf}
}

@article{Papaspiliopoulos2016a,
  title = {Scalable {{Bayesian}} Variable Selection and Model Averaging under Block Orthogonal Design},
  author = {Papaspiliopoulos, Omiros and Rossell, David},
  year = {2016},
  eprint = {1606.03749},
  abstract = {We show how to carry out fully Bayesian variable selection and model averaging in linear models when both the number of observations and covariates are large. We work under the assumption that the Gram matrix is block-diagonal. Apart from orthogonal regression and various contexts where this is satisfied by design, this framework may serve in future work as a basis for computational approximations to more general design matrices with clusters of correlated predictors. Our approach returns the most probable model of any given size without resorting to numerical integration, posterior probabilities for any number of models by evaluating a single one-dimensional integral that can be computed upfront, and other quantities of interest such as variable inclusion probabilities and model averaged regression estimates by carrying out an adaptive, deterministic one-dimensional numerical integration. This integration and model search are done using novel schemes we introduce in this article. We do not require Markov Chain Monte Carlo. The overall computational cost scales linearly with the number of blocks, which can be processed in parallel, and exponentially with the block size, rendering it most adequate in situations where predictors are organized in many moderately-sized blocks.},
  archiveprefix = {arXiv},
  arxivid = {1606.03749},
  keywords = {nosource}
}

@article{pardiDistancebasedMethodsPhylogenetics,
  title = {Distance-\-based Methods in Phylogenetics},
  author = {Pardi, Fabio and Gascuel, Olivier},
  pages = {17},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/ILN895K6/Pardi and Gascuel - Distance-­based methods in phylogenetics.pdf}
}

@article{parino2018analysis,
  title = {Analysis of the Bitcoin Blockchain: {{Socio-economic}} Factors behind the Adoption},
  author = {Parino, Francesco and Beir{\'o}, Mariano G and Gauvin, Laetitia},
  year = {2018},
  journal = {EPJ Data Science},
  volume = {7},
  number = {1},
  pages = {38},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1140/epjds/s13688-018-0170-8},
  keywords = {nosource}
}

@article{Park2010,
  title = {Bayesian Generalize Product Partition Models},
  author = {Park, Ju-hyun and Dunson, David B},
  year = {2010},
  journal = {Statistica Sinica},
  volume = {20},
  pages = {1203--1226},
  keywords = {and phrases,clustering,conditional distribution estimation,dirichlet,generalized p,latent class model,mixture of experts,nonparamet-,nosource,olya urn,process,product partition,ric bayes}
}

@article{parkBayesianLasso2008,
  title = {The {{Bayesian Lasso}}},
  author = {Park, Trevor and Casella, George},
  year = {2008},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {482},
  pages = {681--686},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1198/016214508000000337},
  urldate = {2021-04-01},
  abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
  file = {/home/gkonkamking/Zotero/storage/BNLRQ2IX/Park and Casella - 2008 - The Bayesian Lasso.pdf}
}

@article{parmigiani2003analysis,
  title = {The Analysis of Gene Expression Data: An Overview of Methods and Software},
  author = {Parmigiani, Giovanni and Garrett, Elizabeth S and Irizarry, Rafael A and Zeger, Scott L},
  year = {2003},
  journal = {The analysis of gene expression data},
  pages = {1--45},
  publisher = {Springer},
  doi = {10.1007/0-387-21679-0_1}
}

@article{pati2013posterior,
  title = {Posterior Consistency in Conditional Distribution Estimation},
  author = {Pati, Debdeep and Dunson, David B. and Tokdar, Surya T.},
  year = {2013},
  journal = {Journal of Multivariate Analysis},
  volume = {116},
  pages = {456--472},
  publisher = {Elsevier},
  issn = {0047259X},
  doi = {10.1016/j.jmva.2013.01.011},
  abstract = {A wide variety of priors have been proposed for nonparametric Bayesian estimation of conditional distributions, and there is a clear need for theorems providing conditions on the prior for large support, as well as posterior consistency. Estimation of an uncountable collection of conditional distributions across different regions of the predictor space is a challenging problem, which differs in some important ways from density and mean regression estimation problems. Defining various topologies on the space of conditional distributions, we provide sufficient conditions for posterior consistency focusing on a broad class of priors formulated as predictor-dependent mixtures of Gaussian kernels. This theory is illustrated by showing that the conditions are satisfied for a class of generalized stick-breaking process mixtures in which the stick-breaking lengths are monotone, differentiable functions of a continuous stochastic process. We also provide a set of sufficient conditions for the case where stick-breaking lengths are predictor independent, such as those arising from a fixed Dirichlet process prior. ?? 2013 Elsevier Inc.},
  keywords = {Asymptotics,Bayesian nonparametrics,Density regression,Dependent Dirichlet process,Large support,nosource,Probit stick-breaking process}
}

@article{patil1982diversity,
  title = {Diversity as a Concept and Its Measurement},
  author = {Patil, G P and Taillie, C},
  year = {2012},
  journal = {Journal of the American statistical Association},
  volume = {77379},
  number = {379},
  pages = {548--561},
  publisher = {Taylor \& Francis Group},
  issn = {01621459},
  doi = {10.1080/01621459.1982.10477845},
  abstract = {This paper puts forth the view that diversity is an average property of a community and identifies that property as species rarity. An intrinsic diversity ordering of com-munities is defined and is shown to be equivalent to sto-chastic ordering. Also. the sensitivity of an index to rare species is developed, culminating in a crossing-point theorem and a response theory to perturbations. Diver-sity decompositions, analogous to the analysis of vari-ance, are discussed for two-way classifications and mix-tures. The paper concludes with a brief survey of genetic diversity, linguistic diversity, industrial concentration, and income inequality.},
  isbn = {0162-1459},
  pmid = {2638},
  keywords = {Diversity decornpo-sition,Intrinsic diversity order-ing,Lorenz curves,nosource,Sensitivity of diversity indices,Species rarity}
}

@article{paxtonReflectionReasoningMoral2012,
  title = {Reflection and {{Reasoning}} in {{Moral Judgment}}},
  author = {Paxton, Joseph M. and Ungar, Leo and Greene, Joshua D.},
  year = {2012},
  journal = {Cognitive Science},
  volume = {36},
  number = {1},
  pages = {163--177},
  issn = {1551-6709},
  doi = {10.1111/j.1551-6709.2011.01210.x},
  urldate = {2020-05-04},
  abstract = {While there is much evidence for the influence of automatic emotional responses on moral judgment, the roles of reflection and reasoning remain uncertain. In Experiment 1, we induced subjects to be more reflective by completing the Cognitive Reflection Test (CRT) prior to responding to moral dilemmas. This manipulation increased utilitarian responding, as individuals who reflected more on the CRT made more utilitarian judgments. A follow-up study suggested that trait reflectiveness is also associated with increased utilitarian judgment. In Experiment 2, subjects considered a scenario involving incest between consenting adult siblings, a scenario known for eliciting emotionally driven condemnation that resists reasoned persuasion. Here, we manipulated two factors related to moral reasoning: argument strength and deliberation time. These factors interacted in a manner consistent with moral reasoning: A strong argument defending the incestuous behavior was more persuasive than a weak argument, but only when increased deliberation time encouraged subjects to reflect.},
  copyright = {Copyright {\copyright} 2011 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Dual-process model,Moral decision making,Moral judgment,Moral psychology,Moral reasoning,Morality,Reflection,Social intuitionist model}
}

@article{payne2008implicit,
  title = {Why Do Implicit and Explicit Attitude Tests Diverge? {{The}} Role of Structural Fit.},
  author = {Payne, B Keith and Burkley, Melissa A and Stokes, Mark B},
  year = {2008},
  journal = {Journal of personality and social psychology},
  volume = {94},
  number = {1},
  pages = {16},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{peixoto2018change,
  title = {Change Points, Memory and Epidemic Spreading in Temporal Networks},
  author = {Peixoto, Tiago P and Gauvin, Laetitia},
  year = {2018},
  journal = {Scientific reports},
  volume = {8},
  number = {1},
  pages = {1--10},
  publisher = {Nature Publishing Group},
  doi = {10.1038/s41598-018-33313-1},
  keywords = {nosource}
}

@article{Pennington2003,
  title = {Extrapolating Ecotoxicological Measures from Small Data Sets},
  author = {Pennington, David W.},
  year = {2003},
  month = oct,
  journal = {Ecotoxicology and Environmental Safety},
  volume = {56},
  number = {2},
  pages = {238--250},
  issn = {01476513},
  doi = {10.1016/S0147-6513(02)00089-1},
  isbn = {0147-6513},
  keywords = {assessment factors,Assessment factors,duplicate-citation-key,extrapolation factors,Extrapolation factors,hazardous concentration,Hazardous concentration,nosource,pnec,PNEC,species sensitivity distributions,Species sensitivity distributions (SSD),ssd,uncertainty,Uncertainty}
}

@article{pepe2020covid,
  title = {{{COVID-19}} Outbreak Response, a Dataset to Assess Mobility Changes in {{Italy}} Following National Lockdown},
  author = {Pepe, Emanuele and Bajardi, Paolo and Gauvin, Laetitia and Privitera, Filippo and Lake, Brennan and Cattuto, Ciro and Tizzoni, Michele},
  year = {2020},
  journal = {Scientific data},
  volume = {7},
  number = {1},
  pages = {1--7},
  publisher = {Nature Publishing Group},
  doi = {10.1038/s41597-020-00575-2},
  keywords = {nosource}
}

@article{pepe2020covid,
  title = {{{COVID-19}} Outbreak Response: A First Assessment of Mobility Changes in {{Italy}} Following National Lockdown},
  author = {Pepe, Emanuele and Bajardi, Paolo and Gauvin, Laetitia and Privitera, Filippo and Lake, Brennan and Cattuto, Ciro and Tizzoni, Michele},
  year = {2020},
  journal = {medRxiv : the preprint server for health sciences},
  publisher = {Cold Spring Harbor Laboratory Press},
  keywords = {⛔ No DOI found,nosource}
}

@article{pepe2020covid,
  title = {{{COVID-19}} Outbreak Response, a Dataset to Assess Mobility Changes in {{Italy}} Following National Lockdown},
  author = {Pepe, Emanuele and Bajardi, Paolo and Gauvin, Laetitia and Privitera, Filippo and Lake, Brennan and Cattuto, Ciro and Tizzoni, Michele},
  year = {2020},
  journal = {Scientific data},
  volume = {7},
  number = {1},
  pages = {230},
  publisher = {Nature Publishing Group UK London},
  doi = {10.1038/s41597-020-00575-2}
}

@article{pepeWeightedKaplanMeierStatistics1989,
  title = {Weighted {{Kaplan-Meier Statistics}}: {{A Class}} of {{Distance Tests}} for {{Censored Survival Data}}},
  shorttitle = {Weighted {{Kaplan-Meier Statistics}}},
  author = {Pepe, Margaret Sullivan and Fleming, Thomas R.},
  year = {1989},
  month = jun,
  journal = {Biometrics},
  volume = {45},
  number = {2},
  eprint = {2531492},
  eprinttype = {jstor},
  pages = {497},
  issn = {0006341X},
  doi = {10.2307/2531492},
  urldate = {2020-06-17},
  abstract = {A class of statisticsbased on the integratedweighteddifferencein Kaplan-Meierestimatorsis introducedfor the two-samplecensoreddataproblem.Withpositiveweightfunctionsthesestatistics are intuitivefor and sensitiveagainstthe alternativeof stochasticordering.The standardweighted log-rankstatisticsarenot alwayssensitiveagainstthis alternative,particularlyif the hazardfunctions cross.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/6GHIBASR/Pepe and Fleming - 1989 - Weighted Kaplan-Meier Statistics A Class of Dista.pdf}
}

@article{perman1992size,
  title = {Size-Biased Sampling of {{Poisson}} Point Processes and Excursions},
  author = {Perman, Mihael and Pitman, Jim and Yor, Marc},
  year = {1992},
  journal = {Probability Theory and Related Fields},
  volume = {92},
  number = {1},
  pages = {21--39},
  publisher = {Springer},
  issn = {01788051},
  doi = {10.1007/BF01205234},
  abstract = {Some general formulae are obtained for size-biased sampling from a Poisson point process in an abstract space where the size of a point is defined by an arbitrary strictly positive function. These formulae explain why in certain cases (gamma and stable) the size-biased permutation of the normalized jumps of a subordinator can be represented by a stickbreaking (residual allocation) scheme defined by independent beta random variables. An application is made to length biased sampling of excursions of a Markov process away from a recurrent point of its statespace, with emphasis on the Brownian and Bessel cases when the associated inverse local time is a stable subordinator. Results in this case are linked to generalizations of the arcsine law for the fraction of time spent positive by Brownian motion.},
  keywords = {nosource}
}

@article{Pesaran1998,
  title = {Generalized Impulse Response Analysis in Linear Multivariate Models},
  author = {Pesaran, H Hashem and Shin, Yongcheol},
  year = {1998},
  volume = {58},
  pages = {17--29},
  keywords = {c13,c32,c51,cointegration,forecast error variance decompositions,generalized impulse responses,jel classification,nosource,var}
}

@article{pesce2009response,
  title = {Response of Spring and Summer Riverine Microbial Communities Following Glyphosate Exposure},
  author = {Pesce, St{\'e}phane and Batisson, Isabelle and Bardot, Corinne and Fajon, C{\'e}line and Portelli, Christophe and Montuelle, Bernard and Bohatier, Jacques},
  year = {2009},
  journal = {Ecotoxicology and Environmental Safety},
  volume = {72},
  number = {7},
  pages = {1905--1912},
  publisher = {Elsevier},
  issn = {01476513},
  doi = {10.1016/j.ecoenv.2009.07.004},
  abstract = {Seasonal variation in the response of riverine microbial communities to an environmentally relevant exposure to glyphosate (about 10 {$\mu$}g l-1) was assessed on natural communities collected in spring and summer, using two 14-day microcosm studies. The two experiments showed no major effect of glyphosate on algal biomass (chlorophyll a concentrations), bacterial activity ([3H]thymidine incorporation), or bacterial community diversity (16S PCR-TTGE detection). Effects on algal community composition (genus-level taxonomic identification) and eukaryotic community diversity (18S PCR-DGGE on {$<$}100 {$\mu$}m organisms) were only detected on the samples collected in summer. This work demonstrates that even if the effects of a short pulse of glyphosate (10 {$\mu$}g l-1) on riverine microorganisms seem to be limited, the responses of natural microbial communities to glyphosate exposure (and probably to other pesticide exposures) can clearly vary between the experiments, and can be seasonally dependent. {\copyright} 2009 Elsevier Inc. All rights reserved.},
  isbn = {01476513},
  pmid = {19646758},
  keywords = {Bacteria,Genetic fingerprinting,Glyphosate,Microbial communities,Microcosms,nosource,Periphyton,Phytoplankton,River,Seasonal response}
}

@techreport{PeterCraig2013,
  title = {Exploring Novel Ways of Using Species Sensitivity Distributions to Establish {{PNECs}} for Industrial Chemicals: {{Final}} Report to {{Project Steering Group}}},
  author = {Craig, Peter S.},
  year = {2013},
  keywords = {nosource}
}

@article{petrone1,
  title = {Bayesian Nonparametric Modelling for Spatial Data Using Dirichlet Processes},
  author = {Gelfand, A E and Guindani, M and Petrone, S},
  year = {2007},
  journal = {Bayesian Statistics 8},
  pages = {175--200},
  publisher = {Oxford University Press},
  keywords = {and phrases,gaussian process,generalized stick-breaking,local surface selection,nonstationary process,nosource,process,spatial}
}

@article{PETRONE199769,
  title = {A Note on the {{Dirichlet}} Process Prior in {{Bayesian}} Nonparametric Inference with Partial Exchangeability},
  author = {Petrone, Sonia and Raftery, Adrian E},
  year = {1997},
  journal = {Statistics \& Probability Letters},
  volume = {36},
  number = {1},
  pages = {69--83},
  issn = {0167-7152},
  doi = {10.1016/S0167-7152(97)00050-3},
  abstract = {We consider Bayesian nonparametric inference for continuous-valued partially exchangeable data, when the partition of the observations into groups is unknown. This includes change-point problems and mixture models. As the prior, we consider a mixture of products of Dirichlet processes. We show that the discreteness of the Dirichlet process can have a large effect on inference (posterior distributions and Bayes factors), leading to conclusions that can be different from those that result from a reasonable parametric model. When the observed data are all distinct, the effect of the prior on the posterior is to favor more evenly balanced partitions, and its effect on Bayes factors is to favor more groups. In a hierarchical model with a Dirichlet process as the second-stage prior, the prior can also have a large effect on inference, but in the opposite direction, towards more unbalanced partitions.},
  keywords = {Bayesian nonparametric inference,Dirichlet process,Hierarchical model,nosource,Partial exchangeability,Partition models}
}

@article{petrone2009hybrid,
  title = {Hybrid Dirichlet Mixture Models for Functional Data},
  author = {Petrone, Sonia and Guindani, Michele and Gelfand, Alan E.},
  year = {2009},
  journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
  volume = {71},
  number = {4},
  pages = {755--782},
  publisher = {Wiley Online Library},
  issn = {13697412},
  doi = {10.1111/j.1467-9868.2009.00708.x},
  abstract = {In functional data analysis, curves or surfaces are observed, up to measurement error, at a finite set of locations, for, say, a sample of n individuals. Often, the curves are homogeneous, except perhaps for individual-specific regions that provide heterogeneous behaviour (e.g.`damaged' areas of irregular shape on an otherwise smooth surface). Motivated by applications with functional data of this nature, we propose a Bayesian mixture model, with the aim of dimension reduction, by representing the sample of n curves through a smaller set of canonical curves.We propose a novel prior on the space of probability measures for a random curve which extends the popular Dirichlet priors by allowing local clustering: non-homogeneous portions of a curve can be allocated to different clusters and the n individual curves can be represented as recombinations (hybrids) of a few canonical curves. More precisely, the prior proposed envisions a conceptual hidden factor with k-levels that acts locally on each curve. We discuss several models incorporating this prior and illustrate its performance with simulated and real data sets. We examine theoretical properties of the proposed finite hybrid Dirichlet mixtures, specifically, their behaviour as the number of the mixture components goes to1and their connection with Dirichlet process mixtures.},
  keywords = {Bayesian non-parametrics,Dependent random partitions,Dirichlet process,Finite mixture models,Gaussian process,Labelling measures,nosource,Species sampling priors}
}

@article{pettit1990measuring,
  title = {Measuring the Effect of Observations on {{Bayes}} Factors},
  author = {Pettit, L I and Young, K D S},
  year = {1990},
  journal = {Biometrika},
  volume = {77},
  number = {3},
  pages = {455},
  publisher = {Biometrika Trust},
  issn = {0006-3444},
  keywords = {duplicate-citation-key,nosource}
}

@article{pettit1990measuring,
  title = {{{PETTIT}}\_{{YOUNG}}\_{{Measuring}} the Effect of Observations on {{Bayes}} Factors\_1990.Pdf},
  author = {Pettit, L I and Young, K D S},
  year = {1990},
  journal = {Biometrika},
  volume = {77},
  number = {3},
  pages = {455},
  publisher = {Biometrika Trust},
  issn = {0006-3444},
  keywords = {duplicate-citation-key,nosource}
}

@article{PhysRevLett.112.118701,
  title = {Bayesian Inference of Epidemics on Networks via Belief Propagation},
  author = {Altarelli, Fabrizio and Braunstein, Alfredo and Dall'Asta, Luca and {Lage-Castellanos}, Alejandro and Zecchina, Riccardo},
  year = {2014},
  month = mar,
  journal = {Phys. Rev. Lett.},
  volume = {112},
  number = {11},
  pages = {118701},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.112.118701},
  keywords = {nosource}
}

@article{Phyu2013,
  title = {Assessing the Chronic Toxicity of Atrazine, Permethrin, and Chlorothalonil to the Cladoceran {{Ceriodaphnia}} Cf. Dubia in Laboratory and Natural River Water},
  author = {Phyu, Yin Latt and Palmer, C. G. and Warne, M. St J and Dowse, Renee and Mueller, S. and Chapman, J. C. and Hose, G. C. and Lim, R. P.},
  year = {2013},
  journal = {Archives of Environmental Contamination and Toxicology},
  volume = {64},
  number = {3},
  pages = {419--426},
  issn = {00904341},
  doi = {10.1007/s00244-012-9837-5},
  abstract = {The majority of ecotoxicological data are generated from standard laboratory-based experiments with organisms exposed in nonflowing systems using highly purified water, which contains very low amounts of dissolved organic matter and suspended particulates. However, such experimental conditions are not ecologically relevant. Thus, there is a need to develop more realistic approaches to determining toxicity, including both lethal and sublethal effects. This research provides information on the effect of natural water constituents, such as suspended particulates and dissolved organic matter, in river water (RW) on the chronic toxicity (7-day reproductive impairment) of the pesticides atrazine, chlorothalonil, and permethrin to the freshwater cladoceran Ceriodaphnia cf. dubia. Standard bioassays were conducted under standard laboratory and more environmentally realistic conditions (using RW). The 7-day IC25 (reproduction impairment) values of atrazine, chlorothalonil, and permethrin to C. cf. dubia ranged from 862.4 to {$>$}1000, 51.3 to 66.4, and 0.19 to 0.23 {$\mu$}g/L, respectively. Using the Globally Harmonized System of Classification and Labelling of Chemicals, atrazine is classified as moderately to highly toxic, whereas permethrin and chlorothalonil were both highly toxic. The presence of dissolved organic matter and suspended particles in natural RW did not significantly (p {$>$} 0.05) change the toxicity of any of the pesticides to C. cf. dubia compared with that tested in laboratory water (LW). For the tested pesticides, toxicity testing in LW provided an adequate estimate of the hazard posed.},
  isbn = {0090-4341},
  pmid = {23192589},
  keywords = {nosource}
}

@inproceedings{piaggesi2019predicting,
  title = {Predicting City Poverty Using Satellite Imagery},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition Workshops},
  author = {Piaggesi, Simone and Gauvin, Laetitia and Tizzoni, Michele and Cattuto, Ciro and Adler, Natalia and Verhulst, Stefaan and Young, Andrew and Price, Rhiannan and Ferres, Leo and Panisson, Andr{\'e}},
  year = {2019},
  pages = {90--96},
  keywords = {⛔ No DOI found,nosource}
}

@inproceedings{piaggesi2019predicting,
  title = {Predicting City Poverty Using Satellite Imagery},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition Workshops},
  author = {Piaggesi, Simone and Gauvin, Laetitia and Tizzoni, Michele and Cattuto, Ciro and Adler, Natalia and Verhulst, Stefaan and Young, Andrew and Price, Rhiannan and Ferres, Leo and Panisson, Andr{\'e}},
  year = {2019},
  pages = {90--96},
  keywords = {⛔ No DOI found}
}

@incollection{pielou1975ecological,
  title = {Ecological Diversity},
  booktitle = {Wiley New York},
  author = {Pielou, {\relax EC}},
  year = {1996},
  pages = {165},
  publisher = {Wiley New York},
  isbn = {0-471-68925-4},
  keywords = {duplicate-citation-key,nosource}
}

@book{pielou1975ecological,
  title = {Ecological Diversity},
  author = {Pielou, Evelyn C},
  year = {1975},
  publisher = {Wiley New York},
  keywords = {duplicate-citation-key,nosource}
}

@article{piironen2016hyperprior,
  title = {On the Hyperprior Choice for the Global Shrinkage Parameter in the Horseshoe Prior},
  author = {Piironen, Juho and Vehtari, Aki},
  year = {2016},
  journal = {arXiv preprint arXiv:1610.05559},
  eprint = {1610.05559},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@book{pinheiro2006mixed,
  title = {Mixed-Effects Models in {{S}} and {{S-PLUS}}},
  author = {Pinheiro, Jos{\'e} and Bates, Douglas},
  year = {2006},
  publisher = {Springer science \& business media}
}

@article{pires2002models,
  title = {Models for the Estimation of a 'No Effect Concentration'},
  author = {Pires, Ana M. and Branco, Jo??o A. and Picado, Ana and Mendon??a, Elsa},
  year = {2002},
  journal = {Environmetrics},
  volume = {13},
  number = {1},
  pages = {15--27},
  publisher = {Wiley Online Library},
  issn = {11804009},
  doi = {10.1002/env.501},
  abstract = {NOEC hampered. Thus, NEC calculated as threshold concentration in a non-lineair model},
  keywords = {Daphnia magna reproduction test,No effect concentration (NEC),No observed effect concentration (NOEC),nosource,Resampling methods,Segmented regression,Threshold parameter}
}

@article{pitman,
  title = {Poisson-Kingman Partitions},
  author = {Pitman, Jim},
  year = {2003},
  journal = {Lecture Notes-Monograph Series},
  pages = {1--34},
  publisher = {JSTOR},
  file = {/home/gkonkamking/Zotero/storage/XSE2PQ7D/Pitman - 2003 - Poisson-kingman partitions.pdf}
}

@techreport{pitman1992two,
  title = {The Two-Parameter Generalization of {{Ewens}}' Random Partition Structure},
  author = {Pitman, Jim},
  year = {2003},
  number = {345},
  pages = {1--9},
  institution = {Technical Report 345, Dept. Statistics, UC Berkeley},
  keywords = {duplicate-citation-key,nosource}
}

@article{pitman1995exchangeable,
  title = {Exchangeable and Partially Exchangeable Random Partitions},
  author = {Pitman, Jim},
  year = {1995},
  journal = {Probability Theory and Related Fields},
  volume = {102},
  number = {2},
  pages = {145--158},
  publisher = {Springer},
  issn = {01788051},
  doi = {10.1007/BF01213386},
  abstract = {Call a random partition of the positive integers partially exchange- able if for each finite sequence of positive integers nl,..., nk, the probabil- ity that the partition breaks the first nl +...  nk integers into k particular classes, of sizes nl .... , nk in order of their first elements, has the same value p(na ..... nk) for every possible choice of classes subject to the sizes con- straint. A random partition is exchangeable iff it is partially exchangeable for a symmetric function p(nl,.., nk). A representation is given for partially ex- changeable random partitions which provides a useful variation of Kingman's representation in the exchangeable case. Results are illustrated by the two- parameter generalization of Ewens' partition structure.},
  keywords = {60C05,Mathematics Subject Classification: 60G09,nosource}
}

@article{pitman1996random,
  title = {Random Discrete Distributions Invariant under Size-Biased Permutation},
  author = {Pitman, Jim},
  year = {1996},
  journal = {Advances in Applied Probability},
  volume = {28},
  number = {2},
  pages = {525--539},
  publisher = {JSTOR},
  issn = {00018678},
  doi = {10.2307/1428070},
  abstract = {Invariance of a random discrete distribution under size-biased permutation is equivalent to a conjunction of symmetry conditions on its finite-dimensional distributions. This is applied to characterize residual allocation models with independent factors that are invariant under size-biased permutation. Apart from some exceptional cases and minor modifications, such models form a two-parameter family of generalized Dirichlet distributions.},
  keywords = {Exchangeable random partition,Generalized dirichlet distribution,nosource,Residual allocation model}
}

@article{pitman1996some,
  title = {Some Developments of the Blackwell-Macqueen Urn Scheme},
  author = {Pitman, Jim},
  year = {1996},
  journal = {Lecture Notes-Monograph Series},
  volume = {30},
  pages = {245--267},
  publisher = {JSTOR},
  issn = {07492170},
  doi = {10.2307/4355949},
  abstract = {The \{Blackwell-MacQueen\} description of sampling from a Dirichlet{\textbackslash}nrandom distribution on an abstract space is reviewed, and extended{\textbackslash}nto a general family of random discrete distributions. Results are{\textbackslash}nobtained by application of Kingman's theory of partition structures.},
  keywords = {dirichlet-process,nosource,pitman-yor-process}
}

@article{pitman1997two,
  title = {The Two-Parameter {{Poisson-Dirichlet}} Distribution Derived from a Stable Subordinator},
  author = {Pitman, Jim and Yor, Marc},
  year = {1997},
  journal = {The Annals of Probability},
  volume = {25},
  number = {2},
  pages = {855--900},
  publisher = {Institute of Mathematical Statistics},
  issn = {00911798},
  doi = {10.1214/aop/1024404422},
  abstract = {The two-parameter Poisson-Dirichlet distribution, denoted mathsfPD(alpha, theta) is a probability distribution on the set of decreasing positive sequences with sum 1. The usual Poisson-Dirichlet distribution with a single parameter theta, introduced by Kingman, is mathsfPD(0, theta). Known properties of mathsfPD(0, theta), including the Markov chain description due to Vershik, Shmidt and Ignatov, are generalized to the two-parameter case. The size-biased random permutation of mathsfPD(alpha, theta) is a simple residual allocation model proposed by Engen in the context of species diversity, and rediscovered by Perman and the authors in the study of excursions of Brownian motion and Bessel processes. For \$0 {$<$} alpha {$<$} 1, mathsfPD(alpha, 0) is the asymptotic distribution of ranked lengths of excursions of a Markov chain away from a state whose recurrence time distribution is in the domain of attraction of a stable law of index alpha. Formulae in this case trace back to work of Darling, Lamperti and Wendel in the 1950s and 1960s. The distribution of ranked lengths of excursions of a one-dimensional Brownian motion is mathsfPD(1/2, 0), and the corresponding distribution for a Brownian bredge is mathsfPD(1/2, 1/2). The mathsfPD(alpha, 0) and mathsfPD(alpha, alpha) distributions admit a similar interpretation in terms of the ranked lengths of excursions of a semistable Markov process whose zero set is the range of a stable subordinator of index alpha\$},
  isbn = {0091-1798},
  keywords = {Local time,nosource,Poisson point process,Ranked lengths of excursions,Semistable Markov process,Zero set}
}

@article{pitman2002poisson,
  title = {Poisson-{{Dirichlet}} and {{GEM}} Invariant Distributions for Split-and-Merge Transformations of an Interval Partition 1 {{Introduction}}},
  author = {Pitman, Jim and Hall, Evans},
  year = {2002},
  journal = {Combinatorics, Probability, and Computing},
  volume = {11},
  number = {05},
  pages = {501--514},
  publisher = {Cambridge Univ Press},
  issn = {0963-5483},
  doi = {http://dx.doi.org/10.1017/S0963548302005163},
  abstract = {This paper introduces a split-and-merge transformation of interval partitions which combines some features of one model studied by Gnedin and Kerov [12, 11] and another studied by Tsilevich [30, 31] and Mayer-Wolf, Zeitouni and Zerner [21]. The invariance under this split-and-merge transformation of the interval partition generated by a suitable Poisson process yields a simple proof of the recent result of [21] that a Poisson--Dirichlet distribution is invariant for a closely related fragmentation--coagulation process. Uniqueness and convergence to the invariant measure are established for the split-and-merge transformation of interval partitions, but the corresponding problems for the fragmentation--coagulation process remain open.},
  keywords = {nosource}
}

@incollection{pitman2006combinatorial,
  title = {Combinatorial Stochastic Processes},
  booktitle = {Combinatorial Stochastic Processes},
  author = {Pitman, Jim},
  year = {2006},
  volume = {1875},
  pages = {1--11},
  publisher = {Springer-Verlag},
  keywords = {duplicate-citation-key,nosource}
}

@book{pitman2006combinatorial,
  title = {Combinatorial Stochastic Processes: {{Ecole}} d'{{Et}}\{{\'e}\} de {{Probabilit}}\{{\'e}\}s de Saint-Flour {{XXXII-2002}}},
  author = {Pitman, Jim},
  year = {2006},
  publisher = {Springer},
  keywords = {duplicate-citation-key,nosource}
}

@inproceedings{PK16,
  title = {Parallel Evaluation of a {{DSP}} Algorithm Using Julia},
  booktitle = {Proceedings of the 3rd International Workshop on Software Engineering for Parallel Systems},
  author = {Kourzanov, Peter},
  year = {2016},
  series = {{{SEPS}} 2016},
  pages = {20--24},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/3002125.3002126},
  abstract = {Rapid pace of innovation in industrial research labs requires fast algorithm evaluation cycles. The use of multi-core hardware and distributed clusters is essential to achieve reasonable turnaround times for high-load simulations. Julia's support for these as well as its pervasive multiple dispatch make it very attractive for high-performance technical computing. Our experiments in speeding up a Digital Signal Processing (DSP) Intellectual Property (IP) model simulation for a Wireless LAN (WLAN) product confirm this. We augment standard SystemC High-Level Synthesis (HLS) tool-flow by an interactive worksheet supporting performance visualization and rapid design space exploration cycles.},
  isbn = {978-1-4503-4641-2},
  keywords = {HLS,Julia,LSF,MapReduce,nosource,SystemC}
}

@inproceedings{plummer2003jags,
  title = {{{JAGS}}: A Program for Analysis of {{Bayesian}} Graphical Models Using {{Gibbs}} Sampling},
  booktitle = {Proceedings of the 3rd International Workshop on Distributed Statistical Computing ({{DSC}} 2003), March 20-22, Vienna, Austria. {{ISSN}} 1609-{{395X}}.},
  author = {Plummer, Martyn},
  year = {2003},
  volume = {124},
  pages = {125},
  issn = {1609395X},
  doi = {10.1.1.13.3406},
  isbn = {ISSN 1609-395X},
  keywords = {nosource}
}

@article{Plummer2013,
  title = {{{JAGS}} Version 3.4.0 User Manual},
  author = {Plummer, Martyn},
  year = {2013},
  number = {August},
  pages = {0--41},
  abstract = {JAGS is Just Another Gibbs Sampler. It is a program for the analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) which is not wholly unlike OpenBUGS (http: //www.openbugs.info). JAGS was written with three aims in mind: to have an engine for the BUGS language that runs on Unix; to be extensible, allowing users to write their own functions, distributions, and samplers; and to be a platform for experimentation with ideas in Bayesian modelling. JAGS is designed to work closely with the R language and environment for statistical computation and graphics (http://www.r-project.org). You will find it useful to install the coda package for R to analyze the output. You can also use the rjags package to work directly with JAGS from within R (but note that the rjags package is not described in this manual). JAGS is licensed under the GNU General Public License version 2. You may freely modify and redistribute it under certain conditions (see the file COPYING for details).},
  keywords = {nosource}
}

@article{plummerPenalizedLossFunctions2008,
  title = {Penalized Loss Functions for {{Bayesian}} Model Comparison},
  author = {Plummer, Martyn},
  year = {2008},
  month = jul,
  journal = {Biostatistics},
  volume = {9},
  number = {3},
  pages = {523--539},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxm049},
  urldate = {2021-11-18},
  abstract = {The deviance information criterion (DIC) is widely used for Bayesian model comparison, despite the lack of a clear theoretical foundation. DIC is shown to be an approximation to a penalized loss function based on the deviance, with a penalty derived from a cross-validation argument. This approximation is valid only when the effective number of parameters in the model is much smaller than the number of independent observations. In disease mapping, a typical application of DIC, this assumption does not hold and DIC under-penalizes more complex models. Another deviance-based loss function, derived from the same decision-theoretic framework, is applied to mixture models, which have previously been considered an unsuitable application for DIC},
  file = {/home/gkonkamking/Zotero/storage/5A3D42V5/Plummer - 2008 - Penalized loss functions for Bayesian model compar.pdf}
}

@incollection{PMG15,
  title = {Monte Carlo Methods and Zero Variance Principle},
  author = {Papamarkou, Theodore and Mira, Antonietta and Girolami, Mark},
  editor = {Upadhyay, Satyanshu K and Singh, Umesh and Dey, Dipak K and Loganathan, Appaia},
  year = {2015},
  volume = {Current Tr},
  pages = {457--476},
  publisher = {{Chapman and Hall/CRC}},
  chapter = {22},
  keywords = {nosource}
}

@inproceedings{pmlr-v139-cai21a,
  title = {Finite Mixture Models Do Not Reliably Learn the Number of Components},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Cai, Diana and Campbell, Trevor and Broderick, Tamara},
  editor = {Meila, Marina and Zhang, Tong},
  year = {2021-07-18/2021-07-24},
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {1158--1169},
  publisher = {PMLR},
  abstract = {Scientists and engineers are often interested in learning the number of subpopulations (or components) present in a data set. A common suggestion is to use a finite mixture model (FMM) with a prior on the number of components. Past work has shown the resulting FMM component-count posterior is consistent; that is, the posterior concentrates on the true, generating number of components. But consistency requires the assumption that the component likelihoods are perfectly specified, which is unrealistic in practice. In this paper, we add rigor to data-analysis folk wisdom by proving that under even the slightest model misspecification, the FMM component-count posterior diverges: the posterior probability of any particular finite number of components converges to 0 in the limit of infinite data. Contrary to intuition, posterior-density consistency is not sufficient to establish this result. We develop novel sufficient conditions that are more realistic and easily checkable than those common in the asymptotics literature. We illustrate practical consequences of our theory on simulated and real data.}
}

@inproceedings{poliHyenaHierarchyLarger2023,
  title = {Hyena Hierarchy: {{Towards}} Larger Convolutional Language Models},
  shorttitle = {Hyena Hierarchy},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  year = {2023},
  pages = {28043--28078},
  publisher = {PMLR},
  urldate = {2024-10-10},
  file = {/home/gkonkamking/Zotero/storage/D4RMQIIF/Poli et al. - 2023 - Hyena hierarchy Towards larger convolutional language models.pdf}
}

@article{pollockQuasistationaryMonteCarlo2020,
  title = {Quasi-Stationary {{Monte Carlo}} and the {{ScaLE Algorithm}}},
  author = {Pollock, Murray and Fearnhead, Paul and Johansen, Adam M. and Roberts, Gareth O.},
  year = {2020},
  month = apr,
  journal = {arXiv:1609.03436 [stat]},
  eprint = {1609.03436},
  primaryclass = {stat},
  urldate = {2020-07-13},
  abstract = {This paper introduces a class of Monte Carlo algorithms which are based upon the simulation of a Markov process whose quasi-stationary distribution coincides with a distribution of interest. This differs fundamentally from, say, current Markov chain Monte Carlo methods which simulate a Markov chain whose stationary distribution is the target. We show how to approximate distributions of interest by carefully combining sequential Monte Carlo methods with methodology for the exact simulation of diffusions. The methodology introduced here is particularly promising in that it is applicable to the same class of problems as gradient based Markov chain Monte Carlo algorithms but entirely circumvents the need to conduct Metropolis-Hastings type accept/reject steps whilst retaining exactness: the paper gives theoretical guarantees ensuring the algorithm has the correct limiting target distribution. Furthermore, this methodology is highly amenable to big data problems. By employing a modification to existing na\{{\textbackslash}"{\textbackslash}i\}ve sub-sampling and control variate techniques it is possible to obtain an algorithm which is still exact but has sub-linear iterative cost as a function of data size.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/gkonkamking/Zotero/storage/R8AIVDDN/Pollock et al. - 2020 - Quasi-stationary Monte Carlo and the ScaLE Algorit.pdf}
}

@article{Polson2010,
  title = {Shrink Globally, Act Locally: {{Sparse Bayesian}} Regularization and Prediction},
  author = {Polson, Nicholas G and Scott, James G},
  year = {2010},
  journal = {Bayesian Statistics},
  volume = {9},
  pages = {501--538},
  keywords = {{$\beta$},{$\beta$} 1,{$\beta$} p,{$\sigma$} 2 i,1,and at least some,and phrases,frequentists,is believed to be,l e,many bayesians,n,one-group answers to two-group,questions,sparse,suppose that,where {$\beta$},would assume an exchangeable,y},
  file = {/home/gkonkamking/Zotero/storage/P68GDR47/Polson and Scott - 2010 - Shrink globally, act locally Sparse Bayesian regu.pdf}
}

@article{Pooley2014,
  title = {Hunting down the Chimera of Multiple Disciplinarity in Conservation Science},
  author = {Pooley, Simon P. and Mendelsohn, J. Andrew and {Milner-Gulland}, E. J.},
  year = {2014},
  journal = {Conservation Biology},
  volume = {28},
  number = {1},
  pages = {22--32},
  issn = {08888892},
  doi = {10.1111/cobi.12183},
  abstract = {The consensus is that both ecological and social factors are essential dimensions of conservation research and practice. However, much of the literature on multiple disciplinary collaboration focuses on the difficulties of undertaking it. This review of the challenges of conducting multiple disciplinary collaboration offers a framework for thinking about the diversity and complexity of this endeavor. We focused on conceptual challenges, of which 5 main categories emerged: methodological challenges, value judgments, theories of knowledge, disciplinary prejudices, and interdisciplinary communication. The major problems identified in these areas have proved remarkably persistent in the literature surveyed (c.1960-2012). Reasons for these failures to learn from past experience include the pressure to produce positive outcomes and gloss over disagreements, the ephemeral nature of many such projects and resulting lack of institutional memory, and the apparent complexity and incoherence of the endeavor. We suggest that multiple disciplinary collaboration requires conceptual integration among carefully selected multiple disciplinary team members united in investigating a shared problem or question. We outline a 9-point sequence of steps for setting up a successful multiple disciplinary project. This encompasses points on recruitment, involving stakeholders, developing research questions, negotiating power dynamics and hidden values and conceptual differences, explaining and choosing appropriate methods, developing a shared language, facilitating on-going communications, and discussing data integration and project outcomes. Although numerous solutions to the challenges of multiple disciplinary research have been proposed, lessons learned are often lost when projects end or experienced individuals move on. We urge multiple disciplinary teams to capture the challenges recognized, and solutions proposed, by their researchers while projects are in process. A database of well-documented case studies would showcase theories and methods from a variety of disciplines and their interactions, enable better comparative study and evaluation, and provide a useful resource for developing future projects and training multiple disciplinary researchers.},
  isbn = {1523-1739 (Electronic){\textbackslash}r0888-8892 (Linking)},
  pmid = {24299167},
  keywords = {Conceptual challenges,Humanities,Interdisciplinary,Multidisciplinary,Natural sciences,nosource,Social sciences,Transdisciplinary}
}

@article{porwalComparingMethodsStatistical2022,
  title = {Comparing Methods for Statistical Inference with Model Uncertainty},
  author = {Porwal, Anupreet and Raftery, Adrian E.},
  year = {2022},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {16},
  pages = {e2120737119},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2120737119},
  urldate = {2022-06-27},
  keywords = {\_tablet},
  file = {/home/gkonkamking/Zotero/storage/26VPP2W4/Porwal_Raftery_2022_Comparing methods for statistical inference with model uncertainty.pdf}
}

@article{Posthuma2006,
  title = {Predicted Effects of Toxicant Mixtures Are Confirmed by Changes in Fish Species Assemblages in {{Ohio}}, {{USA}}, Rivers.},
  author = {Posthuma, Leo and {de Zwart}, Dick},
  year = {2006},
  month = apr,
  journal = {Environmental toxicology and chemistry / SETAC},
  volume = {25},
  number = {4},
  eprint = {16629149},
  eprinttype = {pubmed},
  pages = {1094--105},
  issn = {0730-7268},
  abstract = {The purposes of this study were to investigate whether exposure to toxicant mixtures is associated with fish assemblage characteristics in the field and to describe the relationships between predicted chronic and acute mixture risks and observed impacts. Fish abundance and abiotic monitoring data from Ohio, USA, surface waters were compiled and analyzed. Variability of biotic and abiotic parameters was large. Exposure assessment, risk assessment with species-sensitivity distributions, and mixture toxicity rules were used to calculate a relative risk predictor: The multisubstance potentially affected fraction of species (msPAF). Predicted acute and chronic risks ranged from low values to more than 10 and 50\% of species potentially affected, respectively. Pearson correlations between predicted risk and observed assemblage characteristics were nonsignificant for total abundance, number of species, Shannon-Weaver index, and evenness. Moderately significant correlations were found between predicted risk and abundance for 23\% of individual species. Both abundance increases and decreases were observed. Generalized linear model (GLM) regressions revealed significant nonlinear associations between predicted risk and the abundance for 50\% (metals and ammonia) and 55\% (household product ingredients) of the species. Local ecological impact was expressed as the fraction of species expected but not observed, both with and without attribution of impact to mixture exposure. The association between predicted impacted fraction and the fraction of species expected but not observed was not significant. Predicted acute and chronic impacted fractions were associated significantly with the observed fraction of species likely lost by the action of toxicant mixtures under field conditions, with wide confidence bounds. These findings confirm the view that higher mixture impacts are expected in the field at higher msPAF.},
  isbn = {0730-7268},
  pmid = {16629149},
  keywords = {---,Animals,Community responses,duplicate-citation-key,Ecological risk assessment,Field confirmation,Fishes,Fishes: classification,Mixture toxicity,nosource,Ohio,Risk Factors,Rivers,Species-sensitivity distribution,Water Pollutants,Water Pollutants: toxicity}
}

@incollection{Posthuma2010,
  title = {Species Sensitivity Distributions in Ecotoxicology},
  booktitle = {Ecotoxicology},
  author = {Posthuma, Leo and Suter II, Glenn W and Trass, P Teo},
  year = {2002},
  pages = {616},
  publisher = {CRC press},
  abstract = {In spite of the growing importance of Species Sensitivity Distribution models (SSDs) in ecological risk assessments, the conceptual basis, strengths, and weaknesses of using them have not been comprehensively reviewed. This book fills that need. Written by a panel of international experts, Species Sensitivity Distributions in Ecotoxicology reviews the current SSD methods from all angles, compiling for the first time the variety of contemporary applications of SSD-based methods. Beginning with an introduction to SSDs, the chapter authors review the issues surrounding SSDs, synthesizing the positions of advocates and critics with their own analysis of each issue. Finally, they discuss the prospects for future development, paving the way for improved future uses. In sum, this book defines the field of SSD modeling and application. It reveals a lively field, with SSD-applications extending beyond legally adopted quality criteria to other applications such as Life-Cycle Analysis. For anyone developing or revising environmental criteria or standards, this book explores the pros and cons of using the SSD approach. For anyone who needs to apply and interpret SSD-based criteria or standards, the book explains the basis for the numbers, thereby making it possible to correctly apply and defend them. For anyone performing ecological risk assessments, the book covers when and how to use SSDs including alternative assumptions, data treatments, computational methods, and available resources. Species Sensitivity Distributions in Ecotoxicology provides you with a clear picture of these standard models for estimating ecological risks from laboratory toxicity data.},
  isbn = {1-56670-578-9},
  pmid = {158},
  keywords = {nosource}
}

@incollection{Posthuma2010_straalen,
  title = {Theory of Ecological Risk Assessment Based on {{SSDs}}},
  booktitle = {Species Sensitivity Distributions in Ecotoxicology},
  author = {{van Straalen}, Nico M},
  editor = {Posthuma, Leo and Suter II, Glenn W and Traas, Theo P},
  year = {2010},
  pages = {37--48},
  publisher = {CRC press},
  address = {Boca Raton, FL},
  chapter = {Theory of},
  keywords = {nosource}
}

@article{Posthuma2012a,
  title = {Predicted Mixture Toxic Pressure Relates to Observed Fraction of Benthic Macrofauna Species Impacted by Contaminant Mixtures},
  author = {Posthuma, Leo and {de Zwart}, Dick},
  year = {2012},
  month = sep,
  journal = {Environmental Toxicology and Chemistry},
  volume = {31},
  number = {9},
  eprint = {22729941},
  eprinttype = {pubmed},
  pages = {2175--2188},
  issn = {07307268},
  doi = {10.1002/etc.1923},
  abstract = {Species sensitivity distributions (SSDs) quantify fractions of species potentially affected in contaminated environmental compartments using test species sensitivity data. The present study quantitatively describes associations between predicted and observed ecological impacts of contaminant mixtures, based on monitoring data of benthic macroinvertebrates. Local mixture toxic pressures (multisubstance potentially affected fraction of species [msPAF]) were quantified based on measured concentrations of 45 compounds (eight metals, 16 chlorinated organics, mineral oil, 16 polycyclic aromatic hydrocarbons, four polychlorinated biphenyls), using acute as well as chronic 50\%-effective concentration-based SSD-modeling combined with bioavailability and mixture modeling. Acute and chronic toxic pressures were closely related. Generalized linear models (GLMs) were derived to describe taxon abundances as functions of environmental variables (including acute toxic pressure). Acute toxic pressure ranged from 0 to 42\% and was related to abundance for 74\% of the taxa. Habitat-abundance curves were generated using the GLMs and Monte Carlo simulation. Predicted abundances for the taxa were associated with acute mixture toxic pressure in various ways: negative, positive, and optimum abundance changes occurred. Acute toxic pressure (msPAF) was associated almost 1:1 with the observed fraction of taxa exhibiting an abundance reduction of 50\% or more. The findings imply that an increase of mixture toxic pressure associates to increased ecological impacts in the field. This finding is important, given the societal relevance of SSD model outputs in environmental policies. Environ. Toxicol. Chem. 2012; 31: 21752188. (c) 2012 SETAC},
  isbn = {0730-7268},
  pmid = {22729941},
  keywords = {Confirmation,Contaminated sediment,Multisubstance potentially affected fraction,nosource,Species sensitivity distribution,Toxic pressure}
}

@article{Pouillot2009,
  title = {Quantitative Risk Assessment of Listeria Monocytogenes in French Cold-Smoked {{Salmon}}: {{II}}. {{Risk}} Characterization},
  author = {Pouillot, R{\'e}gis and Goulet, V??ronique and {Delignette-Muller}, Marie Laure and Mah??, Aur??lie and Cornu, Marie},
  year = {2009},
  month = jun,
  journal = {Risk Analysis},
  volume = {29},
  number = {6},
  eprint = {19220799},
  eprinttype = {pubmed},
  pages = {806--819},
  issn = {02724332},
  doi = {10.1111/j.1539-6924.2008.01200.x},
  abstract = {A model for the assessment of exposure to Listeria monocytogenes from cold-smoked salmon consumption in France was presented in the first of this pair of articles (Pouillot et al., 2007, Risk Analysis, 27:683-700). In the present study, the exposure model output was combined with an internationally accepted hazard characterization model, adapted to the French situation, to assess the risk of invasive listeriosis from cold-smoked salmon consumption in France in a second-order Monte Carlo simulation framework. The annual number of cases of invasive listeriosis due to cold-smoked salmon consumption in France is estimated to be 307, with a very large credible interval ([10; 12,453]), reflecting data uncertainty. This uncertainty is mainly associated with the dose-response model. Despite the significant uncertainty associated with the predictions, this model provides a scientific base for risk managers and food business operators to manage the risk linked to cold-smoked salmon contaminated with L. monocytogenes. Under the modeling assumptions, risk would be efficiently reduced through a decrease in the prevalence of L. monocytogenes or better control of the last steps of the cold chain (shorter and/or colder storage during the consumer step), whereas reduction of the initial contamination levels of the contaminated products and improvement in the first steps of the cold chain do not seem to be promising strategies. An attempt to apply the recent risk-based concept of FSO (food safety objective) on this example underlines the ambiguity in practical implementation of the risk management metrics and the need for further elaboration on these concepts.},
  isbn = {0272-4332},
  pmid = {19220799},
  keywords = {Listeria monocytogenes,nosource,Risk assessment,Second-order Monte Carlo simulations}
}

@article{Pouillot2010,
  title = {Evaluating Variability and Uncertainty Separately in Microbial Quantitative Risk Assessment Using Two {{R}} Packages},
  author = {Pouillot, R{\'e}gis and {Delignette-Muller}, Marie Laure},
  year = {2010},
  month = sep,
  journal = {International Journal of Food Microbiology},
  volume = {142},
  number = {3},
  eprint = {20674055},
  eprinttype = {pubmed},
  pages = {330--340},
  publisher = {Elsevier B.V.},
  issn = {01681605},
  doi = {10.1016/j.ijfoodmicro.2010.07.011},
  abstract = {Quantitative risk assessment has emerged as a valuable tool to enhance the scientific basis of regulatory decisions in the food safety domain. This article introduces the use of two new computing resources (R packages) specifically developed to help risk assessors in their projects. The first package, "fitdistrplus", gathers tools for choosing and fitting a parametric univariate distribution to a given dataset. The data may be continuous or discrete. Continuous data may be right-, left- or interval-censored as is frequently obtained with analytical methods, with the possibility of various censoring thresholds within the dataset. Bootstrap procedures then allow the assessor to evaluate and model the uncertainty around the parameters and to transfer this information into a quantitative risk assessment model. The second package, "mc2d", helps to build and study two dimensional (or second-order) Monte-Carlo simulations in which the estimation of variability and uncertainty in the risk estimates is separated. This package easily allows the transfer of separated variability and uncertainty along a chain of conditional mathematical and probabilistic models. The usefulness of these packages is illustrated through a risk assessment of hemolytic and uremic syndrome in children linked to the presence of Escherichia coli O157:H7 in ground beef. These R packages are freely available at the Comprehensive R Archive Network (cran.r-project.org). {\copyright} 2010 Elsevier B.V.},
  isbn = {1879-3460 (Electronic) 0168-1605 (Linking)},
  pmid = {20674055},
  keywords = {Distribution fitting,Freeware tools,Microbial quantitative risk assessment,nosource,Second-order Monte Carlo simulations}
}

@article{Prangle2014,
  title = {Semi-Automatic Selection of Summary Statistics for {{ABC}} Model Choice},
  author = {Prangle, Dennis and Fearnhead, Paul and Cox, Murray P. and Biggs, Patrick J. and French, Nigel P.},
  year = {2014},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  volume = {13},
  number = {1},
  eprint = {1302.5624v1},
  pages = {67--82},
  issn = {15446115},
  doi = {10.1515/sagmb-2013-0012},
  abstract = {A central statistical goal is to choose between alternative explanatory models of data. In many modern applications, such as population genetics, it is not possible to apply standard methods based on evaluating the likelihood functions of the models, as these are numerically intractable. Approximate Bayesian computation (ABC) is a commonly used alternative for such situations. ABC simulates data x for many parameter values under each model, which is compared to the observed data x obs. More weight is placed on models under which S(x) is close to S(x obs), where S maps data to a vector of summary statistics. Previous work has shown the choice of S is crucial to the efficiency and accuracy of ABC. This paper provides a method to select good summary statistics for model choice. It uses a preliminary step, simulating many x values from all models and fitting regressions to this with the model as response. The resulting model weight estimators are used as S in an ABC analysis. Theoretical results are given to justify this as approximating low dimensional sufficient statistics. A substantive application is presented: choosing between competing coalescent models of demographic growth for Campylobacter jejuni in New Zealand using multi-locus sequence typing data.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1302.5624v1},
  isbn = {2194-6302},
  pmid = {24323893},
  keywords = {ABC,Campylobacter,Coalescent,MLST,Model selection,nosource,Sufficiency}
}

@article{Prangle2015,
  title = {Adapting the {{ABC}} Distance Function},
  author = {Prangle, Dennis},
  year = {2016},
  journal = {Bayesian Analysis},
  eprint = {1507.00874},
  pages = {1--21},
  issn = {1936-0975},
  doi = {10.1214/16-BA1002},
  abstract = {Approximate Bayesian computation performs approximate inference for models where likelihood computations are expensive or impossible. Instead simulations from the model are performed for various parameter values and accepted if they are close enough to the observations. There has been much progress on deciding which summary statistics of the data should be used to judge closeness, but less work on how to weight them. Typically weights are chosen at the start of the algorithm which normalise the summary statistics to vary on similar scales. However these may not be appropriate in iterative ABC algorithms, where the distribution from which the parameters are proposed is updated. This can substantially alter the resulting distribution of summary statistics, so that different weights are needed for normalisation. This paper presents an iterative ABC algorithm which adaptively updates its weights, without requiring any extra simulations to do so, and demonstrates improved results on test applications.},
  archiveprefix = {arXiv},
  arxivid = {1507.00874},
  keywords = {likelihood-free inference,lotka-,nosource,population monte carlo,quantile distributions}
}

@article{Prangle2016,
  title = {Lazy {{ABC}}},
  author = {Prangle, Dennis},
  year = {2016},
  journal = {Statistics and Computing},
  volume = {26},
  number = {1-2},
  eprint = {1405.7867},
  pages = {171--185},
  issn = {15731375},
  doi = {10.1007/s11222-014-9544-3},
  abstract = {Approximate Bayesian computation (ABC) performs statistical inference for otherwise intractable probability models by accepting parameter proposals when corresponding simulated datasets are sufficiently close to the observations. Producing the large quantity of simulations needed requires considerable computing time. However, it is often clear before a simulation ends that it is unpromising: it is likely to produce a poor match or require excessive time. This paper proposes lazy ABC, an ABC importance sampling algorithm which saves time by sometimes abandoning such simulations. This makes ABC more scalable to applications where simulation is expensive. By using a random stopping rule and appropriate reweighting step, the target distribution is unchanged from that of standard ABC. Theory and practical methods to tune lazy ABC are presented and demonstrated on the computationally demanding spatial extremes application of Erhardt and Smith (2012), producing efficiency gains, in terms of effective sample size per unit CPU time, of roughly 3 times for a 20 location dataset, and 8 times for 35 locations.},
  archiveprefix = {arXiv},
  arxivid = {1405.7867},
  isbn = {1122201495443},
  keywords = {ABC,Epidemics,Importance sampling,nosource,Spatial extremes,Unbiased likelihood estimators}
}

@article{prentice1979hazard,
  title = {Hazard Rate Models with Covariates},
  author = {Prentice, Ross L and Kalbfleisch, John D},
  year = {1979},
  journal = {Biometrics},
  pages = {25--39},
  publisher = {JSTOR},
  keywords = {nosource}
}

@article{Price2012,
  title = {Estimation of Dose-Response Models for Discrete and Continuous Data in Weed Science},
  author = {Price, {\relax WJ} and Shafii, B and Seefeldt, {\relax SS}},
  year = {2012},
  journal = {Weed Technology},
  volume = {26},
  number = {3},
  pages = {587--601},
  issn = {0890-037X},
  doi = {10.1614/WT-D-11-00101.1},
  abstract = {Dose--response analysis is widely used in biological sciences and has application to a variety of risk assessment, bioassay, and calibration problems. In weed science, dose--response methodologies have typically relied on least squares estimation under the assumptions of normal, homoscedastic, and independent errors. Advances in computational abilities and available software, however, have given researchers more flexibility and choices for data analysis when these assumptions are not appropriate. This article will explore these techniques and demonstrate their use to provide researchers with an up-to-date set of tools necessary for analysis of dose--response problems. Demonstrations of the techniques are provided using a variety of data examples from weed science.},
  keywords = {alternative model estimation,bioassay,de,dosis es ampliamente usado,en las ciencias biolo,gicas y tiene aplicacio,lisis de respuesta a,maximum likelihood,n a una variedad,nonlinear models,nosource,treatment comparison}
}

@article{proctor2017integrated,
  title = {Integrated Modeling of Survival Data from Multiple Stressor Ecotoxicology Experiments},
  author = {Proctor, Abigael H and King, Catherine K and Holan, Jessica R and Wotherspoon, Simon J},
  year = {2017},
  journal = {Environmental Science \& Technology},
  publisher = {ACS Publications},
  keywords = {nosource}
}

@article{Project2006,
  title = {Survival and Censored Data},
  author = {Project, Semester and Samartzis, Lefteris},
  year = {2006},
  keywords = {nosource}
}

@article{Protein2014,
  title = {A {{Dirichlet}} Process Model for Detecting Positive Selection in Protein-Coding {{DNA}} Sequences.},
  author = {Huelsenbeck, John P and Jain, Sonia and Frost, Simon W D and Pond, Sergei L Kosakovsky},
  year = {2006},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {103},
  number = {16},
  pages = {6263--6268},
  issn = {0027-8424},
  doi = {10.1073/pnas.0508279103},
  abstract = {Most methods for detecting Darwinian natural selection at the molecular level rely on estimating the rates or numbers of nonsynonymous and synonymous changes in an alignment of protein-coding DNA sequences. In some of these methods, the nonsynonymous rate of substitution is allowed to vary across the sequence, permitting the identification of single amino acid positions that are under positive natural selection. However, it is unclear which probability distribution should be used to describe how the nonsynonymous rate of substitution varies across the sequence. One widely used solution is to model variation in the nonsynonymous rate across the sequence as a mixture of several discrete or continuous probability distributions. Unfortunately, there is little population genetics theory to inform us of the appropriate probability distribution for among-site variation in the nonsynonymous rate of substitution. Here, we describe an approach to modeling variation in the nonsynonymous rate of substitution by using a Dirichlet process mixture model. The Dirichlet process allows there to be a countably infinite number of nonsynonymous rate classes and is very flexible in accommodating different potential distributions for the nonsynonymous rate of substitution. We implemented the model in a fully Bayesian approach, with all parameters of the model considered as random variables.},
  isbn = {0027-8424 (Print)},
  pmid = {16606848},
  keywords = {nosource}
}

@article{prunster2013bayesian,
  title = {A {{Bayesian}} Nonparametric Approach to Modeling Market Share Dynamics},
  author = {Pr{\"u}nster, Igor and Ruggiero, Matteo},
  year = {2013},
  journal = {Bernoulli},
  volume = {19},
  number = {1},
  pages = {64--92},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  keywords = {nosource}
}

@article{quick2017hidden,
  title = {Hidden {{Markov}} Models Reveal Complexity in the Diving Behaviour of Short-Finned Pilot Whales},
  author = {Quick, Nicola J and Isojunno, Saana and Sadykova, Dina and Bowers, Matthew and Nowacek, Douglas P and Read, Andrew J},
  year = {2017},
  journal = {Scientific reports},
  volume = {7},
  pages = {45765},
  publisher = {Nature Publishing Group},
  keywords = {nosource}
}

@article{quinceDESMANNewTool2017,
  title = {{{DESMAN}}: A New Tool for de Novo Extraction of Strains from Metagenomes},
  shorttitle = {{{DESMAN}}},
  author = {Quince, Christopher and Delmont, Tom O. and Raguideau, S{\'e}bastien and Alneberg, Johannes and Darling, Aaron E. and Collins, Gavin and Eren, A. Murat},
  year = {2017},
  month = sep,
  journal = {Genome Biology},
  volume = {18},
  number = {1},
  pages = {181},
  issn = {1474-760X},
  doi = {10.1186/s13059-017-1309-9},
  urldate = {2021-05-02},
  abstract = {We introduce DESMAN for De novo Extraction of Strains from Metagenomes. Large multi-sample metagenomes are being generated but strain variation results in fragmentary co-assemblies. Current algorithms can bin contigs into metagenome-assembled genomes but are unable to resolve strain-level variation. DESMAN identifies variants in core genes and uses co-occurrence across samples to link variants into haplotypes and abundance profiles. These are then searched for against non-core genes to determine the accessory genome of each strain. We validated DESMAN on a complex 50-species 210-genome 96-sample synthetic mock data set and then applied it to the Tara Oceans microbiome.},
  keywords = {Metagenomes,Niche,Strain},
  file = {/home/gkonkamking/pCloudDrive/papers/Quince et al_2017_DESMAN.pdf}
}

@article{quinceMetagenomicsStrainResolution2020,
  title = {Metagenomics {{Strain Resolution}} on {{Assembly Graphs}}},
  author = {Quince, Christopher and Nurk, Sergey and Raguideau, Sebastien and James, Robert and Soyer, Orkun S. and Summers, J. Kimberly and Limasset, Antoine and Eren, A. Murat and Chikhi, Rayan and Darling, Aaron E.},
  year = {2020},
  month = sep,
  journal = {bioRxiv},
  pages = {2020.09.06.284828},
  publisher = {Cold Spring Harbor Laboratory},
  doi = {10.1101/2020.09.06.284828},
  urldate = {2020-10-12},
  abstract = {{$<$}p{$>$}We introduce a novel bioinformatics pipeline, STrain Resolution ON assembly Graphs (STRONG), which identifies strains de novo, when multiple metagenome samples from the same community are available. STRONG performs coassembly, followed by binning into metagenome assembled genomes (MAGs), but uniquely it stores the coassembly graph prior to simplification of variants. This enables the subgraphs for individual single-copy core genes (SCGs) in each MAG to be extracted. It can then thread back reads from the samples to compute per sample coverages for the unitigs in these graphs. These graphs and their unitig coverages are then used in a Bayesian algorithm, BayesPaths, that determines the number of strains present, their sequences or haplotypes on the SCGs and their abundances in each of the samples. Our approach both avoids the ambiguities of read mapping and allows more of the information on co-occurrence of variants in reads to be utilised than if variants were treated independently, whilst at the same time exploiting the correlation of variants across samples that occurs when they are linked in the same strain. We compare STRONG to the current state of the art on synthetic communities and demonstrate that we can recover more strains, more accurately, and with a realistic estimate of uncertainty deriving from the variational Bayesian algorithm employed for the strain resolution. On a real anaerobic digestor time series we obtained strain-resolved SCGs for over 300 MAGs that for abundant community members match those observed from long Nanopore reads.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Quince et al_2020_Metagenomics Strain Resolution on Assembly Graphs.pdf}
}

@article{quinceShotgunMetagenomicsSampling2017,
  title = {Shotgun Metagenomics, from Sampling to Analysis},
  author = {Quince, Christopher and Walker, Alan W. and Simpson, Jared T. and Loman, Nicholas J. and Segata, Nicola},
  year = {2017},
  month = sep,
  journal = {Nature Biotechnology},
  volume = {35},
  number = {9},
  pages = {833--844},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/nbt.3935},
  urldate = {2020-10-12},
  abstract = {The promises and potential pitfalls of shotgun metagenomics, from experimental design to computational analyses, are reviewed.},
  copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english}
}

@article{rafteryFastInferenceLatent2012,
  title = {Fast {{Inference}} for the {{Latent Space Network Model Using}} a {{Case-Control Approximate Likelihood}}},
  author = {Raftery, Adrian E. and Niu, Xiaoyue and Hoff, Peter D. and Yeung, Ka Yee},
  year = {2012},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {21},
  number = {4},
  pages = {901--919},
  issn = {1061-8600, 1537-2715},
  doi = {10.1080/10618600.2012.679240},
  urldate = {2021-02-22},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/C2DCLM25/Raftery et al. - 2012 - Fast Inference for the Latent Space Network Model .pdf}
}

@article{rajkowski2016analysis,
  title = {Analysis of {{MAP}} in {{CRP}} Normal-Normal Model},
  author = {Rajkowski, {\L}ukasz},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.03275},
  eprint = {1606.03275},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@article{Ramanathan1969,
  title = {Journal of the American Statistical},
  author = {{Ramanathan}},
  year = {1969},
  journal = {journal of American Statistical Association},
  volume = {64},
  number = {325},
  pages = {90--101},
  doi = {10.2307/2286841},
  keywords = {nosource}
}

@book{ramsay2002applied,
  title = {Applied Functional Data Analysis: Methods and Case Studies},
  author = {Ramsay, James O and Silverman, Bernard W},
  year = {2002},
  volume = {77},
  publisher = {Citeseer},
  keywords = {nosource}
}

@article{ranciatiBayesianVariableSelection2019,
  title = {Bayesian Variable Selection in Linear Regression Models with Non-Normal Errors},
  author = {Ranciati, Saverio and Galimberti, Giuliano and Soffritti, Gabriele},
  year = {2019},
  month = jun,
  journal = {Statistical Methods \& Applications},
  volume = {28},
  number = {2},
  pages = {323--358},
  issn = {1618-2510, 1613-981X},
  doi = {10.1007/s10260-018-00441-x},
  urldate = {2020-04-10},
  langid = {english},
  keywords = {nosource}
}

@article{randSpontaneousGivingCalculated2012,
  title = {Spontaneous Giving and Calculated Greed},
  author = {Rand, David G. and Greene, Joshua D. and Nowak, Martin A.},
  year = {2012},
  month = sep,
  journal = {Nature},
  volume = {489},
  number = {7416},
  pages = {427--430},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature11467},
  urldate = {2020-04-16},
  abstract = {Economic games are used to investigate the cognitive mechanisms underlying cooperative behaviour, and show that intuition supports cooperation in social dilemmas, whereas reflection can undermine these cooperative impulses.},
  copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english}
}

@article{ranwez2007orthomam,
  title = {{{OrthoMaM}}: {{A}} Database of Orthologous Genomic Markers for Placental Mammal Phylogenetics},
  author = {Ranwez, Vincent and Delsuc, Fr{\'e}d{\'e}ric and Ranwez, Sylvie and Belkhir, Khalid and Tilak, Marie-Ka and Douzery, Emmanuel JP},
  year = {2007},
  journal = {BMC Evolutionary Biology},
  volume = {7},
  number = {1},
  pages = {241},
  publisher = {BioMed Central Ltd},
  issn = {1471-2148},
  doi = {10.1186/1471-2148-7-241},
  abstract = {BACKGROUND:Molecular sequence data have become the standard in modern day phylogenetics. In particular, several long-standing questions of mammalian evolutionary history have been recently resolved thanks to the use of molecular characters. Yet, most studies have focused on only a handful of standard markers. The availability of an ever increasing number of whole genome sequences is a golden mine for modern systematics. Genomic data now provide the opportunity to select new markers that are potentially relevant for further resolving branches of the mammalian phylogenetic tree at various taxonomic levels.DESCRIPTION:The EnsEMBL database was used to determine a set of orthologous genes from 12 available complete mammalian genomes. As targets for possible amplification and sequencing in additional taxa, more than 3,000 exons of length {$>$} 400 bp have been selected, among which 118, 368, 608, and 674 are respectively retrieved for 12, 11, 10, and 9 species. A bioinformatic pipeline has been developed to provide evolutionary descriptors for these candidate markers in order to assess their potential phylogenetic utility. The resulting OrthoMaM (Orthologous Mammalian Markers) database can be queried and alignments can be downloaded through a dedicated web interface http://kimura.univ-montp2.fr/orthomam webcite.CONCLUSION:The importance of marker choice in phylogenetic studies has long been stressed. Our database centered on complete genome information now makes possible to select promising markers to a given phylogenetic question or a systematic framework by querying a number of evolutionary descriptors. The usefulness of the database is illustrated with two biological examples. First, two potentially useful markers were identified for rodent systematics based on relevant evolutionary parameters and sequenced in additional species. Second, a complete, gapless 94 kb supermatrix of 118 orthologous exons was assembled for 12 mammals. Phylogenetic analyses using probabilistic methods unambiguously supported the new placental phylogeny by retrieving the monophyly of Glires, Euarchontoglires, Laurasiatheria, and Boreoeutheria. Muroid rodents thus do not represent a basal placental lineage as it was mistakenly reasserted in some recent phylogenomic analyses based on fewer taxa. We expect the OrthoMaM database to be useful for further resolving the phylogenetic tree of placental mammals and for better understanding the evolutionary dynamics of their genomes, i.e., the forces that shaped coding sequences in terms of selective constraints.},
  isbn = {1471-2148},
  pmid = {18053139},
  keywords = {nosource}
}

@article{rao2009spatial,
  title = {Spatial Normalized Gamma Processes},
  author = {Rao, Vinayak and Teh, Yee Whye},
  year = {2009},
  journal = {Advances in Neural Information Processing Systems},
  volume = {22},
  pages = {1--9},
  publisher = {Citeseer},
  abstract = {Dependent Dirichlet processes (DPs) are dependent sets of random measures, each being marginally DP distributed. They are used in Bayesian nonparametric models when the usual exchangeability assumption does not hold. We propose a simple and general framework to construct dependent DPs by marginalizing and normalizing a single gamma process over an extended space. The result is a set of DPs, each associated with a point in a space such that neighbouring DPs are more dependent. We describe Markov chain Monte Carlo inference involving Gibbs sampling and three different Metropolis-Hastings proposals to speed up convergence. We report an empirical study of convergence on a synthetic dataset and demonstrate an application of the model to topic modeling through time.},
  isbn = {9781615679119},
  keywords = {Computational,Information-Theoretic Learning with,Learning/Statistics \& Optimisation,nosource,Theory \& Algorithms}
}

@book{Rasmussen:2006aa,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K I},
  year = {2006},
  publisher = {MIT Press},
  doi = {10.1.1.86.3414},
  keywords = {duplicate-citation-key,nosource}
}

@article{rasmussenBayesianInferenceHawkes2013,
  title = {Bayesian {{Inference}} for {{Hawkes Processes}}},
  author = {Rasmussen, Jakob Gulddahl},
  year = {2013},
  month = sep,
  journal = {Methodology and Computing in Applied Probability},
  volume = {15},
  number = {3},
  pages = {623--642},
  issn = {1573-7713},
  doi = {10.1007/s11009-011-9272-5},
  urldate = {2024-11-12},
  abstract = {The Hawkes process is a practically and theoretically important class of point processes, but parameter-estimation for such a process can pose various problems. In this paper we explore and compare two approaches to Bayesian inference. The first approach is based on the so-called conditional intensity function, while the second approach is based on an underlying clustering and branching structure in the Hawkes process. For practical use, MCMC (Markov chain Monte Carlo) methods are employed. The two approaches are compared numerically using three examples of the Hawkes process.},
  langid = {english},
  keywords = {60G55,Bayesian inference,Cluster process,Hawkes process,Markov chain Monte Carlo,Missing data,Point process},
  file = {/home/gkonkamking/pCloudDrive/papers/Rasmussen - 2013 - Bayesian Inference for Hawkes Processes.pdf}
}

@article{Rastelli2018,
  title = {Optimal {{Bayesian}} Estimators for Latent Variable Cluster Models},
  author = {Rastelli, Riccardo and Friel, Nial},
  year = {2018},
  month = nov,
  journal = {Statistics and Computing},
  volume = {28},
  number = {6},
  pages = {1169--1186},
  issn = {1573-1375},
  doi = {10.1007/s11222-017-9786-y},
  abstract = {In cluster analysis interest lies in probabilistically capturing partitions of individuals, items or observations into groups, such that those belonging to the same group share similar attributes or relational profiles. Bayesian posterior samples for the latent allocation variables can be effectively obtained in a wide range of clustering models, including finite mixtures, infinite mixtures, hidden Markov models and block models for networks. However, due to the categorical nature of the clustering variables and the lack of scalable algorithms, summary tools that can interpret such samples are not available. We adopt a Bayesian decision theoretical approach to define an optimality criterion for clusterings and propose a fast and context-independent greedy algorithm to find the best allocations. One important facet of our approach is that the optimal number of groups is automatically selected, thereby solving the clustering and the model-choice problems at the same time. We consider several loss functions to compare partitions and show that our approach can accommodate a wide range of cases. Finally, we illustrate our approach on both artificial and real datasets for three different clustering models: Gaussian mixtures, stochastic block models and latent block models for networks.},
  file = {/home/gkonkamking/Zotero/storage/LM2BRJVY/Rastelli and Friel - 2018 - Optimal Bayesian estimators for latent variable cl.pdf}
}

@inproceedings{raySpikeSlabVariational2020,
  title = {Spike and Slab Variational {{Bayes}} for High Dimensional Logistic Regression},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ray, Kolyan and Szabo, Botond and Clara, Gabriel},
  year = {2020},
  volume = {33},
  pages = {14423--14434},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-11-12},
  file = {/home/gkonkamking/pCloudDrive/papers/Ray et al. - 2020 - Spike and slab variational Bayes for high dimensional logistic regression.pdf}
}

@book{rcoreteam,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2015},
  address = {Vienna, Austria},
  organization = {R Foundation for Statistical Computing},
  keywords = {duplicate-citation-key,nosource}
}

@article{rcoreteam,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  booktitle = {Vienna, Austria: {{R}} Foundation for Statistical Computing},
  author = {{RCoreTeam}},
  year = {2012},
  pages = {R Foundation for Statistical Computing, Vienna, Au},
  address = {Vienna, Austria},
  institution = {R Foundation for Statistical Computing},
  isbn = {3-900051-07-0},
  keywords = {duplicate-citation-key,nosource}
}

@article{Ren2008,
  title = {The Dynamic Hierarchical {{Dirichlet}} Process},
  author = {Ren, Lu and Dunson, David B and Carin, Lawrence},
  year = {2008},
  journal = {Proceedings of the 25th International Conference on Machine Learning (2008)},
  number = {Icml},
  pages = {824--831},
  doi = {10.1145/1390156.1390260},
  abstract = {The dynamic hierarchical Dirichlet process (dHDP) is developed to model the time-evolving statistical properties of sequential data sets. The data collected at any time point are represented via a mixture associ- ated with an appropriate underlying model, in the framework of HDP. The statistical properties of data collected at consecutive time points are linked via a random parameter that controls their probabilistic similarity. The sharing mechanisms of the time- evolving data are derived, and a relatively simple Markov Chain Monte Carlo sampler is developed. Experimental results are pre- sented to demonstrate the model.},
  isbn = {9781605582054},
  keywords = {nosource}
}

@article{renBayesianVaryingeffectsVector2024,
  title = {Bayesian Varying-Effects Vector Autoregressive Models for Inference of Brain Connectivity Networks and Covariate Effects in Pediatric Traumatic Brain Injury},
  author = {Ren, Yangfan and Osborne, Nathan and Peterson, Christine B. and DeMaster, Dana M. and {Ewing-Cobbs}, Linda and Vannucci, Marina},
  year = {2024},
  journal = {Human Brain Mapping},
  volume = {45},
  number = {10},
  pages = {e26763},
  issn = {1097-0193},
  doi = {10.1002/hbm.26763},
  urldate = {2024-10-07},
  abstract = {In this article, we develop an analytical approach for estimating brain connectivity networks that accounts for subject heterogeneity. More specifically, we consider a novel extension of a multi-subject Bayesian vector autoregressive model that estimates group-specific directed brain connectivity networks and accounts for the effects of covariates on the network edges. We adopt a flexible approach, allowing for (possibly) nonlinear effects of the covariates on edge strength via a novel Bayesian nonparametric prior that employs a weighted mixture of Gaussian processes. For posterior inference, we achieve computational scalability by implementing a variational Bayes scheme. Our approach enables simultaneous estimation of group-specific networks and selection of relevant covariate effects. We show improved performance over competing two-stage approaches on simulated data. We apply our method on resting-state functional magnetic resonance imaging data from children with a history of traumatic brain injury (TBI) and healthy controls to estimate the effects of age and sex on the group-level connectivities. Our results highlight differences in the distribution of parent nodes. They also suggest alteration in the relation of age, with peak edge strength in children with TBI, and differences in effective connectivity strength between males and females.},
  copyright = {{\copyright} 2024 The Author(s). Human Brain Mapping published by Wiley Periodicals LLC.},
  langid = {english},
  keywords = {brain connectivity,fMRI data,Gaussian process,spike-and-slab prior,traumatic brain injury,variational inference},
  file = {/home/gkonkamking/Zotero/storage/QGAITCWP/Ren et al. - 2024 - Bayesian varying-effects vector autoregressive models for inference of brain connectivity networks a.pdf}
}

@article{reynoldsContextualEffectsReading2008,
  title = {Contextual Effects on Reading Aloud: {{Evidence}} for Pathway Control},
  shorttitle = {Contextual Effects on Reading Aloud},
  author = {Reynolds, Michael and Besner, Derek},
  year = {2008},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {34},
  number = {1},
  pages = {50--64},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1285(Electronic),0278-7393(Print)},
  doi = {10.1037/0278-7393.34.1.50},
  abstract = {Recent evidence suggests that the processes responsible for generating a phonological code from print are flexible in skilled readers. An important goal, therefore, is to identify the conditions that lead to changes in how a phonological code is computed. Five experiments are reported that examine whether phonological processes change as predicted by the pathway control hypothesis when reading aloud words and nonwords. Changes in reading processes were assessed by measuring the effect of predictable switches between stimulus categories across trials. The results of the present experiments are argued to be consistent with the pathway control hypothesis. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Ability,Neural Pathways,nosource,Oral Reading,Phonology,Theories}
}

@article{rheeUnbiasedEstimationSquare2015,
  title = {Unbiased {{Estimation}} with {{Square Root Convergence}} for {{SDE Models}}},
  author = {Rhee, Chang-Han and Glynn, Peter W.},
  year = {2015},
  month = oct,
  journal = {Operations Research},
  volume = {63},
  number = {5},
  pages = {1026--1043},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.2015.1404},
  urldate = {2021-07-05},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/EUISIMDW/Rhee and Glynn - 2015 - Unbiased Estimation with Square Root Convergence f.pdf}
}

@article{riabizOptimalThinningMCMC2020,
  ids = {riabizOptimalThinningMCMC2020a},
  title = {Optimal {{Thinning}} of {{MCMC Output}}},
  author = {Riabiz, Marina and Chen, Wilson and Cockayne, Jon and Swietach, Pawel and Niederer, Steven A. and Mackey, Lester and Oates, Chris J.},
  year = {2020},
  month = jun,
  journal = {arXiv:2005.03952 [math, stat]},
  eprint = {2005.03952},
  primaryclass = {math, stat},
  urldate = {2020-11-01},
  abstract = {The use of heuristics to assess the convergence and compress the output of Markov chain Monte Carlo can be sub-optimal in terms of the empirical approximations that are produced. Typically a number of the initial states are attributed to ``burn in'' and removed, whilst the remainder of the chain is ``thinned'' if compression is also required. In this paper we consider the problem of retrospectively selecting a subset of states, of fixed cardinality, from the sample path such that the approximation provided by their empirical distribution is close to optimal. A novel method is proposed, based on greedy minimisation of a kernel Stein discrepancy, that is suitable for problems where heavy compression is required. Theoretical results guarantee consistency of the method and its effectiveness is demonstrated in the challenging context of parameter inference for ordinary differential equations. Software is available in the Stein Thinning package in both Python and MATLAB.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/gkonkamking/Zotero/storage/TYLN8ILP/Riabiz et al. - 2020 - Optimal Thinning of MCMC Output.pdf}
}

@article{Richardson1997,
  title = {On {{Bayesian}} Analysis of Mixtures with an Unknown Number of Components - {{Discussion}}},
  author = {Richardson, Sylvia and Green, Peter J.},
  year = {1997},
  journal = {Journal of the Royal Statistical Society, Series B (Statistical Methodology)},
  number = {4},
  pages = {731--792},
  issn = {1369-7412},
  doi = {10.1111/1467-9868.00095},
  abstract = {New methodology for fully Bayesian mixture analysis is developed, making use of reversible jump Markov chain Monte Carlo methods that are capable of jumping between the parameter subspaces corresponding to different numbers of components in the mixture. A sample from the full joint distribution of all unknown variables is thereby generated, and this can be used as a basis for a thorough presentation of many aspects of the posterior distribution. The methodology is applied here to the analysis of univariate normal mixtures, using a hierarchical prior model that offers an approach to dealing with weak prior information while avoiding the mathematical pitfalls of using improper priors in the mixture context.},
  arxiv = {1467-9868.00095 [10.1111]},
  arxivid = {10.1111/1467-9868.00095},
  isbn = {1467-9868},
  keywords = {QA273 Probabilities,QA276 Mathematical statistics},
  file = {/home/gkonkamking/Zotero/storage/L9HA6CZ8/Richardson and Green - 1997 - On Bayesian analysis of mixtures with an unknown n.pdf}
}

@incollection{richardsonBayesianModelsSparse2011,
  title = {Bayesian {{Models}} for {{Sparse Regression Analysis}} of {{High Dimensional Data}}*},
  booktitle = {Bayesian {{Statistics}} 9},
  author = {Richardson, Sylvia and Bottolo, Leonardo and Rosenthal, Jeffrey S.},
  editor = {Bernardo, Jos{\'e} M. and Bayarri, M. J. and Berger, James O. and Dawid, A. P. and Heckerman, David and Smith, Adrian F. M. and West, Mike},
  year = {2011},
  month = oct,
  pages = {539--568},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780199694587.003.0018},
  urldate = {2022-03-09},
  abstract = {This paper considers the task of building efficient regression models for sparse multivariate analysis of high dimensional data sets, in particular it focuses on cases where the numbers q of responses Y = (y , 1 {$\leq$} k {$\leq$} q) and p of k predictors X = (xj , 1 {$\leq$} j {$\leq$} p) to analyse jointly are both large with respect to the sample size n, a challenging bi-directional task. The analysis of such data sets arise commonly in genetical genomics, with X linked to the DNA characteristics and Y corresponding to measurements of fundamental biological processes such as transcription, protein or metabolite production. Building on the Bayesian variable selection set-up for the linear model and associated efficient MCMC algorithms developed for single responses, we discuss the generic framework of hierarchical related sparse regressions, where parallel regressions of y on the set of covariates X are linked in a hierarchical k fashion, in particular through the prior model of the variable selection indicators {$\gamma$}kj , which indicate among the covariates xj those which are associated to the response y in each multivariate regression. Structures for the joint model k of the {$\gamma$}kj , which correspond to different compromises between the aims of controlling sparsity and that of enhancing the detection of predictors that are associated with many responses (`hot spots'), will be discussed and a new multiplicative model for the probability structure of the {$\gamma$}kj will be presented. To perform inference for these models in high dimensional set-ups, novel adaptive MCMC algorithms are needed. As sparsity is paramount and most of the associations expected to be zero, new algorithms that progressively focus on part of the space where the most interesting associations occur are of great interest. We shall discuss their formulation and theoretical properties, and demonstrate their use on simulated and real data from genomics.},
  isbn = {978-0-19-969458-7},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/DP3QAYMD/Richardson et al. - 2011 - Bayesian Models for Sparse Regression Analysis of .pdf}
}

@article{richesonThreatAppearingPrejudiced2008,
  title = {The {{Threat}} of {{Appearing Prejudiced}} and {{Race-Based Attentional Biases}}:},
  shorttitle = {The {{Threat}} of {{Appearing Prejudiced}} and {{Race-Based Attentional Biases}}},
  author = {Richeson, Jennifer A. and Trawalter, Sophie},
  year = {2008},
  month = feb,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {1467-9280},
  urldate = {2020-07-08},
  abstract = {The current work tested whether external motivation to respond without prejudice toward Blacks is associated with biased patterns of selective attention that re...},
  langid = {english},
  keywords = {nosource}
}

@article{Ricketts1999,
  title = {A Five-Parameter Logistic Equation for Investigating Asymmetry of Curvature in Baroreflex Studies.},
  author = {Ricketts, J H and a Head, G},
  year = {1999},
  journal = {The American journal of physiology},
  volume = {277},
  number = {2 Pt 2},
  eprint = {10444551},
  eprinttype = {pubmed},
  pages = {R441-54},
  issn = {0002-9513},
  abstract = {Baroreceptor reflex curves are usually analyzed using a symmetric four-parameter function. We wished to ascertain the validity of assuming symmetry in the baroreflex curve and also of constraining the curves to pass through the resting blood pressure and heart rate (HR) values. Therefore, we have investigated the suitability of a new five-parameter asymmetric logistic model for analysis of baroreflex curves from rabbits and dogs. The five-parameter model is an extension of the usual four-parameter model and reduces to that model when the fitted data are symmetrical. Using 30 data sets of blood pressure versus renal sympathetic nerve activity (RSNA) and HR from six conscious rabbits, we compared the five-parameter curves with the four-parameter model. We also tested the effect of forcing these baroreflex curves through the resting point. We found that the five-parameter model reduced the unexplained variation and gave small but important improvements to the estimates of plateaus for RSNA and HR and the HR gain. Although forcing the HR curves through the resting values had little effect, this procedure, when applied to RSNA, produced a worse curve fit by increasing the unexplained variation with alteration to most of the estimated curve parameters. The mean arterial pressure-HR baroreflex relationship from six conscious dogs was also analyzed and showed clear evidence of systematic asymmetry. We conclude that the asymmetric model is a valuable extension to the symmetric logistic model when examining baroreceptor reflexes, giving improved estimates of the parameters and a new approach to examining the mechanisms contributing to baroreflex curve asymmetry. Furthermore, forcing the curves through the resting value is a statistically questionable practice when analyzing RSNA, because it affects the parameter estimates.},
  isbn = {0363-6119},
  pmid = {10444551},
  keywords = {Animals,Baroreflex,Baroreflex: physiology,Blood Pressure,Blood Pressure: physiology,Cardiovascular,Dogs,Female,Heart Rate,Heart Rate: physiology,Kidney,Kidney: innervation,Male,Models,Neurological,nosource,Rabbits,Sympathetic Nervous System,Sympathetic Nervous System: physiology}
}

@article{ridoutGeneratingRandomNumbers2008,
  title = {Generating Random Numbers from a Distribution Specified by~Its~{{Laplace}} Transform},
  author = {Ridout, M. S.},
  year = {2008},
  month = oct,
  journal = {Statistics and Computing},
  volume = {19},
  number = {4},
  pages = {439},
  issn = {1573-1375},
  doi = {10.1007/s11222-008-9103-x},
  urldate = {2022-04-05},
  abstract = {This paper discusses simulation from an absolutely continuous distribution on the positive real line when the Laplace transform of the distribution is known but its density and distribution functions may not be available. We advocate simulation by the inversion method using a modified Newton-Raphson method, with values of the distribution and density functions obtained by numerical transform inversion. We show that this algorithm performs well in a series of increasingly complex examples. Caution is needed in some situations when the numerical Laplace transform inversion becomes unreliable. In particular the algorithm should not be used for distributions with finite range. But otherwise, except for rather pathological distributions, the approach offers a rapid way of generating random samples with minimal user effort. We contrast our approach with an alternative algorithm due to Devroye (Comput. Math. Appl. 7, 547--552, 1981).},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/X6N38FL4/Ridout - 2008 - Generating random numbers from a distribution spec.pdf}
}

@article{risenWhyPeopleAre2008,
  ids = {risenWhyPeopleAre2008a},
  title = {Why People Are Reluctant to Tempt Fate},
  author = {Risen, Jane L. and Gilovich, Thomas},
  year = {2008},
  journal = {Journal of Personality and Social Psychology},
  volume = {95},
  number = {2},
  pages = {293--307},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1315(Electronic),0022-3514(Print)},
  doi = {10.1037/0022-3514.95.2.293},
  abstract = {The present research explored the belief that it is bad luck to "tempt fate." Studies 1 and 2 demonstrated that people do indeed have the intuition that actions that tempt fate increase the likelihood of negative outcomes. Studies 3-6 examined our claim that the intuition is due, in large part, to the combination of the automatic tendencies to attend to negative prospects and to use accessibility as a cue when judging likelihood. Study 3 demonstrated that negative outcomes are more accessible following actions that tempt fate than following actions that do not tempt fate. Studies 4 and 5 demonstrated that the heightened accessibility of negative outcomes mediates the elevated perceptions of likelihood. Finally, Study 6 examined the automatic nature of the underlying processes. The types of actions that are thought to tempt fate as well as the role of society and culture in shaping this magical belief are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Intuition,Magical Thinking,Negativism,Sociocultural Factors}
}

@article{Ritz2005,
  title = {Bioassay Analysis Using r},
  author = {Ritz, Christian and Streibig, Jens C and Ritz, C. and Streibig, J. C.},
  year = {2005},
  journal = {Journal of Statistical Software},
  volume = {12},
  number = {5},
  pages = {1--22},
  issn = {15487660},
  doi = {10.18637/jss.v012.i05},
  abstract = {We describe an add-on package for the language and environment R which allows simultaneous fitting of several non-linear regression models. The focus is on analysis of dose response curves, but the functionality is applicable to arbitrary non-linear regression models. Features of the package is illustrated in examples},
  isbn = {1548-7660},
  keywords = {dose response data,multiple curves,non-linear regression,nosource}
}

@article{Ritz2010,
  title = {Toward a Unified Approach to Dose-Response Modeling in Ecotoxicology},
  author = {Ritz, Christian},
  year = {2010},
  journal = {Environmental Toxicology and Chemistry},
  volume = {29},
  number = {1},
  pages = {220--229},
  issn = {15528618},
  doi = {10.1002/etc.7},
  abstract = {This study reviews dose-response models that are used in ecotoxicology. The focus lies on clarification of differences and similarities between models, and as a side effect, their different guises in ecotoxicology are unravelled. A look at frequently used dose-response models reveals major discrepancies, among other things in naming conventions. Therefore, there is a need for a unified view on dose-response modeling in order to improve the understanding of it and to facilitate communication and comparison of findings across studies, thus realizing its full potential. This study attempts to establish a general framework that encompasses most dose-response models that are of interest to ecotoxicologists in practice. The framework includes commonly used models such as the log-logistic and Weibull models, but also features entire suites of models as found in various guidance documents. An outline on how the proposed framework can be implemented in statistical software systems is also provided.},
  isbn = {0730-7268},
  pmid = {20821438},
  keywords = {Log-logistic models,Logarithm-transformed doses,nosource,Open source statistical software,Parameterizations,Weibull models}
}

@article{ritzDoseResponseAnalysisUsing2015,
  title = {Dose-{{Response Analysis Using R}}},
  author = {Ritz, Christian and Baty, Florent and Streibig, Jens C. and Gerhard, Daniel},
  year = {2015},
  month = dec,
  journal = {PLOS ONE},
  volume = {10},
  number = {12},
  pages = {e0146021},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0146021},
  urldate = {2021-05-25},
  abstract = {Dose-response analysis can be carried out using multi-purpose commercial statistical software, but except for a few special cases the analysis easily becomes cumbersome as relevant, non-standard output requires manual programming. The extension package drc for the statistical environment R provides a flexible and versatile infrastructure for dose-response analyses in general. The present version of the package, reflecting extensions and modifications over the last decade, provides a user-friendly interface to specify the model assumptions about the dose-response relationship and comes with a number of extractors for summarizing fitted models and carrying out inference on derived parameters. The aim of the present paper is to provide an overview of state-of-the-art dose-response analysis, both in terms of general concepts that have evolved and matured over the years and by means of concrete examples.},
  langid = {english},
  keywords = {Binomials,Computer software,Curve fitting,Dose prediction methods,Log dose-response method,Nonlinear least squares method,Normal distribution,User interfaces},
  file = {/home/gkonkamking/Zotero/storage/M7VSG8ML/Ritz et al. - 2015 - Dose-Response Analysis Using R.pdf}
}

@article{RIVM2005,
  title = {The {{RIVM etoxBase}}. {{Database}} for Ecotoxicological Risk Assessment.},
  author = {{RIVM}},
  year = {2005},
  publisher = {{National Institute of Public Health and the Environment}},
  address = {Bilthoven, The Netherlands},
  keywords = {nosource}
}

@article{Rivoirard:2009p6204,
  title = {Bernstein-von Mises Theorem for Linear Functionals of the Density},
  author = {Rivoirard, Vincent and Rousseau, Judith},
  year = {2012},
  journal = {Annals of Statistics},
  volume = {40},
  number = {3},
  eprint = {0908.4167},
  pages = {1489--1523},
  issn = {00905364},
  doi = {10.1214/12-AOS1004},
  abstract = {In this paper, we study the asymptotic posterior distribution of linear functionals of the density. In particular, we give general conditions to obtain a semiparametric version of the Bernstein-Von Mises theorem. We then apply this general result to nonparametric priors based on infinite dimensional exponential families. As a byproduct, we also derive adaptive nonparametric rates of concentration of the posterior distributions under these families of priors on the class of Sobolev and Besov spaces.},
  archiveprefix = {arXiv},
  arxivid = {0908.4167},
  keywords = {Adaptive estimation,Bayesian nonparametric,Bernstein-von Mises,nosource,Rates of convergence}
}

@article{Rivoirard:2011,
  title = {Posterior Concentration Rates for Infinite Dimensional Exponential Families},
  author = {Rivoirard, Vincent and Rousseau, Judith},
  year = {2012},
  journal = {Bayesian Analysis},
  volume = {7},
  number = {2},
  pages = {311--334},
  issn = {19360975},
  doi = {10.1214/12-BA710},
  keywords = {Adaptive estimation,Bayesian non-parametric,nosource,Rates of convergence,Sobolev and Besov balls,Wavelets and Fourier Bases}
}

@article{rizziShorttermForecastsExpected2021,
  title = {Short-Term Forecasts of Expected Deaths},
  author = {Rizzi, Silvia and Vaupel, James W.},
  year = {2021},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {15},
  pages = {e2025324118},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2025324118},
  urldate = {2021-03-30},
  abstract = {We introduce a method for making short-term mortality forecasts of a few months, illustrating it by estimating how many deaths might have happened if some major shock had not occurred. We apply the method to assess excess mortality from March to June 2020 in Denmark and Sweden as a result of the first wave of the coronavirus pandemic; associated policy interventions; and behavioral, healthcare, social, and economic changes. We chose to compare Denmark and Sweden because reliable data were available and because the two countries are similar but chose different responses to COVID-19: Denmark imposed a rather severe lockdown; Sweden did not. We make forecasts by age and sex to predict expected deaths if COVID-19 had not struck. Subtracting these forecasts from observed deaths gives the excess death count. Excess deaths were lower in Denmark than Sweden during the first wave of the pandemic. The later/earlier ratio we propose for shortcasting is easy to understand, requires less data than more elaborate approaches, and may be useful in many countries in making both predictions about the future and the past to study the impact on mortality of coronavirus and other epidemics. In the application to Denmark and Sweden, prediction intervals are narrower and bias is less than when forecasts are based on averages of the last 5 y, as is often done. More generally, later/earlier ratios may prove useful in short-term forecasting of illnesses and births as well as economic and other activity that varies seasonally or periodically.},
  langid = {english}
}

@article{rlp2003,
  title = {Distributional Results for Means of Normalized Random Measures with Independent Increments},
  author = {Regazzini, Eugenio and Lijoi, Antonio and Pr{\"u}nster, Igor},
  year = {2003},
  journal = {Annals of Statistics},
  volume = {31},
  number = {2},
  pages = {560--585},
  publisher = {Institute of Mathematical Statistics},
  issn = {00905364},
  doi = {10.1214/aos/1051027881},
  abstract = {We consider the problem of determining the distribution of means of random probability measures which are obtained by normalizing increasing additive processes. A solution is found by resorting to a well-known inversion formula for characteristic functions due to Gurland. Moreover, expressions of the posterior distributions of those means, in the presence of exchangeable observations, are given. Finally, a section is devoted to the illustration of two examples of statistical relevance.},
  keywords = {(normalized) random measure with independent incre,Dirichlet process,Distribution of means of random probability measur,Increasing additive processes,L??vy measure,nosource}
}

@incollection{robert2007bayesian,
  title = {The {{Bayesian}} Choice: From Decision-Theoretic Foundations to Computational Implementation},
  booktitle = {Elements},
  author = {Robert, Christian P},
  year = {2007},
  volume = {91},
  pages = {602},
  publisher = {Springer Verlag},
  issn = {01621459},
  doi = {10.1007/0-387-71599-1},
  abstract = {This graduate-level textbook presents an introduction to Bayesian statistics and decision theory. Its scope covers both the basic ideas of statistical theory, and also some of the more modern and advanced topics of Bayesian statistics such as complete class theorems, the Stein effect, Bayesian model choice, hierarchical and empirical Bayes modeling, Monte Carlo integration, including Gibbs sampling and other MCMC techniques. The second edition includes a new chapter on model choice (Chapter 7) and the chapter on Bayesian calculations (6) has been extensively revised. Chapter 4 includes a new section on dynamic models. In Chapter 3, the material on noninformative priors has been expanded, and Chapter 10 has been supplemented with more examples. The Bayesian Choice will be suitable as a text for courses on Bayesian analysis, decision theory or a combination of them.},
  isbn = {978-0-387-71598-8},
  pmid = {14802466},
  keywords = {nosource}
}

@article{robertsCouplingErgodicityAdaptive2007,
  title = {Coupling and {{Ergodicity}} of {{Adaptive Markov Chain Monte Carlo Algorithms}}},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {2007},
  month = jun,
  journal = {Journal of Applied Probability},
  volume = {44},
  number = {2},
  pages = {458--475},
  issn = {0021-9002, 1475-6072},
  doi = {10.1239/jap/1183667414},
  urldate = {2020-11-24},
  abstract = {We consider basic ergodicity properties of adaptive Markov chain Monte Carlo algorithms under minimal assumptions, using coupling constructions. We prove convergence in distribution and a weak law of large numbers. We also give counterexamples to demonstrate that the assumptions we make are not redundant.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/BVP2VUVR/Roberts and Rosenthal - 2007 - Coupling and Ergodicity of Adaptive Markov Chain M.pdf}
}

@article{robertsExamplesAdaptiveMCMC2009,
  title = {Examples of {{Adaptive MCMC}}},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {2009},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {18},
  number = {2},
  pages = {349--367},
  publisher = {Taylor \& Francis},
  issn = {1061-8600},
  doi = {10.1198/jcgs.2009.06134},
  urldate = {2020-10-26},
  abstract = {We investigate the use of adaptive MCMC algorithms to automatically tune the Markov chain parameters during a run. Examples include the Adaptive Metropolis (AM) multivariate algorithm of Haario, Saksman, and Tamminen (2001), Metropolis-within-Gibbs algorithms for nonconjugate hierarchical models, regionally adjusted Metropolis algorithms, and logarithmic scalings. Computer simulations indicate that the algorithms perform very well compared to nonadaptive algorithms, even in high dimension.},
  file = {/home/gkonkamking/pCloudDrive/papers/Roberts and Rosenthal - 2009 - Examples of Adaptive MCMC.pdf}
}

@article{robertsOptimalScalingVarious2001,
  title = {Optimal {{Scaling}} for {{Various Metropolis-Hastings Algorithms}}},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {2001},
  journal = {Statistical Science},
  volume = {16},
  number = {4},
  eprint = {3182776},
  eprinttype = {jstor},
  pages = {351--367},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  urldate = {2020-10-26},
  abstract = {We review and extend results related to optimal scaling of Metropolis-Hastings algorithms. We present various theoretical results for the high-dimensional limit. We also present simulation studies which confirm the theoretical results in finite-dimensional contexts.},
  file = {/home/gkonkamking/Downloads/Roberts and Rosenthal - 2001 - Optimal Scaling for Various Metropolis-Hastings Al.pdf}
}

@article{Robin2010,
  title = {Uncovering Latent Structure in Valued Graphs : {{A}} Variational Approach},
  author = {Mariadassou, Mahendra and Robin, St{\'e}phane and Vacher, Corinne},
  year = {2010},
  journal = {The Annals of Applied Statistics},
  volume = {4},
  number = {2},
  pages = {715--742},
  issn = {1932-6157},
  doi = {10.1214/10-AOAS361},
  keywords = {nosource}
}

@article{Robinson2003,
  title = {Protein Evolution with Dependence among Codons Due to Tertiary Structure},
  author = {Robinson, Douglas M. and Jones, David T. and Kishino, Hirohisa and Goldman, Nick and Thorne, Jeffrey L.},
  year = {2003},
  journal = {Molecular Biology and Evolution},
  volume = {20},
  number = {10},
  pages = {1692--1704},
  issn = {07374038},
  doi = {10.1093/molbev/msg184},
  abstract = {Markovian models of protein evolution that relax the assumption of independent change among codons are considered. With this comparatively realistic framework, an evolutionary rate at a site can depend both on the state of the site and on the states of surrounding sites. By allowing a relatively general dependence structure among sites, models of evolution can reflect attributes of tertiary structure. To quantify the impact of protein structure on protein evolution, we analyze protein-coding DNA sequence pairs with an evolutionary model that incorporates effects of solvent accessibility and pairwise interactions among amino acid residues. By explicitly considering the relationship between nonsynonymous substitution rates and protein structure, this approach can lead to refined detection and characterization of positive selection. Analyses of simulated sequence pairs indicate that parameters in this evolutionary model can be well estimated. Analyses of lysozyme c and annexin V sequence pairs yield the biologically reasonable result that amino acid replacement rates are higher when the replacements lead to energetically favorable proteins than when they destabilize the proteins. Although the focus here is evolutionary dependence among codons that is associated with protein structure, the statistical approach is quite general and could be applied to diverse cases of evolutionary dependence where surrogates for sequence fitness can be measured or modeled.},
  isbn = {0737-4038},
  pmid = {12885968},
  keywords = {Bayesian,Evolution,Markov chain Monte Carlo,nosource,Protein structure}
}

@article{rockovaBayesianEstimationSparse2018,
  title = {Bayesian Estimation of Sparse Signals with a Continuous Spike-and-Slab Prior},
  author = {Ro{\v c}kov{\'a}, Veronika},
  year = {2018},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {46},
  number = {1},
  pages = {401--437},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/17-AOS1554},
  urldate = {2022-03-08},
  abstract = {We introduce a new framework for estimation of sparse normal means, bridging the gap between popular frequentist strategies (LASSO) and popular Bayesian strategies (spike-and-slab). The main thrust of this paper is to introduce the family of Spike-and-Slab LASSO (SS-LASSO) priors, which form a continuum between the Laplace prior and the point-mass spike-and-slab prior. We establish several appealing frequentist properties of SS-LASSO priors, contrasting them with these two limiting cases. First, we adopt the penalized likelihood perspective on Bayesian modal estimation and introduce the framework of Bayesian penalty mixing with spike-and-slab priors. We show that the SS-LASSO global posterior mode is (near) minimax rate-optimal under squared error loss, similarly as the LASSO. Going further, we introduce an adaptive two-step estimator which can achieve provably sharper performance than the LASSO. Second, we show that the whole posterior keeps pace with the global mode and concentrates at the (near) minimax rate, a property that is known {\textbackslash}textsl\{not to hold\} for the single Laplace prior. The minimax-rate optimality is obtained with a suitable class of independent product priors (for known levels of sparsity) as well as with dependent mixing priors (adapting to the unknown levels of sparsity). Up to now, the rate-optimal posterior concentration has been established only for spike-and-slab priors with a point mass at zero. Thus, the SS-LASSO priors, despite being continuous, possess similar optimality properties as the ``theoretically ideal'' point-mass mixtures. These results provide valuable theoretical justification for our proposed class of priors, underpinning their intuitive appeal and practical potential.},
  keywords = {62F15,62J99,asymptotic minimaxity,Lasso,posterior concentration,spike-and-slab},
  file = {/home/gkonkamking/Zotero/storage/4P5GCW98/Ročková - 2018 - Bayesian estimation of sparse signals with a conti.pdf}
}

@article{rockovaDynamicVariableSelection2021,
  title = {Dynamic {{Variable Selection}} with {{Spike-and-Slab Process Priors}}},
  author = {Rockova, Veronika and McAlinn, Kenichiro},
  year = {2021},
  month = jan,
  journal = {Bayesian Analysis},
  volume = {16},
  number = {1},
  pages = {233--269},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1199},
  urldate = {2021-05-31},
  abstract = {We address the problem of dynamic variable selection in time series regression with unknown residual variances, where the set of active predictors is allowed to evolve over time. To capture time-varying variable selection uncertainty, we introduce new dynamic shrinkage priors for the time series of regression coefficients. These priors are characterized by two main ingredients: smooth parameter evolutions and intermittent zeroes for modeling predictive breaks. More formally, our proposed Dynamic Spike-and-Slab (DSS) priors are constructed as mixtures of two processes: a spike process for the irrelevant coefficients and a slab autoregressive process for the active coefficients. The mixing weights are themselves time-varying and depend on lagged values of the series. Our D S S priors are probabilistically coherent in the sense that their stationary distribution is fully known and characterized by spike-and-slab marginals. For posterior sampling over dynamic regression coefficients, model selection indicators as well as unknown dynamic residual variances, we propose a Dynamic SSVS algorithm based on forward-filtering and backward-sampling. To scale our method to large data sets, we develop a Dynamic EMVS algorithm for MAP smoothing. We demonstrate, through simulation and a topical macroeconomic dataset, that D S S priors are very effective at separating active and noisy coefficients. Our fast implementation significantly extends the reach of spike-and-slab methods to big time series data.},
  keywords = {Autoregressive mixture processes,Dynamic sparsity,MAP smoothing,spike and slab,stationarity},
  file = {/home/gkonkamking/Zotero/storage/CTQITCYQ/Rockova and McAlinn - 2021 - Dynamic Variable Selection with Spike-and-Slab Pro.pdf}
}

@article{rockovaEMVSEMApproach2014,
  title = {{{EMVS}}: {{The EM Approach}} to {{Bayesian Variable Selection}}},
  shorttitle = {{{EMVS}}},
  author = {Ro{\v c}kov{\'a}, Veronika and George, Edward I.},
  year = {2014},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {109},
  number = {506},
  pages = {828--846},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2013.869223},
  urldate = {2021-03-08},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Ročková_George_2014_EMVS.pdf}
}

@article{rockovaSpikeSlabLASSO2018,
  title = {The {{Spike-and-Slab LASSO}}},
  author = {Ro{\v c}kov{\'a}, Veronika and George, Edward I.},
  year = {2018},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {521},
  pages = {431--444},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1260469},
  urldate = {2021-05-31},
  abstract = {Despite the wide adoption of spike-and-slab methodology for Bayesian variable selection, its potential for penalized likelihood estimation has largely been overlooked. In this article, we bridge this gap by cross-fertilizing these two paradigms with the Spike-and-Slab LASSO procedure for variable selection and parameter estimation in linear regression. We introduce a new class of self-adaptive penalty functions that arise from a fully Bayes spike-and-slab formulation, ultimately moving beyond the separable penalty framework. A virtue of these nonseparable penalties is their ability to borrow strength across coordinates, adapt to ensemble sparsity information and exert multiplicity adjustment. The Spike-and-Slab LASSO procedure harvests efficient coordinate-wise implementations with a path-following scheme for dynamic posterior exploration. We show on simulated data that the fully Bayes penalty mimics oracle performance, providing a viable alternative to cross-validation. We develop theory for the separable and nonseparable variants of the penalty, showing rate-optimality of the global mode as well as optimal posterior concentration when p {$>$} n. Supplementary materials for this article are available online.},
  keywords = {High-dimensional regression,LASSO,Penalized likelihood,Posterior concentration,Spike-and-Slab,Variable selection},
  file = {/home/gkonkamking/Zotero/storage/3GBL537S/Ročková and George - 2018 - The Spike-and-Slab LASSO.pdf}
}

@article{rodea-palomaresHiddenDriversLowdose2016,
  title = {Hidden Drivers of Low-Dose Pharmaceutical Pollutant Mixtures Revealed by the Novel {{GSA-QHTS}} Screening Method},
  author = {{Rodea-Palomares}, Ismael and {Gonzalez-Pleiter}, Miguel and Gonzalo, Soledad and Rosal, Roberto and Leganes, Francisco and Sabater, Sergi and Casellas, Maria and {Mu{\~n}oz-Carpena}, Rafael and {Fern{\'a}ndez-Pi{\~n}as}, Francisca},
  year = {2016},
  month = sep,
  journal = {Science Advances},
  volume = {2},
  number = {9},
  pages = {e1601272},
  publisher = {American Association for the Advancement of Science},
  issn = {2375-2548},
  doi = {10.1126/sciadv.1601272},
  urldate = {2020-03-17},
  abstract = {The ecological impacts of emerging pollutants such as pharmaceuticals are not well understood. The lack of experimental approaches for the identification of pollutant effects in realistic settings (that is, low doses, complex mixtures, and variable environmental conditions) supports the widespread perception that these effects are often unpredictable. To address this, we developed a novel screening method (GSA-QHTS) that couples the computational power of global sensitivity analysis (GSA) with the experimental efficiency of quantitative high-throughput screening (QHTS). We present a case study where GSA-QHTS allowed for the identification of the main pharmaceutical pollutants (and their interactions), driving biological effects of low-dose complex mixtures at the microbial population level. The QHTS experiments involved the integrated analysis of nearly 2700 observations from an array of 180 unique low-dose mixtures, representing the most complex and data-rich experimental mixture effect assessment of main pharmaceutical pollutants to date. An ecological scaling-up experiment confirmed that this subset of pollutants also affects typical freshwater microbial community assemblages. Contrary to our expectations and challenging established scientific opinion, the bioactivity of the mixtures was not predicted by the null mixture models, and the main drivers that were identified by GSA-QHTS were overlooked by the current effect assessment scheme. Our results suggest that current chemical effect assessment methods overlook a substantial number of ecologically dangerous chemical pollutants and introduce a new operational framework for their systematic identification. Novel GSA-QHTS screens out drivers of ecological effects from low-dose pharmaceutical mixtures typically missed by current policy. Novel GSA-QHTS screens out drivers of ecological effects from low-dose pharmaceutical mixtures typically missed by current policy.},
  chapter = {Research Article},
  copyright = {Copyright {\copyright} 2016, The Authors. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  langid = {english},
  keywords = {nosource}
}

@article{RodneySIMooreD2008,
  title = {{{SSD Master Version}} 2.0. {{Determination}} of Hazardous Concentrations with Species Sensitivity Distributions. {{Intrinsik Inc}}.},
  author = {Rodney SI, Moore D, Teed RS.},
  year = {2008},
  keywords = {duplicate-citation-key,nosource}
}

@article{Rodrigue2009,
  title = {Computational Methods for Evaluating Phylogenetic Models of Coding Sequence Evolution with Dependence between Codons},
  author = {Rodrigue, Nicolas and Kleinman, Claudia L. and Philippe, Herv{\'e} and Lartillot, Nicolas},
  year = {2009},
  month = jul,
  journal = {Molecular Biology and Evolution},
  volume = {26},
  number = {7},
  pages = {1663--1676},
  issn = {07374038},
  doi = {10.1093/molbev/msp078},
  abstract = {In recent years, molecular evolutionary models formulated as site-interdependent Markovian codon substitution processes have been proposed as means of mechanistically accounting for selective features over long-range evolutionary scales. Under such models, site interdependencies are reflected in the use of a simplified protein tertiary structure representation and predefined statistical potential, which, along with mutational parameters, mediate nonsynonymous rates of substitution; rates of synonymous events are solely mediated by mutational parameters. Although theoretically attractive, the models are computationally challenging, and the methods used to manipulate them still do not allow for quantitative model evaluations in a multiple-sequence context. Here, we describe Markov chain Monte Carlo computational methodologies for sampling parameters from their posterior distribution under site-interdependent codon substitution models within a phylogenetic context and allowing for Bayesian model assessment and ranking. Specifically, the techniques we expound here can form the basis of posterior predictive checking under these models and can be embedded within thermodynamic integration algorithms for computing Bayes factors. We illustrate the methods using two data sets and find that although current forms of site-interdependent models of codon substitution provide an improved fit, they are outperformed by the extended site-independent versions. Altogether, the methodologies described here should enable a quantified contrasting of alternative ways of modeling structural constraints, or other site-interdependent criteria, and establish if such formulations can match (or supplant) site-independent model extensions.},
  isbn = {1537-1719 (Electronic){\textbackslash}n0737-4038 (Linking)},
  pmid = {19383983},
  keywords = {Auxiliary variables,Bayes factors,Data augmentation,Markov chain Monte Carlo,nosource,Posterior predictive checking,Protein tertiary structure},
  file = {/home/gkonkamking/Zotero/storage/PPEAY5HT/Rodrigue et al. - 2009 - Computational methods for evaluating phylogenetic .pdf}
}

@article{Rodrigue2010,
  title = {Mutation-Selection Models of Coding Sequence Evolution with Site-Heterogeneous Amino Acid Fitness Profiles},
  author = {Rodrigue, Nicolas and Philippe, Herv{\'e} and Lartillot, Nicolas},
  year = {2010},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {107},
  number = {10},
  pages = {4629--4634},
  issn = {0027-8424},
  doi = {10.1073/pnas.0910915107},
  abstract = {Modeling the interplay between mutation and selection at the molecular level is key to evolutionary studies. To this end, codon-based evolutionary models have been proposed as pertinent means of studying long-range evolutionary patterns and are widely used. However, these approaches have not yet consolidated results from amino acid level phylogenetic studies showing that selection acting on proteins displays strong site-specific effects, which translate into heterogeneous amino acid propensities across the columns of alignments; related codon-level studies have instead focused on either modeling a single selective context for all codon columns, or a separate selective context for each codon column, with the former strategy deemed too simplistic and the latter deemed overparameterized. Here, we integrate recent developments in nonparametric statistical approaches to propose a probabilistic model that accounts for the heterogeneity of amino acid fitness profiles across the coding positions of a gene. We apply the model to a dozen real protein-coding gene alignments and find it to produce biologically plausible inferences, for instance, as pertaining to site-specific amino acid constraints, as well as distributions of scaled selection coefficients. In their account of mutational features as well as the heterogeneous regimes of selection at the amino acid level, the modeling approaches studied here can form a backdrop for several extensions, accounting for other selective features, for variable population size, or for subtleties of mutational features, all with parameterizations couched within population-genetic theory.},
  isbn = {0027-8424},
  pmid = {20176949},
  keywords = {nosource},
  file = {/home/gkonkamking/Zotero/storage/P4NYH4NC/Rodrigue et al. - 2010 - Mutation-selection models of coding sequence evolu.pdf}
}

@article{Rodrigue2013,
  title = {On the Statistical Interpretation of Site-Specific Variables in Phylogeny-Based Substitution Models},
  author = {Rodrigue, Nicolas},
  year = {2013},
  journal = {Genetics},
  volume = {193},
  number = {2},
  pages = {557--564},
  issn = {00166731},
  doi = {10.1534/genetics.112.145722},
  abstract = {Phylogeny-based modeling of heterogeneity across the positions of multiple-sequence alignments has generally been approached from two main perspectives. The first treats site specificities as random variables drawn from a statistical law, and the likelihood function takes the form of an integral over this law. The second assigns distinct variables to each position, and, in a maximum-likelihood context, adjusts these variables, along with global parameters, to optimize a joint likelihood function. Here, it is emphasized that while the first approach directly enjoys the statistical guaranties of traditional likelihood theory, the latter does not, and should be approached with particular caution when the site-specific variables are high dimensional. Using a phylogeny-based mutation-selection framework, it is shown that the difference in interpretation of site-specific variables explains the incongruities in recent studies regarding distributions of selection coefficients.},
  pmid = {23222651},
  keywords = {infinitely-many-parameters trap,maximum likelihood,nosource,over-fitting,random variable}
}

@article{Rodrigue2014,
  title = {Site-Heterogeneous Mutation-Selection Models within the {{PhyloBayes-MPI}} Package},
  author = {Rodrigue, Nicolas and Lartillot, Nicolas},
  year = {2014},
  journal = {Bioinformatics},
  volume = {30},
  number = {7},
  pages = {1020--1021},
  issn = {14602059},
  doi = {10.1093/bioinformatics/btt729},
  abstract = {MOTIVATION: In recent years, there has been an increasing interest in the potential of codon substitution models for a variety of applications. However, the computational demands of these models have sometimes lead to the adoption of oversimplified assumptions, questionable statistical methods or a limited focus on small data sets.{\textbackslash}n{\textbackslash}nRESULTS: Here, we offer a scalable, message-passing-interface-based Bayesian implementation of site-heterogeneous codon models in the mutation-selection framework. Our software jointly infers the global mutational parameters at the nucleotide level, the branch lengths of the tree and a Dirichlet process governing across-site variation at the amino acid level. We focus on an example estimation of the distribution of selection coefficients from an alignment of several hundred sequences of the influenza PB2 gene, and highlight the site-specific characterization enabled by such a modeling approach. Finally, we discuss future potential applications of the software for conducting evolutionary inferences.Availability and implementation: The models are implemented within the PhyloBayes-MPI package, (available at phylobayes.org) along with usage details in the accompanying manual.{\textbackslash}n{\textbackslash}nCONTACT: nicolas.rodrigue@ucalgary.ca.},
  pmid = {24351710},
  keywords = {nosource}
}

@article{Rodriguez2008,
  title = {The Nested Dirichlet Process},
  author = {Rodr{\'i}guez, Abel and Dunson, David B. and Gelfand, Alan E.},
  year = {2008},
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {483},
  pages = {1131--1154},
  issn = {0162-1459},
  doi = {10.1198/016214508000000553},
  abstract = {In multicenter studies, subjects in different centers may have different outcome distri- butions. This article is motivated by the problem of nonparametric modeling of these distributions, borrowing information across centers while also allowing centers to be clustered. Starting with a stick- breaking representation of the Dirichlet process (DP), we replace the random atoms with random prob- ability measures drawn from a DP. This results in a nested Dirichlet process (nDP) prior, which can be placed on the collection of distributions for the different centers, with centers drawn from the same DP component automatically clustered together. Theoretical properties are discussed, and an efficient MCMC algorithm is developed for computation. The methods are illustrated using a simulation study and an application to quality of care in US hospitals.},
  keywords = {and phrases,bayes,clustering,dependent dirichlet process,gibbs sampler,hierarchical model,nonparametric,nosource,random probability measure}
}

@article{rodriguez2008bayesian,
  title = {Bayesian Dynamic Density Estimation},
  author = {Rodriguez, Abel and Ter Horst, Enrique},
  year = {2008},
  journal = {Bayesian Analysis},
  volume = {3},
  number = {2},
  pages = {339--365},
  publisher = {International Society for Bayesian Analysis},
  keywords = {nosource}
}

@article{rodriguez2010latent,
  title = {Latent Stick-Breaking Processes},
  author = {Rodr{\'i}guez, Abel and Dunson, David B. and Gelfand, Alan E.},
  year = {2010},
  journal = {Journal of the American Statistical Association},
  volume = {105},
  number = {490},
  keywords = {nosource}
}

@article{rodriguez2011nonparametric,
  title = {Nonparametric {{Bayesian}} Models through Probit Stick-Breaking Processes},
  author = {Rodriguez, Abel and Dunson, David B. and Rodr{\'i}guez, Abel and Dunson, David B. and Rodriguez, Abel and Dunson, David B.},
  year = {2011},
  journal = {Bayesian Analysis},
  volume = {6},
  number = {1},
  pages = {145--177},
  publisher = {International Society for Bayesian Analysis},
  issn = {19360975},
  doi = {10.1214/11-BA605},
  abstract = {We describe a novel class of Bayesian nonparametric priors based on stick-breaking constructions where the weights of the process are constructed as probit transformations of normal random variables. We show that these priors are extremely flexible, allowing us to generate a great variety of models while preserving computational simplicity. Particular emphasis is placed on the construction of rich temporal and spatial processes, which are applied to two problems in finance and ecology.},
  arxiv = {NIHMS150003},
  arxivid = {NIHMS150003},
  isbn = {2122633255},
  pmid = {24358072},
  keywords = {Data augmentation,Mixture model,Nonparametric bayes,nosource,Random probability measure,Spatial data,Stick-breaking prior,Time series}
}

@article{roseLessonBypassing2013,
  title = {The {{Lesson}} of {{Bypassing}}},
  author = {Rose, David and Nichols, Shaun},
  year = {2013},
  month = dec,
  journal = {Review of Philosophy and Psychology},
  volume = {4},
  number = {4},
  pages = {599--619},
  issn = {1878-5166},
  doi = {10.1007/s13164-013-0154-3},
  urldate = {2020-05-11},
  abstract = {The idea that incompatibilism is intuitive is one of the key motivators for incompatibilism. Not surprisingly, then philosophers who defend incompatibilism often claim that incompatibilism is the natural, commonsense view about free will and moral responsibility (e.g., Pereboom 2001, Kane Journal of Philosophy 96:217--240 1999, Strawson 1986). And a number of recent studies find that people give apparently incompatibilist responses in vignette studies. When participants are presented with a description of a causal deterministic universe, they tend to deny that people are morally responsible in that universe. Although this suggests that people are intuitive incompatibilists, Eddy Nahmias and Dylan Murray, in a recent series of important papers, have developed an important challenge to this interpretation. They argue that people confuse determinism with bypassing, the idea that one's mental states lack causal efficacy. Murray and Nahmias present new experiments that seem to confirm the bypassing hypothesis. In this paper, we use structural equation modeling to re-examine the issue. We find support instead for an incompatibilist explanation of the bypassing results, i.e., incompatibilist judgments seem to cause bypassing judgments. We hypothesize that this phenomenon occurs because people think of decisions as essentially indeterministic; thus, when confronted with a description of determinism they tend to think that decisions do not even occur. We provide evidence for this in three subsequent studies which show that many participants deny that people make decisions in a deterministic universe; by contrast, most participants tend to allow that people add numbers in a deterministic universe. Together, these studies suggest that bypassing results don't reflect a confusion, but rather the depth of the incompatibilist intuition.},
  langid = {english},
  keywords = {nosource}
}

@article{rosenblatt1956central,
  title = {A Central Limit Theorem and a Strong Mixing Condition},
  author = {Rosenblatt, M.},
  year = {1956},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {42},
  number = {1},
  pages = {43--47},
  publisher = {National Academy of Sciences},
  issn = {0027-8424},
  doi = {10.1073/pnas.42.1.43},
  keywords = {nosource}
}

@article{ross2002feels,
  title = {It Feels like Yesterday: {{Self-esteem}}, Valence of Personal Past Experiences, and Judgments of Subjective Distance.},
  author = {Ross, Michael and Wilson, Anne E},
  year = {2002},
  journal = {Journal of personality and social psychology},
  volume = {82},
  number = {5},
  pages = {792},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{ROSSINI2018,
  title = {Quantifying Prediction Uncertainty for Functional-and-Scalar to Functional Autoregressive Models under Shape Constraints},
  author = {Rossini, Jacopo and Canale, Antonio},
  year = {2018},
  journal = {Journal of Multivariate Analysis},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2018.10.007},
  keywords = {Demand and offer model,Functional bootstrap,Functional ridge regression,nosource}
}

@article{Rossini2018,
  title = {Quantifying Prediction Uncertainty for Functional and Scalar to Functional Autoregressive Models.},
  booktitle = {Journal of Multivariate Analysis},
  author = {Rossini, J and Canale, A},
  year = {2018},
  keywords = {nosource}
}

@article{rossParameterEstimationPopulation2006,
  title = {On Parameter Estimation in Population Models},
  author = {Ross, J. V. and Taimre, T. and Pollett, P. K.},
  year = {2006},
  month = dec,
  journal = {Theoretical Population Biology},
  volume = {70},
  number = {4},
  pages = {498--510},
  issn = {0040-5809},
  doi = {10.1016/j.tpb.2006.08.001},
  urldate = {2021-03-23},
  abstract = {We describe methods for estimating the parameters of Markovian population processes in continuous time, thus increasing their utility in modelling real biological systems. A general approach, applicable to any finite-state continuous-time Markovian model, is presented, and this is specialised to a computationally more efficient method applicable to a class of models called density-dependent Markov population processes. We illustrate the versatility of both approaches by estimating the parameters of the stochastic SIS logistic model from simulated data. This model is also fitted to data from a population of Bay checkerspot butterfly (Euphydryas editha bayensis), allowing us to assess the viability of this population.},
  langid = {english},
  keywords = {Cross-entropy method,Density dependence,Markov chains,Stochastic SIS logistic model}
}

@article{rossParameterEstimationPopulation2009,
  title = {On Parameter Estimation in Population Models {{II}}: {{Multi-dimensional}} Processes and Transient Dynamics},
  shorttitle = {On Parameter Estimation in Population Models {{II}}},
  author = {Ross, J.V. and Pagendam, D.E. and Pollett, P.K.},
  year = {2009},
  month = mar,
  journal = {Theoretical Population Biology},
  volume = {75},
  number = {2-3},
  pages = {123--132},
  issn = {00405809},
  doi = {10.1016/j.tpb.2008.12.002},
  urldate = {2021-03-23},
  langid = {english}
}

@article{rothPyCloneStatisticalInference2014,
  title = {{{PyClone}}: Statistical Inference of Clonal Population Structure in Cancer},
  shorttitle = {{{PyClone}}},
  author = {Roth, Andrew and Khattra, Jaswinder and Yap, Damian and Wan, Adrian and Laks, Emma and Biele, Justina and Ha, Gavin and Aparicio, Samuel and {Bouchard-C{\^o}t{\'e}}, Alexandre and Shah, Sohrab P.},
  year = {2014},
  month = apr,
  journal = {Nature Methods},
  volume = {11},
  number = {4},
  pages = {396--398},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/nmeth.2883},
  urldate = {2021-05-02},
  abstract = {The hierarchical Bayesian model identifies and quantifies clonal populations in tumors from deep-sequenced somatic mutations.},
  copyright = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/WIUY59FD/Roth et al. - 2014 - PyClone statistical inference of clonal populatio.pdf}
}

@article{rottenstreichMoneyKissesElectric2016,
  title = {Money, {{Kisses}}, and {{Electric Shocks}}: {{On}} the {{Affective Psychology}} of {{Risk}}:},
  shorttitle = {Money, {{Kisses}}, and {{Electric Shocks}}},
  author = {Rottenstreich, Yuval and Hsee, Christopher K.},
  year = {2016},
  month = may,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {10.1111/1467-9280.00334},
  urldate = {2020-06-09},
  abstract = {Prospect theory's S-shaped weighting function is often said to reflect the psychophysics of chance. We propose an affective rather than psychophysical deconstru...},
  langid = {english},
  keywords = {nosource}
}

@article{rousseau2010rates,
  title = {{{RATES}} of Convergence for the Posterior Distributions of Mixtures of Betas and Adaptive Nonparametric Estimation of the Density},
  author = {Rousseau, Judith},
  year = {2010},
  journal = {Annals of Statistics},
  volume = {38},
  number = {1},
  eprint = {1001.1615},
  pages = {146--180},
  publisher = {Institute of Mathematical Statistics},
  issn = {00905364},
  doi = {10.1214/09-AOS703},
  abstract = {In this paper, we investigate the asymptotic properties of nonparametric Bayesian mixtures of Betas for estimating a smooth density on \$[0,1]\$. We consider a parametrization of Beta distributions in terms of mean and scale parameters and construct a mixture of these Betas in the mean parameter, while putting a prior on this scaling parameter. We prove that such Bayesian nonparametric models have good frequentist asymptotic properties. We determine the posterior rate of concentration around the true density and prove that it is the minimax rate of concentration when the true density belongs to a H{\textbackslash}"\{o\}lder class with regularity \$\$, for all positive \$\$, leading to a minimax adaptive estimating procedure of the density. We also believe that the approximating results obtained on these mixtures of Beta densities can be of interest in a frequentist framework.},
  archiveprefix = {arXiv},
  arxivid = {1001.1615},
  keywords = {Adaptive estimation,Bayesian nonparametric,Kernel,Mixtures of Betas,nosource,Rates of convergence}
}

@article{rousseauAsymptoticBehaviourPosterior2011,
  title = {Asymptotic Behaviour of the Posterior Distribution in Overfitted Mixture Models},
  author = {Rousseau, Judith and Mengersen, Kerrie},
  year = {2011},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {5},
  pages = {689--710},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2011.00781.x},
  urldate = {2021-07-29},
  abstract = {Summary. We study the asymptotic behaviour of the posterior distribution in a mixture model when the number of components in the mixture is larger than the true number of components: a situation which is commonly referred to as an overfitted mixture. We prove in particular that quite generally the posterior distribution has a stable and interesting behaviour, since it tends to empty the extra components. This stability is achieved under some restriction on the prior, which can be used as a guideline for choosing the prior. Some simulations are presented to illustrate this behaviour.},
  langid = {english},
  keywords = {Asymptotic behaviour,Bayesian methods,Mixture models,Overfitting,Posterior concentration},
  file = {/home/gkonkamking/pCloudDrive/papers/Rousseau_Mengersen_2011_Asymptotic behaviour of the posterior distribution in overfitted mixture models.pdf}
}

@article{Roux1996,
  title = {Substance-Specific Water Quality Criteria for the Protection of {{South African}} Freshwater Ecosystems: {{Methods}} for Derivation and Initial Results for Some Inorganic Toxic Substances},
  author = {Roux, D. J. and Jooste, S. H J and MacKay, H. M.},
  year = {1996},
  journal = {South African Journal of Science},
  volume = {92},
  number = {4},
  pages = {198--206},
  issn = {00382353},
  abstract = {Freshwater ecosystems form the resource base on which water users, such as the agricultural, recreational, domestic and industrial sectors, depend. These essential resource therefore need to be protected and maintained in a healthy state. The Department of Water Affairs and Forestry is currently developing water quality criteria for the protection of South African freshwater ecosystems, to complement the existing National Water Quality Guidelines for domestic, industrial, agricultural and recreational use. This paper describes the methodology for the derivation of in-stream water quality criteria for inorganic toxic substances. Criteria are calculated from the results of acute and chronic toxicity tests on a number of representative species, using local data where available, and relying on international databases to supplement local information. Conservative numerical criteria are provided for aluminium, ammonia, arsenic, boron, cadmium, chlorine, chromium, copper, cyanide, fluoride, lead, manganese, mercury, molybdenum, selenium, vanadium and zinc. For each toxic substance, threshold levels at which chronic and acute toxicity effects on aquatic biota can be expected are indicated. The criteria can be applied in water quality evaluation, impact assessment, and in the setting of discharge permit conditions.},
  isbn = {0038-2353},
  keywords = {nosource}
}

@article{roveroStandardizedAssessmentForest2020,
  title = {A Standardized Assessment of Forest Mammal Communities Reveals Consistent Functional Composition and Vulnerability across the Tropics},
  author = {Rovero, Francesco and Ahumada, Jorge and Jansen, Patrick A. and Sheil, Douglas and Alvarez, Patricia and Boekee, Kelly and Espinosa, Santiago and Lima, Marcela Guimar{\~a}es Moreira and Martin, Emanuel H. and O'Brien, Timothy G. and Salvador, Julia and Santos, Fernanda and Rosa, Melissa and Zvoleff, Alexander and Sutherland, Chris and Tenan, Simone},
  year = {2020},
  journal = {Ecography},
  volume = {43},
  number = {1},
  pages = {75--84},
  issn = {1600-0587},
  doi = {10.1111/ecog.04773},
  urldate = {2020-04-10},
  abstract = {The understanding of global diversity patterns has benefitted from a focus on functional traits and how they relate to variation in environmental conditions among assemblages. Distant communities in similar environments often share characteristics, and for tropical forest mammals, this functional trait convergence has been demonstrated at coarse scales (110--200 km resolution), but less is known about how these patterns manifest at fine scales, where local processes (e.g. habitat features and anthropogenic activities) and biotic interactions occur. Here, we used standardized camera trapping data and a novel analytical method that accounts for imperfect detection to assess how the functional composition of terrestrial mammal communities for two traits -- trophic guild and body mass -- varies across 16 protected areas in tropical forests and three continents, in relation to the extent of protected habitat and anthropogenic pressures. We found that despite their taxonomic differences, communities generally have a consistent trophic guild composition, and respond similarly to these factors. Insectivores were found to be sensitive to the size of protected habitat and surrounding human population density. Body mass distribution varied little among communities both in terms of central tendency and spread, and interestingly, community average body mass declined with proximity to human settlements. Results indicate predicted trait convergence among assemblages at the coarse scale reflects consistent functional composition among communities at the local scale, suggesting that broadly similar habitats and selective pressures shaped communities with similar trophic strategies and responses to drivers of change. These similarities provide a foundation for assessing assemblages under anthropogenic threats and sharing conservation measures.},
  langid = {english},
  keywords = {community structure,conservation,functional traits,mammals,nosource,trophic guild,tropical forest}
}

@misc{royConvergenceDiagnosticsMarkov2019,
  title = {Convergence Diagnostics for {{Markov}} Chain {{Monte Carlo}}},
  author = {Roy, Vivekananda},
  year = {2019},
  month = oct,
  number = {arXiv:1909.11827},
  eprint = {1909.11827},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2022-06-03},
  abstract = {Markov chain Monte Carlo (MCMC) is one of the most useful approaches to scientific computing because of its flexible construction, ease of use and generality. Indeed, MCMC is indispensable for performing Bayesian analysis. Two critical questions that MCMC practitioners need to address are where to start and when to stop the simulation. Although a great amount of research has gone into establishing convergence criteria and stopping rules with sound theoretical foundation, in practice, MCMC users often decide convergence by applying empirical diagnostic tools. This review article discusses the most widely used MCMC convergence diagnostic tools. Some recently proposed stopping rules with firm theoretical footing are also presented. The convergence diagnostics and stopping rules are illustrated using three detailed examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {{60J05, 62F15, 65C40},Statistics - Computation},
  file = {/home/gkonkamking/Zotero/storage/G2BRPUVV/Roy - 2019 - Convergence diagnostics for Markov chain Monte Car.pdf}
}

@article{RSLLBHGL16,
  title = {Computational High-Throughput Screening of Fluid Permeability in Heterogeneous Fiber Materials},
  author = {R{\"o}ding, Magnus and Schuster, Erich and Logg, Katarina and Lundman, Malin and Bergstr{\"o}m, Per and Hanson, Charlotta and Geb{\"a}ck, Tobias and Lor{\'e}n, Niklas},
  year = {2016},
  journal = {Soft Matter},
  volume = {12},
  pages = {6293--6299},
  abstract = {We explore computational high-throughput screening as a design strategy for heterogeneous, isotropic fiber materials. Fluid permeability, a key property in the design of soft porous materials, is systematically studied using a multi-scale lattice Boltzmann framework. After characterizing microscopic permeability as a function of solid volume fraction in the microstructure, we perform high-throughput computational screening of in excess of 35 000 macrostructures consisting of a continuous bulk interrupted by spherical/elliptical domains with either lower or higher microscopic permeability (hence with two distinct microscopic solid volume fractions and therefore two distinct microscopic permeabilities) to assess which parameters determine macroscopic permeability for a fixed average solid volume fraction. We conclude that the fractions of bulk and domains and the distribution of solid volume fraction between them are the primary determinants of macroscopic permeability, and that a substantial increase in permeability compared to the corresponding homogenous material is attainable.},
  keywords = {nosource}
}

@article{rstan-software:2014,
  title = {{{RStan}}: The r Interface to Stan, Version 2.8.0},
  author = {{Stan Development Team}},
  year = {2015},
  pages = {1--22},
  abstract = {In this vignette we present the RStan package rstan for using Stan in R. Stan is a package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo. We illustrate the features of RStan through an example in Gelman et al. (2003).},
  keywords = {nosource}
}

@article{Rubach2012,
  title = {Species Traits as Predictors for Intrinsic Sensitivity of Aquatic Invertebrates to the Insecticide Chlorpyrifos},
  author = {Rubach, Mascha N. and Baird, Donald J. and Boerwinkel, Marie Claire and Maund, Stephen J. and Roessink, Ivo and Van Den Brink, Paul J.},
  year = {2012},
  month = oct,
  journal = {Ecotoxicology},
  volume = {21},
  number = {7},
  pages = {2088--2101},
  issn = {09639292},
  doi = {10.1007/s10646-012-0962-8},
  abstract = {Ecological risk assessment (ERA) has followed a taxonomy-based approach, making the assumption that related species will show similar sensitivity to toxicants, and using safety factors or species sensitivity distributions to extrapolate from tested to untested species. In ecology it has become apparent that taxonomic approaches may have limitations for the description and understanding of species assemblages in nature. Therefore it has been proposed that the inclusion of species traits in ERA could provide a useful and alternative description of the systems under investigation. At the same time, there is a growing recognition that the use of mechanistic approaches in ERA, including conceptual and quantitative models, may improve predictive and extrapolative power. Purposefully linking traits with mechanistic effect models could add value to taxonomy-based ERA by improving our understanding of how structural and functional system facets may facilitate inter-species extrapolation. Here, we explore whether and in what ways traits can be linked purposefully to mechanistic effect models to predict intrinsic sensitivity using available data on the acute sensitivity and toxicokinetics of a range of freshwater arthropods exposed to chlorpyrifos. The results of a quantitative linking of seven different endpoints and twelve traits demonstrate that while quantitative links between traits and/or trait combinations and process based (toxicokinetic) model parameters can be established, the use of simple traits to predict classical sensitivity endpoints yields little insight. Remarkably, neither of the standard sensitivity values, i.e. the LC(50) or EC(50), showed a strong correlation with traits. Future research in this area should include a quantitative linking of toxicodynamic parameter estimations and physiological traits, and requires further consideration of how mechanistic trait-process/parameter links can be used for prediction of intrinsic sensitivity across species for different substances in ERA.},
  isbn = {0963-9292},
  pmid = {22711550},
  keywords = {nosource,Pesticides,Prediction,Sensitivity,Species traits,Toxicokinetics-toxicodynamics}
}

@article{rubin1984bayesianly,
  title = {Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician},
  author = {Rubin, Donald B},
  year = {1984},
  journal = {The Annals of Statistics},
  volume = {12},
  number = {4},
  pages = {1151--1172},
  publisher = {Institute of Mathematical Statistics},
  keywords = {nosource}
}

@article{rue2009approximate,
  title = {Approximate {{Bayesian}} Inference for Latent {{Gaussian}} Models by Using Integrated Nested {{Laplace}} Approximations},
  author = {Rue, H{\aa}vard and Martino, Sara and Chopin, Nicolas},
  year = {2009},
  journal = {Journal of the royal statistical society: Series b (statistical methodology)},
  volume = {71},
  number = {2},
  pages = {319--392},
  publisher = {Wiley Online Library},
  issn = {1467-9868},
  doi = {DOI 10.1111/j.1467-9868.2008.00700.x},
  abstract = {Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models,},
  isbn = {1369-7412},
  keywords = {additive mixed models,approximate bayesian inference,gaussian markov random fields,generalized,laplace approximation,nosource,parallel computing,sparse matrices,structured additive regression models}
}

@article{Ruggiero2009,
  title = {Countable Representation for Infinite Dimensional Diffusions Derived from the Two-Parameter Poisson-Dirichlet Process},
  author = {Ruggiero, Matteo and Walker, Stephen G.},
  year = {2009},
  journal = {Electronic Communications in Probability},
  volume = {14},
  pages = {501--517},
  issn = {1083589X},
  keywords = {Gibbs sampler,Infinite-dimensional diffusion,nosource,Population process,Stationary distribution,Two-parameter poisson-dirichlet process}
}

@article{runggaldier2001sufficient,
  title = {Sufficient Conditions for Finite Dimensionality of Filters in Discrete Time: {{A Laplace}} Transform-Based Approach},
  author = {Runggaldier, Wolfgang J and Spizzichino, Fabio},
  year = {2001},
  journal = {Bernoulli},
  pages = {211--221},
  publisher = {JSTOR},
  keywords = {nosource}
}

@article{RZRB16,
  title = {Approximate \{\vphantom\}{{B}}\vphantom\{\}ayesian Computation for Estimating Number Concentrations of Monodisperse Nanoparticles in Suspension by Optical Microscopy},
  author = {R{\"o}ding, Magnus and Zagato, Elisa and Remaut, Katrien and Braeckmans, Kevin},
  year = {2016},
  journal = {Physical Review E},
  volume = {93},
  pages = {63311},
  abstract = {We present an approximate Bayesian computation scheme for estimating number concentrations of monodisperse diffusing nanoparticles in suspension by optical particle tracking microscopy. The method is based on the probability distribution of the time spent by a particle inside a detection region. We validate the method on suspensions of well-controlled reference particles. We illustrate its usefulness with an application in gene therapy, applying the method to estimate number concentrations of plasmid DNA molecules and the average number of DNA molecules complexed with liposomal drug delivery particles.},
  keywords = {nosource}
}

@article{sabbaghDistinctInfluenceFilter2013,
  title = {Distinct Influence of Filter Strips on Acute and Chronic Pesticide Aquatic Environmental Exposure Assessments across {{U}}.{{S}}. {{EPA}} Scenarios},
  author = {Sabbagh, George J. and {Mu{\~n}oz-Carpena}, Rafael and Fox, Garey A.},
  year = {2013},
  month = jan,
  journal = {Chemosphere},
  volume = {90},
  number = {2},
  pages = {195--202},
  issn = {0045-6535},
  doi = {10.1016/j.chemosphere.2012.06.034},
  urldate = {2020-03-17},
  abstract = {Vegetative filter strips (VFS) are proposed for protection of receiving water bodies and aquatic organisms from pesticides in runoff, but there is debate regarding the efficiency and filter size requirements. This debate is largely due to the belief that no quantitative methodology exists for predicting runoff buffer efficiency when conducting acute and/or chronic environmental exposure assessments. Previous research has proposed a modeling approach that links the U.S. Environmental Protection Agency's (EPA's) PRZM/EXAMS with a well-tested process-based model for VFS (VFSMOD). In this research, we apply the modeling framework to determine (1) the most important input factors for quantifying mass reductions of pesticides by VFS in aquatic exposure assessments relative to three distinct U.S. EPA scenarios encompassing a wide range of conditions; (2) the expected range in percent reductions in acute and chronic estimated environmental concentrations (EECs); and (3) the differential influence of VFS when conducting acute versus chronic exposure assessments. This research utilized three, 30-yr U.S. EPA scenarios: Illinois corn, California tomato, and Oregon wheat. A global sensitivity analysis (GSA) method identified the most important input factors based on discrete uniform probability distributions for five input factors: VFS length (VL), organic-carbon sorption coefficient (Koc), half-lives in both water and soil phases, and application timing. For percent reductions in acute and chronic EECs, VL and application timing were consistently the most important input factors independent of EPA scenario. The potential ranges in acute and chronic EECs varied as a function of EPA scenario and application timing. Reductions in acute EECs were typically less than percent reductions in chronic EECs because acute exposure was driven primarily by large individual rainfall and runon events. Importantly, generic specification of VFS design characteristics equal across scenarios should be avoided. The revised pesticide assessment modeling framework offers the ability to elucidate the complex and non-linear relationships that can inform targeted VFS design specifications.},
  langid = {english},
  keywords = {Exposure assessment,nosource,Pesticides,Sensitivity analysis,Vegetative filter strips,VFSMOD}
}

@article{sahakyan2008intentional,
  title = {Intentional Forgetting Is Easier after Two" Shots" than One.},
  author = {Sahakyan, Lili and Delaney, Peter F and Waldum, Emily R},
  year = {2008},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {34},
  number = {2},
  pages = {408},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{sakamoto1986akaike,
  title = {Akaike Information Criterion Statistics},
  author = {Sakamoto, Yosiyuki and Ishiguro, Makio and Kitagawa, Genshiro},
  year = {1986},
  journal = {Dordrecht, The Netherlands: D. Reidel},
  volume = {81},
  number = {10.5555},
  pages = {26853},
  publisher = {Taylor \& Francis},
  keywords = {nosource}
}

@article{Sala2012,
  title = {{{SSD-based}} Rating System for the Classification of Pesticide Risk on Biodiversity.},
  author = {Sala, Serenella and Migliorati, Sonia and Monti, Gianna S and Vighi, Marco},
  year = {2012},
  month = may,
  journal = {Ecotoxicology (London, England)},
  volume = {21},
  number = {4},
  eprint = {22270357},
  eprinttype = {pubmed},
  pages = {1050--62},
  issn = {1573-3017},
  doi = {10.1007/s10646-012-0858-7},
  abstract = {A novel approach, based on Species sensitivity distribution (SSD), is proposed for the development of an index for classifying ecotoxicological pesticide risk in surface waters. In this approach, the concept of TER (Toxicity Exposure Ratio), commonly used in traditional risk indices, is substituted by the concept of PAF (Potentially Affected Fraction), which takes into account several species within the biological community of interest, rather than just a small number of indicator species assumed as being representative of the ecosystem. The procedure represents a probabilistic tool to quantitatively assess the ecotoxicological risk on biodiversity considering the distribution of toxicological sensitivity. It can be applied to assess chemical risk on generic aquatic and terrestrial communities as well as on site-specific natural communities. Examples of its application are shown for some pesticides in freshwater ecosystems. In order to overcome the problem of insufficient reliable ecotoxicological data, a methodology and related algorithms are proposed for predicting SSD curves for chemicals that do not have sufficient available data. The methodology is applicable within congeneric classes of chemicals and has been tested and statistically validated on a group of organophosphorus insecticides. Values and limitations of the approach are discussed.},
  isbn = {0963-9292},
  pmid = {22270357},
  keywords = {Aquatic community,Biodiversity,Chemical,Chemical: classification,Chemical: toxicity,duplicate-citation-key,Ecosystem,Ecotoxicology,Environmental Monitoring,Environmental Monitoring: methods,Fresh Water,Fresh Water: chemistry,Nonparametric tests,nosource,Organophosphorus Compounds,Organophosphorus Compounds: classification,Organophosphorus Compounds: toxicity,Pesticide,Pesticides,Pesticides: classification,Pesticides: toxicity,Reproducibility of Results,Risk Assessment,Risk index,Species sensitivity distribution,Water Pollutants}
}

@article{salehiClonalFitnessInferred2021,
  title = {Clonal Fitness Inferred from Time-Series Modelling of Single-Cell Cancer Genomes},
  author = {Salehi, Sohrab and Kabeer, Farhia and Ceglia, Nicholas and Andronescu, Mirela and Williams, Marc J. and Campbell, Kieran R. and Masud, Tehmina and Wang, Beixi and Biele, Justina and Brimhall, Jazmine and Gee, David and Lee, Hakwoo and Ting, Jerome and Zhang, Allen W. and Tran, Hoa and O'Flanagan, Ciara and Dorri, Fatemeh and Rusk, Nicole and {de Algara}, Teresa Ruiz and Lee, So Ra and Cheng, Brian Yu Chieh and Eirew, Peter and Kono, Takako and Pham, Jenifer and Grewal, Diljot and Lai, Daniel and Moore, Richard and Mungall, Andrew J. and Marra, Marco A. and McPherson, Andrew and {Bouchard-C{\^o}t{\'e}}, Alexandre and Aparicio, Samuel and Shah, Sohrab P.},
  year = {2021},
  month = jul,
  journal = {Nature},
  volume = {595},
  number = {7868},
  pages = {585--590},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03648-3},
  urldate = {2022-03-28},
  abstract = {Progress in defining genomic fitness landscapes in cancer, especially those defined by copy number alterations (CNAs), has been impeded by lack of time-series single-cell sampling of polyclonal populations and temporal statistical models1--7. Here we generated 42,000 genomes from multi-year time-series single-cell whole-genome sequencing of breast epithelium and primary triple-negative breast cancer (TNBC) patient-derived xenografts (PDXs), revealing the nature of CNA-defined clonal fitness dynamics induced by TP53 mutation and cisplatin chemotherapy. Using a new Wright--Fisher population genetics model8,9 to infer clonal fitness, we found that TP53 mutation alters the fitness landscape, reproducibly distributing fitness over a larger number of clones associated with distinct CNAs. Furthermore, in TNBC PDX models with mutated TP53, inferred fitness coefficients from CNA-based genotypes accurately forecast experimentally enforced clonal competition dynamics. Drug treatment in three long-term serially passaged TNBC PDXs resulted in cisplatin-resistant clones emerging from low-fitness phylogenetic lineages in the untreated setting. Conversely, high-fitness clones from treatment-naive controls were eradicated, signalling an inversion of the fitness landscape. Finally, upon release of drug, selection pressure dynamics were reversed, indicating a fitness cost of treatment resistance. Together, our findings define clonal fitness linked to both CNA and therapeutic resistance in polyclonal tumours.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Cancer genomics,Preclinical research},
  file = {/home/gkonkamking/Zotero/storage/BPSF5BJG/Salehi et al. - 2021 - Clonal fitness inferred from time-series modelling.pdf}
}

@article{salomond2013adaptive,
  title = {Adaptive {{Bayes}} Test for Monotonicity},
  author = {Salomond, Jean Bernard},
  year = {2014},
  journal = {Springer Proceedings in Mathematics and Statistics},
  volume = {63},
  eprint = {1303.6466v1},
  pages = {29--33},
  issn = {21941009},
  doi = {10.1007/978-3-319-02084-6__7},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1303.6466v1},
  isbn = {9783319020839},
  keywords = {duplicate-citation-key,nosource}
}

@inproceedings{sapienza2015anomaly,
  title = {Anomaly Detection in Temporal Graph Data: {{An}} Iterative Tensor Decomposition and Masking Approach},
  booktitle = {International Workshop on Advanced Analytics and Learning on Temporal Data, {{AALTD}} 2015},
  author = {Sapienza, Anna and Panisson, Andr{\'e} and Wu, {\relax JTK} and Gauvin, Laetitia and Cattuto, Ciro},
  year = {2015},
  keywords = {⛔ No DOI found,nosource}
}

@article{sapienza2015anomaly,
  title = {Anomaly Detection in Temporal Graph Data: {{An}} Iterative Tensor},
  author = {Sapienza, A and Panisson, A and Wu, {\relax JTK} and Gauvin, L and Cattuto, C},
  year = {2015},
  keywords = {⛔ No DOI found,nosource}
}

@inproceedings{sapienza2015detecting,
  title = {Detecting Anomalies in Time-Varying Networks Using Tensor Decomposition},
  booktitle = {2015 {{IEEE}} International Conference on Data Mining Workshop ({{ICDMW}})},
  author = {Sapienza, Anna and Panisson, Andr{\'e} and Wu, Joseph and Gauvin, Laetitia and Cattuto, Ciro},
  year = {2015},
  pages = {516--523},
  doi = {10.1109/ICDMW.2015.128},
  organization = {IEEE},
  keywords = {nosource}
}

@article{sapienza2018estimating,
  title = {Estimating the Outcome of Spreading Processes on Networks with Incomplete Information: {{A}} Dimensionality Reduction Approach},
  author = {Sapienza, Anna and Barrat, Alain and Cattuto, Ciro and Gauvin, Laetitia},
  year = {2018},
  journal = {Physical Review E},
  volume = {98},
  number = {1},
  pages = {012317},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.98.012317},
  keywords = {nosource}
}

@article{sarkar2016,
  title = {Bayesian Nonparametric Modeling of Higher Order Markov Chains},
  author = {Sarkar, Abhra and Dunson, David B},
  year = {2016},
  journal = {Journal of the American Statistical Association},
  volume = {111},
  number = {516},
  pages = {1791--1803},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.2015.1115763},
  keywords = {nosource}
}

@article{Sarkar2019,
  title = {Bayesian Higher Order Hidden Markov Models},
  author = {Sarkar, Abhra and Dunson, David B.},
  year = {2019},
  journal = {arXiv preprint arXiv:1805.12201v2},
  eprint = {1805.12201v2},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@article{sarkissianFolkMoralRelativism2011,
  title = {Folk {{Moral Relativism}}},
  author = {Sarkissian, Hagop and Park, John and Tien, David and Wright, Jennifer Cole and Knobe, Joshua},
  year = {2011},
  journal = {Mind \& Language},
  volume = {26},
  number = {4},
  pages = {482--505},
  issn = {1468-0017},
  doi = {10.1111/j.1468-0017.2011.01428.x},
  urldate = {2020-05-04},
  abstract = {It has often been suggested that people's ordinary understanding of morality involves a belief in objective moral truths and a rejection of moral relativism. The results of six studies call this claim into question. Participants did offer apparently objectivist moral intuitions when considering individuals from their own culture, but they offered increasingly relativist intuitions considering individuals from increasingly different cultures or ways of life. The authors hypothesize that people do not have a fixed commitment to moral objectivism but instead tend to adopt different views depending on the degree to which they consider radically different perspectives on moral questions.},
  copyright = {{\copyright} 2011 Blackwell Publishing Ltd},
  langid = {english},
  keywords = {nosource}
}

@book{sarkka2013bayesian,
  title = {Bayesian Filtering and Smoothing},
  author = {S{\"a}rkk{\"a}, Simo},
  year = {2013},
  volume = {3},
  publisher = {Cambridge University Press},
  keywords = {nosource}
}

@book{sato1999levy,
  title = {L{\'e}vy Processes and Infinitely Divisible Distributions},
  author = {Sato, Ken-Iti},
  year = {1999},
  series = {Cambridge {{Studies}} in {{Advanced Mathematics}}},
  volume = {68},
  publisher = {Cambridge University Press},
  keywords = {nosource}
}

@article{savaniWhatCountsChoice2010,
  title = {What {{Counts}} as a {{Choice}}?: {{U}}.{{S}}. {{Americans Are More Likely Than Indians}} to {{Construe Actions}} as {{Choices}}},
  shorttitle = {What {{Counts}} as a {{Choice}}?},
  author = {Savani, Krishna and Markus, Hazel Rose and Naidu, N. V. R. and Kumar, Satishchandra and Berlia, Neha},
  year = {2010},
  month = jan,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  doi = {10.1177/0956797609359908},
  urldate = {2020-06-19},
  abstract = {People everywhere select among multiple alternatives, but are they always making choices? In five studies, we found that people in U.S. American contexts, where...},
  langid = {english},
  keywords = {nosource}
}

@article{schachtnerBayesianApproachLee2014,
  title = {A {{Bayesian}} Approach to the {{Lee}}--{{Seung}} Update Rules for {{NMF}}},
  author = {Schachtner, R. and Poeppel, G. and Tom{\'e}, A. M. and Lang, E. W.},
  year = {2014},
  month = aug,
  journal = {Pattern Recognition Letters},
  volume = {45},
  pages = {251--256},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2014.04.013},
  urldate = {2023-10-06},
  abstract = {NMF is a Blind Source Separation technique decomposing multivariate non-negative data sets into meaningful non-negative basis components and non-negative weights. In its canonical form an NMF algorithm was proposed by Lee and Seung (1999) [31] employing multiplicative update rules. In this study we show how the latter follow from a new variational Bayes NMF algorithm VBNMF employing a Gaussian noise kernel.},
  keywords = {Bayesian optimality criteria,Lee--Seung update rules,Variational Bayes NMF}
}

@article{schafferContrastiveKnowledgeSurveyed2012,
  title = {Contrastive {{Knowledge Surveyed}}},
  author = {Schaffer, Jonathan and Knobe, Joshua},
  year = {2012},
  journal = {No{\^u}s},
  volume = {46},
  number = {4},
  pages = {675--708},
  issn = {1468-0068},
  doi = {10.1111/j.1468-0068.2010.00795.x},
  urldate = {2020-05-04},
  copyright = {{\copyright} 2010 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {nosource}
}

@article{scheiplSpikeSlabGAMBayesianVariable2011,
  title = {{{spikeSlabGAM}}: {{Bayesian Variable Selection}}, {{Model Choice}} and {{Regularization}} for {{Generalized Additive Mixed Models}} in {{R}}},
  shorttitle = {{{spikeSlabGAM}}},
  author = {Scheipl, Fabian},
  year = {2011},
  month = sep,
  journal = {Journal of Statistical Software},
  volume = {43},
  number = {1},
  pages = {1--24},
  issn = {1548-7660},
  doi = {10.18637/jss.v043.i14},
  urldate = {2020-04-10},
  copyright = {Copyright (c) 2010 Fabian Scheipl},
  langid = {english}
}

@article{scheiplSpikeSlabPriorsFunction2012,
  title = {Spike-and-{{Slab Priors}} for {{Function Selection}} in {{Structured Additive Regression Models}}},
  author = {Scheipl, Fabian and Fahrmeir, Ludwig and Kneib, Thomas},
  year = {2012},
  journal = {Journal of the American Statistical Association},
  volume = {107},
  number = {500},
  eprint = {23427352},
  eprinttype = {jstor},
  pages = {1518--1532},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  urldate = {2021-09-20},
  abstract = {Structured additive regression (STAR) provides a general framework for complex Gaussian and non-Gaussian regression models, with predictors comprising arbitrary combinations of nonlinear functions and surfaces, spatial effects, varying coefficients, random effects, and further regression terms. The large flexibility of STAR makes function selection a challenging and important task, aiming at (1) selecting the relevant covariates, (2) choosing an appropriate and parsimonious representation of the impact of covariates on the predictor, and (3) determining the required interactions. We propose a spike-and-slab prior structure for function selection that allows to include or exclude single coefficients as well as blocks of coefficients representing specific model terms. A novel multiplicative parameter expansion is required to obtain good mixing and convergence properties in a Markov chain Monte Carlo simulation approach and is shown to induce desirable shrinkage properties. In simulation studies and with (real) benchmark classification data, we investigate sensitivity to hyperparameter settings and compare performance to competitors. The flexibility and applicability of our approach are demonstrated in an additive piecewise exponential model with time-varying effects for right-censored survival times of intensive care patients with sepsis. Geoadditive and additive mixed logit model applications are discussed in an extensive online supplement.},
  file = {/home/gkonkamking/Zotero/storage/MGG6P9SL/Scheipl et al. - 2012 - Spike-and-Slab Priors for Function Selection in St.pdf}
}

@article{schelling1980micromotives,
  title = {Micromotives and Macrobehaviour {{NY}} Norton and Co, 1978, Traduction Fran\{{\c c}\}aise},
  author = {Schelling, Thomas C},
  year = {1980},
  journal = {La tyrannie des petites d\{{\'e}\}cisions},
  keywords = {nosource}
}

@book{schelling2006micromotives,
  title = {Micromotives and Macrobehavior},
  author = {Schelling, Thomas C},
  year = {2006},
  publisher = {WW Norton \& Company},
  keywords = {nosource}
}

@book{schervish1995theory,
  title = {Theory of Statistics},
  author = {Schervish, M},
  year = {1995},
  publisher = {Springer},
  isbn = {978-1-4612-8708-7},
  keywords = {nosource}
}

@article{schloss2005introducing,
  title = {Introducing {{DOTUR}} , a Computer Program for Defining Operational Taxonomic Units and Estimating Species Richness},
  author = {Schloss, Patrick D and Handelsman, Jo},
  year = {2005},
  journal = {Applied and Environmental Microbiology},
  volume = {71},
  number = {3},
  pages = {1501--1506},
  publisher = {Am Soc Microbiol},
  issn = {0099-2240, 1098-5336},
  doi = {10.1128/AEM.71.3.1501},
  abstract = {Although copious qualitative information describes the members of the diverse microbial communities on Earth, statistical approaches for quantifying and comparing the numbers and compositions of lineages in communities are lacking. We present a method that addresses the challenge of assigning sequences to oper- ational taxonomic units (OTUs) based on the genetic distances between sequences. We developed a computer program, DOTUR, which assigns sequences to OTUs by using either the furthest, average, or nearest neighbor algorithm for each distance level. DOTUR uses the frequency at which each OTU is observed to construct rarefaction and collector's curves for various measures of richness and diversity. We analyzed 16S rRNA gene libraries derived from Scottish and Amazonian soils and the Sargasso Sea with DOTUR, which assigned sequences to OTUs rapidly and reliably based on the genetic distances between sequences and identified previous inconsistencies and errors in assigning sequences to OTUs. An analysis of the two 16S rRNA gene libraries from soil demonstrated that they do not contain enough sequences to support a claim that they contain different numbers of bacterial lineages with statistical confidence (P {$>$} 0.05), nor do they contain enough sequences to provide a robust estimate of species richness when an OTU is defined as containing sequences that are no more than 3\% different from each other. In contrast, the richness of OTUs at the 3\% level in the Sargasso Sea collection began to plateau after the sampling of 690 sequences. We anticipate that an equivalent extent of sampling for soil would require sampling more than 10,000 sequences, almost 100 times the size of typical sequence collections obtained from soil. An},
  isbn = {0099-2240 (Print)},
  pmid = {15746353},
  keywords = {duplicate-citation-key,nosource}
}

@article{Schmon2019a,
  title = {Bernoulli Race Particle Filters},
  author = {Schmon, Sebastian M and Doucet, Arnaud and Deligiannidis, George},
  year = {2019},
  number = {3},
  eprint = {1903.00939},
  abstract = {When the weights in a particle filter are not available analytically, standard resampling methods cannot be employed. To circumvent this problem state-of-the-art algorithms replace the true weights with non-negative unbiased estimates. This algorithm is still valid but at the cost of higher variance of the resulting filtering estimates in comparison to a particle filter using the true weights. We propose here a novel algorithm that allows for resampling according to the true intractable weights when only an unbiased estimator of the weights is available. We demonstrate our algorithm on several examples.},
  archiveprefix = {arXiv},
  arxivid = {1903.00939},
  isbn = {1903.00939v1},
  keywords = {bernoulli factory,nosource,particle filters,pfs,random weight particle,sequentially approximate the joint}
}

@article{scholzStrainlevelMicrobialEpidemiology2016,
  title = {Strain-Level Microbial Epidemiology and Population Genomics from Shotgun Metagenomics},
  author = {Scholz, Matthias and Ward, Doyle V. and Pasolli, Edoardo and Tolio, Thomas and Zolfo, Moreno and Asnicar, Francesco and Truong, Duy Tin and Tett, Adrian and Morrow, Ardythe L. and Segata, Nicola},
  year = {2016},
  month = may,
  journal = {Nature Methods},
  volume = {13},
  number = {5},
  pages = {435--438},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/nmeth.3802},
  urldate = {2020-10-12},
  abstract = {PanPhlAn detects strains and characterizes strain-specific gene content and activity within metagenomic and metatranscriptomic samples for microbial population analysis and epidemiology.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {nosource}
}

@article{Schroer2004,
  title = {Comparison of Laboratory Single Species and Field Population-Level Effects of the Pyrethroid Insecticide ?-{{Cyhalothrin}} on Freshwater Invertebrates},
  author = {Schroer, a. F. W. and Belgers, J. D. M. and Brock, C. M. and Matser, a. M. and Maund, S. J. and Brink, P. J.},
  year = {2004},
  month = apr,
  journal = {Archives of Environmental Contamination and Toxicology},
  volume = {46},
  number = {3},
  pages = {324--335},
  issn = {0090-4341},
  doi = {10.1007/s00244-003-2315-3},
  isbn = {00904341 (ISSN)},
  pmid = {15195804},
  keywords = {duplicate-citation-key,nosource}
}

@article{schwartz1965bayes,
  title = {On Bayes Procedures},
  author = {Schwartz, Lorraine},
  year = {1965},
  journal = {Zeitschrift f??r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
  volume = {4},
  number = {1},
  pages = {10--26},
  publisher = {Springer},
  issn = {00443719},
  doi = {10.1007/BF00535479},
  abstract = {A result of Doob regarding consistency of Bayes estimators is extended to a large class of Bayes decision procedures in which the loss functions are not necessarily convex. Rather weak conditions are given under which the Bayes procedures are consistent. One set involves restrictions on the a priori distribution and follows an example in which the choice of a priori distribution determines whether the Bayes estimators are consistent. Another example shows that the maximum likelihood estimators may be consistent when the Bayes estimators are not. However, the conditions given are of an essentially weaker nature than those established for consistency of maximum likelihood estimators.},
  pmid = {16591192},
  keywords = {nosource}
}

@article{schwarzASSIMILATIONCONTRASTEFFECTS1991,
  title = {{{ASSIMILATION AND CONTRAST EFFECTS IN PART-WHOLE QUESTION SEQUENCES}}: {{A CONVERSATIONAL LOGIC ANALYSIS}}},
  shorttitle = {{{ASSIMILATION AND CONTRAST EFFECTS IN PART-WHOLE QUESTION SEQUENCES}}},
  author = {Schwarz, Norbert and Strack, Fritz and Mai, Hans-Peter},
  year = {1991},
  month = jan,
  journal = {Public Opinion Quarterly},
  volume = {55},
  number = {1},
  pages = {3--23},
  publisher = {Oxford Academic},
  issn = {0033-362X},
  doi = {10.1086/269239},
  urldate = {2020-07-07},
  abstract = {Abstract.  A theoretical model of the emergence of assimilation and contrast effects in part-whole question sequences is presented. When one specific question p},
  langid = {english},
  keywords = {nosource}
}

@article{schwarzEvaluatingPsychologicalResearch2016,
  title = {Evaluating {{Psychological Research Requires More Than Attention}} to the {{N}}: {{A Comment}} on {{Simonsohn}}'s (2015) ``{{Small Telescopes}}''},
  shorttitle = {Evaluating {{Psychological Research Requires More Than Attention}} to the {{N}}},
  author = {Schwarz, Norbert and Clore, Gerald L.},
  year = {2016},
  month = oct,
  journal = {Psychological Science},
  volume = {27},
  number = {10},
  pages = {1407--1409},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797616653102},
  urldate = {2020-03-10},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/B9D97BSG/Schwarz and Clore - 2016 - Evaluating Psychological Research Requires More Th.pdf}
}

@article{scott2002bayesian,
  title = {Bayesian Methods for Hidden {{Markov}} Models: {{Recursive}} Computing in the 21st Century},
  author = {Scott, Steven L},
  year = {2002},
  journal = {Journal of the American Statistical Association},
  volume = {97},
  number = {457},
  pages = {337--351},
  publisher = {Taylor \& Francis},
  keywords = {nosource}
}

@article{scottBayesEmpiricalBayesMultiplicity2010,
  title = {Bayes and {{Empirical-Bayes Multiplicity Adjustment}} in the {{Variable-Selection Problem}}},
  author = {Scott, James G. and Berger, James O.},
  year = {2010},
  journal = {The Annals of Statistics},
  volume = {38},
  number = {5},
  eprint = {29765241},
  eprinttype = {jstor},
  pages = {2587--2619},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  urldate = {2024-10-17},
  abstract = {This paper studies the multiplicity-correction effect of standard Bayesian variable-selection priors in linear regression. Our first goal is to clarify when, and how, multiplicity correction happens automatically in Bayesian analysis, and to distinguish this correction from the Bayesian Ockham's-razor effect. Our second goal is to contrast empirical-Bayes and fully Bayesian approaches to variable selection through examples, theoretical results and simulations. Considerable differences between the two approaches are found. In particular, we prove a theorem that characterizes a surprising aymptotic discrepancy between fully Bayes and empirical Bayes. This discrepancy arises from a different source than the failure to account for hyperparameter uncertainty in the empirical-Bayes estimate. Indeed, even at the extreme, when the empirical-Bayes estimate converges asymptotically to the true variable-inclusion probability, the potential for a serious difference remains.},
  file = {/home/gkonkamking/Zotero/storage/YB6FIVZF/Scott and Berger - 2010 - Bayes and Empirical-Bayes Multiplicity Adjustment in the Variable-Selection Problem.pdf}
}

@article{scricciolo_convergence_2006,
  title = {Convergence Rates for Bayesian Density Estimation of Infinite-Dimensional Exponential Families},
  author = {Scricciolo, Catia},
  year = {2006},
  journal = {Annals of Statistics},
  volume = {34},
  number = {6},
  eprint = {0708.0175},
  pages = {2897--2920},
  issn = {00905364},
  doi = {10.1214/009053606000000911},
  abstract = {We study the rate of convergence of posterior distributions in density estimation problems for log-densities in periodic Sobolev classes characterized by a smoothness parameter p. The posterior expected density provides a nonparametric estimation procedure attaining the optimal minimax rate of convergence under Hellinger loss if the posterior distribution achieves the optimal rate over certain uniformity classes. A prior on the density class of interest is induced by a prior on the coefficients of the trigonometric series expansion of the log-density. We show that when p is known, the posterior distribution of a Gaussian prior achieves the optimal rate provided the prior variances die off sufficiently rapidly. For a mixture of normal distributions, the mixing weights on the dimension of the exponential family are assumed to be bounded below by an exponentially decreasing sequence. To avoid the use of infinite bases, we develop priors that cut off the series at a sample-size-dependent truncation point. When the degree of smoothness is unknown, a finite mixture of normal priors indexed by the smoothness parameter, which is also assigned a prior, produces the best rate. A rate-adaptive estimator is derived.},
  archiveprefix = {arXiv},
  arxivid = {0708.0175},
  keywords = {Bayesian adaptive density estimation,Infinite-dimensional exponential family,nosource,Posterior distribution,Rate of convergence,Sieve prior}
}

@article{scricciolo2012adaptive,
  title = {Adaptive Bayesian Density Estimation in {{Lp-metrics}} with Pitman-Yor or Normalized Inverse-Gaussian Process Kernel Mixtures},
  author = {Scricciolo, Catia},
  year = {2014},
  journal = {Bayesian Analysis},
  volume = {9},
  number = {2},
  pages = {475--520},
  issn = {19316690},
  doi = {10.1214/14-BA863},
  keywords = {Adaptation,Nonparametric density estimation,Normalized inverse-gaussian process,Pitman-yor process,Posterior contraction rate,Sinc kernel},
  file = {/home/gkonkamking/Zotero/storage/3XVSAAQD/Scricciolo - 2014 - Adaptive bayesian density estimation in Lp-metrics.pdf}
}

@article{scruccaMclust5Clustering2016,
  title = {Mclust 5: {{Clustering}}, {{Classification}} and {{Density Estimation Using Gaussian Finite Mixture Models}}},
  shorttitle = {Mclust 5},
  author = {Scrucca, Luca and Fop, Michael and Murphy, T. Brendan and Raftery, Adrian E.},
  year = {2016},
  month = aug,
  journal = {The R journal},
  volume = {8},
  number = {1},
  pages = {289--317},
  issn = {2073-4859},
  urldate = {2020-05-07},
  abstract = {Finite mixture models are being used increasingly to model a wide variety of random phenomena for clustering, classification and density estimation. mclust is a powerful and popular package which allows modelling of data as a Gaussian finite mixture with different covariance structures and different numbers of mixture components, for a variety of purposes of analysis. Recently, version 5 of the package has been made available on CRAN. This updated version adds new covariance structures, dimension reduction capabilities for visualisation, model selection criteria, initialisation strategies for the EM algorithm, and bootstrap-based inference, making it a full-featured R package for data analysis via finite mixture modelling.},
  pmcid = {PMC5096736},
  pmid = {27818791},
  keywords = {nosource}
}

@incollection{sebastiani2003bayesian,
  title = {Bayesian Clustering of Gene Expression Dynamics},
  booktitle = {The Analysis of Gene Expression Data},
  author = {Sebastiani, Paola and Ramoni, Marco and Kohane, Isaac S},
  year = {2003},
  pages = {409--427},
  publisher = {Springer}
}

@article{Seefeldt2008,
  title = {Feature Log-Logistic Analysis of Herbicide Dose-Response Relationshipsi},
  author = {Seefeldt, Steven S and Jensen, Jens Erik and Fuerst, E Patrick and Technology, Weed and Jun, No Apr and Seefeldt, Steven S and Jensen, Jens Erik and Fuerst, E Patrick},
  year = {2008},
  volume = {9},
  number = {2},
  pages = {218--227},
  keywords = {nosource}
}

@inproceedings{SEKBK13,
  title = {Novel Algebras for Advanced Analytics in \{\vphantom\}{{J}}\vphantom\{\}ulia},
  booktitle = {High Performance Extreme Computing Conference ({{HPEC}}), 2013 {{IEEE}}},
  author = {Shah, Viral B and Edelman, Alan and Karpinski, Stefan and Bezanson, Jeff and Kepner, Jeremy},
  year = {2013},
  month = sep,
  pages = {1--4},
  doi = {10.1109/HPEC.2013.6670347},
  abstract = {A linear algebraic approach to graph algorithms that exploits the sparse adjacency matrix representation of graphs can provide a variety of benefits. These benefits include syntactic simplicity, easier implementation, and higher performance. One way to employ linear algebra techniques for graph algorithms is to use a broader definition of matrix and vector multiplication. We demonstrate through the use of the Julia language system how easy it is to explore semirings using linear algebraic methodologies.},
  keywords = {high level languages,linear algebra,mathematics co,nosource}
}

@article{Semenzin2007,
  title = {Improving Ecological Risk Assessment by Including Bioavailability into Species Sensitivity Distributions: An Example for Plants Exposed to Nickel in Soil.},
  author = {Semenzin, Elena and Temminghoff, Erwin J M and Marcomini, Antonio},
  year = {2007},
  month = jul,
  journal = {Environmental pollution (Barking, Essex : 1987)},
  volume = {148},
  number = {2},
  eprint = {17240027},
  eprinttype = {pubmed},
  pages = {642--7},
  issn = {0269-7491},
  doi = {10.1016/j.envpol.2006.11.019},
  abstract = {The variability of species sensitivity distribution (SSD) due to contaminant bioavailability in soil was explored by using nickel as metal of concern. SSDs of toxicity test results of Avena sativa L. originating from different soils and expressed as total content and available (0.01 M CaCl2) extractable concentration were compared to SSDs for terrestrial plants derived from literature toxicity data. Also the 'free' nickel (Ni2+) concentration was calculated and compared. The results demonstrated that SSDs based on total nickel content highly depend on the experimental conditions set up for toxicity testing (i.e. selected soil and pH value) and thus on metal bioavailability in soil, resulting in an unacceptable uncertainty for ecological risk estimation. The use in SSDs of plant toxicity data expressed as 0.01 M CaCl2 extractable metal strongly reduced the uncertainty in the SSD curve and thus can improve the ERA procedure remarkably by taking bioavailability into account.},
  isbn = {0415093074},
  pmid = {17240027},
  keywords = {Avena sativa,Avena sativa: metabolism,Bioavailability,Biological Availability,Calcium Chloride,Calcium Chloride: chemistry,duplicate-citation-key,Ecological risk assessment (ERA),Ecosystem,Environmental Exposure,Environmental Exposure: adverse effects,Heavy metals,Hydrogen-Ion Concentration,Nickel,Nickel: analysis,Nickel: pharmacokinetics,nosource,Plant toxicity,Plants,Plants: metabolism,Risk Assessment,Risk Assessment: methods,Soil,Soil Pollutants,Soil Pollutants: analysis,Soil Pollutants: pharmacokinetics,Soil: analysis,Species sensitivity distributions (SSDs),Toxicity Tests}
}

@article{Sensitivity2004,
  title = {Acute-to-chronic Species Sensitivity Distribution Extrapolation},
  author = {Duboudin, C and Ciffroy, P and Magaud, H},
  year = {2004},
  journal = {{\dots} toxicology and chemistry},
  volume = {23},
  number = {7},
  pages = {1774--1785},
  keywords = {acute,chronic extrapolation,duplicate-citation-key,nosource,risk assessment,species sensitivity distribution}
}

@article{Sensitivity2004,
  title = {Acute-to-Chronic Species Sensitivity Distribution Extrapolation},
  author = {Duboudin, C{\'e}dric and Ciffroy, Philippe and Magaud, H{\'e}l{\`e}ne},
  year = {2004},
  journal = {Environmental Toxicology and Chemistry},
  volume = {23},
  number = {7},
  pages = {1774},
  publisher = {Wiley Online Library},
  issn = {0730-7268},
  doi = {10.1897/1551-5028(2004)023<1774:ASSDE>2.0.CO;2},
  abstract = {Seeking to make greater use of available data for risk assessment of substances, we constructed, for the situation in which chronic data are limited or even nonexistent but acute data are relatively large, an acute to chronic transformation (ACT) methodology based on the concept of species sensitivity distributions (SSDs). This ACT methodology uses a comparison of acute and chronic SSDs, separately for vertebrate data (with 22 substances) and for invertebrate data (with 15 substances). Rather than comparing an acute toxicity value with a chronic value, as when calculating an acute to chronic ratio (ACR), samples of acute and chronic data corresponding to the same category of species were compared. Starting from a sample of acute data, the ACT methodology showed relationships that enable the creation of a sample of predicted chronic values. This sample can then be used to calculate a predicted chronic hazardous concentration potentially affecting 5\% of species (HC5\%), just as with a sample of real chronic toxicity values. This ACT approach was tested on 11 substances. For each substance, the real chronic HC5\% and the predicted chronic HC5\% were calculated and compared. The ratio between chronic HC5\% and ACT HC5\% was, on average, 1.6 and did not exceed 4.4 for the 11 substances studied.},
  isbn = {0730-7268},
  pmid = {15230330},
  keywords = {acute,chronic extrapolation,duplicate-citation-key,nosource,risk assessment,species sensitivity distribution}
}

@article{sermaidisMarkovChainMonte2013,
  title = {Markov {{Chain Monte Carlo}} for {{Exact Inference}} for {{Diffusions}}},
  author = {Sermaidis, Giorgos and Papaspiliopoulos, Omiros and Roberts, Gareth O. and Beskos, Alexandros and Fearnhead, Paul},
  year = {2013},
  journal = {Scandinavian Journal of Statistics},
  volume = {40},
  number = {2},
  pages = {294--321},
  issn = {1467-9469},
  doi = {10.1111/j.1467-9469.2012.00812.x},
  urldate = {2024-07-30},
  abstract = {ABSTRACT. We develop exact Markov chain Monte Carlo methods for discretely sampled, directly and indirectly observed diffusions. The qualification `exact' refers to the fact that the invariant and limiting distribution of the Markov chains is the posterior distribution of the parameters free of any discretization error. The class of processes to which our methods directly apply are those which can be simulated using the most general to date exact simulation algorithm. The article introduces various methods to boost the performance of the basic scheme, including reparametrizations and auxiliary Poisson sampling. We contrast both theoretically and empirically how this new approach compares to irreducible high frequency imputation, which is the state-of-the-art alternative for the class of processes we consider, and we uncover intriguing connections. All methods discussed in the article are tested on typical examples.},
  copyright = {{\copyright} 2012 Board of the Foundation of the Scandinavian Journal of Statistics},
  langid = {english},
  keywords = {exact inference,exact simulation,Markov chain Monte Carlo,stochastic differential equation,transition density},
  file = {/home/gkonkamking/pCloudDrive/papers/Sermaidis et al_2013_Markov Chain Monte Carlo for Exact Inference for Diffusions.pdf}
}

@article{sethuraman1994constructive,
  title = {A Constructive Definition of {{Dirichlet}} Priors},
  author = {Sethuraman, J.},
  year = {1994},
  journal = {Statistica Sinica},
  volume = {4},
  pages = {639--650},
  doi = {***},
  abstract = {In this paper we give a simple and new constructive definition of Dirichlet measures removing the restriction that the basic space should be Rk. We also give complete, self contained proofs of the three basic results for Dirichlet measures: 1. The Dirichlet measure is a probability measure on the space of all probability measures. 2. It gives probability one to the subset of discrete probability measures. 3. The posterior distribution is also a Dirichlet measure.},
  keywords = {nosource}
}

@article{shafer2008tutorial,
  title = {A Tutorial on Conformal Prediction.},
  author = {Shafer, Glenn and Vovk, Vladimir},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {3},
  keywords = {⛔ No DOI found}
}

@article{shafirChoosingRejectingWhy1993,
  title = {Choosing versus Rejecting: {{Why}} Some Options Are Both Better and Worse than Others},
  shorttitle = {Choosing versus Rejecting},
  author = {Shafir, Eldar},
  year = {1993},
  month = jul,
  journal = {Memory \& Cognition},
  volume = {21},
  number = {4},
  pages = {546--556},
  issn = {1532-5946},
  doi = {10.3758/BF03197186},
  urldate = {2020-07-08},
  abstract = {A previously unobserved pattern of choice behavior is predicted and corroborated. In line with the principle of compatibility, according to which the weighting of inputs is enhanced by their compatibility with output, the positive and negative dimensions of options (their pros and cons) are expected to loom larger when one is choosing and when one is rejecting, respectively. Subjects are presented with pairs of options, one of which---theenriched option---has more positive as well as more negative dimensions than does the other,impoverished, option. Because positive dimensions are weighted more heavily in choosing than in rejecting, and negative dimensions are weighted more heavily in rejecting than in choosing, the enriched option tends to be chosen and rejected relatively more often than the impoverished option. These findings are extended to nonbinary decision problems, and their implications for the rational theory of choice and for everyday decisions are discussed.},
  langid = {english},
  keywords = {nosource}
}

@article{shah2015contingency,
  title = {Contingency and Entrenchment in Protein Evolution under Purifying Selection},
  author = {Shah, Premal and McCandlish, David M. and Plotkin, Joshua B.},
  year = {2015},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {25},
  eprint = {1404.4005},
  pages = {E3226-E3235},
  publisher = {National Acad Sciences},
  issn = {0027-8424},
  doi = {10.1073/pnas.1412933112},
  abstract = {The fitness contribution of an allele at one genetic site may depend on the states of other sites, a phenomenon known as epistasis. Epistasis can profoundly influence the process of evolution in populations under selection, and shape the course of protein evolution across divergent species. Whereas epistasis among adaptive substitutions has been the subject of extensive study, relatively little is known about epistasis under purifying selection. Here we use mechanistic models of thermodynamic stability in a ligand-binding protein to explore computationally the structure of epistatic interactions among substitutions that fix in protein sequences under purifying selection. We find that the selection coefficients of mutations that are nearly neutral when they fix are highly conditional on the presence of preceding mutations. In addition, substitutions which are initially neutral become increasingly entrenched over time due to antagonistic epistasis with subsequent substitutions. Our evolutionary model includes insertions and deletions, as well as point mutations, which allows us to quantify epistasis between these classes of mutations, and also to study the evolution of protein length. We find that protein length remains largely constant over time, because indels are more deleterious than point mutations. Our results imply that, even under purifying selection, protein sequence evolution is highly contingent on history and it cannot be predicted by the phenotypic effects of mutations introduced into the wildtype sequence alone.},
  archiveprefix = {arXiv},
  arxivid = {1404.4005},
  isbn = {0027-8424},
  pmid = {26056312},
  keywords = {nosource}
}

@article{Shao2000,
  title = {Estimation for Hazardous Concentrations Based on {{NOEC}} Toxicity Data: An Alternative Approach},
  author = {Shao, Quanxi},
  year = {2000},
  month = sep,
  journal = {Environmetrics},
  volume = {11},
  number = {5},
  pages = {583--595},
  issn = {1180-4009},
  doi = {10.1002/1099-095X(200009/10)11:5<583::AID-ENV456>3.0.CO;2-X},
  keywords = {bootstrapping,Bootstrapping,burr iii distribution,Burr III distribution,cml,CML,delta-method,Delta-method,duplicate-citation-key,limiting distribution,Limiting distribution,noec,NOEC,nosource}
}

@article{shapiro1965analysis,
  title = {An Analysis of Variance Test for Normality (Complete Samples)},
  author = {Shapiro, S S and Wilk, M B},
  year = {1965},
  journal = {Biometrika},
  volume = {52},
  number = {3/4},
  pages = {591--611},
  publisher = {JSTOR},
  issn = {0006-3444},
  doi = {10.1093/biomet/52.3-4.591},
  abstract = {No abstract},
  isbn = {00063444},
  pmid = {3963},
  keywords = {Kolmogorov-Smirnov,Normality,nosource,Shapiro-Wilk,W test}
}

@article{Shaw-Allen,
  title = {{{CADDIS SSD Generator}}},
  author = {{Shaw-Allen}, P. and Suter II, G.W.},
  keywords = {duplicate-citation-key,nosource}
}

@article{shawCommentsMethodsApproximating1996,
  title = {Comments on "{{Methods}} for {{Approximating Integrals}} in {{Statistics}} with {{Special Emphasis}} on {{Bayesian Integration Problems}}" by {{Michael Evans}} and {{Tim Swartz}}},
  author = {Shaw, J. E. H. and Genz, Alan and Monahan, John and Schervish, Mark J. and Wasserman, Larry},
  year = {1996},
  journal = {Statistical Science},
  volume = {11},
  number = {1},
  eprint = {2246200},
  eprinttype = {jstor},
  pages = {54--64},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  urldate = {2023-08-01},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/pCloudDrive/papers/Shaw et al_1996_Comments on Methods for Approximating Integrals in Statistics with Special.pdf}
}

@article{Shen:2001p194,
  title = {Rates of Convergence of Posterior Distributions},
  author = {Shen, Xiaotong and Wasserman, Larry},
  year = {2001},
  journal = {Annals of Statistics},
  volume = {29},
  number = {3},
  pages = {687--714},
  issn = {00905364},
  doi = {10.1214/aos/1009210686},
  abstract = {We compute the rate at which the posterior distribution{\textbackslash}n{\textbackslash}t{\textbackslash}t{\textbackslash}t{\textbackslash}tconcentrates around the true parameter value. The spaces we work in are quite{\textbackslash}n{\textbackslash}t{\textbackslash}t{\textbackslash}t{\textbackslash}tgeneral and include in finite dimensional cases. The rates are driven by two{\textbackslash}n{\textbackslash}t{\textbackslash}t{\textbackslash}t{\textbackslash}tquantities: the size of the space, as measured by bracketing entropy, and the{\textbackslash}n{\textbackslash}t{\textbackslash}t{\textbackslash}t{\textbackslash}tdegree to which the prior concentrates in a small ball around the true{\textbackslash}n{\textbackslash}t{\textbackslash}t{\textbackslash}t{\textbackslash}tparameter. We consider two examples.},
  keywords = {Asymptotic inference,Bayesian inference,Non-parametric models,nosource,Sieves}
}

@article{shen2012adaptive,
  title = {Adaptive {{Bayesian}} Procedures Using Random Series Prior},
  author = {Shen, Weining and Ghosal, Subhashis},
  year = {2012},
  journal = {Preprint},
  keywords = {duplicate-citation-key,nosource}
}

@article{shen2012adaptive,
  title = {{{MCMC-free}} Adaptive {{Bayesian}} Procedures Using Random Series Prior},
  author = {Shen, Weining},
  year = {2008},
  journal = {Preprint},
  number = {1},
  pages = {1--25},
  keywords = {adaptive convergence rate,b-splines,duplicate-citation-key,gaussian process,mcmc-free computa-,nosource,random series prior,tion}
}

@article{shenhavFEASTFastExpectationmaximization2019,
  title = {{{FEAST}}: Fast Expectation-Maximization for Microbial Source Tracking},
  shorttitle = {{{FEAST}}},
  author = {Shenhav, Liat and Thompson, Mike and Joseph, Tyler A. and Briscoe, Leah and Furman, Ori and Bogumil, David and Mizrahi, Itzhak and Pe'er, Itsik and Halperin, Eran},
  year = {2019},
  month = jul,
  journal = {Nature Methods},
  volume = {16},
  number = {7},
  pages = {627--632},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0431-x},
  urldate = {2024-01-24},
  abstract = {A major challenge of analyzing the compositional structure of microbiome data is identifying its potential origins. Here, we introduce fast expectation-maximization microbial source tracking (FEAST), a ready-to-use scalable framework that can simultaneously estimate the contribution of thousands of potential source environments in a timely manner, thereby helping unravel the origins of complex microbial communities (https://github.com/cozygene/FEAST). The information gained from FEAST may provide insight into quantifying contamination, tracking the formation of developing microbial communities, as well as distinguishing and characterizing bacteria-related health conditions.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Ecology,Microbiology},
  file = {/home/gkonkamking/pCloudDrive/papers/Shenhav et al_2019_FEAST.pdf}
}

@article{Shi2014,
  title = {Weighted Species Sensitivity Distribution Method to Derive Site-Specific Quality Criteria for Copper in {{Tai Lake}}, {{China}}},
  author = {Shi, Rui and Yang, Chunhui and Su, Runhua and Jin, Jiarui and Chen, Yi and Liu, Hongling and Giesy, John P. and Yu, Hongxia},
  year = {2014},
  journal = {Environmental Science and Pollution Research},
  volume = {21},
  number = {22},
  pages = {12968--12978},
  issn = {16147499},
  doi = {10.1007/s11356-014-3156-5},
  abstract = {Tai Lake (Ch: Taihu), which is the largest lake in Jiangsu province, China, has been affected by human activities. As part of a concerted effort to improve water quality to protect the integrity of the Tai Lake ecosystem, a water quality criterion (WQC) was developed for copper (Cu) II. The acute WQC was based on 440 values for acute toxicity of Cu to 24 species from 6 phyla, 16 families, and 20 genera. In addition, 255 values for chronic toxicity of Cu to 10 species from 5 phyla, 8 families, and 9 genera were used to derive chronic WQC. Instead of using a traditional approach based species sensitivity distributions (SSD), a weighted species sensitivity distribution (WSSD) approach was used to calculate the cumulative probability based on endemic species to Tai Lake. Acute and chronic WQC developed by use of the WSSD were 5.3 and 3.7 mug Cu/L, respectively. While the WQC values were comparable to those of other countries, there were slight differences due to variability in species composition of different regions. The site-specific criteria indicated that the current standard set for surface water by the Chinese government might not be protective of aquatic organisms in Tai Lake.},
  isbn = {1614-7499 (Electronic){\textbackslash}r0944-1344 (Linking)},
  pmid = {24984917},
  keywords = {Asia,Model,nosource,Probabilistic,Statistics,Weighted species sensitivity distributions}
}

@article{shinFunctionalHorseshoePriors2020,
  title = {Functional {{Horseshoe Priors}} for {{Subspace Shrinkage}}},
  author = {Shin, Minsuk and Bhattacharya, Anirban and Johnson, Valen E.},
  year = {2020},
  month = oct,
  journal = {Journal of the American Statistical Association},
  volume = {115},
  number = {532},
  pages = {1784--1797},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2019.1654875},
  urldate = {2022-06-25},
  abstract = {We introduce a new shrinkage prior on function spaces, called the functional horseshoe (fHS) prior, that encourages shrinkage toward parametric classes of functions. Unlike other shrinkage priors for parametric models, the fHS shrinkage acts on the shape of the function rather than inducing sparsity on model parameters. We study the efficacy of the proposed approach by showing an adaptive posterior concentration property on the function. We also demonstrate consistency of the model selection procedure that thresholds the shrinkage parameter of the fHS prior. We apply the fHS prior to nonparametric additive models and compare its performance with procedures based on the standard horseshoe prior and several penalized likelihood approaches. We find that the new procedure achieves smaller estimation error and more accurate model selection than other procedures in several simulated and real examples. Supplementary materials for this article, which contain additional simulated and real data examples, MCMC diagnostics, and proofs of the theoretical results, are available online.},
  pmid = {33716358},
  keywords = {Additive model,Bayesian shrinkage,Nonparametric regression,Posterior contraction},
  file = {/home/gkonkamking/Zotero/storage/RB8ZUFCD/Shin et al. - 2020 - Functional Horseshoe Priors for Subspace Shrinkage.pdf}
}

@article{shiUltrarapidMetagenotypingHuman2020,
  title = {Ultra-Rapid Metagenotyping of the Human Gut Microbiome},
  author = {Shi, Zhou Jason and Dimitrov, Boris and Zhao, Chunyu and Nayfach, Stephen and Pollard, Katherine S.},
  year = {2020},
  month = jun,
  journal = {bioRxiv},
  pages = {2020.06.12.149336},
  publisher = {Cold Spring Harbor Laboratory},
  doi = {10.1101/2020.06.12.149336},
  urldate = {2021-05-02},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Sequence variation is used to quantify population structure and identify genetic determinants of phenotypes that vary within species. In the human microbiome and other environments, single nucleotide polymorphisms (SNPs) are frequently detected by aligning metagenomic sequencing reads to catalogs of genes or genomes. But this requires high-performance computing and enough read coverage to distinguish SNPs from sequencing errors. We solved these problems by developing the GenoTyper for Prokaytotes (GT-Pro), a suite of novel methods to catalog SNPs from genomes and use exact k-mer matches to perform ultra-fast reference-based SNP calling from metagenomes. Compared to read alignment, GT-Pro is more accurate and two orders of magnitude faster. We discovered 104 million SNPs in 909 human gut species, characterized their global population structure, and tracked pathogenic strains. GT-Pro democratizes strain-level microbiome analysis by making it possible to genotype hundreds of metagenomes on a personal computer.{$<$}/p{$><$}h3{$>$}Software availability{$<$}/h3{$>$} {$<$}p{$>$}GT-Pro is available at https://github.com/zjshi/gt-pro.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english}
}

@article{Silva2006,
  title = {The Log of Gravity},
  author = {Silva, J M C Santos and Tenreyro, Silvana},
  year = {2006},
  volume = {88},
  number = {November},
  pages = {641--658},
  keywords = {nosource}
}

@book{silverman1986density,
  title = {Density Estimation for Statistics and Data Analysis},
  author = {Silverman, Bernard W},
  year = {1986},
  volume = {26},
  publisher = {CRC press},
  keywords = {nosource}
}

@unpublished{Simini,
  title = {A Universal Model for Mobility and Migration Patterns},
  booktitle = {Nature},
  author = {Simini, Filippo and Gonz{\'a}lez, Marta Cecilia and Maritan, Amos and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2013},
  volume = {484},
  number = {7392},
  eprint = {1111.0586},
  issn = {1476-4687},
  doi = {10.1038/nature10856},
  abstract = {Nature 484, 96 (2012). doi:10.1038/nature10856},
  archiveprefix = {arXiv},
  arxivid = {1111.0586},
  isbn = {1476-4687 (Electronic){\textbackslash}r0028-0836 (Linking)},
  pmid = {22367540},
  keywords = {nosource}
}

@article{simmonsFalsePositivePsychologyUndisclosed2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  urldate = {2020-02-11},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  keywords = {disclosure,methodology,motivated reasoning,publication},
  file = {/home/gkonkamking/Zotero/storage/3PSNEB7V/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@article{simonsohnSmallTelescopesDetectability2015,
  title = {Small {{Telescopes}}: {{Detectability}} and the {{Evaluation}} of {{Replication Results}}},
  shorttitle = {Small {{Telescopes}}},
  author = {Simonsohn, Uri},
  year = {2015},
  month = may,
  journal = {Psychological Science},
  volume = {26},
  number = {5},
  pages = {559--569},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797614567341},
  urldate = {2020-03-10},
  abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating ``unsuccessful'' replication attempts (i.e., studies yielding p {$>$} .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) ``protecting'' true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular.},
  langid = {english},
  keywords = {nosource}
}

@techreport{simonsohnSpecificationCurveDescriptive2019,
  type = {{{SSRN Scholarly Paper}}},
  title = {Specification {{Curve}}: {{Descriptive}} and {{Inferential Statistics}} on {{All Reasonable Specifications}}},
  shorttitle = {Specification {{Curve}}},
  author = {Simonsohn, Uri and Simmons, Joseph P. and Nelson, Leif D.},
  year = {2019},
  month = oct,
  number = {ID 2694998},
  address = {Rochester, NY},
  institution = {Social Science Research Network},
  doi = {10.2139/ssrn.2694998},
  urldate = {2020-03-05},
  abstract = {Empirical results often hinge on data analytic decisions that are simultaneously defensible, arbitrary, and motivated. To mitigate this problem we introduce Specification-Curve Analysis, which consists of three steps: (i) identifying the set of theoretically justified, statistically valid, and non-redundant analytic specifications, (ii) displaying alternative results graphically, allowing the identification of decisions producing different results, and (iii) conducting statistical tests to determine whether as a whole results are inconsistent with the null hypothesis. We illustrate its use by applying it to three published findings. One proves robust, one weak, one not robust at all.},
  langid = {english},
  keywords = {p-hacking,Specification Curve},
  file = {/home/gkonkamking/Zotero/storage/5XNR3DM2/Simonsohn et al. - 2019 - Specification Curve Descriptive and Inferential S.pdf}
}

@article{simpson1949measurement,
  title = {Measurement of Diversity},
  author = {Simpson, E. H.},
  year = {1949},
  journal = {Nature},
  volume = {163},
  number = {4148},
  pages = {688--688},
  publisher = {New York},
  issn = {0028-0836},
  doi = {10.1038/163688a0},
  abstract = {Simpson Index calculation},
  isbn = {0028-0836},
  keywords = {nosource}
}

@inproceedings{singerWhyWeRead2017,
  title = {Why {{We Read Wikipedia}}},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web}}},
  author = {Singer, Philipp and Lemmerich, Florian and West, Robert and Zia, Leila and Wulczyn, Ellery and Strohmaier, Markus and Leskovec, Jure},
  year = {2017},
  month = apr,
  pages = {1591--1600},
  publisher = {International World Wide Web Conferences Steering Committee},
  address = {Perth Australia},
  doi = {10.1145/3038912.3052716},
  urldate = {2023-10-23},
  abstract = {Wikipedia is one of the most popular sites on the Web, with millions of users relying on it to satisfy a broad range of information needs every day. Although it is crucial to understand what exactly these needs are in order to be able to meet them, little is currently known about why users visit Wikipedia. The goal of this paper is to fill this gap by combining a survey of Wikipedia readers with a log-based analysis of user activity. Based on an initial series of user surveys, we build a taxonomy of Wikipedia use cases along several dimensions, capturing users' motivations to visit Wikipedia, the depth of knowledge they are seeking, and their knowledge of the topic of interest prior to visiting Wikipedia. Then, we quantify the prevalence of these use cases via a large-scale user survey conducted on live Wikipedia with almost 30,000 responses. Our analyses highlight the variety of factors driving users to Wikipedia, such as current events, media coverage of a topic, personal curiosity, work or school assignments, or boredom. Finally, we match survey responses to the respondents' digital traces in Wikipedia's server logs, enabling the discovery of behavioral patterns associated with specific use cases. For instance, we observe long and fast-paced page sequences across topics for users who are bored or exploring randomly, whereas those using Wikipedia for work or school spend more time on individual articles focused on topics such as science. Our findings advance our understanding of reader motivations and behavior on Wikipedia and can have implications for developers aiming to improve Wikipedia's user experience, editors striving to cater to their readers' needs, third-party services (such as search engines) providing access to Wikipedia content, and researchers aiming to build tools such as recommendation engines.},
  isbn = {978-1-4503-4913-0},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/IKM49C26/Singer et al. - 2017 - Why We Read Wikipedia.pdf}
}

@article{singh2010seasonal,
  title = {Seasonal Diatom Variations with Reference to Physico-Chemical Properties of Water of {{Mansagar}} Lake of {{Jaipur}}, {{Rajasthan}}},
  author = {Singh, Meenakshi and Lodha, Payal and Singh, Gajendra Pal},
  year = {2010},
  journal = {Research Journal of Agricultural Sciences},
  volume = {1},
  number = {4},
  pages = {451--457},
  keywords = {nosource}
}

@article{sjolander1996dirichlet,
  title = {Dirichlet Mixtures: A Method for Improved Detection of Weak but Significant Protein Sequence Homology.},
  author = {Sj{\"o}lander, K and Karplus, K and Brown, M and Hughey, R and Krogh, a and Mian, I S and Haussler, D},
  year = {1996},
  journal = {Computer applications in the biosciences : CABIOS},
  volume = {12},
  number = {4},
  pages = {327--345},
  publisher = {Oxford Univ Press},
  issn = {0266-7061},
  doi = {10.1093/bioinformatics/12.4.327},
  abstract = {We present a method for condensing the information in multiple alignments of proteins into a mixture of Dirichlet densities over amino acid distributions. Dirichlet mixture densities are designed to be combined with observed amino acid frequencies to form estimates of expected amino acid probabilities at each position in a profile, hidden Markov model or other statistical model. These estimates give a statistical model greater generalization capacity, so that remotely related family members can be more reliably recognized by the model. This paper corrects the previously published formula for estimating these expected probabilities, and contains complete derivations of the Dirichlet mixture formulas, methods for optimizing the mixtures to match particular databases, and suggestions for efficient implementation.},
  isbn = {0266-7061 (Print)},
  pmid = {8902360},
  keywords = {nosource}
}

@article{Smetanova2014,
  title = {Do Predictions from {{Species Sensitivity Distributions}} Match with Field Data?},
  author = {Smetanov{\'a}, S. and Bl{\'a}ha, L. and Liess, M. and Sch{\"a}fer, R. B. and Beketov, M. A.},
  year = {2014},
  month = jun,
  journal = {Environmental Pollution},
  volume = {189},
  eprint = {24657606},
  eprinttype = {pubmed},
  pages = {126--133},
  publisher = {Elsevier Ltd},
  issn = {18736424},
  doi = {10.1016/j.envpol.2014.03.002},
  abstract = {Species Sensitivity Distribution (SSD) is a statistical model that can be used to predict effects of contaminants on biological communities, but only few comparisons of this model with field studies have been conducted so far. In the present study we used measured pesticides concentrations from streams in Germany, France, and Finland, and we used SSD to calculate msPAF (multiple substance potentially affected fraction) values based on maximum toxic stress at localities. We compared these SSD-based predictions with the actual effects on stream invertebrates quantified by the SPEARpesticides bioindicator. The results show that the msPAFs correlated well with the bioindicator, however, the generally accepted SSD threshold msPAF of 0.05 (5\% of species are predicted to be affected) severely underestimated the observed effects (msPAF values causing significant effects are 2-1000-times lower). These results demonstrate that validation with field data is required to define the appropriate thresholds for SSD predictions. {\copyright} 2014 Elsevier Ltd. All rights reserved.},
  isbn = {0269-7491},
  pmid = {24657606},
  keywords = {Freshwater,nosource,Pollution,Risk assessment,Rivers,Statistical modelling}
}

@article{smillieStrainTrackingReveals2018,
  title = {Strain {{Tracking Reveals}} the {{Determinants}} of {{Bacterial Engraftment}} in the {{Human Gut Following Fecal Microbiota Transplantation}}},
  author = {Smillie, Christopher S. and Sauk, Jenny and Gevers, Dirk and Friedman, Jonathan and Sung, Jaeyun and Youngster, Ilan and Hohmann, Elizabeth L. and Staley, Christopher and Khoruts, Alexander and Sadowsky, Michael J. and Allegretti, Jessica R. and Smith, Mark B. and Xavier, Ramnik J. and Alm, Eric J.},
  year = {2018},
  month = feb,
  journal = {Cell Host \& Microbe},
  volume = {23},
  number = {2},
  pages = {229-240.e5},
  issn = {1931-3128},
  doi = {10.1016/j.chom.2018.01.003},
  urldate = {2020-10-12},
  abstract = {Fecal microbiota transplantation (FMT) from healthy donor to patient is a treatment for microbiome-associated diseases. Although the success of FMT requires donor bacteria to engraft in the patient's gut, the forces governing engraftment in humans are unknown. Here we use an ongoing clinical experiment, the treatment of recurrent Clostridium difficile infection, to uncover the rules of engraftment in humans. We built a statistical model that predicts which bacterial species will engraft in a given host, and developed Strain Finder, a method to infer strain genotypes and track them over time. We find that engraftment can be predicted largely from the abundance and phylogeny of bacteria in the donor and the pre-FMT patient. Furthermore, donor strains within a species engraft in an all-or-nothing manner and previously undetected strains frequently colonize patients receiving FMT. We validated these findings for metabolic syndrome, suggesting that the same principles of engraftment extend to other indications.},
  langid = {english},
  keywords = {bacterial engraftment,fecal microbiota transplant,fecal transplant,FMT,human microbiome,human microbiota,strain inference,strain tracking}
}

@article{Smit2008,
  title = {Species Sensitivity Distributions for Suspended Clays, Sediment Burial, and Grain Size Change in the Marine Environment},
  author = {Smit, Mathijs G.D. and Holthaus, Karlijn I.E. and Trannum, Hilde C. and Neff, Jerry M. and {Kjeilen-Eilertsen}, Grete and Jak, Robbert G. and Singsaas, Ivar and Huijbregts, Mark A.J. and Hendriks, A. Jan},
  year = {2008},
  month = apr,
  journal = {Environmental Toxicology and Chemistry},
  volume = {27},
  number = {4},
  pages = {1006},
  issn = {0730-7268},
  doi = {10.1897/07-339.1},
  abstract = {Assessment of the environmental risk of discharges, containing both chemicals and suspended solids (e.g., drilling discharges to the marine environment), requires an evaluation of the effects of both toxic and nontoxic pollutants. To date, a structured evaluation scheme that can be used for prognostic risk assessments for nontoxic stress is lacking. In the present study we challenge this lack of information by the development of marine species sensitivity distributions (SSDs) for three nontoxic stressors: suspended clays, burial by sediment, and change in sediment grain size. Through a literature study, effect levels were obtained for suspended clays, as well as for burial of biota. Information on the species preference range for median grain size was used to assess the sensitivity of marine species to changes in grain size. The 50\% hazardous concentrations (HC50) for suspended barite and bentonite based on 50\% effect concentrations (EC50s) were 3,010 and 1,830 mg/L, respectively. For burial the 50\% hazardous level (HL50) was 5.4 cm. For change in median grain size, two SSDs were constructed; one for reducing and one for increasing the median grain size. The HL50 for reducing the median grain size was 17.8 mum. For increasing the median grain size this value was 305 mum. The SSDs have been constructed by using information related to offshore oil- and gas-related activities. Nevertheless, the results of the present study may have broader implications. The hypothesis of the present study is that the SSD methodology developed for the evaluation of toxic stress can also be applied to evaluate nontoxic stressors, facilitating the incorporation of nontoxic stressors in prognostic risk assessment tools.},
  pmid = {18333685},
  keywords = {Aluminum Silicates,Animals,Chemical,Chemical: toxicity,Geologic Sediments,nosource,Risk Assessment,Sensitivity and Specificity,Water Pollutants}
}

@article{Smit2008a,
  title = {Time and Concentration Dependency in the Potentially Affected Fraction of Species: The Case of Hydrogen Peroxide Treatment of Ballast Water.},
  author = {Smit, Mathijs G D and Ebbens, Eltjo and Jak, Robbert G and Huijbregtst, Mark a J},
  year = {2008},
  journal = {Environmental toxicology and chemistry / SETAC},
  volume = {27},
  number = {3},
  pages = {746--753},
  issn = {0730-7268},
  doi = {10.1897/07-343.1},
  abstract = {Transport of large volumes of ballast water contributes greatly to invasions of species. Hydrogen peroxide (H2O2) can be used as a disinfectant to prevent the spread of exotic species via ballast water. Instead of using environmental risk assessment techniques for protecting a certain fraction of the species from being affected, the present study aimed to apply these techniques to define treatment regimes of H2O2 and effectively eliminate as many species as possible. Based on time-dependent dose-response curves for five marine species (Corophium volutator, Artemia salina, Brachionus plicatilis, Dunaliella teriolecta, and Skeletonema costatum), time-dependent species-sensitivity distributions (SSDs) were derived for different effect sizes. The present study showed that H2O2 can be used effectively to treat ballast water but that relatively high concentrations and long treatment durations are required to eliminate the vast majority of species in ballast water. The described toxicant effectiveness approach using SSDs also has other potential fields of application, including short-term application of biocides.},
  isbn = {1552-8618},
  pmid = {17983276},
  keywords = {species-sensitivity distributions},
  file = {/home/gkonkamking/Zotero/storage/3GNYL722/Smit et al. - 2008 - Time and concentration dependency in the potential.pdf}
}

@article{Smith1993,
  title = {Extrapolation Methods for Setting Ecological Standards for Water Quality: Statistical and Ecological Concerns},
  author = {Smith, Eric P. and Cairns, John},
  year = {1993},
  month = sep,
  journal = {Ecotoxicology},
  volume = {2},
  number = {3},
  pages = {203--219},
  issn = {0963-9292},
  doi = {10.1007/BF00116425},
  pmid = {24201582},
  keywords = {aquatic organisms,complex and sometimes subtle,comprised of a potentially,duplicate-citation-key,ecosystems are complex entities,extrapolation,including predator-prey,interactions and components,introduction and objectives,large number of species,nosource,prediction,toxicity,with}
}

@article{Smith2007,
  title = {Boa : {{An}} r Package for {{MCMC}} Output Convergence},
  author = {Smith, Brian J},
  year = {2007},
  journal = {Journal of Statistical Software},
  volume = {21},
  number = {11},
  pages = {1--37},
  issn = {15487660},
  abstract = {Markov chain Monte Carlo (MCMC) is the most widely used method of estimating joint posterior distributions in Bayesian analysis. The idea of MCMC is to iteratively produce parameter values that are representative samples from the joint posterior. Unlike frequentist analysis where iterative model fitting routines are monitored for convergence to a single point, MCMC output is monitored for convergence to a distribution. Thus, specialized diagnostic tools are needed in the Bayesian setting. To this end, the R package boa was created. This manuscript presents the user's manual for boa, which outlines the use of and methodology upon which the software is based. Included is a description of the menu system, data management capabilities, and statistical/graphical methods for convergence assessment and posterior inference. Throughout the manual, a linear regression example is used to illustrate the software.},
  isbn = {1548-7660},
  keywords = {bayesian analysis,Bayesian analysis,convergence diagnostics,markov chain monte carlo,Markov chain Monte Carlo,nosource,posterior,posterior inference,R}
}

@article{smithMonitoringRenalTransplants1983,
  title = {Monitoring {{Renal Transplants}}: {{An Application}} of the {{Multiprocess Kalman Filter}}},
  shorttitle = {Monitoring {{Renal Transplants}}},
  author = {Smith, A. F. M. and West, M.},
  year = {1983},
  journal = {Biometrics},
  volume = {39},
  number = {4},
  eprint = {2531322},
  eprinttype = {jstor},
  pages = {867--878},
  publisher = {[Wiley, International Biometric Society]},
  issn = {0006-341X},
  doi = {10.2307/2531322},
  urldate = {2021-07-29},
  abstract = {The multiprocess Kalman filter offers a powerful general framework for the modelling and analysis of noisy time series which are subject to abrupt changes in pattern. It has considerable potential application to many forms of biological series used in clinical monitoring. In particular, the approach can be used to provide on-line probabilities of whether changes have occurred, as well as to identify the type of change that is involved. In this paper, we extend and illustrate the methodology within the context of a particular case study. The general features of the problem, and the approach adopted, will be seen to have wide application.},
  file = {/home/gkonkamking/Zotero/storage/3K2DWTJ3/Smith and West - 1983 - Monitoring Renal Transplants An Application of th.pdf}
}

@article{smithScalableMicrobialStrain2022,
  title = {Scalable {{Microbial Strain Inference}} in {{Metagenomic Data Using StrainFacts}}},
  author = {Smith, Byron J. and Li, Xiangpeng and Shi, Zhou Jason and Abate, Adam and Pollard, Katherine S.},
  year = {2022},
  journal = {Frontiers in Bioinformatics},
  volume = {2},
  issn = {2673-7647},
  doi = {10.3389/fbinf.2022.867386},
  urldate = {2023-12-19},
  abstract = {While genome databases are nearing a complete catalog of species commonly inhabiting the human gut, their representation of intraspecific diversity is lacking for all but the most abundant and frequently studied taxa. Statistical deconvolution of allele frequencies from shotgun metagenomic data into strain genotypes and relative abundances is a promising approach, but existing methods are limited by computational scalability. Here we introduce StrainFacts, a method for strain deconvolution that enables inference across tens of thousands of metagenomes. We harness a ``fuzzy'' genotype approximation that makes the underlying graphical model fully differentiable, unlike existing methods. This allows parameter estimates to be optimized with gradient-based methods, speeding up model fitting by two orders of magnitude. A GPU implementation provides additional scalability. Extensive simulations show that StrainFacts can perform strain inference on thousands of metagenomes and has comparable accuracy to more computationally intensive tools. We further validate our strain inferences using single-cell genomic sequencing from a human stool sample. Applying StrainFacts to a collection of more than 10,000 publicly available human stool metagenomes, we quantify patterns of strain diversity, biogeography, and linkage-disequilibrium that agree with and expand on what is known based on existing reference genomes. StrainFacts paves the way for large-scale biogeography and population genetic studies of microbiomes using metagenomic data.},
  file = {/home/gkonkamking/pCloudDrive/papers/Smith et al_2022_Scalable Microbial Strain Inference in Metagenomic Data Using StrainFacts.pdf}
}

@article{SO14,
  title = {On the Use of Conformal Maps for the Acceleration of Convergence of the Trapezoidal Rule and {{Sinc}} Numerical Methods},
  author = {Slevinsky, Richard Mika{\"e}l and Olver, Sheehan},
  year = {2014},
  abstract = {We investigate the use of conformal maps for the acceleration of convergence of the trapezoidal rule and Sinc numerical methods. The conformal map is a polynomial adjustment to the sinh map, and allows the treatment of a finite number of singularities in the complex plane. In the case where locations are unknown, the so-called Sinc-Pad{\'e} approximants are used to provide approximate results. This adaptive method is shown to have almost the same convergence properties. We use the conformal maps to generate high accuracy solutions to several challenging integrals, nonlinear waves, and multidimensional integrals.},
  keywords = {nosource}
}

@article{SOG16,
  title = {Second-Order Switching Time Optimization for Switched Dynamical Systems},
  author = {Stellato, B and {Ober-Bl{\"o}baum}, S and Goulart, P{\~.}J.},
  year = {2016},
  abstract = {Switching time optimization arises in finite-horizon optimal control for switched systems where, given a sequence of continuous dynamics, one minimizes a cost function with respect to the switching times. We propose an efficient method for computing the optimal switching times for switched linear and nonlinear systems. A novel second-order optimization algorithm is introduced where, at each iteration, the dynamics are linearized over an underlying time grid to compute the cost function, the gradient and the Hessian efficiently. With the proposed method, the most expensive operations at each iteration are shared between the cost function and its derivatives, thereby greatly reducing the computational burden. We implemented the algorithm in the Julia package SwitchTimeOpt allowing the user to easily solve switching time optimization problems. In the case of linear dynamics, many operations can be further simplified and benchmarks show that our approach is able to provide optimal solutions in just a few ms. In the case of nonlinear dynamics, two examples show that our method provides optimal solutions with up to two orders of magnitude time reductions over state-of-the-art approaches.},
  keywords = {nosource}
}

@article{Song2010,
  title = {Modelling the Scaling Properties of Human Mobility},
  author = {Song, Chaoming and Koren, Tal and Wang, Pu and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2010},
  month = sep,
  journal = {Nature Physics},
  volume = {6},
  number = {10},
  eprint = {1010.0436},
  pages = {1--6},
  publisher = {Nature Publishing Group},
  issn = {1745-2473},
  doi = {10.1038/NPHYS1760},
  abstract = {Nature Physics 6, 1 (2010). doi:10.1038/nphys1760},
  archiveprefix = {arXiv},
  arxivid = {1010.0436},
  isbn = {1745-2473},
  keywords = {nosource}
}

@article{song2017deepmob,
  title = {{{DeepMob}}: Learning Deep Knowledge of Human Emergency Behavior and Mobility from Big and Heterogeneous Data},
  author = {Song, Xuan and Shibasaki, Ryosuke and Yuan, Nicholos Jing and Xie, Xing and Li, Tao and Adachi, Ryutaro},
  year = {2017},
  journal = {ACM Transactions on Information Systems (TOIS)},
  volume = {35},
  number = {4},
  pages = {1--19},
  publisher = {ACM New York, NY, USA},
  keywords = {nosource}
}

@misc{SpeciesSensitivityToxic,
  title = {Species {{Sensitivity}} to {{Toxic Substances}}: {{Evolution}}, {{Ecology}} and {{Applications}} - {{ProQuest}}},
  shorttitle = {Species {{Sensitivity}} to {{Toxic Substances}}},
  urldate = {2020-12-11},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  howpublished = {https://search.proquest.com/openview/3124c046e8a9b9fc555873d238b1dc40/1?pq-origsite=gscholar\&cbl=2049529},
  langid = {english}
}

@article{spiegelhalter2002bayesian,
  title = {Bayesian Measures of Model Complexity Anf Fit},
  author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, P. Bradley and {van der Linde}, Angelika},
  year = {2002},
  journal = {Journal of the Royal Statistical Society Series B (Statistical Methodology)},
  volume = {64},
  number = {4},
  pages = {583--639},
  publisher = {Wiley Online Library},
  issn = {1369-7412},
  doi = {10.1111/1467-9868.00353},
  abstract = {We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure PD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general PD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the 'hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding PD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.},
  isbn = {1369-7412},
  pmid = {6175412381464386404},
  keywords = {Bayesian model comparison,Decision theory,Deviance information criterion,Effective number of parameters,Hierarchical models,Information theory,Leverage,Markov chain Monte Carlo methods,Model dimension,nosource}
}

@article{spiegelhalterDevianceInformationCriterion2014,
  title = {The Deviance Information Criterion: 12 Years On},
  shorttitle = {The Deviance Information Criterion},
  author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and {van der Linde}, Angelika},
  year = {2014},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {76},
  number = {3},
  eprint = {24774528},
  eprinttype = {jstor},
  pages = {485--493},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {1369-7412},
  urldate = {2024-11-20},
  abstract = {The essentials of our paper of 2002 are briefly summarized and compared with other criteria for model comparison. After some comments on the paper's reception and influence, we consider criticisms and proposals for improvement made by us and others.},
  file = {/home/gkonkamking/pCloudDrive/papers/Spiegelhalter et al. - 2014 - The deviance information criterion 12 years on.pdf}
}

@article{Spielman2015,
  title = {The Relationship between {{dN}}/{{dS}} and Scaled Selection Coefficients},
  author = {Spielman, Stephanie J. and Wilke, Claus O.},
  year = {2015},
  journal = {Molecular Biology and Evolution},
  volume = {32},
  number = {4},
  pages = {1097--1108},
  issn = {15371719},
  doi = {10.1093/molbev/msv003},
  abstract = {Numerous computational methods exist to assess the mode and strength of natural selection in protein-coding sequences, yet how distinct methods relate to one another remains largely unknown. Here, we elucidate the relationship between two widely used phylogenetic modeling frameworks: dN/dS models and mutation-selection (MutSel) models. We derive a mathematical relationship between dN/dS and scaled selection coefficients, the focal parameters of MutSel models, and use this relationship to gain deeper insight into the behaviors, limitations, and applicabilities of these two modeling frameworks. We prove that, if all synonymous changes are neutral, standard MutSel models correspond to dN/dS {$\leq$} 1. However, if synonymous codons differ in fitness, dN/dS can take on arbitrarily high values even if all selection is purifying. Thus, the MutSel modeling framework cannot necessarily accommodate positive, diversifying selection, while dN/dS cannot distinguish between purifying selection on synonymous codons and positive selection on amino acids. We further propose a new benchmarking strategy of dN/dS inferences against MutSel simulations and demonstrate that the widely used Goldman-Yang-style dN/dS models yield substantially biased dN/dS estimates on realistic sequence data. In contrast, the less frequently used Muse-Gaut-style models display much less bias. Strikingly, the least-biased and most precise dN/dS estimates are never found in the models with the best fit to the data, measured through both AIC and BIC scores. Thus, selecting models based on goodness-of-fit criteria can yield poor parameter estimates if the models considered do not precisely correspond to the underlying mechanism that generated the data. In conclusion, establishing mathematical links among modeling frameworks represents a novel, powerful strategy to pinpoint previously unrecognized model limitations and strengths.},
  pmid = {25576365},
  keywords = {dN/dS,Markov models of sequence evolution,mutation-selection models,nosource,protein evolution,scaled selection coefficients}
}

@article{Spiess2010,
  title = {An Evaluation of {{R2}} as an Inadequate Measure for Nonlinear Models in Pharmacological and Biochemical Research: A {{Monte Carlo}} Approach.},
  author = {Spiess, Andrej-Nikolai and Neumeyer, Natalie},
  year = {2010},
  journal = {BMC pharmacology},
  volume = {10},
  pages = {6},
  issn = {1471-2210},
  doi = {10.1186/1471-2210-10-6},
  abstract = {BACKGROUND: It is long known within the mathematical literature that the coefficient of determination R(2) is an inadequate measure for the goodness of fit in nonlinear models. Nevertheless, it is still frequently used within pharmacological and biochemical literature for the analysis and interpretation of nonlinear fitting to data. RESULTS: The intensive simulation approach undermines previous observations and emphasizes the extremely low performance of R(2) as a basis for model validity and performance when applied to pharmacological/biochemical nonlinear data. In fact, with the 'true' model having up to 500 times more strength of evidence based on Akaike weights, this was only reflected in the third to fifth decimal place of R(2). In addition, even the bias-corrected R(2)(adj) exhibited an extreme bias to higher parametrized models. The bias-corrected AICc and also BIC performed significantly better in this respect. CONCLUSION: Researchers and reviewers should be aware that R(2) is inappropriate when used for demonstrating the performance or validity of a certain nonlinear model. It should ideally be removed from scientific literature dealing with nonlinear model fitting or at least be supplemented with other methods such as AIC or BIC or used in context to other models in question.},
  isbn = {1471-2210},
  pmid = {20529254},
  keywords = {Biochemistry,Biomedical Research,Biomedical Research: statistics \& numerical data,Computer Simulation,Data Interpretation,Models,Monte Carlo Method,Nonlinear Dynamics,nosource,Pharmacology,Statistical}
}

@article{Sporns2013,
  title = {The Structure and Function of Complex Networks},
  author = {Newman, M. E. J.},
  year = {2003},
  month = sep,
  journal = {SIAM Review},
  volume = {45},
  number = {2},
  eprint = {24876274},
  eprinttype = {pubmed},
  pages = {167--256},
  issn = {1958-5969},
  doi = {10.1137/S003614450342480},
  abstract = {An increasing number of theoretical and empirical studies approach the function of the human brain from a network perspective. The analysis of brain networks is made feasible by the development of new imaging acquisition methods as well as new tools from graph theory and dynamical systems. This review surveys some of these methodological advances and summarizes recent findings on the architecture of structural and functional brain networks. Studies of the structural connectome reveal several modules or network communities that are interlinked by hub regions mediating communication processes between modules. Recent network analyses have shown that network hubs form a densely linked collective called a "rich club," centrally positioned for attracting and dispersing signal traffic. In parallel, recordings of resting and task-evoked neural activity have revealed distinct resting-state networks that contribute to functions in distinct cognitive domains. Network methods are increasingly applied in a clinical context, and their promise for elucidating neural substrates of brain and mental disorders is discussed. Abstract available from the publisher. Abstract available from the publisher.},
  arxiv = {0303516 [arXiv:cond-mat]},
  arxivid = {arXiv:cond-mat/0303516},
  isbn = {00361445},
  pmid = {24174898},
  keywords = {Animals,Brain,Brain Mapping,Brain Mapping: methods,Brain: anatomy \& histology,Brain: physiology,Humans,Nerve Net,Nerve Net: anatomy \& histology,Nerve Net: physiology,Neural Pathways,Neural Pathways: anatomy \& histology,Neural Pathways: physiology,Neuroimaging,Neuroimaging: methods,nosource,Psychomotor Performance,Psychomotor Performance: physiology,Rest,Rest: physiology}
}

@article{sporysheva2013bivariate,
  title = {Bivariate Species Sampling Models},
  author = {Sporysheva, Polina and Petrone, Sonia},
  year = {2013},
  journal = {Manuscript under preparation},
  keywords = {nosource}
}

@article{squire19881936,
  title = {Why the 1936 {{Literary Digest}} Poll Failed},
  author = {Squire, Peverill},
  year = {1988},
  journal = {Public Opinion Quarterly},
  volume = {52},
  number = {1},
  pages = {125--133},
  publisher = {Oxford University Press},
  doi = {10.1086/269085}
}

@article{Srivastava2005,
  title = {{{BIODIVERSITY-ECOSYSTEM FUNCTION RESEARCH}}: {{Is}} It Relevant to Conservation?},
  author = {Srivastava, Diane S. and Vellend, Mark},
  year = {2005},
  journal = {Annual Review of Ecology, Evolution, and Systematics},
  volume = {36},
  number = {1},
  pages = {267--294},
  issn = {1543-592X},
  doi = {10.1146/annurev.ecolsys.36.102003.152636},
  abstract = {It has often been argued that conserving biodiversity is necessary for maintaining ecosystem functioning. We critically evaluate the current evidence for this argument. Although there is substantial evidence that diversity is able to affect function, particularly for plant communities, it is unclear if these patterns will hold for realistic scenarios of extinctions, multitrophic communities, or larger spatial scales. Experiments are conducted at small spatial scales, the very scales at which diversity tends to increase owing to exotics. Stressors may affect function by many pathways, and diversity-mediated effects on function may be a minor pathway, except in the case of multiple-stressor insurance effects. In general, the conservation case is stronger for stability measures of function than stock and flux measures, in part because it is easier to attribute value unambiguously to stability and in part because stock and flux measures of functions are anticipated to be more affected by multitrophic dynamics. Nor is biodiversity-ecosystem function theory likely to help conservation managers in practical decisions, except in the particular case of restoration. We give recommendations for increasing the relevance of this area of research for conservation. CR - Copyright \&\#169; 2005 Annual Reviews},
  isbn = {1543-592X},
  pmid = {426},
  keywords = {abstract it has often,been argued that conserving,biodiversity is necessary for,current evidence for,diversity,ecosystem services,extinction,maintaining ecosystem functioning,nosource,species richness,stability,we critically evaluate the}
}

@article{stadlbauer2022radiophysiomics,
  title = {Radiophysiomics: Brain Tumors Classification by Machine Learning and Physiological {{MRI}} Data},
  author = {Stadlbauer, Andreas and Marhold, Franz and Oberndorfer, Stefan and Heinz, Gertraud and Buchfelder, Michael and Kinfe, Thomas M and {Meyer-B{\"a}se}, Anke},
  year = {2022},
  journal = {Cancers},
  volume = {14},
  number = {10},
  pages = {2363},
  publisher = {MDPI},
  doi = {10.3390/cancers14102363}
}

@misc{stan-manual:2014,
  title = {Stan Modeling Language {{Users Guide}} and {{Reference Manual}}},
  author = {{Stan Development Team}},
  year = {2021},
  keywords = {duplicate-citation-key,nosource}
}

@article{stan-software:2015,
  title = {Stan: {{A C}}++ Library for Probability and Sampling, Version 2.7.0},
  booktitle = {Http://{{Mc-Stan}}.{{Org}}/},
  author = {{Stan Development Team} and {Stan Developement Team}},
  year = {2015},
  keywords = {nosource}
}

@book{Stanford,
  title = {Statistical Methods for Physical Science},
  author = {Stanford, John and Vardeman, Stepen},
  year = {1994},
  abstract = {This volume of Methods of Experimental Physics provides an extensive introduction to probability and statistics in many areas of the physical sciences, with an emphasis on the emerging area of spatial statistics. The scope of topics covered is wide-ranging-the text discusses a variety of the most commonly used classical methods and addresses newer methods that are applicable or potentially important. The chapter authors motivate readers with their insightful discussions, augmenting their material withKey Features* Examines basic probability, including coverage of standard distributions, time series models, and Monte Carlo methods* Describes statistical methods, including basic inference, goodness of fit, maximum likelihood, and least squares* Addresses time series analysis, including filtering and spectral analysis* Includes simulations of physical experiments* Features applications of statistics to atmospheric physics and radio astronomy* Covers the increasingly important area of modern statistical computing},
  isbn = {978-0-08-086016-9},
  keywords = {nosource}
}

@article{stanleyClusteringNetworkLayers2016,
  title = {Clustering {{Network Layers}} with the {{Strata Multilayer Stochastic Block Model}}},
  author = {Stanley, Natalie and Shai, Saray and Taylor, Dane and Mucha, Peter J.},
  year = {2016},
  month = apr,
  journal = {IEEE Transactions on Network Science and Engineering},
  volume = {3},
  number = {2},
  pages = {95--105},
  issn = {2327-4697},
  doi = {10.1109/TNSE.2016.2537545},
  urldate = {2023-01-13},
  abstract = {Multilayer networks are a useful data structure for simultaneously capturing multiple types of relationships between a set of nodes. In such networks, each relational definition gives rise to a layer. While each layer provides its own set of information, community structure across layers can be collectively utilized to discover and quantify underlying relational patterns between nodes. To concisely extract information from a multilayer network, we propose to identify and combine sets of layers with meaningful similarities in community structure. In this paper, we describe the ``strata multilayer stochastic block model'' (sMLSBM), a probabilistic model for multilayer community structure. The central extension of the model is that there exist groups of layers, called ``strata'', which are defined such that all layers in a given stratum have community structure described by a common stochastic block model (SBM). That is, layers in a stratum exhibit similar node-to-community assignments and SBM probability parameters. Fitting the sMLSBM to a multilayer network provides a joint clustering that yields node-to-community and layer-to-stratum assignments, which cooperatively aid one another during inference. We describe an algorithm for separating layers into their appropriate strata and an inference technique for estimating the SBM parameters for each stratum. We demonstrate our method using synthetic networks and a multilayer network inferred from data collected in the Human Microbiome Project.},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Stanley et al_2016_Clustering Network Layers with the Strata Multilayer Stochastic Block Model.pdf}
}

@misc{stantonParticleBasedInference2024,
  title = {Particle {{Based Inference}} for {{Continuous-Discrete State Space Models}}},
  author = {Stanton, Christopher and Beskos, Alexandros},
  year = {2024},
  month = jul,
  number = {arXiv:2407.15666},
  eprint = {2407.15666},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.15666},
  urldate = {2024-07-30},
  abstract = {This article develops a methodology allowing application of the complete machinery of particle-based inference methods upon what we call the class of continuous-discrete State Space Models (CD-SSMs). Such models correspond to a latent continuous-time It{\textbackslash}{\textasciicircum}o diffusion process which is observed with noise at discrete time instances. Due to the continuous-time nature of the hidden signal, standard Feynman-Kac formulations and their accompanying particle-based approximations have to overcome several challenges, arising mainly due to the following considerations: (i) finite-time transition densities of the signal are typically intractable; (ii) ancestors of sampled signals are determined w.p.{\textasciitilde}1, thus cannot be resampled; (iii) diffusivity parameters given a sampled signal yield Dirac distributions. We overcome all above issues by introducing a framework based on carefully designed proposals and transformations thereof. That is, we obtain new expressions for the Feynman-Kac model that accommodate the effects of a continuous-time signal and overcome induced degeneracies. The constructed formulations will enable use of the full range of particle-based algorithms for CD-SSMs: for filtering/smoothing and parameter inference, whether online or offline. Our framework is compatible with guided proposals in the filtering steps that are essential for efficient algorithmic performance in the presence of informative observations or in higher dimensions, and is applicable for a very general class of CD-SSMs, including the case when the signal is modelled as a hypo-elliptic diffusion. Our methods can be immediately incorporated to available software packages for particle-based algorithms.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/gkonkamking/pCloudDrive/papers/Stanton_Beskos_2024_Particle Based Inference for Continuous-Discrete State Space Models.pdf}
}

@article{steegenIncreasingTransparencyMultiverse2016,
  title = {Increasing {{Transparency Through}} a {{Multiverse Analysis}}},
  author = {Steegen, Sara and Tuerlinckx, Francis and Gelman, Andrew and Vanpaemel, Wolf},
  year = {2016},
  month = sep,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {5},
  pages = {702--712},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691616658637},
  urldate = {2020-02-11},
  abstract = {Empirical research inevitably includes constructing a data set by processing raw data into a form ready for statistical analysis. Data processing often involves choices among several reasonable options for excluding, transforming, and coding data. We suggest that instead of performing only one analysis, researchers could perform a multiverse analysis, which involves performing all analyses across the whole set of alternatively processed data sets corresponding to a large set of reasonable scenarios. Using an example focusing on the effect of fertility on religiosity and political attitudes, we show that analyzing a single data set can be misleading and propose a multiverse analysis as an alternative practice. A multiverse analysis offers an idea of how much the conclusions change because of arbitrary choices in data construction and gives pointers as to which choices are most consequential in the fragility of the result.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/GZ62UC24/Steegen et al. - 2016 - Increasing Transparency Through a Multiverse Analy.pdf}
}

@techreport{Stephen1985,
  title = {Guidelines for Deriving Numerical National Water Quality Criteria for the Protection of Aquatic Organisms and Their Uses},
  author = {Stephen, {\relax CE} and Mount, {\relax DI} and Hansen, {\relax DJ}},
  year = {1985},
  keywords = {duplicate-citation-key,nosource}
}

@book{stephens1986goodness,
  title = {Goodness-of-Fit Techniques},
  author = {Stephens, Michael A},
  year = {1986},
  volume = {68},
  publisher = {Marcel Dekker},
  keywords = {nosource}
}

@article{stephensAncestralInferencePopulation2003,
  title = {Ancestral {{Inference}} in {{Population Genetics Models}} with {{Selection}} (with {{Discussion}})},
  author = {Stephens, Matthew and Donnelly, Peter},
  year = {2003},
  journal = {Australian \& New Zealand Journal of Statistics},
  volume = {45},
  number = {4},
  pages = {395--430},
  issn = {1467-842X},
  doi = {10.1111/1467-842X.00295},
  urldate = {2021-07-06},
  abstract = {A new algorithm is presented for exact simulation from the conditional distribution of the genealogical history of a sample, given the composition of the sample, for population genetics models with general diploid selection. The method applies to the usual diffusion approximation of evolution at a single locus, in a randomly mating population of constant size, for mutation models in which the distribution of the type of a mutant does not depend on the type of the progenitor allele; this includes any model with only two alleles. The new method is applied to ancestral inference for the two-allele case, both with genic selection and heterozygote advantage and disadvantage, where one of the alleles is assumed to have resulted from a unique mutation event. The paper describes how the method could be used for inference when data are also available at neutral markers linked to the locus under selection. It also informally describes and constructs the non-neutral Fleming--Viot measure-valued diffusion.},
  langid = {english},
  keywords = {ancestral selection graph,coalescent,Fleming--Viot diffusion,importance sampling,intra-allelic genealogy,nosource,perfect simulation},
  file = {/home/gkonkamking/pCloudDrive/papers/Stephens_Donnelly_2003_Ancestral Inference in Population Genetics Models with Selection (with.pdf}
}

@article{Straalen1989,
  title = {Ecotoxicological Evaluation of Soil Quality Criteria},
  author = {{van Straalen}, Nico M. and Denneman, Carl A.J.},
  year = {1989},
  journal = {Ecotoxicology and Environmental Safety},
  volume = {18},
  number = {3},
  pages = {241--251},
  issn = {01476513},
  doi = {10.1016/0147-6513(89)90018-3},
  abstract = {To implement the Soil Protection Act of 1986, the Dutch Ministry of Housing, Physical Planning, and Environment has recently proposed a list of soil quality reference values. These values are, as yet, insufficiently based on ecotoxicological evidence. In this paper, a three-step procedure of risk assessment for soil contaminants is proposed. Arguing from experimental results concerning no observed effect concentrations for a set of selected soil organisms, the method aims at protecting a certain fraction of soil life, taking factors such as soil organic matter and clay content into account. When applied to cadmium, a concentration protecting 95\% of soil invertebrates is estimated as 0.16 {$\mu$}g/g for a standard soil. The value of 0.8 {$\mu$}g/g, as proposed by the Dutch authorities, may, given the present variation and uncertainty of toxicity data, protect about 85\% of the soil invertebrate fauna. It is concluded that even low levels of cadmium in soil may endanger the functioning of some sensitive soil animal species.},
  keywords = {nosource}
}

@book{strogatz2014nonlinear,
  title = {Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering},
  author = {Strogatz, Steven H},
  year = {2014},
  publisher = {Westview press},
  keywords = {nosource}
}

@article{sturtz2005r2winbugs,
  title = {{{R2WinBUGS}}: A Package for Running {{WinBUGS}} from r},
  author = {Sturtz, Sibylle and Ligges, Uwe and Gelman, Andrew G},
  year = {2005},
  journal = {Journal of Statistical Software},
  volume = {12},
  number = {3},
  pages = {1--16},
  issn = {1548-7660},
  doi = {10.1103/PhysRevLett.44.1404},
  abstract = {The R2WinBUGS package provides convenient functions to call WinBUGS from R. It automatically writes the data and scripts in a format readable by WinBUGS for processing in batch mode, which is possible since version 1.4. After the WinBUGS process has finished, it is possible either to read the resulting data into R by the package itself---which gives a compact graphical summary of inference and convergence diagnostics---or to use the facilities of the coda package for further analyses of the output. Examples are given to demonstrate the usage of this package.},
  isbn = {1548-7660},
  keywords = {interface,MCMC,nosource,R,WinBUGS}
}

@article{Suarez2013,
  title = {Bayesian Clustering of Functional Data Using Local Features},
  author = {Suarez, Adam and Ghosal, Subhashis},
  year = {2013},
  journal = {Bayesian Analysis},
  volume = {1},
  number = {1},
  pages = {1--15},
  issn = {1936-0975},
  doi = {10.1214/14-BA925},
  keywords = {dirichlet process prior,Dirichlet process priorProcess Prior,exploratory analysis,nosource,wavelets,waveletsWave}
}

@article{Sugimoto2009,
  title = {A {{Wald-type}} Variance Estimation for the Nonparametric Distribution Estimators for Doubly Censored Data},
  author = {Sugimoto, Tomoyuki},
  year = {2011},
  month = jul,
  journal = {Annals of the Institute of Statistical Mathematics},
  volume = {63},
  number = {4},
  pages = {645--670},
  issn = {00203157},
  doi = {10.1007/s10463-009-0251-3},
  isbn = {1046300902513},
  keywords = {Double censoring,Efficient Fisher information,Empirical likelihood,Integral equations,nosource,Profile likelihood,Self-consistent equations,Variance formula}
}

@article{sundqvistDirectionalGeneticDifferentiation2016,
  title = {Directional Genetic Differentiation and Relative Migration},
  author = {Sundqvist, Lisa and Keenan, Kevin and Zackrisson, Martin and Prod{\"o}hl, Paulo and Kleinhans, David},
  year = {2016},
  journal = {Ecology and Evolution},
  volume = {6},
  number = {11},
  pages = {3461--3475},
  issn = {2045-7758},
  doi = {10.1002/ece3.2096},
  urldate = {2022-06-29},
  abstract = {Understanding the population structure and patterns of gene flow within species is of fundamental importance to the study of evolution. In the fields of population and evolutionary genetics, measures of genetic differentiation are commonly used to gather this information. One potential caveat is that these measures assume gene flow to be symmetric. However, asymmetric gene flow is common in nature, especially in systems driven by physical processes such as wind or water currents. As information about levels of asymmetric gene flow among populations is essential for the correct interpretation of the distribution of contemporary genetic diversity within species, this should not be overlooked. To obtain information on asymmetric migration patterns from genetic data, complex models based on maximum-likelihood or Bayesian approaches generally need to be employed, often at great computational cost. Here, a new simpler and more efficient approach for understanding gene flow patterns is presented. This approach allows the estimation of directional components of genetic divergence between pairs of populations at low computational effort, using any of the classical or modern measures of genetic differentiation. These directional measures of genetic differentiation can further be used to calculate directional relative migration and to detect asymmetries in gene flow patterns. This can be done in a user-friendly web application called divMigrate-online introduced in this study. Using simulated data sets with known gene flow regimes, we demonstrate that the method is capable of resolving complex migration patterns under a range of study designs.},
  langid = {english},
  keywords = {Allele frequency data,asymmetric migration,directional gene flow,dispersal},
  file = {/home/gkonkamking/MEGA/work/Papers/Sundqvist et al_2016_Directional genetic differentiation and relative migration.pdf}
}

@article{Sunn??ker2013,
  title = {Approximate Bayesian Computation},
  author = {Sunnaker, Mikael and Busetto, Alberto Giovanni and Numminen, Elina and Corander, Jukka and Foll, Matthieu and Dessimoz, Christophe},
  year = {2013},
  journal = {PLoS Computational Biology},
  volume = {9},
  number = {1},
  issn = {1553734X},
  doi = {10.1371/journal.pcbi.1002803},
  abstract = {Approximate Bayesian computation (ABC) constitutes a class of computational methods rooted in Bayesian statistics. In all model-based statistical inference, the likelihood function is of central importance, since it expresses the probability of the observed data under a particular statistical model, and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evaluate. ABC methods bypass the evaluation of the likelihood function. In this way, ABC methods widen the realm of models for which statistical inference can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection. ABC has rapidly gained popularity over the last years and in particular for the analysis of complex problems arising in biological sciences (e.g., in population genetics, ecology, epidemiology, and systems biology).},
  isbn = {1553-7358 (Electronic){\textbackslash}r1553-734X (Linking)},
  pmid = {23341757},
  keywords = {nosource}
}

@incollection{survival-book,
  title = {Modeling Survival Data: {{Extending}} the Cox Model},
  booktitle = {Technometrics},
  author = {Therneau, Terry and Grambsch, Patricia},
  year = {2002},
  volume = {44},
  pages = {85--86},
  publisher = {Springer},
  address = {New York},
  issn = {0040-1706},
  doi = {10.1198/tech.2002.s656},
  abstract = {This is a book for statistical practitioners, particularly those who design and analyze studies for survival and event history data. Its goal is to extend the toolkit beyond the basic triad provided by most statistical packages: the Kaplan-Meier estimator, log-rank test, and Cox regression model.},
  isbn = {0-387-98784-3},
  pmid = {43993527},
  keywords = {nosource}
}

@article{survival-package,
  title = {A Package for Survival Analysis in {{S}}},
  author = {Therneau, T},
  year = {2012},
  keywords = {nosource}
}

@article{SuterIi1999,
  title = {Ecological Risk Assessment in a Large {{River}}--{{Reservoir}}: 2. {{Fish}} Community},
  author = {Suter II, Glenn W and Barnthouse, Lawrence W and Efroymson, Rebecca A and Jager, Henriette},
  year = {1999},
  journal = {Environmental Toxicology and Chemistry},
  volume = {18},
  number = {4},
  pages = {589--598},
  issn = {0730-7268},
  doi = {10.1897/1551-5028(1999)018<0589:ERAIAL>2.3.CO;2},
  abstract = {---This paper summarizes the assessment of risks to fishes in the Clinch River Operable Unit due to contaminants released by the U.S. Department of Energy's activities on its Oak Ridge Reservation in Tennessee. This paper focuses on the most contaminated area, the Poplar Creek (PC) embayment. The assessment is of interest because of its use of five distinct lines of evidence: fish community surveys, fish body burdens, toxicity tests of ambient waters, suborganismal bioindicators, and single chemical toxicity tests. None of these lines of evidence provided unambiguous evidence of a significant risk, but the surveys indicated that the fish community in PC was depauperate, polychlorinated biphenyl body burdens may have been at toxic levels in catfish, one of the three tests of ambient water showed clear toxicity, some of the indicators were indicative of toxic effects, and concentrations that have been toxic in the laboratory were detected periodically. Interpretation was further complicated by upstream contamination of both the Clinch River and PC. The risk characterization was performed by evaluating each line of evidence separately and then weighing the evidence using an ecoepidemiological approach.},
  keywords = {ecological risk assessment,nosource}
}

@incollection{suttonBayesianVariableSelection2020,
  title = {Bayesian {{Variable Selection}}},
  booktitle = {Case {{Studies}} in {{Applied Bayesian Data Science}}: {{CIRM Jean-Morlet Chair}}, {{Fall}} 2018},
  author = {Sutton, Matthew},
  editor = {Mengersen, Kerrie L. and Pudlo, Pierre and Robert, Christian P.},
  year = {2020},
  series = {Lecture {{Notes}} in {{Mathematics}}},
  pages = {121--135},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-42553-1_5},
  urldate = {2022-09-07},
  abstract = {In this chapter we survey Bayesian approaches for variable selection and model choice in regression models. We explore the methodological developments and computational approaches for these methods. In conclusion we note the available software for their implementation.},
  isbn = {978-3-030-42553-1},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Sutton_2020_Bayesian Variable Selection.pdf}
}

@article{sytsmaTwoConceptionsSubjective2010,
  title = {Two Conceptions of Subjective Experience},
  author = {Sytsma, Justin and Machery, Edouard},
  year = {2010},
  month = nov,
  journal = {Philosophical Studies},
  volume = {151},
  number = {2},
  pages = {299--327},
  issn = {1573-0883},
  doi = {10.1007/s11098-009-9439-x},
  urldate = {2020-05-04},
  abstract = {Do philosophers and ordinary people conceive of subjective experience in the same way? In this article, we argue that they do not and that the philosophical concept of phenomenal consciousness does not coincide with the folk conception. We first offer experimental support for the hypothesis that philosophers and ordinary people conceive of subjective experience in markedly different ways. We then explore experimentally the folk conception, proposing that for the folk, subjective experience is closely linked to valence. We conclude by considering the implications of our findings for a central issue in the philosophy of mind, the hard problem of consciousness.},
  langid = {english}
}

@article{Sz2013,
  title = {Rspear : {{Calculate SPEAR}} in r Install Rspear Functions in Rspear},
  author = {Szocs, Eduard},
  year = {2013},
  number = {4},
  pages = {1--10},
  keywords = {nosource}
}

@article{szabo2013empirical,
  title = {Empirical {{Bayes}} Scaling of {{Gaussian}} Priors in the White Noise Model},
  author = {Szab{\'o}, B T and {van der Vaart}, A W and {van Zanten}, J H},
  year = {2013},
  journal = {Electronic Journal of Statistics},
  volume = {7},
  number = {1},
  pages = {991--1018},
  publisher = {Institute of Mathematical Statistics},
  issn = {19357524},
  doi = {10.1214/13-EJS798},
  keywords = {Adaptation,Bandwidth,Gaussian white noise,Hyper-rectangle,Normal means model,nosource,Rate of contraction}
}

@article{szymkow2013warmer,
  title = {Warmer Hearts, Warmer Rooms},
  author = {Szymkow, Aleksandra and Chandler, Jesse and IJzerman, Hans and Parzuchowski, Michal and Wojciszke, Bogdan},
  year = {2013},
  journal = {Social Psychology},
  publisher = {Hogrefe Publishing},
  keywords = {nosource}
}

@article{taddyAutoregressiveMixtureModels2010,
  title = {Autoregressive {{Mixture Models}} for {{Dynamic Spatial Poisson Processes}}: {{Application}} to {{Tracking Intensity}} of {{Violent Crime}}},
  shorttitle = {Autoregressive {{Mixture Models}} for {{Dynamic Spatial Poisson Processes}}},
  author = {Taddy, Matthew A.},
  year = {2010},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {105},
  number = {492},
  pages = {1403--1417},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1198/jasa.2010.ap09655},
  urldate = {2023-10-29},
  abstract = {This article develops a set of tools for smoothing and prediction with dependent point event patterns. The methodology is motivated by the problem of tracking weekly maps of violent crime events, but is designed to be straightforward to adapt to a wide variety of alternative settings. In particular, a Bayesian semiparametric framework is introduced for modeling correlated time series of marked spatial Poisson processes. The likelihood is factored into two independent components: the set of total integrated intensities and a series of process densities. For the former it is assumed that Poisson intensities are realizations from a dynamic linear model. In the latter case, a novel class of dependent stick-breaking mixture models are proposed to allow nonparametric density estimates to evolve in discrete time. This, a simple and flexible new model for dependent random distributions, is based on autoregressive time series of marginally beta random variables applied as correlated stick-breaking proportions. The approach allows for marginal Dirichlet process priors at each time and adds only a single new correlation term to the static model specification. Sequential Monte Carlo algorithms are described for online inference with each model component, and marginal likelihood calculations form the basis for inference about parameters governing temporal dynamics. Simulated examples are provided to illustrate the methodology, and we close with results for the motivating application of tracking violent crime in Cincinnati.},
  keywords = {Autoregressive beta process,Bayes factors,Bayesian nonparametric,Dependent Dirichlet process,Particle learning,Poisson DLM,Sequential Monte Carlo,Stick-breaking,Time dependent mixture model},
  file = {/home/gkonkamking/pCloudDrive/papers/Taddy_2010_Autoregressive Mixture Models for Dynamic Spatial Poisson Processes.pdf}
}

@book{tadesseHandbookBayesianVariable2021,
  title = {Handbook of {{Bayesian Variable Selection}}},
  author = {Tadesse, Mahlet G. and Vannucci, Marina},
  year = {2021},
  month = dec,
  publisher = {CRC Press},
  abstract = {Bayesian variable selection has experienced substantial developments over the past 30 years with the proliferation of large data sets. Identifying relevant variables to include in a model allows simpler interpretation, avoids overfitting and multicollinearity, and can provide insights into the mechanisms underlying an observed phenomenon. Variable selection is especially important when the number of potential predictors is substantially larger than the sample size and sparsity can reasonably be assumed. The Handbook of Bayesian Variable Selection provides a comprehensive review of theoretical, methodological and computational aspects of Bayesian methods for variable selection. The topics covered include spike-and-slab priors, continuous shrinkage priors, Bayes factors, Bayesian model averaging, partitioning methods, as well as variable selection in decision trees and edge selection in graphical models. The handbook targets graduate students and established researchers who seek to understand the latest developments in the field. It also provides a valuable reference for all interested in applying existing methods and/or pursuing methodological extensions. Features:  Provides a comprehensive review of methods and applications of Bayesian variable selection. Divided into four parts: Spike-and-Slab Priors; Continuous Shrinkage Priors; Extensions to various Modeling; Other Approaches to Bayesian Variable Selection. Covers theoretical and methodological aspects, as well as worked out examples with R code provided in the online supplement.  Includes contributions by experts in the field. Supported by a website with code, data, and other supplementary material},
  googlebooks = {Cn1TEAAAQBAJ},
  isbn = {978-1-00-051025-6},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / Bayesian Analysis,Mathematics / Probability \& Statistics / Regression Analysis}
}

@article{Tamuri2012,
  title = {Estimating the Distribution of Selection Coefficients from Phylogenetic Data Using Sitewise Mutation-Selection Models},
  author = {Tamuri, Asif U. and {dos Reis}, Mario and Goldstein, Richard A.},
  year = {2012},
  journal = {Genetics},
  volume = {190},
  number = {3},
  pages = {1101--1115},
  issn = {00166731},
  doi = {10.1534/genetics.111.136432},
  abstract = {Estimation of the distribution of selection coefficients of mutations is a long-standing issue in molecular evolution. In addition to population-based methods, the distribution can be estimated from DNA sequence data by phylogenetic-based models. Previous models have generally found unimodal distributions where the probability mass is concentrated between mildly deleterious and nearly neutral mutations. Here we use a sitewise mutation-selection phylogenetic model to estimate the distribution of selection coefficients among novel and fixed mutations (substitutions) in a data set of 244 mammalian mitochondrial genomes and a set of 401 PB2 proteins from influenza. We find a bimodal distribution of selection coefficients for novel mutations in both the mitochondrial data set and for the influenza protein evolving in its natural reservoir, birds. Most of the mutations are strongly deleterious with the rest of the probability mass concentrated around mildly deleterious to neutral mutations. The distribution of the coefficients among substitutions is unimodal and symmetrical around nearly neutral substitutions for both data sets at adaptive equilibrium. About 0.5\% of the nonsynonymous mutations and 14\% of the nonsynonymous substitutions in the mitochondrial proteins are advantageous, with 0.5\% and 24\% observed for the influenza protein. Following a host shift of influenza from birds to humans, however, we find among novel mutations in PB2 a trimodal distribution with a small mode of advantageous mutations.},
  pmid = {22209901},
  keywords = {nosource}
}

@article{Tataru2016,
  title = {Statistical Inference in the {{Wright}}--{{Fisher}} Model Using Allele Frequency Data},
  author = {Tataru, Paula and Simonsen, Maria and Bataillon, Thomas and Hobolth, Asger},
  year = {2017},
  journal = {Systematic Biology},
  volume = {66},
  number = {1},
  pages = {e30-e46},
  issn = {1063-5157},
  doi = {10.1093/sysbio/syw056},
  abstract = {The Wright-Fisher model provides an elegant mathematical framework for understanding allele frequency data. In particular, the model can be used to infer the demographic history of species and identify loci under selection. A crucial quantity for inference under the Wright-Fisher model is the distribution of allele frequencies (DAF). Despite the apparent simplicity of the model, the calculation of the DAF is challenging. We review and discuss strategies for approximating the DAF, and how these are used in methods that perform inference from allele frequency data. Various evolutionary forces can be incorporated in the Wright-Fisher model, and we consider these in turn. We begin our review with the basic bi-allelic Wright-Fisher model where random genetic drift is the only evolutionary force. We then consider mutation, migration, and selection. In particular, we compare diffusion-based and moment-based methods in terms of accuracy, computational efficiency, and analytical tractability. We conclude with a brief overview of the multi-allelic process with a general mutation model. [Allele frequency, diffusion, inference, moments, selection, Wright-Fisher.].},
  pmid = {27486182},
  keywords = {nosource}
}

@article{tavareLinearBirthdeathProcess2018,
  title = {The Linear Birth-death Process: An Inferential Retrospective},
  shorttitle = {The Linear Birth-death Process},
  author = {Tavar{\'e}, Simon},
  year = {2018},
  month = dec,
  journal = {Advances in Applied Probability},
  volume = {50},
  number = {A},
  pages = {253--269},
  publisher = {Cambridge University Press},
  issn = {0001-8678, 1475-6064},
  doi = {10.1017/apr.2018.84},
  urldate = {2022-03-28},
  abstract = {In this paper we provide an introduction to statistical inference for the classical linear birth-death process, focusing on computational aspects of the problem in the setting of discretely observed processes. The basic probabilistic properties are given in Section 2, focusing on computation of the transition functions. This is followed by a brief discussion of simulation methods in Section 3, and of frequentist methods in Section 4. Section 5 is devoted to Bayesian methods, from rejection sampling to Markov chain Monte Carlo and approximate Bayesian computation. In Section 6 we consider the time-inhomogeneous case. The paper ends with a brief discussion in Section 7.},
  langid = {english},
  keywords = {62P10,Approximate Bayesian computation,estimation,Markov chain Monte Carlo,Primary 62M05,Secondary 62F15},
  file = {/home/gkonkamking/Zotero/storage/HRBSNP2T/Tavaré - 2018 - The linear birth‒death process an inferential ret.pdf}
}

@article{tavareLinedescentGenealogicalProcesses1984,
  title = {Line-of-Descent and Genealogical Processes, and Their Applications in Population Genetics Models},
  author = {Tavar{\'e}, Simon},
  year = {1984},
  month = oct,
  journal = {Theoretical Population Biology},
  volume = {26},
  number = {2},
  pages = {119--164},
  issn = {00405809},
  doi = {10.1016/0040-5809(84)90027-3},
  urldate = {2020-04-21},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/LST39LQH/Tavaré - 1984 - Line-of-descent and genealogical processes, and th.pdf}
}

@article{taylor-rodriguezJointSpeciesDistribution2017,
  title = {Joint {{Species Distribution Modeling}}: {{Dimension Reduction Using Dirichlet Processes}}},
  shorttitle = {Joint {{Species Distribution Modeling}}},
  author = {{Taylor-Rodr{\'i}guez}, Daniel and Kaufeld, Kimberly and Schliep, Erin M. and Clark, James S. and Gelfand, Alan E.},
  year = {2017},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {12},
  number = {4},
  pages = {939--967},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/16-BA1031},
  urldate = {2020-02-19},
  abstract = {Species distribution models are used to evaluate the variables that affect the distribution and abundance of species and to predict biodiversity. Historically, such models have been fitted to each species independently. While independent models can provide useful information regarding distribution and abundance, they ignore the fact that, after accounting for environmental covariates, residual interspecies dependence persists. With stacking of individual models, misleading behaviors, may arise. In particular, individual models often imply too many species per location. Recently developed joint species distribution models have application to presence--absence, continuous or discrete abundance, abundance with large numbers of zeros, and discrete, ordinal, and compositional data. Here, we deal with the challenge of joint modeling for a large number of species. To appreciate the challenge in the simplest way, with just presence/absence (binary) response and say, SS{$<$}math alttext="\$S\$" overflow="scroll"{$><$}mi{$>$}S{$<$}/mi{$><$}/math{$>$} species, we have an SS{$<$}math alttext="\$S\$" overflow="scroll"{$><$}mi{$>$}S{$<$}/mi{$><$}/math{$>$}-way contingency table with 2S2S{$<$}math alttext="\$2{\textasciicircum}\{S\}\$" overflow="scroll"{$><$}msup{$><$}mn{$>$}2{$<$}/mn{$><$}mi{$>$}S{$<$}/mi{$><$}/msup{$><$}/math{$>$} cell probabilities. Even if SS{$<$}math alttext="\$S\$" overflow="scroll"{$><$}mi{$>$}S{$<$}/mi{$><$}/math{$>$} is as small as 100100{$<$}math alttext="\$100\$" overflow="scroll"{$><$}mn{$>$}100{$<$}/mn{$><$}/math{$>$} this is an enormous table, infeasible to work with without some structure to reduce dimension. We develop a computationally feasible approach to accommodate a large number of species (say order 103103{$<$}math alttext="\$10{\textasciicircum}\{3\}\$" overflow="scroll"{$><$}msup{$><$}mn{$>$}10{$<$}/mn{$><$}mn{$>$}3{$<$}/mn{$><$}/msup{$><$}/math{$>$}) that allows us to: 1) assess the dependence structure across species; 2) identify clusters of species that have similar dependence patterns; and 3) jointly predict species distributions. To do so, we build hierarchical models capturing dependence between species at the first or ``data'' stage rather than at a second or ``mean'' stage. We employ the Dirichlet process for clustering in a novel way to reduce dimension in the joint covariance structure. This last step makes computation tractable. We use Forest Inventory Analysis (FIA) data in the eastern region of the United States to demonstrate our method. It consists of presence--absence measurements for 112 tree species, observed east of the Mississippi. As a proof of concept for our dimension reduction approach, we also include simulations using continuous and binary data.},
  langid = {english},
  mrnumber = {MR3724974},
  zmnumber = {1384.62317},
  keywords = {abundance,hierarchical model,latent variables,Markov chain Monte Carlo,nosource,presence--absence}
}

@incollection{TDDM14,
  title = {Seven More Languages in Seven Weeks: {{Languages}} That Are Shaping the Future},
  author = {Tate, Bruce A and Dees, Ian and Daoud, Frederic and Moffitt, Jack},
  year = {2014},
  edition = {1},
  publisher = {Pragmatic Bookshelf},
  chapter = {6},
  keywords = {nosource}
}

@article{teh2006hierarchical,
  ids = {Teh2004},
  title = {Hierarchical Dirichlet Processes},
  author = {Teh, Y. and Jordan, M. and Beal, Matthew J. and Blei, David M.},
  year = {2006},
  journal = {J. Am. Stat. Assoc.},
  volume = {101},
  number = {476},
  eprint = {1210.6738v2},
  pages = {1566--1581},
  publisher = {ASA},
  issn = {0162-1459},
  doi = {10.1198/016214506000000302},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1210.6738v2},
  isbn = {0162-1459},
  pmid = {242869700023},
  keywords = {nosource}
}

@inproceedings{teh2009indian,
  title = {Indian Buffet Processes with Power-Law Behavior},
  booktitle = {Advances in Neural Information Processing Systems 22},
  author = {Teh, Yee Whye and G{\"o}r{\"u}r, Dilan},
  year = {2009},
  pages = {1838--1846},
  abstract = {The Indian buffet process (IBP) is an exchangeable distribution over binary matrices used in Bayesian nonparametric featural models. In this paper we propose a three-parameter generalization of the IBP exhibiting power-law behavior. We achieve this by generalizing the beta process (the de Finetti measure of the IBP) to the stable-beta process and deriving the IBP corresponding to it. We find interesting relationships between the stable-beta process and the Pitman-Yor process (another stochastic process used in Bayesian nonparametric models with interesting power-law properties). We derive a stick-breaking construction for the stable-beta process, and find that our power-law IBP is a good model for word occurrences in document corpora.},
  isbn = {978-1-61567-911-9},
  keywords = {nosource}
}

@article{Tello2012,
  title = {Selective Pressure of Antibiotic Pollution on Bacteria of Importance to Public Health},
  author = {Tello, Alfredo and Austin, Brian and Telfer, Trevor C.},
  year = {2012},
  month = aug,
  journal = {Environmental Health Perspectives},
  volume = {120},
  number = {8},
  pages = {1100--1106},
  issn = {00916765},
  doi = {10.1289/ehp.1104650},
  abstract = {BACKGROUND: Many bacteria of clinical importance survive and may grow in different environments. Antibiotic pollution may exert on them a selective pressure leading to an increase in the prevalence of resistance. OBJECTIVES: In this study we sought to determine whether environmental concentrations of antibiotics and concentrations representing action limits used in environmental risk assessment may exert a selective pressure on clinically relevant bacteria in the environment. METHODS: We used bacterial inhibition as an assessment end point to link antibiotic selective pressures to the prevalence of resistance in bacterial populations. Species sensitivity distributions were derived for three antibiotics by fitting log-logistic models to end points calculated from minimum inhibitory concentration (MIC) distributions based on worldwide data collated by the European Committee on Antimicrobial Susceptibility Testing (EUCAST). To place bacteria represented in these distributions in a broader context, we performed a brief phylogenetic analysis. The potentially affected fraction of bacterial genera at measured environmental concentrations of antibiotics and environmental risk assessment action limits was used as a proxy for antibiotic selective pressure. Measured environmental concentrations and environmental risk assessment action limits were also directly compared to wild-type cut-off values.Results: The potentially affected fraction of bacterial genera estimated based on antibiotic concentrations measured in water environments is {$\leq$} 7\%. We estimated that measured environmental concentrations in river sediments, swine feces lagoons, liquid manure, and farmed soil inhibit wild-type populations in up to 60\%, 92\%, 100\%, and 30\% of bacterial genera, respectively. At concentrations used as action limits in environmental risk assessment, erythromycin and ciprofloxacin were estimated to inhibit wild-type populations in up to 25\% and 76\% of bacterial genera. CONCLUSIONS: Measured environmental concentrations of antibiotics, as well as concentrations representing environmental risk assessment action limits, are high enough to exert a selective pressure on clinically relevant bacteria that may lead to an increase in the prevalence of resistance.},
  isbn = {1552-9924 (Electronic) 0091-6765 (Linking)},
  pmid = {22571927},
  keywords = {Antibiotic pollution,Antibiotic resistance,Minimum inhibitory concentration distributions,nosource,Risk assessment,Species sensitivity distributions}
}

@article{tero2006physarum,
  title = {Physarum Solver: {{A}} Biologically Inspired Method of Road-Network Navigation},
  author = {Tero, Atsushi and Kobayashi, Ryo and Nakagaki, Toshiyuki},
  year = {2006},
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {363},
  number = {1},
  pages = {115--119},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{tero2010rules,
  title = {Rules for Biologically Inspired Adaptive Network Design},
  author = {Tero, Atsushi and Takagi, Seiji and Saigusa, Tetsu and Ito, Kentaro and Bebber, Dan P and Fricker, Mark D and Yumiki, Kenji and Kobayashi, Ryo and Nakagaki, Toshiyuki},
  year = {2010},
  journal = {Science},
  volume = {327},
  number = {5964},
  pages = {439--442},
  publisher = {American Association for the Advancement of Science},
  keywords = {nosource}
}

@inproceedings{thibaux2007hierarchical,
  title = {Hierarchical \{\vphantom\}{{B}}\vphantom\{\}eta Processes and the \{\vphantom\}{{I}}\vphantom\{\}ndian Buffet Process},
  booktitle = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics ({{AISTATS}})},
  author = {Thibaux, R and Jordan, M I},
  year = {2007},
  pages = {564--571},
  issn = {15324435},
  abstract = {We show that the beta process is the de{\textbackslash}n{\textbackslash}nFinetti mixing distribution underlying the In-{\textbackslash}n{\textbackslash}ndian buffet process of [2]. This result shows{\textbackslash}n{\textbackslash}nthat the beta process plays the role for the{\textbackslash}n{\textbackslash}nIndian buffet process that the Dirichlet pro-{\textbackslash}n{\textbackslash}ncess plays for the Chinese restaurant process,{\textbackslash}n{\textbackslash}na parallel that guides us in deriving analogs{\textbackslash}n{\textbackslash}nfor the beta process of the many known ex-{\textbackslash}n{\textbackslash}ntensions of the Dirichlet process. In partic-{\textbackslash}n{\textbackslash}nular we define Bayesian hierarchies of beta{\textbackslash}n{\textbackslash}nprocesses and use the connection to the beta{\textbackslash}n{\textbackslash}nprocess to develop posterior inference algo-{\textbackslash}n{\textbackslash}nrithms for the Indian buffet process. We also{\textbackslash}n{\textbackslash}npresent an application to document classifi-{\textbackslash}n{\textbackslash}ncation, exploring a relationship between the{\textbackslash}n{\textbackslash}nhierarchical beta process and smoothed naive{\textbackslash}n{\textbackslash}nBayes models.},
  keywords = {Beta process,Chinese restaurant process,Dirichlet process,Indian buffet process,nosource}
}

@article{thomas2006making,
  title = {Making {{BUGS}} Open},
  author = {Thomas, Andrew and O'Hara, Bob and Ligges, Uew and Sturtz, Sibylle},
  year = {2006},
  journal = {R News},
  volume = {6},
  number = {1},
  pages = {12--17},
  pmid = {2579},
  keywords = {nosource}
}

@article{thomsonSimultaneousParameterEstimation2019,
  title = {Simultaneous Parameter Estimation and Variable Selection via the Logit-Normal Continuous Analogue of the Spike-and-Slab Prior},
  author = {Thomson, W. and Jabbari, S. and Taylor, A. E. and Arlt, W. and Smith, D. J.},
  year = {2019},
  month = jan,
  journal = {Journal of The Royal Society Interface},
  volume = {16},
  number = {150},
  pages = {20180572},
  publisher = {Royal Society},
  doi = {10.1098/rsif.2018.0572},
  urldate = {2020-04-10},
  abstract = {We introduce a Bayesian prior distribution, the logit-normal continuous analogue of the spike-and-slab, which enables flexible parameter estimation and variable/model selection in a variety of settings. We demonstrate its use and efficacy in three case studies---a simulation study and two studies on real biological data from the fields of metabolomics and genomics. The prior allows the use of classical statistical models, which are easily interpretable and well known to applied scientists, but performs comparably to common machine learning methods in terms of generalizability to previously unseen data.}
}

@article{Thorne1996,
  title = {Combining Protein Evolution and Secondary Structure.},
  author = {Thorne, J L and Goldman, Nick and Jones, D T},
  year = {1996},
  journal = {Molecular biology and evolution},
  volume = {13},
  number = {5},
  pages = {666--673},
  issn = {0737-4038},
  doi = {10.1093/oxfordjournals.molbev.a025627},
  abstract = {An evolutionary model that combines protein secondary structure and amino acid replacement is introduced. It allows likelihood analysis of aligned protein sequences and does not require the underlying secondary (or tertiary) structures of these sequences to be known. One component of the model describes the organization of secondary structure along a protein sequence and another specifies the evolutionary process for each category of secondary structure. A database of proteins with known secondary structures is used to estimate model parameters representing these two components. Phylogeny, the third component of the model, can be estimated from the data set of interest. As an example, we employ our model to analyze a set of sucrose synthase sequences. For the evolution of sucrose synthase, a parametric bootstrap approach indicates that our model is statistically preferable to one that ignores secondary structure.},
  isbn = {0737-4038 (Print){\textbackslash}r0737-4038 (Linking)},
  pmid = {8676741},
  keywords = {hidden markov model,maximum,nosource,phylogeny,protein structure,ular evolution}
}

@article{tierney,
  title = {Markov Chains for Exploring Posterior Distributions},
  author = {Tierney, Luke},
  year = {1994},
  journal = {the Annals of Statistics},
  pages = {1701--1728},
  publisher = {JSTOR},
  keywords = {nosource}
}

@article{ties1993demographic,
  title = {Demographic and Health Surveys ({{DHS}}: Contributions and Limitations},
  author = {Ties Boerma, J and Sommerfelt, A Elisabeth},
  year = {1993},
  journal = {World health statistics quarterly 1993; 46 (4): 222-226},
  keywords = {⛔ No DOI found}
}

@article{Tilman2001,
  title = {Diversity and Productivity in a Long-Term Grassland Experimen ... {{Diversity}} and Productivity in a Long-Term Grassland Experiment Diversity and Productivity in a Long-Term Grassland Experimen ...},
  author = {Tilman, David and Reich, Peter B and Knops, Johannes and Wedin, David and Mielke, Troy and Lehman, Clarence},
  year = {2009},
  journal = {October},
  volume = {294},
  number = {2008},
  eprint = {11679667},
  eprinttype = {pubmed},
  pages = {9--11},
  issn = {00368075},
  doi = {10.1126/science.1060391},
  abstract = {Plant diversity and niche complementarity had progressively stronger effects on ecosystem functioning during a 7-year experiment, with 16-species plots attaining 2.7 times greater biomass than monocultures. Diversity effects were neither transients nor explained solely by a few productive or unviable species. Rather, many higher-diversity plots outperformed the best monoculture. These results help resolve debate over biodiversity and ecosystem functioning, show effects at higher than expected diversity levels, and demonstrate, for these ecosystems, that even the best-chosen monocultures cannot achieve greater productivity or carbon stores than higher-diversity sites.},
  isbn = {0036-8075 (Print){\textbackslash}r0036-8075 (Linking)},
  pmid = {11679667},
  keywords = {*Biomass,*Ecosystem,*Plant Development,Analysis of Variance,Fabaceae/growth \& development,Minnesota,nosource,Poaceae/*growth \& development,Regression Analysis,Seasons}
}

@article{titsias2016statistical,
  title = {Statistical Inference in Hidden {{Markov}} Models Using K-Segment Constraints},
  author = {Titsias, Michalis K and Holmes, Christopher C and Yau, Christopher},
  year = {2016},
  journal = {Journal of the American Statistical Association},
  volume = {111},
  number = {513},
  pages = {200--215},
  publisher = {Taylor \& Francis},
  keywords = {nosource}
}

@article{Tizzoni2014,
  title = {On the Use of Human Mobility Proxies for Modeling Epidemics},
  author = {Tizzoni, Michele and Bajardi, Paolo and Decuyper, Adeline and Kon Kam King, Guillaume and Schneider, Christian M. and Blondel, Vincent and Smoreda, Zbigniew and Gonz{\'a}lez, Marta Cecilia and Colizza, Vittoria},
  year = {2014},
  journal = {PLoS Computational Biology},
  volume = {10},
  number = {7},
  eprint = {1309.7272},
  issn = {15537358},
  doi = {10.1371/journal.pcbi.1003716},
  abstract = {The spatial dissemination of a directly transmitted infectious disease in a population is driven by population movements from one region to another allowing mixing and importation. Public health policy and planning may thus be more accurate if reliable descriptions of population movements can be considered in the epidemic evaluations. Next to census data, generally available in developed countries, alternative solutions can be found to describe population movements where official data is missing. These include mobility models, such as the radiation model, and the analysis of mobile phone activity records providing individual geo-temporal information. Here we explore to what extent mobility proxies, such as mobile phone data or mobility models, can effectively be used in epidemic models for influenza-like-illnesses and how they compare to official census data. By focusing on three European countries, we find that phone data matches the commuting patterns reported by census well but tends to overestimate the number of commuters, leading to a faster diffusion of simulated epidemics. The order of infection of newly infected locations is however well preserved, whereas the pattern of epidemic invasion is captured with higher accuracy by the radiation model for centrally seeded epidemics and by phone proxy for peripherally seeded epidemics.},
  archiveprefix = {arXiv},
  arxivid = {1309.7272},
  copyright = {All rights reserved},
  pmid = {25010676},
  file = {/home/gkonkamking/Zotero/storage/26EVHHYU/Tizzoni et al. - 2014 - On the use of human mobility proxies for modeling .pdf}
}

@article{TO14,
  title = {The Automatic Solution of Partial Differential Equations Using a Global Spectral Method},
  author = {Townsend, Alex and Olver, Sheehan},
  year = {2014},
  abstract = {A spectral method for solving linear partial differential equations (PDEs) with variable coefficients and general boundary conditions defined on rectangular domains is described, based on separable representations of partial differential operators and the one-dimensional ultraspherical spectral method. If a partial differential operator is of splitting rank 2, such as the operator associated with Poisson or Helmholtz, the corresponding PDE is solved via a generalized Sylvester matrix equation, and a bivariate polynomial approximation of the solution of degree (n\_x,n\_y) is computed in O(n\_x n\_y){\textasciicircum}\{3/2\} operations. Partial differential operators of splitting rank {$>$}=3 are solved via a linear system involving a block-banded matrix in O(min(n\_x{\textasciicircum}\{3\} n\_y,n\_x n\_y{\textasciicircum}\{3\})) operations. Numerical examples demonstrate the applicability of our 2D spectral method to a broad class of PDEs, which includes elliptic and dispersive time-evolution equations. The resulting PDE solver is written in \{{\textbackslash}sc Matlab\} and is publicly available as part of \{{\textbackslash}sc Chebfun\}. It can resolve solutions requiring over a million degrees of freedom in under 60 seconds. An experimental implementation in the Julia language can currently perform the same solve in 10 seconds.},
  keywords = {nosource}
}

@article{toch2019analyzing,
  title = {Analyzing Large-Scale Human Mobility Data: A Survey of Machine Learning Methods and Applications},
  author = {Toch, Eran and Lerner, Boaz and {Ben-Zion}, Eyal and {Ben-Gal}, Irad},
  year = {2019},
  journal = {Knowledge and Information Systems},
  volume = {58},
  number = {3},
  pages = {501--523},
  publisher = {Springer},
  keywords = {nosource}
}

@article{tokdar2006posterior,
  title = {Posterior Consistency of Dirichlet Location-Scale Mixture of Normals in Density Estimation and Regression},
  author = {Tokdar, Surya T.},
  year = {2006},
  journal = {Sankhya: The Indian Journal of Statistics},
  volume = {68},
  number = {1},
  pages = {90--110},
  publisher = {JSTOR},
  issn = {09727671},
  keywords = {Density estimation,Dirichlet process,Location-scale mixtures,nosource,Posterior consistency,Regression}
}

@article{tomasiPARAFACMissingValues2005,
  title = {{{PARAFAC}} and Missing Values},
  author = {Tomasi, Giorgio and Bro, Rasmus},
  year = {2005},
  month = feb,
  journal = {Chemometrics and Intelligent Laboratory Systems},
  volume = {75},
  number = {2},
  pages = {163--180},
  issn = {0169-7439},
  doi = {10.1016/j.chemolab.2004.07.003},
  urldate = {2022-07-22},
  abstract = {Missing values are a common occurrence in chemometrics data, and different approaches have been proposed to deal with them. In this work, two different concepts based on two algorithms are compared in their efficiency in dealing with incomplete data when fitting the PARAFAC model: single imputation (SI) combined with a standard PARAFAC-alternating least squares (ALS) algorithm, and fitting the model only to the existing elements using a computationally more expensive method (Levenberg--Marquadt) appropriately modified and optimised. The performance of these two algorithms and the effect of the incompleteness of the data on the final model have been evaluated on the basis of a Monte Carlo study and real data sets with different amounts and patterns of missing values (randomly missing values, randomly missing spectra/vectors, and systematically missing spectra/vectors). The evaluation is based on the quality of the solution as well as on computational aspects (time requirement and number of iterations). The results show that a PARAFAC model can be correctly determined even when a large fraction of the data is missing (up to 70\%), and that the pattern matters more than the fraction of missing values. Computationally, the Levenberg--Marquadt-based approach appeared superior for the pattern of missing values typical of fluorescence measurements when the fraction of missing elements exceeded 30\%.},
  langid = {english},
  keywords = {Fluorescence,INDAFAC,Missing values,PARAFAC},
  file = {/home/gkonkamking/pCloudDrive/papers/Tomasi_Bro_2005_PARAFAC and missing values.pdf}
}

@article{Toni2009,
  title = {Approximate {{Bayesian}} Computation Scheme for Parameter Inference and Model Selection in Dynamical Systems},
  author = {Toni, Tina and Welch, David and Strelkowa, Natalja and Ipsen, Andreas and Stumpf, Michael P. H.},
  year = {2009},
  number = {July 2008},
  eprint = {0901.1925},
  pages = {187--202},
  doi = {10.1098/rsif.2008.0172},
  abstract = {Approximate Bayesian computation methods can be used to evaluate posterior distributions without having to calculate likelihoods. In this paper we discuss and apply an approximate Bayesian computation (ABC) method based on sequential Monte Carlo (SMC) to estimate parameters of dynamical models. We show that ABC SMC gives information about the inferability of parameters and model sensitivity to changes in parameters, and tends to perform better than other ABC approaches. The algorithm is applied to several well known biological systems, for which parameters and their credible intervals are inferred. Moreover, we develop ABC SMC as a tool for model selection; given a range of different mathematical descriptions, ABC SMC is able to choose the best model using the standard Bayesian model selection apparatus.},
  archiveprefix = {arXiv},
  arxivid = {0901.1925},
  keywords = {bayesian model selection,dynamical systems,nosource,parameter estimation,sampling,sequential importance,sequential monte carlo,sloppy parameters}
}

@book{torraModelingDecisionsArtificial2020,
  title = {Modeling {{Decisions}} for {{Artificial Intelligence}}: 17th {{International Conference}}, {{MDAI}} 2020, {{Sant Cugat}}, {{Spain}}, {{September}} 2--4, 2020, {{Proceedings}}},
  shorttitle = {Modeling {{Decisions}} for {{Artificial Intelligence}}},
  editor = {Torra, Vicen{\c c} and Narukawa, Yasuo and Nin, Jordi and Agell, N{\'u}ria},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12256},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-57524-3},
  urldate = {2021-12-13},
  isbn = {978-3-030-57523-6 978-3-030-57524-3},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/98TFQF5Q/Torra et al. - 2020 - Modeling Decisions for Artificial Intelligence 17.pdf}
}

@article{torricelli2020weg2vec,
  title = {Weg2vec: {{Event}} Embedding for Temporal Networks},
  author = {Torricelli, Maddalena and Karsai, M{\'a}rton and Gauvin, Laetitia},
  year = {2020},
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {1--11},
  publisher = {Nature Publishing Group},
  doi = {10.1038/s41598-020-63221-2},
  keywords = {nosource}
}

@article{totterdellBayesianHiddenMarkov2017,
  title = {Bayesian Hidden {{Markov}} Models in {{DNA}} Sequence Segmentation Using {{R}}: The Case of {{Simian Vacuolating}} Virus ({{SV40}})},
  shorttitle = {Bayesian Hidden {{Markov}} Models in {{DNA}} Sequence Segmentation Using {{R}}},
  author = {Totterdell, James A. and Nur, Darfiana and Mengersen, Kerrie L.},
  year = {2017},
  month = sep,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {87},
  number = {14},
  pages = {2799--2827},
  publisher = {Taylor \& Francis},
  issn = {0094-9655},
  doi = {10.1080/00949655.2017.1344666},
  urldate = {2024-06-04},
  abstract = {Segmentation models aim to partition compositionally heterogeneous domains into homogeneous segments which may be reflective of biological function. Due to the latent nature of the segments a natural approach to segmentation that has gained favour recently uses Bayesian hidden Markov models (HMMs). Concomitantly in the last few decades, the free R programming language has become a dominant tool for computational statistics, visualization and data science. Therefore, this paper aims to fully exploit R to fit a Bayesian HMM for DNA segmentation. The joint posterior distribution of parameters in the model to be considered is derived followed by the algorithms that can be used for estimation. Functions following these algorithms (Gibbs Sampling, Data Augmentation and Label Switching) are then fully implemented in R. The methodology is assessed through extensive simulation studies and then being applied to analyse Simian Vacuolating virus (SV40). It is concluded that: (1) the algorithms and functions in R can correctly estimate sequence segmentation if the HMM structure is assumed; (2) the performance of the model improves with sequence length; (3) R is reasonably fast for short to medium sequence lengths and number of segments and (4) the segmentation of SV40 appears to correspond with the two major transcripts, early and late, that regulate the expression of SV40 genes.},
  keywords = {62F15,62M10,62P10,65C40,Bayesian modelling,data augmentation,DNA sequence,Gibbs sampler algorithm,hidden Markov models,label switching algorithm,R statistical software,segmentation modelling,Simian Vacuolating virus (SV40)},
  file = {/home/gkonkamking/pCloudDrive/papers/Totterdell et al_2017_Bayesian hidden Markov models in DNA sequence segmentation using R.pdf}
}

@article{touw2015series,
  title = {A Series of {{PDB-related}} Databanks for Everyday Needs},
  author = {Touw, Wouter G. and Baakman, Coos and Black, Jon and Te Beek, Tim A H and Krieger, E. and Joosten, Robbie P. and Vriend, Gert},
  year = {2015},
  journal = {Nucleic Acids Research},
  volume = {43},
  number = {D1},
  pages = {D364-D368},
  publisher = {Oxford Univ Press},
  issn = {13624962},
  doi = {10.1093/nar/gku1028},
  abstract = {We present a series of databanks (http://swift.cmbi.ru.nl/gv/facilities/) that hold information that is computationally derived from Protein Data Bank (PDB) entries and that might augment macromolecular structure studies. These derived databanks run parallel to the PDB, i.e. they have one entry per PDB entry. Several of the well-established databanks such as HSSP, PDBREPORT and PDB\_REDO have been updated and/or improved. The software that creates the DSSP databank, for example, has been rewritten to better cope with {$\pi$}-helices. A large number of databanks have been added to aid computational structural biology; some examples are lists of residues that make crystal contacts, lists of contacting residues using a series of contact definitions or lists of residue accessibilities. PDB files are not the optimal presentation of the underlying data for many studies. We therefore made a series of databanks that hold PDB files in an easier to use or more consistent representation. The BDB databank holds X-ray PDB files with consistently represented B-factors. We also added several visualization tools to aid the users of our databanks.},
  isbn = {13624962 (Electronic)},
  pmid = {25352545},
  keywords = {nosource}
}

@article{trippa2009bayesian,
  title = {Bayesian Nonparametric Binary Regression via Random Tessellations},
  author = {Trippa, Lorenzo and Muliere, Pietro},
  year = {2009},
  journal = {Statistics and Probability Letters},
  volume = {79},
  number = {21},
  pages = {2273--2280},
  publisher = {Elsevier},
  issn = {01677152},
  doi = {10.1016/j.spl.2009.07.026},
  abstract = {A Bayesian nonparametric model for binary random variables is introduced. The characterization of the probability model is based on the Dirichlet process and on the Poisson hyperplane tessellation model. These two stochastic models are combined in order to adapt, under the hypothesis of partial exchangeability, the reinforcement mechanism of the P??lya urn scheme. A Gibbs sampling algorithm for implementing predictive inference is illustrated and an application of the inferential procedure is discussed. ?? 2009 Elsevier B.V. All rights reserved.},
  keywords = {nosource}
}

@article{trippamuller,
  title = {The Multivariate Beta Process and an Extension of the {{Polya}} Tree Model},
  author = {Trippa, Lorenzo and Muller, Peter and Johnson, Wesley},
  year = {2011},
  journal = {Biometrika},
  volume = {98},
  number = {1},
  pages = {17--34},
  issn = {00063444},
  doi = {10.1093/biomet/asq072},
  abstract = {We introduce a novel stochastic process that we term the multivariate beta process. The process is defined for modelling-dependent random probabilities and has beta marginal distributions. We use this process to define a probability model for a family of unknown distributions indexed by covariates. The marginal model for each distribution is a Polya tree prior. An important feature of the proposed prior is the easy centring of the nonparametric model around any parametric regression model. We use the model to implement nonparametric inference for survival distributions. The nonparametric model that we introduce can be adopted to extend the support of prior distributions for parametric regression models.},
  pmid = {23956460},
  keywords = {Dependent random probability measures,Multivariate beta process,nosource,Polya tree distribution}
}

@article{TSPK14,
  title = {Annealed Important Sampling for Models with Latent Variables},
  author = {Tran, M.-N. and Strickland, C and Pitt, M K and Kohn, R},
  year = {2014},
  abstract = {This paper is concerned with Bayesian inference when the likelihood is analytically intractable but can be unbiasedly estimated. We propose an annealed importance sampling procedure for estimating expectations with respect to the posterior. The proposed algorithm is useful in cases where finding a good proposal density is challenging, and when estimates of the marginal likelihood are required. The effect of likelihood estimation is investigated, and the results provide guidelines on how to set up the precision of the likelihood estimation in order to optimally implement the procedure. The methodological results are empirically demonstrated in several simulated and real data examples.},
  keywords = {nosource}
}

@book{tsybakov2009introduction,
  title = {Introduction to Nonparametric Estimation},
  author = {Tsybakov, A B},
  year = {2009},
  publisher = {Springer Verlag},
  keywords = {duplicate-citation-key,nosource}
}

@book{tsybakov2009introduction,
  title = {Introducation to Nonparametric Estimation},
  author = {Tsybakov, Alexandre B.},
  year = {2009},
  publisher = {Springer Verlag},
  issn = {01727397},
  doi = {10.1007/978-0-387-98135-2},
  isbn = {978-0-387-79051-0},
  pmid = {15772297},
  keywords = {duplicate-citation-key,nosource}
}

@article{TTO14,
  title = {Fast Computation of \{\vphantom\}{{Gauss}}\vphantom\{\} Quadrature Nodes and Weights on the Whole Real Line},
  author = {Townsend, Alex and Trogdon, Thomas and Olver, Sheehan},
  year = {2014},
  abstract = {A fast and accurate algorithm for the computation of Gauss-Hermite and generalized Gauss-Hermite quadrature nodes and weights is presented. The algorithm is based on Newton's method with carefully selected initial guesses for the nodes and a fast evaluation scheme for the associated orthogonal polynomial. In the Gauss-Hermite case the initial guesses and evaluation scheme rely on explicit asymptotic formulas. For generalized Gauss-Hermite, the initial guesses are furnished by sampling a certain equilibrium measure and the associated polynomial evaluated via a Riemann-Hilbert reformulation. In both cases the n-point quadrature rule is computed in O(n) operations to an accuracy that is close to machine precision. For sufficiently large n, some of the quadrature weights have a value less than the smallest positive normalized floating-point number in double precision and we exploit this fact to achieve a complexity as low as O(sqrt(n))).},
  keywords = {nosource}
}

@article{turekAutomatedParameterBlocking2017,
  title = {Automated {{Parameter Blocking}} for {{Efficient Markov Chain Monte Carlo Sampling}}},
  author = {Turek, Daniel and de Valpine, Perry and Paciorek, Christopher J. and {Anderson-Bergman}, Clifford},
  year = {2017},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {12},
  number = {2},
  pages = {465--490},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/16-BA1008},
  urldate = {2022-03-03},
  abstract = {Markov chain Monte Carlo (MCMC) sampling is an important and commonly used tool for the analysis of hierarchical models. Nevertheless, practitioners generally have two options for MCMC: utilize existing software that generates a black-box ``one size fits all" algorithm, or the challenging (and time consuming) task of implementing a problem-specific MCMC algorithm. Either choice may result in inefficient sampling, and hence researchers have become accustomed to MCMC runtimes on the order of days (or longer) for large models. We propose an automated procedure to determine an efficient MCMC block-sampling algorithm for a given model and computing platform. Our procedure dynamically determines blocks of parameters for joint sampling that result in efficient MCMC sampling of the entire model. We test this procedure using a diverse suite of example models, and observe non-trivial improvements in MCMC efficiency for many models. Our procedure is the first attempt at such, and may be generalized to a broader space of MCMC algorithms. Our results suggest that substantive improvements in MCMC efficiency may be practically realized using our automated blocking procedure, or variants thereof, which warrants additional study and application.},
  keywords = {block sampling,integrated autocorrelation time,MCMC,Metropolis--Hastings,Mixing,NIMBLE},
  file = {/home/gkonkamking/Zotero/storage/8NHXEAQW/Turek et al. - 2017 - Automated Parameter Blocking for Efficient Markov .pdf}
}

@article{turing1952chemical,
  title = {The Chemical Basis of Morphogenesis},
  author = {Turing, Alan Mathison},
  year = {1952},
  journal = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
  volume = {237},
  number = {641},
  pages = {37--72},
  publisher = {The Royal Society},
  keywords = {nosource}
}

@techreport{Turnbull1972,
  title = {Nonparametric Estimation of a Survivor Function with Doubly Censored Data},
  booktitle = {Journal of the American Statistical Association},
  author = {Turnbull, Bruce W},
  year = {1974},
  volume = {69},
  number = {345},
  pages = {169--173},
  keywords = {nosource}
}

@article{Turnbull1976,
  title = {The Empirical Distribution Function with Arbitrarily Grouped , Censored and Truncated Data Point in Time Is to Be Incorporated},
  author = {Turnbull, Bruce W.},
  year = {1975},
  journal = {Journal of the Royal Statistical Society},
  volume = {38},
  number = {3},
  pages = {290--295},
  issn = {00359246},
  doi = {10.2307/2984980},
  abstract = {This paper is concerned with the non-parametric estimation of a distribution function F, when the data are complete due to grouping, censoring and/or truncation. Using the idea of self-consistency, a simple algorithm is constructed and shown to converge monotonically to yield a maximum likelihood estimate of F. An application to hypothesis testing is indicated.},
  isbn = {00359246},
  pmid = {315},
  keywords = {censoring,empirical distribution function,grouping,kaplan-meier product limit estimator,logrank test,maximum likelihood,multinomial distribution,newton-raphson,nosource,self-consistency,survival curve,truncation}
}

@article{Turner2010,
  title = {A Review of Parameter Learning Methods Based on Approximate Versions of the Method of Moments},
  booktitle = {Unpublished Manuscript, Cambridge University},
  author = {Turner, RE E},
  year = {2010},
  number = {2},
  pages = {1--6},
  keywords = {nosource}
}

@article{UHZB14,
  title = {Generalized Low Rank Models},
  author = {Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd, Stephen},
  year = {2014},
  abstract = {Principal components analysis (PCA) is a well-known technique for approximating a data set represented by a matrix by a low rank matrix. Here, we extend the idea of PCA to handle arbitrary data sets consisting of numerical, Boolean, categorical, ordinal, and other data types. This framework encompasses many well known techniques in data analysis, such as nonnegative matrix factorization, matrix completion, sparse and robust PCA, k-means, k-SVD, and maximum margin matrix factorization. The method handles heterogeneous data sets, and leads to coherent schemes for compressing, denoising, and imputing missing entries across all data types simultaneously. It also admits a number of interesting interpretations of the low rank factors, which allow clustering of examples or of features. We propose several parallel algorithms for fitting generalized low rank models, and describe implementations and numerical results.},
  keywords = {nosource}
}

@inproceedings{UMZHDB14,
  title = {Convex Optimization in Julia},
  booktitle = {{{HPTCDL}}'14 Proceedings of the 1st Workshop on High Performance Technical Computing in Dynamic Languages},
  author = {Udell, Madeleine and Mohan, Karanveer and Zeng, David and Hong, Jenny and Diamond, Steven and Boyd, Stephen},
  year = {2014},
  pages = {18--28},
  publisher = {ACM},
  address = {New York},
  doi = {10.1109/HPTCDL.2014.5},
  abstract = {This paper describes Convex, a convex optimization modeling framework in Julia. Convex translates problems from a user-friendly functional language into an abstract syntax tree describing the problem. This concise representation of the global structure of the problem allows Convex to infer whether the problem complies with the rules of disciplined convex programming (DCP), and to pass the problem to a suitable solver. These operations are carried out in Julia using multiple dispatch, which dramatically reduces the time required to verify DCP compliance and to parse a problem into conic form. Convex then automatically chooses an appropriate backend solver to solve the conic form problem.},
  keywords = {nosource}
}

@misc{unitednationsdepartmentofeconomicandsocialaffairspopulationdivisionWorldPopulationProspects2019,
  title = {World {{Population Prospects}} 2019: {{Highlights}}. {{ST}}/{{ESA}}/{{SER}}.{{A}}/423.},
  author = {{United Nations, Department of Economic {and} Social Affairs, Population Division}},
  year = {2019},
  keywords = {nosource}
}

@misc{unitednationsWorldUrbanizationProspects2018,
  title = {World {{Urbanization Prospects}}: {{The}} 2018 {{Revision}}: Key Facts},
  author = {United Nations},
  year = {2018},
  keywords = {nosource}
}

@techreport{USEPASSD,
  title = {Guidelines for Ecological Risk Assessment.},
  author = {{USEPA}},
  year = {1998},
  address = {Washington, DC.},
  institution = {US Environmental Protection Agency},
  keywords = {nosource}
}

@book{uteng2018urban,
  title = {Urban Mobilities in the Global South},
  author = {Uteng, Tanu Priya and Lucas, Karen},
  year = {2018},
  publisher = {Routledge London},
  keywords = {nosource}
}

@phdthesis{V14,
  title = {Parallelle Abstracties Voor Het Programmeren van \{\vphantom\}{{GPU}}'s\vphantom\{\} in \{\vphantom\}{{Julia}}\vphantom\{\} ({{Parallel}} Abstractions for Programming \{\vphantom\}{{GPUs}}\vphantom\{\} in \{\vphantom\}{{Julia}}\vphantom\{\})},
  booktitle = {Afstudeerwerk {{FEA}}, Universiteit {{UGent}}},
  author = {Verstraete, Pieter and De Sutter, Bjorn and Besard, Tim},
  year = {2014},
  abstract = {This master's thesis explores the possibility to provide access to the computing power of a GPU from the high-level programming language Julia. An important requirement here is to keep the programmer's productivity at the same high level as if he would use Julia without a GPU. Indeed, very specialized and detailed technical knowledge is needed in order to program a GPU, making it complex and time-consuming. In many modern scientific domains quite a lot of brute computing power is required, but often these domains lack the technical expertise to use GPUs in an efficient manner. The purpose of this thesis is to provide access to a GPU from Julia in a way that shields the GPU details from the programmer. In a first step we define and implement in Julia abstractions that can be executed in parallel on the GPU. Next we adapt the Julia compiler such that it can translate these abstractions to GPU code. The resulting compiler infrastructure manages the GPU in a way that is transparent to the programmer. Finally we evaluate the abstractions and compiler infrastructure in the context of a concrete application, namely the trace transform.},
  keywords = {nosource}
}

@article{vaggi2017rich,
  title = {The Rich and the Poor: {{A}} Note on Countries' Classification},
  author = {Vaggi, Gianni},
  year = {2017},
  journal = {PSL Quarterly Review},
  volume = {70},
  number = {279},
  keywords = {nosource}
}

@article{van_der_vaart_rates_2008,
  title = {Rates of Contraction of Posterior Distributions Based on {{Gaussian}} Process Priors},
  author = {Van Der Vaart, A. W. and Van Zanten, J. H.},
  year = {2008},
  journal = {Annals of Statistics},
  volume = {36},
  number = {3},
  pages = {1435--1463},
  issn = {00905364},
  doi = {10.1214/009053607000000613},
  keywords = {Bayesian inference,Classification,Nonparametric density estimation,Nonparametric regression,nosource,Rate of convergence}
}

@article{van1997development,
  title = {Development of Prosocial, Individualistic, and Competitive Orientations: Theory and Preliminary Evidence.},
  author = {Van Lange, Paul AM and De Bruin, Ellen and Otten, Wilma and Joireman, Jeffrey A},
  year = {1997},
  journal = {Journal of personality and social psychology},
  volume = {73},
  number = {4},
  pages = {733},
  publisher = {American Psychological Association},
  keywords = {nosource}
}

@article{van2002european,
  title = {European History of Species Sensitivity Distributions},
  author = {Straalen, van and {NM} and Leeuwen, van and {CJ} and Straalen, van and {NM} and Leeuwen, van and {CJ}},
  year = {2002},
  pages = {19--34},
  doi = {doi:10.1201/9781420032314.ch3},
  isbn = {978-1-56670-578-3},
  keywords = {nosource}
}

@article{van2009adaptive,
  title = {Adaptive {{Bayesian}} Estimation Using a {{Gaussian}} Random Field with Inverse Gamma Bandwidth},
  author = {{van der Vaart}, Aad W and {van Zanten}, J Harry},
  year = {2009},
  journal = {The Annals of Statistics},
  volume = {37},
  number = {5B},
  pages = {2655--2675},
  publisher = {Institute of Mathematical Statistics},
  keywords = {nosource}
}

@incollection{VandenBrink2001,
  title = {Species Sensitivity Distribution Concept for Predicting Field Effects: (Non-)Confirmation of the Concept Using Semifield Experiments},
  booktitle = {Species Sensitivity Distributions in Ecotoxicology},
  author = {{van den Brink}, Paul J. and Brock, Theo C. M. and Posthuma, Leo},
  editor = {Posthuma, Leo and Suter II, Glenn W and Traas, Theo P},
  year = {2001},
  pages = {155--193},
  publisher = {CRC press},
  address = {Boca Raton, FL},
  chapter = {9},
  keywords = {nosource}
}

@article{VandenBrink2011,
  title = {Traits-Based Approaches in Bioassessment and Ecological Risk Assessment: {{Strengths}}, Weaknesses, Opportunities and Threats},
  author = {{Van den Brink}, Paul J. and Alexander, Alexa C. and Desrosiers, M{\'e}lanie and Goedkoop, Willem and Goethals, Peter L M and Liess, Matthias and Dyer, Scott D.},
  year = {2011},
  month = apr,
  journal = {Integrated Environmental Assessment and Management},
  volume = {7},
  number = {2},
  eprint = {20981837},
  eprinttype = {pubmed},
  pages = {198--208},
  issn = {15513793},
  doi = {10.1002/ieam.109},
  abstract = {We discuss the application of traits-based bioassessment approaches in retrospective bioassessment as well as in prospective ecological risk assessments in regulatory frameworks. Both approaches address the interaction between species and stressors and their consequences at different levels of biological organization, but the fact that a specific species may be less abundant in a potentially impacted site compared with a reference site is, regrettably, insufficient to provide diagnostic information. Species traits may, however, overcome the problems associated with taxonomy-based bioassessment. Trait-based approaches could provide signals regarding what environmental factors may be responsible for the impairment and, thereby, provide causal insight into the interaction between species and stressors. For development of traits-based (TBA), traits should correspond to specific types of stressors or suites of stressors. In this paper, a strengths, weaknesses, opportunities, and threats (SWOT) analysis of TBA in both applications was used to identify challenges and potentials. This paper is part of a series describing the output of the TERA (Traits-based ecological risk assessment: Realising the potential of ecoinformatics approaches in ecotoxicology) Workshop held between 7 and 11 September, 2009, in Burlington, Ontario, Canada. The recognized strengths were that traits are transferrable across geographies, add mechanistic and diagnostic knowledge, require no new sampling methodology, have an old tradition, and can supplement taxonomic analysis. Weaknesses include autocorrelation, redundancy, and inability to protect biodiversity directly. Automated image analysis, combined with genetic and biotechnology tools and improved data analysis to solve autocorrelation problems were identified as opportunities, whereas low availability of trait data, their transferability, their quantitative interpretation, the risk of developing nonrelevant traits, low quality of historic databases, and their standardization were listed as threats.},
  isbn = {1551-3793},
  pmid = {20981837},
  keywords = {Bioassessment,Biomonitoring,Ecological risk assessment,nosource,Traits}
}

@article{VandenBrink2011a,
  title = {Traits-Based Ecological Risk Assessment ({{TERA}}): {{Realizing}} the Potential of Ecoinformatics Approaches in Ecotoxicology},
  author = {{Van den Brink}, Paul J. and Rubach, Mascha N. and Culp, Joseph M. and Pascoe, Timothy J. and Maund, Stephen J. and Baird, Donald J.},
  year = {2011},
  month = apr,
  journal = {Integrated Environmental Assessment and Management},
  volume = {7},
  number = {2},
  eprint = {20886604},
  eprinttype = {pubmed},
  pages = {169--171},
  issn = {15513793},
  doi = {10.1002/ieam.103},
  isbn = {1551-3793},
  pmid = {20886604},
  keywords = {Ecology,Ecology: methods,Ecotoxicology,Ecotoxicology: methods,nosource,Risk Assessment,Risk Assessment: methods}
}

@article{VanderHoeven2001,
  title = {Estimating the 5-Percentile of the Species Sensitivity Distributions without Any Assumptions about the Distribution.},
  author = {{van der Hoeven}, N},
  year = {2001},
  month = feb,
  journal = {Ecotoxicology (London, England)},
  volume = {10},
  number = {1},
  eprint = {11227815},
  eprinttype = {pubmed},
  pages = {25--34},
  issn = {0963-9292},
  abstract = {A non-parametric method is described to estimate the hazardous concentration for p\% of the species, the HCp, and the confidence limits for this value, the HCp(alpha). For this method, all observed sensitivities are ordered from high to low sensitivity. The HC5 is the k-th observation were k is the largest integer below 0.05 x (n + 1). It is described how the HCp(alpha) can be calculated. A table is presented for easy calculation of several conservative confidence limits for the HC5. For the HC5(0.05), a second table is presented which can be used to interpolate between the conservative estimate of the HC5(0.05) and the next higher observed concentration. The non-parametric HC5 and HC5(0.05) estimation is illustrated with an example of sensitivity data on malathion. For this data set, the log-normal HC5 and HC5(0.05) estimations appear to be conservative compared with the non-parametric estimate. It is stressed that HC5 and HC5(0.05) estimates will often be much more affected by the non-randomness of the species set for which sensitivity data are available than by the choice of the statistical method.},
  pmid = {11227815},
  keywords = {Animals,Beta distribution,Cadmium Chloride,Cadmium Chloride: toxicity,Crustacea,duplicate-citation-key,Ecology,Hazardeous concentration,Hazardous Substances,Hazardous Substances: toxicity,Insecticides,Insecticides: toxicity,Insects,Malathion,Malathion: toxicity,Mollusca,Nonparametric,nosource,Statistics,Vertebrates}
}

@article{VanderPas2015,
  title = {The Horseshoe Estimator: {{Posterior}} Concentration around Nearly Black Vectors},
  author = {{van der Pas}, S. L. and Kleijn, B. J. and {van der Vaart}, A. W.},
  year = {2015},
  journal = {Electronic Journal of Statistics},
  volume = {8},
  number = {February},
  eprint = {1404.0202},
  pages = {2585--2618},
  issn = {19357524},
  doi = {10.1214/14-EJS962},
  abstract = {We consider the horseshoe estimator due to Carvalho, Polson and Scott (2010) for the multivariate normal mean model in the situation that the mean vector is sparse in the nearly black sense. We assume the frequentist framework where the data is generated according to a fixed mean vector. We show that if the number of nonzero parameters of the mean vector is known, the horseshoe estimator attains the minimax \$\_2\$ risk, possibly up to a multiplicative constant. We provide conditions under which the horseshoe estimator combined with an empirical Bayes estimate of the number of nonzero means still yields the minimax risk. We furthermore prove an upper bound on the rate of contraction of the posterior distribution around the horseshoe estimator, and a lower bound on the posterior variance. These bounds indicate that the posterior distribution of the horseshoe prior may be more informative than that of other one-component priors, including the Lasso.},
  archiveprefix = {arXiv},
  arxivid = {1404.0202},
  keywords = {Bayesian inference,Empirical bayes,Horseshoe prior,Normal means model,nosource,Posterior contraction,Sparsity,Worst case risk}
}

@article{VanOudenhove2011,
  title = {Temperature Limits Trail Following Behaviour through Pheromone Decay in Ants},
  author = {Van Oudenhove, Louise and Billoir, Elise and Boulay, Rapha??l and Bernstein, Carlos and Cerd??, Xim},
  year = {2011},
  month = dec,
  journal = {Naturwissenschaften},
  volume = {98},
  number = {12},
  eprint = {22038287},
  eprinttype = {pubmed},
  pages = {1009--1017},
  issn = {00281042},
  doi = {10.1007/s00114-011-0852-6},
  abstract = {In Mediterranean habitats, temperature affects both ant foraging behaviour and community structure. Many studies have shown that dominant species often forage at lower temperature than subordinates. Yet, the factors that constrain dominant species foraging activity in hot environments are still elusive. We used the dominant ant Tapinoma nigerrimum as a model species to test the hypothesis that high temperatures hinder trail following behaviour by accelerating pheromone degradation. First, field observations showed that high temperatures ({$>$} 30{$^\circ$}C) reduce the foraging activity of T. nigerrimum independently of the daily and seasonal rhythms of this species. Second, we isolated the effect of high temperatures on pheromone trail efficacy from its effect on worker physiology. A marked substrate was heated during 10 min (five temperature treatments from 25{$^\circ$}C to 60{$^\circ$}C), cooled down to 25{$^\circ$}C, and offered in a test choice to workers. At hot temperature treatments ({$>$}40{$^\circ$}C), workers did not discriminate the previously marked substrate. High temperatures appeared therefore to accelerate pheromone degradation. Third, we assessed the pheromone decay dynamics by a mechanistic model fitted with Bayesian inference. The model predicted ant choice through the evolution of pheromone concentration on trails as a function of both temperature and time since pheromone deposition. Overall, our results highlighted that the effect of high temperatures on recruitment intensity was partly due to pheromone evaporation. In the Mediterranean ant communities, this might affect dominant species relying on chemical recruitment, more than subordinate ant species, less dependent on chemical communication and less sensitive to high temperatures.},
  pmid = {22038287},
  keywords = {Ant foraging behaviour,Bayesian inference,Mechanistic model,Mediterranean ant community,nosource,Tapinoma nigerrimum,Trail pheromone}
}

@article{vanrossumDiversitySpeciesInterpreting2020,
  title = {Diversity within Species: Interpreting Strains in Microbiomes},
  shorttitle = {Diversity within Species},
  author = {Van Rossum, Thea and Ferretti, Pamela and Maistrenko, Oleksandr M. and Bork, Peer},
  year = {2020},
  month = sep,
  journal = {Nature Reviews Microbiology},
  volume = {18},
  number = {9},
  pages = {491--506},
  publisher = {Nature Publishing Group},
  issn = {1740-1534},
  doi = {10.1038/s41579-020-0368-1},
  urldate = {2020-10-12},
  abstract = {Studying within-species variation has traditionally been limited to culturable bacterial isolates and low-resolution microbial community fingerprinting. Metagenomic sequencing and technical advances have enabled culture-free, high-resolution strain and subspecies analyses at high throughput and in complex environments. This holds great scientific promise but has also led to an overwhelming number of methods and terms to describe infraspecific variation. This Review aims to clarify these advances by focusing on the diversity within bacterial and archaeal species in the context of microbiomics. We cover foundational microevolutionary concepts relevant to population genetics and summarize how within-species variation can be studied and stratified directly within microbial communities with a focus on metagenomics. Finally, we describe how common applications of within-species variation can be achieved using metagenomic data. We aim to guide the selection of appropriate terms and analytical approaches to facilitate researchers in benefiting from the increasing availability of large, high-resolution microbiome genetic sequencing data.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/HQT9LZK4/Van Rossum et al. - 2020 - Diversity within species interpreting strains in .pdf}
}

@article{VanStraalen2002,
  title = {Threshold Models for Species Sensitivity Distributions Applied to Aquatic Risk Assessment for Zinc.},
  author = {{van Straalen}, Nico M},
  year = {2002},
  month = jul,
  journal = {Environmental toxicology and pharmacology},
  volume = {11},
  number = {3-4},
  eprint = {21782599},
  eprinttype = {pubmed},
  pages = {167--72},
  issn = {1382-6689},
  abstract = {Species sensitivity distributions (SSDs) are used in ecological risk assessment to derive maximum acceptable concentrations of toxicants in the environment from a limited set of ecotoxicity data obtained in the laboratory. Such distributions usually employ continuous bell-shaped functions such as the normal and the logistic distribution, which have the disadvantage that an arbitrary cut-off value must be chosen (usually the 5-percentile) to designate the concentration below which the fraction of species exposed above their no-effect level is considered acceptably small. In this paper the possibility is explored of introducing a true no-effect principle in the SSD framework by considering models with a finite lower threshold. Four of these distributions are elaborated, the uniform, triangular, exponential and Weibull distributions. The mathematical representations of these functions were re-parameterized allowing direct estimation of the threshold parameter by nonlinear regression. By way of example, a data set comprising chronic ecotoxicity of zinc to 21 different aquatic organisms was used. The exponential distribution did not describe the data well. The other distributions provided estimates for HC(0) (hazardous concentration for none of the species) between 1.66 and 7.83 {$\mu$}g/l. The triangular distribution fitted best to the data and was consistent with previous models. Since threshold-SSDs incorporate a true no-effect level they may be better communicable as a principle for environmental protection in comparison to the approach based on '95\% protection'.},
  isbn = {3120444707},
  pmid = {21782599},
  keywords = {aquatic,Aquatic,duplicate-citation-key,models,Models,nosource,risk assessment,Risk assessment,species sensitivity distribution,Species sensitivity distribution,statistics,Statistics,triangular,Triangular,zinc,Zinc}
}

@article{VanVlaardingen2004,
  title = {{{ETX}} 2.0},
  author = {{van Vlaardingen}, P. L. A. and Traas, T. P. and Wintersen, A. M. and Aldenberg, T.},
  year = {2004},
  keywords = {duplicate-citation-key,nosource}
}

@article{VanWijngaarden2010,
  title = {The Species Sensitivity Distribution Approach Compared to a Microcosm Study: A Case Study with the Fungicide Fluazinam.},
  author = {a {van Wijngaarden}, R P and Arts, G H P and Belgers, J D M and Boonstra, H and Roessink, I and Schroer, a F W and Brock, T C M},
  year = {2010},
  month = feb,
  journal = {Ecotoxicology and environmental safety},
  volume = {73},
  number = {2},
  eprint = {19837458},
  eprinttype = {pubmed},
  pages = {109--22},
  publisher = {Elsevier},
  issn = {1090-2414},
  doi = {10.1016/j.ecoenv.2009.09.019},
  abstract = {We assessed the sensitivity of freshwater organisms (invertebrates and algae) to the fungicide Shirlan (active ingredient fluazinam) in single-species laboratory tests and in microcosms. Species sensitivity distribution (SSD) curves were constructed by means of acute toxicity data for 14 invertebrate species, since algae were much less sensitive. The EC(10)-based SSD gave a median HC(5) value of 0.6microgL(-1) and a 90\% confidence interval of 0.1-1.9 microgL(-1). The EC(50)-based SSD gave a median HC(5) value of 3.9 microgL(-1) (90\% confidence interval: 0.9-9.9 microgL(-1)). The microcosms were treated four times with Shirlan (concentration range: 0.4-250 microgL(-1)). Responses of the microcosm communities were followed. The 2 microgL(-1) treatment was the no-observed-effect concentration (NOEC(microcosm)). The 10 microgL(-1) treatment resulted in short-term effects on a few zooplankton taxa. Clear effects were observed at 50 and 250 microgL(-1). The responses in the microcosms were in line with the toxicity data for the tested lab species. The median EC(10)-based HC(5) and the lower limit EC(50)-based HC(5) were lower, and the median EC(50)-based HC(5) was slightly higher than the NOEC(microcosm). This is consistent with other studies that compared SSDs with responses in model ecosystems that received repeated applications of pesticides.},
  isbn = {1090-2414 (Electronic){\textbackslash}r0147-6513 (Linking)},
  pmid = {19837458},
  keywords = {Acute,Acute toxicity,Aminopyridines,Aminopyridines: administration \& dosage,Aminopyridines: toxicity,Animals,Aquatic risk assessment,Chemical,Chemical: administration \& dosag,Chemical: toxicity,Community-level effects,duplicate-citation-key,EC50,Ecosystem,Eukaryota,Eukaryota: drug effects,Freshwater Biology,Fungicides,HC5,Industrial,Industrial: administration \& dosage,Industrial: toxicity,Invertebrates,Invertebrates: classification,Invertebrates: drug effects,Invertebrates: physiology,Lethal Dose 50,Model ecosystem,NOEC,nosource,Risk Assessment,Species Specificity,SSD,Time Factors,Toxicity Tests,Water Pollutants,Zooplankton,Zooplankton: classification,Zooplankton: drug effects}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and ukasz Kaiser, {\L} and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-08-06},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  keywords = {⛔ No DOI found},
  file = {/home/gkonkamking/pCloudDrive/papers/Vaswani et al_2017_Attention is All you Need.pdf}
}

@article{vats2018revisiting,
  title = {Revisiting the Gelman-Rubin Diagnostic},
  author = {Vats, Dootika and Knudson, Christina},
  year = {2018},
  journal = {arXiv preprint arXiv:1812.09384},
  eprint = {1812.09384},
  archiveprefix = {arXiv},
  keywords = {nosource}
}

@article{Vehtari2014,
  title = {{{WAIC}} and Cross-Validation in {{Stan}}},
  author = {Vehtari, Aki and Gelman, Andrew G},
  year = {2014},
  number = {May},
  pages = {1--15},
  abstract = {Preprint},
  keywords = {applicable information criterion,bayesian computation,deviance information criterion,dic,k -fold cross-validation,leave-one-out cross-,loo-cv,nosource,r,validation,watanabe-akaike information criterion,widely}
}

@article{Vehtari2016,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  year = {2016},
  journal = {Statistics and Computing},
  eprint = {1507.04544v4},
  pages = {1--20},
  publisher = {Springer US},
  issn = {15731375},
  doi = {10.1007/s11222-016-9696-4},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1507.04544v4},
  keywords = {Bayesian computation,K-fold cross-validation,Leave-one-out cross-validation (LOO),nosource,Pareto smoothed importance sampling (PSIS),Stan,Widely applicable information criterion (WAIC)}
}

@inproceedings{venterMobilityAccessAll2019,
  title = {From {{Mobility}} to {{Access}} for {{All}}: {{Expanding Urban Transportation Choices}} in the {{Global South}}},
  author = {Venter, C and Mahendra, A and Hidalgo, D},
  year = {2019},
  address = {Washington, DC: World Resources Institute},
  keywords = {nosource}
}

@article{Verdonck,
  ids = {Verdonck2002},
  title = {Probabilistic Ecological Risk Assessment Framework for Chemical Substances},
  author = {Verdonck, {\relax FAM} and Jaworska, J},
  year = {2002},
  journal = {Integrated Assessment {\dots}},
  pages = {144--149},
  keywords = {atrazine,bootstrap,nosource,water quality}
}

@article{Verdonck2001,
  title = {Determining Environmental Standards Using Bootstrapping, Bayesian and Maximum Likelihood Techniques: A Comparative Study},
  author = {Verdonck, Frederic AM and Jaworska, Joanna and Thas, Olivier and a Vanrolleghem, Peter},
  year = {2001},
  month = nov,
  journal = {Analytica Chimica Acta},
  volume = {446},
  number = {1-2},
  pages = {427--436},
  issn = {00032670},
  doi = {10.1016/S0003-2670(01)00938-2},
  isbn = {0003-2670},
  keywords = {bootstrapping,Bootstrapping,duplicate-citation-key,environmental standards,Environmental standards,nosource,risk assessment,Risk assessment,small sample size,Small sample size,uncertainty,Uncertainty,variability,Variability}
}

@article{Verdonck2003,
  title = {Limitations of Current Risk Characterization Methods in Probabilistic Environmental Risk Assessment},
  author = {Verdonck, {\relax FAM} and Aldenberg, T},
  year = {2003},
  journal = {Environmental {\dots}},
  volume = {22},
  number = {9},
  pages = {2209--2213},
  isbn = {0730-7268},
  pmid = {12959553},
  keywords = {duplicate-citation-key,joint probability curve,nosource,probabilistic risk calculation}
}

@article{Versteeg1999,
  title = {Understanding Single-Species and Model Ecosystem Sensitivity: {{Data-based}} Comparison},
  author = {Versteeg, Donald J and Belanger, Scott E and Carr, Gregory J},
  year = {1999},
  journal = {Environmental Toxicology and Chemistry},
  volume = {18},
  number = {6},
  pages = {1329--1346},
  issn = {0730-7268},
  doi = {10.1897/1551-5028(1999)018<1329:ussame>2.3.co;2},
  abstract = {---Risk assessments for compounds released to the environment typically rely on single-species toxicity studies to predict concentrations at which effects may be observed. These single-species toxicity studies are usually conducted with a few species, cultured under optimum conditions (diet, temperature, light, etc.) and tested in clean water with constant exposure to the compound of interest. Chronic toxicity data are then extrapolated to the ecosystem during risk assessments to predict concentrations that will not adversely impact the environment. Several approaches have been developed that apply statistical methods to estimate toxicant concentrations adversely affecting a small percentage of single species (e.g., 5\%). There are several rarely stated, and infrequently tested, biological and statistical assumptions required to make this extrapolation. One test of the ability to use single-species toxicity data to protect ecosystems is to compare effects on single species with effects on experimental and natural ecosystems (e.g., microcosms, model ecosystems, field). Towards this end, we summarized the chronic single-species and experimental ecosystem data on a variety of substances (n ϭ 11), including heavy metals, pesticides, surfactants, and general organic and inorganic compounds. Single-species data were summarized as genus-specific geometric means using the NOEC or EC20 concentration. Genus mean values spanned a range of values with genera being affected at concentrations above and below those causing effects on model ecosystems. Geometric mean model ecosystem no effect concentrations corresponded to concentrations expected to exceed the NOEC of 10 to 52\% of genera. This analysis suggests that laboratory-generated single-species chronic studies can be used to establish concentrations protective of model ecosystem, and likely whole ecosystem, effects. Further, the use of the 5\% of genera affected level is conservative relative to mean model ecosystem data but is a fairly good predictor of the lower 95\% confidence interval on the mean model ecosystem NOEC.},
  isbn = {0730-7268},
  keywords = {duplicate-citation-key,nosource,risk assessment}
}

@article{vidoni1999exponential,
  title = {Exponential Family State Space Models Based on a Conjugate Latent Process},
  author = {Vidoni, Paolo},
  year = {1999},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {61},
  number = {1},
  pages = {213--221},
  publisher = {Wiley Online Library},
  keywords = {nosource}
}

@article{vihrsApproximateBayesianInference,
  title = {Approximate {{Bayesian}} Inference for a Spatial Point Process Model Exhibiting Regularity and Random Aggregation},
  author = {Vihrs, Ninna and M{\o}ller, Jesper and Gelfand, Alan E.},
  journal = {Scandinavian Journal of Statistics},
  volume = {n/a},
  number = {n/a},
  issn = {1467-9469},
  doi = {10.1111/sjos.12509},
  urldate = {2021-04-26},
  abstract = {In this article, we propose a doubly stochastic spatial point process model with both aggregation and repulsion. This model combines the ideas behind Strauss processes and log Gaussian Cox processes. The likelihood for this model is not expressible in closed form but it is easy to simulate realizations under the model. We therefore explain how to use approximate Bayesian computation (ABC) to carry out statistical inference for this model. We suggest a method for model validation based on posterior predictions and global envelopes. We illustrate the ABC procedure and model validation approach using both simulated point patterns and a real data example.},
  langid = {english},
  keywords = {approximate Bayesian computation,doubly stochastic process,log Gaussian Cox process,model comparison,nosource,posterior prediction,Strauss process}
}

@article{viroliDeepGaussianMixture2019,
  title = {Deep {{Gaussian}} Mixture Models},
  author = {Viroli, Cinzia and McLachlan, Geoffrey J.},
  year = {2019},
  month = jan,
  journal = {Statistics and Computing},
  volume = {29},
  number = {1},
  pages = {43--51},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-017-9793-z},
  urldate = {2021-11-23},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/E7ZPWCG3/Viroli and McLachlan - 2019 - Deep Gaussian mixture models.pdf}
}

@book{VonMises1964,
  title = {Mathematical Theory of Probability and Statistics},
  author = {Von Mises, Richard and Geiringer, Hilda},
  year = {1964},
  edition = {Academic P},
  volume = {75},
  address = {New York},
  keywords = {nosource}
}

@article{vonmisesUberAufteilungsUnd,
  title = {{\"U}ber {{Aufteilungs-}} Und {{Besetzungswahrscheinlichkeiten}}},
  author = {Von Mises, Richard},
  journal = {Revue de la facult{\'e} des sciences de l'Universit{\'e} d'Istanbul},
  volume = {4},
  pages = {145--163}
}

@article{vulTemporalSelectionSuppressed2008,
  title = {Temporal {{Selection}} Is {{Suppressed}}, {{Delayed}}, and {{Diffused During}} the {{Attentional Blink}}:},
  shorttitle = {Temporal {{Selection}} Is {{Suppressed}}, {{Delayed}}, and {{Diffused During}} the {{Attentional Blink}}},
  author = {Vul, Edward and Nieuwenstein, Mark and Kanwisher, Nancy},
  year = {2008},
  month = jan,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {1467-9280},
  urldate = {2020-08-18},
  abstract = {How does temporal selection work, and along what dimensions does it vary from one instance to the next? We explored these questions using a phenomenon in which ...},
  copyright = {{\copyright} 2008 Association for Psychological Science},
  langid = {english},
  keywords = {nosource}
}

@article{wade2013predictive,
  title = {A Predictive Study of Dirichlet Process Mixture Models for Curve Fitting.},
  author = {Wade, Sara and Walker, Stephen G and Petrone, Sonia},
  year = {2014},
  journal = {Scandinavian journal of statistics, theory and applications},
  volume = {41},
  number = {3},
  pages = {580--605},
  issn = {0303-6898 (Print)},
  doi = {10.1111/sjos.12047},
  abstract = {This paper examines the use of Dirichlet process (DP) mixtures for curve fitting. An important modelling aspect in this setting is the choice between constant or covariate-dependent weights. By examining the problem of curve fitting from a predictive perspective, we show the advantages of using covariate-dependent weights. These advantages are a result of the incorporation of covariate proximity in the latent partition. However, closer examination of the partition yields further complications, which arise from the vast number of total partitions. To overcome this, we propose to modify the probability law of the random partition to strictly enforce the notion of covariate proximity, while still maintaining certain properties of the DP. This allows the distribution of the partition to depend on the covariate in a simple manner and greatly reduces the total number of possible partitions, resulting in improved curve fitting and faster computations. Numerical illustrations are presented.},
  pmid = {25395718},
  keywords = {duplicate-citation-key,nosource}
}

@article{wade2013predictive,
  title = {A Predictive Study of \{\vphantom\}{{D}}\vphantom\{\}irichlet Process Mixture Models for Curve Fitting},
  author = {Wade, Sara and Walker, Stephen G and Petrone, Sonia},
  year = {2013},
  journal = {Manuscript in preparation},
  keywords = {duplicate-citation-key,nosource}
}

@article{wade2018bayesian,
  title = {Bayesian Cluster Analysis: {{Point}} Estimation and Credible Balls (with Discussion)},
  author = {Wade, Sara and Ghahramani, Zoubin},
  year = {2018},
  journal = {Bayesian Analysis},
  volume = {13},
  number = {2},
  pages = {559--626},
  publisher = {International Society for Bayesian Analysis},
  file = {/home/gkonkamking/Zotero/storage/VBVLZK4M/Wade and Ghahramani - 2018 - Bayesian cluster analysis Point estimation and cr.pdf}
}

@article{Wadsworth2017,
  title = {An Integrative {{Bayesian Dirichlet-multinomial}} Regression Model for the Analysis of Taxonomic Abundances in Microbiome Data},
  author = {Wadsworth, W Duncan and Argiento, Raffaele and Guindani, Michele and {Galloway-Pena}, Jessica and Shelburne, Samuel A and Vannucci, Marina},
  year = {2017},
  month = feb,
  journal = {BMC Bioinformatics},
  volume = {18},
  number = {1},
  pages = {94},
  issn = {1471-2105},
  doi = {10.1186/s12859-017-1516-0},
  abstract = {The Human Microbiome has been variously associated with the immune-regulatory mechanisms involved in the prevention or development of many non-infectious human diseases such as autoimmunity, allergy and cancer. Integrative approaches which aim at associating the composition of the human microbiome with other available information, such as clinical covariates and environmental predictors, are paramount to develop a more complete understanding of the role of microbiome in disease development.},
  keywords = {nosource}
}

@article{Wagner1991,
  title = {Estimation of Ecotoxicological Protection Levels from {{NOEC}} Toxicity Data},
  author = {Wagner, Connie and Lokke, Hans},
  year = {1991},
  journal = {Water Research},
  volume = {25},
  number = {10},
  pages = {1237--1242},
  issn = {00431354},
  doi = {10.1016/0043-1354(91)90062-U},
  abstract = {An extrapolation method is presented for the evaluation of the effect of toxic compounds on all species in a community from single species tests on selected species representing the community. The method assumes log-normal distribution of the toxicity data. By extrapolation, a lower statistical tolerance limit is determined so that one can assert with a certain probability that only a certain percentage of all the species in the community are influenced. The method was tested on ecotoxicological data for 11 aquatic species, and it has been compared with two existing methods when used for a low number of species. The first implies unwarranted assumptions and resulted in considerably higher critical concentrations than the present method when used for a low number of species. The second method applies log-logistic confidence limits and yielded critical concentrations which were in agreement with the presented method. ?? 1991.},
  isbn = {0043-1354},
  keywords = {ecotoxicology,extrapolation,log-logistic distribution,log-normal distribution,NOEC,nosource,safety factor,tolerance limit}
}

@article{walker1999bayesian,
  title = {Bayesian Nonparametric Inference for Random Distributions and Related Functions},
  author = {Walker, Stephen G and Damien, Paul and Laud, Purushottam W and Smith, Adrian F M},
  year = {1999},
  journal = {Journal Of The Royal Statistical Society Series B-Statistical Methodology},
  volume = {61},
  number = {3},
  pages = {485--527},
  publisher = {Wiley Online Library},
  issn = {1369-7412},
  doi = {10.1111/1467-9868.00190},
  abstract = {In recent years, Bayesian nonparametric inference, both theoretical and computational, has witnessed considerable advances. However, these advances have not received a full critical and comparative analysis of their scope, impact and limitations in statistical modelling; many aspects of the theory and methods remain a mystery to practitioners and many open questions remain. In this paper, we discuss and illustrate the rich modelling and analytic possibilities that are available to the statistician within the Bayesian nonparametric and/or semiparametric framework.},
  keywords = {dirichlet process,exchangeability,le,nosource,survival model,tree}
}

@article{Walker2000representations,
  ids = {10.1093/biomet/87.2.477},
  title = {Representations of Levy Processes without Gaussian Components},
  author = {Walker, Stephen and Damien, Paul},
  year = {2000},
  journal = {Biometrika},
  volume = {87},
  number = {2},
  eprint = {2673477\{\%\}5Cnpapers2://publication/uuid/A6607175-B2D8-4C0A-8901-D0BF84665AFF},
  eprinttype = {jstor},
  pages = {477--483},
  publisher = {Biometrika Trust},
  issn = {0006-3444},
  doi = {10.1093/biomet/87.2.477},
  abstract = {We consider Levy processes without Gaussian components, known as pure jump processes, with a view to simulating such a process in order to implement full Bayesian nonparametric analyses involving the modelling of continuous time stochastic processes. In particular, from practical and theoretical perspectives, we investigate a recent representation of Wolpert Ickstadt (1998).},
  isbn = {0006-3444},
  keywords = {nosource}
}

@article{walker2001bayesian,
  title = {On Bayesian Consistency},
  author = {Walker, Stephen and Hjort, Nils},
  year = {2001},
  journal = {Journal of the Royal Statistical Society. Series B. Statistical Methodology},
  volume = {63},
  number = {4},
  eprint = {2680668\{\%\}5Cnpapers2://publication/uuid/953F8C5E-8582-4F52-958E-5CD7226C26F9},
  eprinttype = {jstor},
  pages = {811--821},
  publisher = {Wiley Online Library},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00314},
  abstract = {We consider a sequence of posterior distributions based on a data-dependent prior (which we shall refer to as a pseudoposterior distribution) and establish simple conditions under which the sequence is Hellinger consistent. It is shown how investigations into these pseudo-posteriors assist with the understanding of some true posterior distributions, including Polya trees, the infinite dimensional exponential family and mixture models.},
  keywords = {nosource}
}

@article{walker2003bivariate,
  title = {A Bivariate Dirichlet Process},
  author = {Walker, Stephen and Muliere, Pietro},
  year = {2003},
  journal = {Statistics and Probability Letters},
  volume = {64},
  number = {1},
  pages = {1--7},
  publisher = {Elsevier},
  issn = {01677152},
  doi = {10.1016/S0167-7152(03)00124-X},
  abstract = {This paper introduces a bivariate Dirichlet process for modelling a partially exchangeable sequence of observables. The proposed model would be relevant when two distributions are unknown but are thought to be close to each other. For two random distributions with the same marginals, the belief in the degree of closeness is expressed through the correlation between masses assigned to equal sets. ?? 2003 Elsevier B.V. All rights reserved.},
  keywords = {Correlation,Dirichlet-multinomial process,Exchangeability,nosource,Partial exchangeability}
}

@article{walker2004new,
  title = {New Approaches to {{Bayesian}} Consistency},
  author = {Walker, Stephen},
  year = {2004},
  journal = {Annals of Statistics},
  volume = {32},
  number = {5},
  pages = {2028--2043},
  publisher = {Institute of Mathematical Statistics},
  issn = {00905364},
  doi = {10.1214/009053604000000409},
  abstract = {We use martingales to study Bayesian consistency. We derive sufficient conditions for both Hettinger and Kullback-Leibler consistency, which do not rely on the use of a sieve. Alternative sufficient conditions for Hellinger consistency are also found and demonstrated on examples.},
  arxiv = {0503672 [math]},
  arxivid = {math/0503672},
  keywords = {Hellinger consistency,Kullback-Leibler consistency,Martingale sequence,nosource,Predictive density}
}

@article{walker2005data,
  title = {Data Tracking and the Understanding of {{Bayesian}} Consistency},
  author = {Walker, Stephen G. and Lijoi, Antonio and Pr{\"u}nster, Igor},
  year = {2005},
  journal = {Biometrika},
  volume = {92},
  number = {4},
  pages = {765--778},
  publisher = {Biometrika Trust},
  issn = {00063444},
  doi = {10.1093/biomet/92.4.765},
  keywords = {Bayesian consistency,Bayesian density estimation,Hellinger distance,Kullback-Leibler divergence,nosource,Weak neighbourhood}
}

@article{walker2007rates,
  title = {On Rates of Convergence for Posterior Distributions under Misspecification},
  author = {Lian, Heng},
  year = {2007},
  journal = {The Annals of Statistics},
  volume = {35},
  number = {April 2015},
  pages = {8},
  publisher = {Institute of Mathematical Statistics},
  issn = {0361-0926},
  doi = {10.1080/03610920802478375},
  abstract = {We extend the approach of Walker (2003, 2004) to the case of misspecified models. A sufficient condition for establishing rates of convergence is given based on a key identity involving martingales, which does not require construction of tests. We also show roughly that the result obtained by using tests can also be obtained by our approach, which demonstrates the potential wider applicability of this method.},
  arxiv = {0702126 [math]},
  arxivid = {math/0702126},
  keywords = {duplicate-citation-key,nosource}
}

@article{walker2007rates,
  title = {On Rates of Convergence for Posterior Distributions in Infinite-Dimensional Models},
  author = {Walker, Stephen G and Lijoi, Antonio and Pr{\"u}nster, Igor},
  year = {2007},
  journal = {The Annals of Statistics},
  volume = {35},
  number = {2},
  pages = {738--746},
  publisher = {Institute of Mathematical Statistics},
  keywords = {duplicate-citation-key,nosource}
}

@article{walker2007sampling,
  title = {Sampling the {{Dirichlet}} Mixture Model with Slices},
  author = {Walker, Stephen G},
  year = {2007},
  journal = {Communications in Statistics---Simulation and Computation},
  volume = {36},
  number = {1},
  pages = {45--54},
  publisher = {Taylor \& Francis},
  doi = {10.1080/03610910601096262}
}

@article{Wang2008,
  title = {Development of Species Sensitivity Distributions and Estimation of {{HC}}(5) of Organochlorine Pesticides with Five Statistical Approaches.},
  author = {Wang, Bin and Yu, Gang and Huang, Jun and Hu, Hongying},
  year = {2008},
  month = nov,
  journal = {Ecotoxicology (London, England)},
  volume = {17},
  number = {8},
  eprint = {18463978},
  eprinttype = {pubmed},
  pages = {716--24},
  issn = {0963-9292},
  doi = {10.1007/s10646-008-0220-2},
  abstract = {Eighteen organochlorine pesticides (OCPs) were studied to develop species sensitivity distributions (SSDs) and calculate hazardous concentration thresholds for 5\% of species (HC5), using both parametric (log-normal and log-logistic) and nonparametric bootstrap methods. In order to avoid picking repetitive values in each resample when performing bootstrap, and to determine the influence of fluctuation of toxicity data of single species on the SSDs and HC5, a modified bootstrap method was introduced, which can generate unrepetitive sampling data other than original elements in datasets. This method can enlarge a dataset without any assumption of a special distribution. Combined with parametric methods, modified bootstrap was also used to develop SSDs and determine HC5. The HC5 estimated by five approaches coincide well with each other with good positive correlation. Even if there is intra-species variation in a certain range of toxicity data; SSDs and HC5 are not very sensitive to the local fluctuation of toxicity of single species. The studied OCPs were classified according to their estimated HC5. A lower HC5 indicates higher ecological toxicity potentials. Endrin, DDTs and Endosulfan are OCPs with very high ecological toxicity potential. alpha-HCH has the lowest ecological toxicity potential in the studied OCPs. For OCPs with high ecological potential, more attention should be paid to their ecological risk.},
  isbn = {0963-9292},
  pmid = {18463978},
  keywords = {Animals,Biological,Chlorinated,Chlorinated: toxicity,Computer Simulation,duplicate-citation-key,Environmental Pollutants,Environmental Pollutants: toxicity,hc 5 {\'a},Hydrocarbons,method {\'a},Models,nosource,organochlorine pesticide {\'a} parametric,Pesticides,Pesticides: toxicity,species sensitivity distribution {\'a},Statistical}
}

@article{Wang2014,
  title = {Comparison of Species Sensitivity Distributions for Species from {{China}} and the {{USA}}},
  author = {Wang, Xiaonan and Yan, Zhenguang and Liu, Zhengtao and Zhang, Cong and Wang, Weili and Li, Handong},
  year = {2014},
  journal = {Environmental Science and Pollution Research},
  volume = {21},
  number = {1},
  pages = {168--176},
  issn = {09441344},
  doi = {10.1007/s11356-013-2110-2},
  isbn = {1135601321},
  keywords = {Acute toxicity data,China,Native species,nosource,Priority pollutant,Species sensitivity distribution,USA,Water quality criteria}
}

@article{Wang2015,
  title = {Non-Parametric Kernel Density Estimation of Species Sensitivity Distributions in Developing Water Quality Criteria of Metals},
  author = {Wang, Ying and Wu, Fengchang and Giesy, John P. and Feng, Chenglian and Liu, Yuedan and Qin, Ning and Zhao, Yujie},
  year = {2015},
  journal = {Environmental Science and Pollution Research},
  volume = {22},
  number = {18},
  pages = {13980--13989},
  issn = {16147499},
  doi = {10.1007/s11356-015-4602-8},
  isbn = {1135601546028},
  keywords = {Hazard,HC5,Metals,nosource,Probabilistic,SSD,Taxa}
}

@article{wang2015forecasting,
  title = {Forecasting Elections with Non-Representative Polls},
  author = {Wang, Wei and Rothschild, David and Goel, Sharad and Gelman, Andrew},
  year = {2015},
  journal = {International Journal of Forecasting},
  volume = {31},
  number = {3},
  pages = {980--991},
  publisher = {Elsevier},
  doi = {10.1016/j.ijforecast.2014.06.001},
  file = {/home/gkonkamking/pCloudDrive/papers/Wang et al_2015_Forecasting elections with non-representative polls.pdf}
}

@article{Wang2016,
  title = {Comparative Study of Species Sensitivity Distributions Based on Non-Parametric Kernel Density Estimation for Some Transition Metals},
  author = {Wang, Ying and Feng, Chenglian and Liu, Yuedan and Zhao, Yujie and Li, Huixian and Zhao, Tianhui and Guo, Wenjing},
  year = {2016},
  journal = {Environmental Pollution},
  pages = {1--8},
  publisher = {Elsevier Ltd},
  issn = {02697491},
  doi = {10.1016/j.envpol.2016.11.084},
  keywords = {nosource}
}

@article{wang2017bayesian,
  title = {Bayesian Nonparametric Clustering and Association Studies for Candidate {{SNP}} Observations},
  author = {Wang, Charlotte and Ruggeri, Fabrizio and Hsiao, Chuhsing K and Argiento, Raffaele},
  year = {2017},
  journal = {International Journal of Approximate Reasoning},
  volume = {80},
  pages = {19--35},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{wangClusteringComplexNetworks2008,
  title = {Clustering Complex Networks and Biological Networks by Nonnegative Matrix Factorization with Various Similarity Measures},
  author = {Wang, Rui-Sheng and Zhang, Shihua and Wang, Yong and Zhang, Xiang-Sun and Chen, Luonan},
  year = {2008},
  month = dec,
  journal = {Neurocomputing},
  volume = {72},
  number = {1-3},
  pages = {134--141},
  issn = {09252312},
  doi = {10.1016/j.neucom.2007.12.043},
  urldate = {2021-12-09},
  abstract = {Identifying community structure in complex networks is closely related to clustering of data in other areas without an underlying network structure. In this paper, we propose a nonnegative matrix factorization (NMF)-based method for finding community structure. We first evaluate several similarity measures, such as diffusion kernel similarity, shortest path based similarity on several widely wellstudied networks. Then, we apply NMF with diffusion kernel similarity to a large biological network, which demonstrates that our method can find biologically meaningful functional modules. Comparison with other algorithms also indicates the good performance of our method.},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Wang et al_2008_Clustering complex networks and biological networks by nonnegative matrix.pdf}
}

@article{wangDimensionreducedNonparametricMaximum2008,
  title = {Dimension-Reduced Nonparametric Maximum Likelihood Computation for Interval-Censored Data},
  author = {Wang, Yong},
  year = {2008},
  month = jan,
  journal = {Computational Statistics \& Data Analysis},
  volume = {52},
  number = {5},
  pages = {2388--2402},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2007.10.018},
  urldate = {2020-07-02},
  abstract = {A general technique is proposed for efficient computation of the nonparametric maximum likelihood estimate (NPMLE) of a survival function. The main idea is to include a new support interval that has the largest gradient value between inclusively every two neighbouring support intervals in the support set at each iteration. It is thus able to expand the support set exponentially fast during the initial stage of computation and tends to produce the same support set of the NPMLE afterward. The use of the proposed technique needs to be combined with an algorithm that can effectively find and remove redundant support intervals, for example, the constrained Newton method, the iterative convex minorant algorithm and the subspace-based Newton method. Numerical studies show that the dimension-reducing technique works very well, especially for purely interval-censored data, where a significant computational improvement via dimension reduction is possible. Strengths and weaknesses of various algorithms are also discussed and demonstrated.},
  langid = {english},
  keywords = {Censored data,Constrained Newton method,Iterative convex minorant algorithm,Nonparametric maximum likelihood,Subspace-based Newton method,Survival function},
  file = {/home/gkonkamking/Downloads/wang2008.pdf}
}

@article{wangEfficientComputationNonparametric2013,
  title = {Efficient Computation of Nonparametric Survival Functions via a Hierarchical Mixture Formulation},
  author = {Wang, Yong and Taylor, Stephen M.},
  year = {2013},
  month = nov,
  journal = {Statistics and Computing},
  volume = {23},
  number = {6},
  pages = {713--725},
  issn = {1573-1375},
  doi = {10.1007/s11222-012-9341-9},
  urldate = {2020-07-02},
  abstract = {We propose a new algorithm for computing the maximum likelihood estimate of a nonparametric survival function for interval-censored data, by extending the recently-proposed constrained Newton method in a hierarchical fashion. The new algorithm makes use of the fact that a mixture distribution can be recursively written as a mixture of mixtures, and takes a divide-and-conquer approach to break down a large-scale constrained optimization problem into many small-scale ones, which can be solved rapidly. During the course of optimization, the new algorithm, which we call the hierarchical constrained Newton method, can efficiently reallocate the probability mass, both locally and globally, among potential support intervals. Its convergence is theoretically established based on an equilibrium analysis. Numerical study results suggest that the new algorithm is the best choice for data sets of any size and for solutions with any number of support intervals.},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/UVBBZXRH/Wang and Taylor - 2013 - Efficient computation of nonparametric survival fu.pdf}
}

@article{wangSurveyBayesianDeep2020,
  title = {A {{Survey}} on {{Bayesian Deep Learning}}},
  author = {Wang, Hao and Yeung, Dit-Yan},
  year = {2020},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {53},
  number = {5},
  pages = {108:1--108:37},
  issn = {0360-0300},
  doi = {10.1145/3409383},
  urldate = {2024-02-14},
  abstract = {A comprehensive artificial intelligence system needs to not only perceive the environment with different ``senses'' (e.g., seeing and hearing) but also infer the world's conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks, such as visual object recognition and speech recognition, using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models.1 In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and, in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, and so on. We also discuss the relationship and differences between Bayesian deep learning and other related topics, such as Bayesian treatment of neural networks.},
  keywords = {Bayesian networks,Deep learning,generative models,probabilistic graphical models},
  file = {/home/gkonkamking/Zotero/storage/IK553UPY/Wang and Yeung - 2020 - A Survey on Bayesian Deep Learning.pdf}
}

@article{Warne2002,
  title = {Derivation of the {{Australian}} and {{New Zealand}} Water Quality Guidelines for Toxicants},
  author = {Warne, Michael St J},
  year = {2002},
  journal = {Australasian Journal of Ecotoxicology},
  volume = {7},
  number = {2000},
  pages = {123--136},
  keywords = {assessment factor,nosource,statistical distribution,toxicants,trigger values}
}

@article{Warne2008,
  title = {Noec and Loec Data Should No Longer Be Generated or Used},
  author = {Warne, Michael St J and Dam, Rick Van},
  year = {2008},
  volume = {14},
  pages = {1--5},
  isbn = {1323-3475},
  pmid = {665706831945866},
  keywords = {duplicate-citation-key,nosource}
}

@article{Warne2013,
  title = {Revisions to the Derivation of the {{Australian}} and {{New Zealand}} Guidelines for Toxicants in Fresh and Marine Waters},
  author = {Warne, M. St J and Batley, G. E. and Braga, O. and Chapman, J. C. and Fox, David R. and Hickey, C. W. and Stauber, J. L. and Van Dam, R.},
  year = {2014},
  month = jun,
  journal = {Environmental Science and Pollution Research},
  volume = {21},
  number = {1},
  eprint = {23797706},
  eprinttype = {pubmed},
  pages = {51--60},
  issn = {09441344},
  doi = {10.1007/s11356-013-1779-6},
  abstract = {The Australian and New Zealand Guidelines for Fresh and Marine Water Quality are a key document in the Australian National Water Quality Management Strategy. These guidelines released in 2000 are currently being reviewed and updated. The revision is being co-ordinated by the Australian Department of Sustainability, Environment, Water, Population and Communities, while technical matters are dealt with by a series of Working Groups. The revision will be evolutionary in nature reflecting the latest scientific developments and a range of stakeholder desires. Key changes will be: increasing the types and sources of data that can be used; working collaboratively with industry to permit the use of commercial-in-confidence data; increasing the minimum data requirements; including a measure of the uncertainty of the trigger value; improving the software used to calculate trigger values; increasing the rigour of site-specific trigger values; improving the method for assessing the reliability of the trigger values; and providing guidance of measures of toxicity and toxicological endpoints that may, in the near future, be appropriate for trigger value derivation. These changes will markedly improve the number and quality of the trigger values that can be derived and will increase end-users' ability to understand and implement the guidelines in a scientifically rigorous manner.},
  isbn = {1135601317796},
  pmid = {23797706},
  keywords = {Australia,Environmental quality standards,New Zealand,nosource,Species sensitivity distribution,Toxicity}
}

@article{warnePracticalGuidePseudomarginal2020,
  title = {A Practical Guide to Pseudo-Marginal Methods for Computational Inference in Systems Biology},
  author = {Warne, David J. and Baker, Ruth E. and Simpson, Matthew J.},
  year = {2020},
  month = jul,
  journal = {Journal of Theoretical Biology},
  volume = {496},
  pages = {110255},
  issn = {0022-5193},
  doi = {10.1016/j.jtbi.2020.110255},
  urldate = {2021-11-08},
  abstract = {For many stochastic models of interest in systems biology, such as those describing biochemical reaction networks, exact quantification of parameter uncertainty through statistical inference is intractable. Likelihood-free computational inference techniques enable parameter inference when the likelihood function for the model is intractable but the generation of many sample paths is feasible through stochastic simulation of the forward problem. The most common likelihood-free method in systems biology is approximate Bayesian computation that accepts parameters that result in low discrepancy between stochastic simulations and measured data. However, it can be difficult to assess how the accuracy of the resulting inferences are affected by the choice of acceptance threshold and discrepancy function. The pseudo-marginal approach is an alternative likelihood-free inference method that utilises a Monte Carlo estimate of the likelihood function. This approach has several advantages, particularly in the context of noisy, partially observed, time-course data typical in biochemical reaction network studies. Specifically, the pseudo-marginal approach facilitates exact inference and uncertainty quantification, and may be efficiently combined with particle filters for low variance, high-accuracy likelihood estimation. In this review, we provide a practical introduction to the pseudo-marginal approach using inference for biochemical reaction networks as a series of case studies. Implementations of key algorithms and examples are provided using the Julia programming language; a high performance, open source programming language for scientific computing (https://github.com/davidwarne/Warne2019\_GuideToPseudoMarginal).},
  langid = {english},
  keywords = {Bayesian inference,Biochemical reaction networks,Markov chain Monte Carlo,Pseudo-marginal methods,Stochastic differential equations},
  file = {/home/gkonkamking/Zotero/storage/G79YDC66/Warne et al. - 2020 - A practical guide to pseudo-marginal methods for c.pdf}
}

@book{Warren-Hicks1998,
  title = {Uncertainty Analysis in Ecological Risk Assessment},
  author = {{Warren-Hicks}, {\relax WJ} and Moore, {\relax DRJ}},
  year = {1998},
  number = {July},
  keywords = {duplicate-citation-key,nosource}
}

@incollection{warren2010application,
  title = {Application of Uncertainty Analysis to Ecological Risks of Pesticides},
  author = {{Warren-Hicks}, William J. and Hart, Andy},
  year = {2010},
  pages = {123--141},
  publisher = {CRC Press},
  chapter = {7},
  isbn = {978-1-4398-0734-7},
  keywords = {ISBN 9781439807347,nosource}
}

@article{Wasserman2000,
  title = {Asymptotic Inference for Mixture Models by Using Data-Dependent Priors},
  author = {Wasserman, L.},
  year = {2000},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {62},
  pages = {159--180},
  issn = {1369-7412},
  doi = {10.1111/1467-9868.00226},
  abstract = {For certain mixture models, improper priors are undesirable because they yield improper posteriors. However, proper priors may be undesirable because they require subjective input. We propose the use of specially chosen data-dependent priors. We show that, in some cases, data-dependent priors are the only priors that produce intervals with second-order correct frequentist coverage. The resulting posterior also has another interpretation: it is the product of a fixed prior and a pseudolikelihood.},
  isbn = {13697412},
  keywords = {coverage,mixtures,non-informative priors,nosource}
}

@article{wasserman2018topological,
  title = {Topological Data Analysis},
  author = {Wasserman, Larry},
  year = {2018},
  journal = {Annual Review of Statistics and Its Application},
  volume = {5},
  pages = {501--532},
  publisher = {Annual Reviews},
  doi = {10.1146/annurev-statistics-031017-100045},
  file = {/home/gkonkamking/Zotero/storage/FBH7TLYS/Wasserman - 2018 - Topological data analysis.pdf}
}

@incollection{Watson2005,
  title = {Ecosystem Services Provided by Birds},
  booktitle = {Annals of the New York Academy of Sciences},
  author = {Whelan, Christopher J. and Wenny, Daniel G. and Marquis, Robert J.},
  year = {2008},
  volume = {1134},
  pages = {25--60},
  publisher = {ISLAND PRESS},
  address = {Washington DC},
  issn = {00778923},
  doi = {10.1196/annals.1439.003},
  abstract = {Ecosystem services are natural processes that benefit humans. Birds contribute the four types of services recognized by the UN Millennium Ecosystem Assessment-provisioning, regulating, cultural, and supporting services. In this review, we concentrate primarily on supporting services, and to a lesser extent, provisioning and regulating services. As members of ecosystems, birds play many roles, including as predators, pollinators, scavengers, seed dispersers, seed predators, and ecosystem engineers. These ecosystem services fall into two subcategories: those that arise via behavior (like consumption of agricultural pests) and those that arise via bird products (like nests and guano). Characteristics of most birds make them quite special from the perspective of ecosystem services. Because most birds fly, they can respond to irruptive or pulsed resources in ways generally not possible for other vertebrates. Migratory species link ecosystem processes and fluxes that are separated by great distances and times. Although the economic value to humans contributed by most, if not all, of the supporting services has yet to be quantified, we believe they are important to humans. Our goals for this review are 1) to lay the groundwork on these services to facilitate future efforts to estimate their economic value, 2) to highlight gaps in our knowledge, and 3) to point to future directions for additional research.},
  isbn = {0077-8923},
  pmid = {18566089},
  keywords = {Birds,Ecosystem engineering,Ecosystem services,Excavation,Guano,Nests,nosource,Pest control,Pollination,Reciprocal nutrient fluxes,Scavenging,Seed dispersal}
}

@incollection{weisberg2005applied,
  title = {Applied Linear Regression},
  booktitle = {New York},
  author = {Weisberg, Sanford},
  year = {2005},
  volume = {528},
  publisher = {John Wiley {\textbackslash}\& Sons},
  issn = {00401706},
  doi = {10.2307/1269895},
  isbn = {0-07-553109-7},
  pmid = {250053125},
  keywords = {nosource}
}

@article{Wheeler2002,
  title = {Species Sensitivity Distributions: Data and Model Choice.},
  author = {Wheeler, J R and Grist, E P M and Leung, K M Y and Morritt, D and Crane, M},
  year = {2002},
  month = jan,
  journal = {Marine pollution bulletin},
  volume = {45},
  number = {1-12},
  eprint = {12398385},
  eprinttype = {pubmed},
  pages = {192--202},
  issn = {0025-326X},
  abstract = {Species sensitivity distributions (SSDs) are increasingly incorporated into ecological risk assessment procedures. Although these new techniques offer a more transparent approach to risk assessment they demand more and superior quality data. Issues of data quantity and quality are especially important for marine datasets that tend to be smaller (and have fewer standard test methods) when compared with freshwater data. An additional source of uncertainty when using SSDs is appropriate selection from the range of methods used in their construction. We show through examples the influence of data quantity, data quality, and choice of model. We then show how regulatory decisions may be affected by these factors.},
  isbn = {0025-326X},
  pmid = {12398385},
  keywords = {data quality,data quantity,Decision Making,duplicate-citation-key,ecological risk assessment,Ecology,model fit,Models,nosource,Quality Control,Risk Assessment,species sensitivity distribution,Theoretical,Water Pollutants,Water Pollutants: adverse effects}
}

@article{Wheeler2002a,
  title = {Freshwater to Saltwater Toxicity Extrapolation Using Species Sensitivity Distributions.},
  author = {Wheeler, James R and Leung, Kenneth M Y and Morritt, David and Sorokin, Neal and Rogers, Howard and Toy, Robin and Holt, Martin and Whitehouse, Paul and Crane, Mark},
  year = {2002},
  month = nov,
  journal = {Environmental toxicology and chemistry / SETAC},
  volume = {21},
  number = {11},
  eprint = {12389927},
  eprinttype = {pubmed},
  pages = {2459--67},
  issn = {0730-7268},
  abstract = {There is generally a lack of saltwater ecotoxicity data for risk assessment purposes, leaving an unknown margin of uncertainty in saltwater assessments that utilize surrogate freshwater data. Consequently, a need for sound scientific advice on the suitability of using freshwater data to extrapolate to saltwater effects exists. Here we use species sensitivity distributions to determine if freshwater datasets are adequately protective of saltwater species assemblages for 21 chemical substances. For ammonia and the metal compounds among these data, freshwater data were generally protective because freshwater organisms tended to be more sensitive. In contrast, for pesticide and narcotic compounds, saltwater species tended to be more sensitive and a suitable uncertainty factor would need to be applied to surrogate freshwater data. Biological and physicochemical factors contribute to such differences in freshwater and saltwater species sensitivities, but the species compositions of datasets used are also important.},
  isbn = {1552-8618},
  pmid = {12389927},
  keywords = {Animals,Chemical,Chemical: toxicity,duplicate-citation-key,Fresh Water,Fresh Water: chemistry,nosource,Predictive Value of Tests,saltwater data,Seawater,Seawater: chemistry,Species Specificity,Toxicity Tests,Water Pollutants}
}

@article{williamsKeepingOnesDistance2008,
  title = {Keeping {{One}}'s {{Distance}}: {{The Influence}} of {{Spatial Distance Cues}} on {{Affect}} and {{Evaluation}}},
  shorttitle = {Keeping {{One}}'s {{Distance}}},
  author = {Williams, Lawrence E. and Bargh, John A.},
  year = {2008},
  month = mar,
  journal = {Psychological Science},
  publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
  issn = {1467-9280},
  urldate = {2020-08-18},
  abstract = {Current conceptualizations of psychological distance (e.g., construal-level theory) refer to the degree of overlap between the self and some other person, place...},
  copyright = {{\copyright} 2008 Association for Psychological Science},
  langid = {english},
  keywords = {nosource}
}

@inproceedings{williamson2010dependent,
  title = {Dependent Indian Buffet Processes},
  booktitle = {Proc. {{International}} Conference on Artificial Intelligence and Statistics},
  author = {Williamson, Sinead and Orbanz, Peter and Ghahramani, Zoubin},
  year = {2010},
  organization = {Citeseer},
  keywords = {duplicate-citation-key,nosource}
}

@inproceedings{williamson2010dependent,
  title = {Dependent Indian Bu ↵ et Processes},
  booktitle = {Proc. {{International}} Conference on Artificial Intelligence and Statistics},
  author = {Williamson, Sinead and Orbanz, Peter},
  year = {2010},
  volume = {6},
  organization = {Citeseer},
  keywords = {duplicate-citation-key,nosource}
}

@article{Wilson1967,
  title = {A Statistical Theory of Spatial Distribution Models},
  author = {Wilson, Alan},
  year = {1967},
  journal = {Transportation research},
  volume = {1},
  number = {3},
  pages = {253--269},
  issn = {0041-1647},
  doi = {10.1016/0041-1647(67)90035-4},
  keywords = {nosource}
}

@misc{wilsonCaseBayesianDeep2020,
  title = {The {{Case}} for {{Bayesian Deep Learning}}},
  author = {Wilson, Andrew Gordon},
  year = {2020},
  month = jan,
  number = {arXiv:2001.10995},
  eprint = {2001.10995},
  primaryclass = {cs, stat},
  institution = {arXiv},
  urldate = {2024-02-15},
  abstract = {The key distinguishing property of a Bayesian approach is marginalization instead of optimization, not the prior, or Bayes rule. Bayesian inference is especially compelling for deep neural networks. (1) Neural networks are typically underspecified by the data, and can represent many different but high performing models corresponding to different settings of parameters, which is exactly when marginalization will make the biggest difference for both calibration and accuracy. (2) Deep ensembles have been mistaken as competing approaches to Bayesian methods, but can be seen as approximate Bayesian marginalization. (3) The structure of neural networks gives rise to a structured prior in function space, which reflects the inductive biases of neural networks that help them generalize. (4) The observed correlation between parameters in flat regions of the loss and a diversity of solutions that provide good generalization is further conducive to Bayesian marginalization, as flat regions occupy a large volume in a high dimensional space, and each different solution will make a good contribution to a Bayesian model average. (5) Recent practical advances for Bayesian deep learning provide improvements in accuracy and calibration compared to standard training, while retaining scalability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/gkonkamking/Zotero/storage/3YVSR5UQ/Wilson - 2020 - The Case for Bayesian Deep Learning.pdf}
}

@article{wolpert,
  title = {Poisson/Gamma Random Field Models for Spatial Statistics},
  author = {Wolpert, Robert L and Ickstadt, Katja},
  year = {1998},
  journal = {Biometrika},
  volume = {85},
  number = {2},
  pages = {251--267},
  publisher = {Biometrika Trust},
  keywords = {nosource},
  file = {/home/gkonkamking/pCloudDrive/papers/Wolpert_Ickstadt_1998_Poisson-gamma random field models for spatial statistics.pdf}
}

@misc{wolpertLectureNotesStationary2021,
  title = {Lecture {{Notes}} on {{Stationary Gamma Processes}}},
  author = {Wolpert, Robert L.},
  year = {2021},
  month = may,
  number = {arXiv:2106.00087},
  eprint = {2106.00087},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.00087},
  urldate = {2023-10-29},
  abstract = {For each \${\textbackslash}lambda{$>$}0\$ and every square-integrable infinitely-divisible (ID) distribution there exists at least one stationary stochastic process \$t{\textbackslash}mapsto X\_t\$ with the specified distribution for \$X\_1\$ and with first-order autoregressive (AR(1)) structure in the sense that the autocorrelation of \$X\_s\$ and \$X\_t\$ is \${\textbackslash}exp(-{\textbackslash}lambda{\textbar}s-t{\textbar})\$ for all indices \$s,t\$. For the special case of the standard Normal distribution, the process \$X\_t\$ is unique -- namely, the first-order autoregressive Ornstein-Uhlenbeck velocity process. The process \$X\_t\$ is also uniquely determined if \$X\_1\$ is accorded the unit rate Poisson distribution. For the Gamma distribution, however, \$X\_t\$ is {\textbackslash}emph\{not\} determined uniquely. In these lecture notes we describe six distinct processes with the same univariate marginal distributions and AR(1) autocorrelation function. We explore a few of their properties and describe methods of simulating their sample paths.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability},
  file = {/home/gkonkamking/pCloudDrive/papers/Wolpert_2021_Lecture Notes on Stationary Gamma Processes.pdf}
}

@article{wonham1964some,
  title = {Some Applications of Stochastic Differential Equations to Optimal Nonlinear Filtering},
  author = {Wonham, W Murray},
  year = {1964},
  journal = {Journal of the Society for Industrial and Applied Mathematics, Series A: Control},
  volume = {2},
  number = {3},
  pages = {347--369},
  publisher = {SIAM},
  keywords = {nosource}
}

@article{woskie2021early,
  title = {Early Social Distancing Policies in {{Europe}}, Changes in Mobility \& {{COVID-19}} Case Trajectories: {{Insights}} from {{Spring}} 2020},
  author = {Woskie, Liana R and Hennessy, Jonathan and Espinosa, Valeria and Tsai, Thomas C and Vispute, Swapnil and Jacobson, Benjamin H and Cattuto, Ciro and Gauvin, Laetitia and Tizzoni, Michele and Fabrikant, Alex and others},
  year = {2021},
  journal = {Plos one},
  volume = {16},
  number = {6},
  pages = {e0253071},
  publisher = {Public Library of Science San Francisco, CA USA},
  doi = {10.1371/journal.pone.0253071},
  keywords = {nosource}
}

@article{wu2008kullback,
  title = {Kullback {{Leibler}} Property of Kernel Mixture Priors in {{Bayesian}} Density Estimation},
  author = {Wu, Yuefeng and Ghosal, Subhashis},
  year = {2008},
  journal = {Electronic Journal of Statistics},
  volume = {2},
  eprint = {0710.2746v2},
  pages = {298--331},
  publisher = {Institute of Mathematical Statistics},
  issn = {1935-7524},
  doi = {10.1214/07-EJS130},
  abstract = {Positivity of the prior probability of Kullback-Leibler neighborhood around the true density, commonly known as the Kullback-Leibler property, plays a fundamental role in posterior consistency. A popular prior for Bayesian estimation is given by a Dirichlet mixture, where the kernels are chosen depending on the sample space and the class of densities to be estimated. The Kullback-Leibler property of the Dirichlet mixture prior has been shown for some special kernels like the normal density or Bernstein polynomial, under appropriate conditions. In this paper, we obtain easily verifiable sufficient conditions, under which a prior obtained by mixing a general kernel possesses the Kullback-Leibler property. We study a wide variety of kernel used in practice, including the normal, \$t\$, histogram, gamma, Weibull densities and so on, and show that the Kullback-Leibler property holds if some easily verifiable conditions are satisfied at the true density. This gives a catalog of conditions required for the Kullback-Leibler property, which can be readily used in applications.{\textbackslash}n{\textbackslash}nPublished in: Electronic Journal of Statistics 2008, Vol. 2, 298-331},
  archiveprefix = {arXiv},
  arxivid = {arXiv:0710.2746v2},
  keywords = {nosource}
}

@article{Wu2010,
  title = {China Embarking on Development of Its Own National Water Quality Criteria System},
  author = {Wu, Fengchang and Meng, Wei and Zhao, Xiaoli and Li, Huixian and Zhang, Ruiqing and Cao, Yujing and Liao, Haiqing},
  year = {2010},
  journal = {Environmental science \& technology},
  volume = {44},
  number = {21},
  pages = {7992--7993},
  issn = {1520-5851},
  doi = {10.1021/es1029365},
  isbn = {0013-936X},
  pmid = {20857904},
  keywords = {nosource}
}

@article{Wu2016,
  title = {Comparison of Species Sensitivity Distributions Constructed with Predicted Acute Toxicity Data from Interspecies Correlation Estimation Models and Measured Acute Data for {{Benzo}}[a]Pyrene.},
  author = {Wu, Jiangyue and Yan, Zhenguang and Yi, Xianliang and Lin, Yufei and Ni, Jianbin and Gao, Xiang and Liu, Zhengtao and Shi, Xiaoyong},
  year = {2016},
  month = feb,
  journal = {Chemosphere},
  volume = {144},
  pages = {2183--8},
  issn = {1879-1298},
  doi = {10.1016/j.chemosphere.2015.10.099},
  abstract = {Benzo[a]pyrene (BaP) is a priority Polycyclic Aromatic Hydrocarbon (PAH), which is toxic to aquatic organisms and has been widely detected in the environment. However, ecological risk assessment for BaP is hard to perform because of the absence of water quality criteria (WQC) and lack of toxicity data for this chemical. To fill in the data gaps, a interspecies correlation estimation (ICE) model was developed by USEPA to predict toxicity values for multiple species from the toxicity estimate for one species. In order to validate the applicability of the ICE model for BaP, measured-based-species sensitivity distributions (SSDs) generated using eight Chinese native aquatic species were compared with ICE-based-SSDs generated using the data predicted from three surrogate species (Lepomis macrochirus, Cyprinus carpio and Daphnia magna). The results showed that there were no significant differences between the two SSD curves and the two hazardous concentrations for the 5\% of species (HC5) derived from measured acute toxicity data and ICE-based predicted data. The ICE model was verified as a valid approach for generating SSDs with limited toxicity data.},
  pmid = {26595312},
  keywords = {Benzo[a]pyrene,Interspecies correlation estimation,Native species,nosource,Species sensitivity distribution,Water quality criteria}
}

@article{Xie2008,
  title = {A Recursive Method for Structural Learning of Directed Acyclic Graphs},
  author = {Xie, Xianchao},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  pages = {459--483},
  issn = {15324435},
  abstract = {In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is first decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efficiency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method.},
  isbn = {1532-4435},
  keywords = {bayesian network,conditional independence,decomposition,directed acyclic graph,nosource}
}

@article{Xing2014,
  title = {A Comparison of Statistical Methods for Deriving Freshwater Quality Criteria for the Protection of Aquatic Organisms},
  author = {Xing, Liqun and Liu, Hongling and Zhang, Xiaowei and Hecker, Markus and Giesy, John P. and Yu, Hongxia},
  year = {2014},
  journal = {Environmental Science and Pollution Research},
  volume = {21},
  number = {1},
  pages = {159--167},
  issn = {09441344},
  doi = {10.1007/s11356-013-1462-y},
  abstract = {Species sensitivity distributions (SSDs) are increasingly used in both ecological risk assessment and derivation of water quality criteria. However, there has been debate about the choice of an appropriate approach for derivation of water quality criteria based on SSDs because the various methods can generate different values. The objective of this study was to compare the differences among various methods. Data sets of acute toxicities of 12 substances to aquatic organisms, representing a range of classes with different modes of action, were studied. Nine typical statistical approaches, including parametric and nonparametric methods, were used to construct SSDs for 12 chemicals. Water quality criteria, expressed as hazardous concentration for 5 \% of species (HC(5)), were derived by use of several approaches. All approaches produced comparable results, and the data generated by the different approaches were significantly correlated. Variability among estimates of HC(5) of all inclusive species decreased with increasing sample size, and variability was similar among the statistical methods applied. Of the statistical methods selected, the bootstrap method represented the best-fitting model for all chemicals, while log-triangle and Weibull were the best models among the parametric methods evaluated. The bootstrap method was the primary choice to derive water quality criteria when data points are sufficient (more than 20). If the available data are few, all other methods should be constructed, and that which best describes the distribution of the data was selected.},
  pmid = {23314707},
  keywords = {Hazardous concentration for 5  of species (HC5),nosource,Species sensitivity distribution (SSD),Statistical methods,Water quality criteria}
}

@inproceedings{xingBayesianMultipopulationHaplotype2006,
  title = {Bayesian Multi-Population Haplotype Inference via a Hierarchical Dirichlet Process Mixture},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  author = {Xing, Eric P. and Sohn, Kyung-Ah and Jordan, Michael I. and Teh, Yee-Whye},
  year = {2006},
  pages = {1049--1056},
  publisher = {ACM Press},
  address = {Pittsburgh, Pennsylvania},
  doi = {10.1145/1143844.1143976},
  urldate = {2022-07-18},
  abstract = {Uncovering the haplotypes of single nucleotide polymorphisms and their population demography is essential for many biological and medical applications. Methods for haplotype inference developed thus far---including methods based on coalescence, finite and infinite mixtures, and maximal parsimony---ignore the underlying population structure in the genotype data. As noted by Pritchard (2001), different populations can share certain portion of their genetic ancestors, as well as have their own genetic components through migration and diversification. In this paper, we address the problem of multipopulation haplotype inference. We capture cross-population structure using a nonparametric Bayesian prior known as the hierarchical Dirichlet process (HDP) (Teh et al., 2006), conjoining this prior with a recently developed Bayesian methodology for haplotype phasing known as DP-Haplotyper (Xing et al., 2004). We also develop an efficient sampling algorithm for the HDP based on a two-level nested Po{\textasciiacute}lya urn scheme. We show that our model outperforms extant algorithms on both simulated and real biological data.},
  isbn = {978-1-59593-383-6},
  langid = {english},
  file = {/home/gkonkamking/pCloudDrive/papers/Xing et al_2006_Bayesian multi-population haplotype inference via a hierarchical dirichlet.pdf}
}

@article{xu2013block,
  title = {A Block Coordinate Descent Method for Regularized Multiconvex Optimization with Applications to Nonnegative Tensor Factorization and Completion},
  author = {Xu, Yangyang and Yin, Wotao},
  year = {2013},
  journal = {SIAM Journal on imaging sciences},
  volume = {6},
  number = {3},
  pages = {1758--1789},
  publisher = {SIAM},
  keywords = {nosource}
}

@article{Xu2015,
  title = {Key Issues for the Development and Application of the Species Sensitivity Distribution ({{SSD}}) Model for Ecological Risk Assessment},
  author = {Xu, Fu-Liu and Li, Yi-Long and Wang, Yin and He, Wei and Kong, Xiang-Zhen and Qin, Ning and Liu, Wen-Xiu and Wu, Wen-Jing and Jorgensen, Sven Erik},
  year = {2015},
  journal = {Ecological Indicators},
  volume = {54},
  pages = {227--237},
  publisher = {Elsevier Ltd},
  issn = {1470160X},
  doi = {10.1016/j.ecolind.2015.02.001},
  abstract = {The species sensitivity distribution (SSD) model is one of the most commonly used methods for ecological risk assessment based on the potentially affected fraction (PAF) of and the combined PAF (msPAF) as quantitative indicators. There are usually four steps for the development of SSD models and their applications: (1) obtain the toxicity data of the pollutants; (2) fit the SSD curves; (3) calculate the potentially affected fractions (PAFs) of the individual pollutants for the ecological risk assessment of an individual pollutant; and (4) calculate the accumulated multi-substance potentially affected fractions (msPAFs) for the joint ecological risk assessment of multiple pollutants. Among the above mentioned four steps, the first two steps are paramount. In the present study, the following six key issues are discussed: (1) how to select the appropriate species, (2) how to preprocess the toxicity data collected from the ecotoxicity database, (3) how to transform the acute toxicity data into chronic data, (4) how to best fit the toxicity data, (5) how to calculate the msPAF of multiple pollutants, and (6) how to determine the uncertainty of the SSD model''. In response to these questions, several principles were proposed to select appropriate species; three data processing methods, including the geometric mean, weight assigning and using all raw data without processing, were compared to determine the appropriate method for the DDT (dichloro diphenyl trichloroethane) toxicity data preprocessing. The method of acute to chronic ratio (ACR) and binary correlation analysis were contrasted using the zinc toxicity data for the transformation of the acute toxicity data into chronic data. The Burr III, Loglogistic and Lognormal models were compared to determine the best fit model using the DDT toxicity data for invertebrates. The concentration addition or response addition were discussed to calculate msPAF according to the toxic model of action (TMoA). The uncertainties of the SSD models for five heavy metals and for eight polycyclic aromatic hydrocarbons (PAHs) were performed. The comparison of the coefficients of variation (CVs) for the toxicity data and exposure levels in Lake Chaohu for eight polycyclic aromatic hydrocarbons (PAHs) were also presented to demonstrate the uncertainties of the ecological risks assessed by the SSD model based on 5000 Monte Carlo simulations.},
  keywords = {Best fit model,Data preprocessing,Ecological risk assessment,nosource,Species selection,Species sensitivity distribution model,Uncertainty analysis}
}

@article{xuBayesianVariableSelection2015,
  title = {Bayesian {{Variable Selection}} and {{Estimation}} for {{Group Lasso}}},
  author = {Xu, Xiaofan and Ghosh, Malay},
  year = {2015},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {10},
  number = {4},
  pages = {909--936},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/14-BA929},
  urldate = {2020-04-10},
  abstract = {The paper revisits the Bayesian group lasso and uses spike and slab priors for group variable selection. In the process, the connection of our model with penalized regression is demonstrated, and the role of posterior median for thresholding is pointed out. We show that the posterior median estimator has the oracle property for group variable selection and estimation under orthogonal designs, while the group lasso has suboptimal asymptotic estimation rate when variable selection consistency is achieved. Next we consider bi-level selection problem and propose the Bayesian sparse group selection again with spike and slab priors to select variables both at the group level and also within a group. We demonstrate via simulation that the posterior median estimator of our spike and slab models has excellent performance for both variable selection and estimation.},
  langid = {english},
  mrnumber = {MR3432244},
  zmnumber = {1334.62132},
  keywords = {Gibbs sampling,group variable selection,median thresholding,spike and slab prior},
  file = {/home/gkonkamking/Zotero/storage/IQ79ZG9D/Xu and Ghosh - 2015 - Bayesian Variable Selection and Estimation for Gro.pdf}
}

@article{xuMADBayesTumor2015,
  title = {{{MAD Bayes}} for {{Tumor Heterogeneity}}---{{Feature Allocation With Exponential Family Sampling}}},
  author = {Xu, Yanxun and M{\"u}ller, Peter and Yuan, Yuan and Gulukota, Kamalakar and Ji, Yuan},
  year = {2015},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {110},
  number = {510},
  pages = {503--514},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2014.995794},
  urldate = {2021-05-04},
  abstract = {We propose small-variance asymptotic approximations for inference on tumor heterogeneity (TH) using next-generation sequencing data. Understanding TH is an important and open research problem in biology. The lack of appropriate statistical inference is a critical gap in existing methods that the proposed approach aims to fill. We build on a hierarchical model with an exponential family likelihood and a feature allocation prior. The proposed implementation of posterior inference generalizes similar small-variance approximations proposed by Kulis and Jordan and Broderick, Kulis, and Jordan for inference with Dirichlet process mixture and Indian buffet process prior models under normal sampling. We show that the new algorithm can successfully recover latent structures of different haplotypes and subclones and is magnitudes faster than available Markov chain Monte Carlo samplers. The latter are practically infeasible for high-dimensional genomics data. The proposed approach is scalable, easy to implement, and benefits from the flexibility of Bayesian nonparametric models. More importantly, it provides a useful tool for applied scientists to estimate cell subtypes in tumor samples. R code is available on http://www.ma.utexas.edu/users/yxu/. Supplementary materials for this article are available online.},
  pmid = {26170513},
  keywords = {Bayesian nonparametric,Bregman divergence,Feature allocation,Indian buffet process,Next-generation sequencing,Tumor heterogeneity},
  file = {/home/gkonkamking/Zotero/storage/RFUT6FW7/Xu et al. - 2015 - MAD Bayes for Tumor Heterogeneity—Feature Allocati.pdf}
}

@article{yangComputationalComplexityHighdimensional2016,
  title = {On the Computational Complexity of High-Dimensional {{Bayesian}} Variable Selection},
  author = {Yang, Yun and Wainwright, Martin J. and Jordan, Michael I.},
  year = {2016},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {44},
  number = {6},
  issn = {0090-5364},
  doi = {10.1214/15-AOS1417},
  urldate = {2022-06-03},
  langid = {english},
  file = {/home/gkonkamking/Zotero/storage/LN8UAEZT/Yang et al. - 2016 - On the computational complexity of high-dimensiona.pdf}
}

@misc{yangPosteriorDistributionNumber2020,
  title = {Posterior {{Distribution}} for the {{Number}} of {{Clusters}} in {{Dirichlet Process Mixture Models}}},
  author = {Yang, Chiao-Yu and Xia, Eric and Ho, Nhat and Jordan, Michael I.},
  year = {2020},
  month = oct,
  number = {arXiv:1905.09959},
  eprint = {1905.09959},
  primaryclass = {cs, math, stat},
  institution = {arXiv},
  doi = {10.48550/arXiv.1905.09959},
  urldate = {2022-05-16},
  abstract = {Dirichlet process mixture models (DPMM) play a central role in Bayesian nonparametrics, with applications throughout statistics and machine learning. DPMMs are generally used in clustering problems where the number of clusters is not known in advance, and the posterior distribution is treated as providing inference for this number. Recently, however, it has been shown that the DPMM is inconsistent in inferring the true number of components in certain cases. This is an asymptotic result, and it would be desirable to understand whether it holds with finite samples, and to more fully understand the full posterior. In this work, we provide a rigorous study for the posterior distribution of the number of clusters in DPMM under different prior distributions on the parameters and constraints on the distributions of the data. We provide novel lower bounds on the ratios of probabilities between \$s+1\$ clusters and \$s\$ clusters when the prior distributions on parameters are chosen to be Gaussian or uniform distributions.},
  archiveprefix = {arXiv},
  keywords = {{62C10, 62G20, 62G99},Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/gkonkamking/Zotero/storage/HMNANHBZ/Yang et al. - 2020 - Posterior Distribution for the Number of Clusters .pdf;/home/gkonkamking/Zotero/storage/MZM7CIE4/Yang et al. - 2020 - Posterior Distribution for the Number of Clusters .pdf}
}

@article{yatesSubclonalDiversificationPrimary2015,
  title = {Subclonal Diversification of Primary Breast Cancer Revealed by Multiregion Sequencing},
  author = {Yates, Lucy R. and Gerstung, Moritz and Knappskog, Stian and Desmedt, Christine and Gundem, Gunes and Van Loo, Peter and Aas, Turid and Alexandrov, Ludmil B. and Larsimont, Denis and Davies, Helen and Li, Yilong and Ju, Young Seok and Ramakrishna, Manasa and Haugland, Hans Kristian and Lilleng, Peer Kaare and {Nik-Zainal}, Serena and McLaren, Stuart and Butler, Adam and Martin, Sancha and Glodzik, Dominic and Menzies, Andrew and Raine, Keiran and Hinton, Jonathan and Jones, David and Mudie, Laura J. and Jiang, Bing and Vincent, Delphine and {Greene-Colozzi}, April and Adnet, Pierre-Yves and Fatima, Aquila and Maetens, Marion and Ignatiadis, Michail and Stratton, Michael R. and Sotiriou, Christos and Richardson, Andrea L. and L{\o}nning, Per Eystein and Wedge, David C. and Campbell, Peter J.},
  year = {2015},
  month = jul,
  journal = {Nature Medicine},
  volume = {21},
  number = {7},
  pages = {751--759},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/nm.3886},
  urldate = {2021-05-02},
  abstract = {Whole-genome and targeted sequencing of multiple sections of each of 50 tumors reveals varying degrees of subclonal diversification and genomic heterogeneity.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english}
}

@article{yau2011bayesian,
  title = {Bayesian Non-Parametric Hidden {{Markov}} Models with Applications in Genomics},
  author = {Yau, Christopher and Papaspiliopoulos, Omiros and Roberts, Gareth O. and Holmes, Christopher},
  year = {2011},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {1},
  pages = {37--57},
  publisher = {Wiley Online Library},
  issn = {13697412},
  doi = {10.1111/j.1467-9868.2010.00756.x},
  abstract = {We consider the development of Bayesian Nonparametric methods for product partition models such as Hidden Markov Models and change point models. Our approach uses a Mixture of Dirichlet Process (MDP) model for the unknown sampling distribution (likelihood) for the observations arising in each state and a computationally efficient data augmentation scheme to aid inference. The method uses novel MCMC methodology which combines recent retrospective sampling methods with the use of slice sampler variables. The methodology is computationally efficient, both in terms of MCMC mixing properties, and robustness to the length of the time series being investigated. Moreover, the method is easy to implement requiring little or no user-interaction. We apply our methodology to the analysis of genomic copy number variation.},
  pmid = {21687778},
  keywords = {Block Gibbs sampler,Copy number variation,Local and global clustering,nosource,Partial exchangeability,Partition models,Retrospective sampling}
}

@article{ye2010compositional,
  title = {Compositional Adjustment of {{Dirichlet}} Mixture Priors.},
  author = {Ye, Xugang and Yu, Yi-Kuo and Altschul, Stephen F},
  year = {2010},
  journal = {Journal of computational biology : a journal of computational molecular cell biology},
  volume = {17},
  number = {12},
  pages = {1607--20},
  publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
  issn = {1557-8666},
  doi = {10.1089/cmb.2010.0117},
  abstract = {Dirichlet mixture priors provide a Bayesian formalism for scoring alignments of protein profiles to individual sequences, which can be generalized to constructing scores for multiple-alignment columns. A Dirichlet mixture is a probability distribution over multinomial space, each of whose components can be thought of as modeling a type of protein position. Applied to the simplest case of pairwise sequence alignment, a Dirichlet mixture is equivalent to an implied symmetric substitution matrix. For alphabets of even size L, Dirichlet mixtures with L/2 components and symmetric substitution matrices have an identical number of free parameters. Although this suggests the possibility of a one-to-one mapping between the two formalisms, we show that there are some symmetric matrices no Dirichlet mixture can imply, and others implied by many distinct Dirichlet mixtures. Dirichlet mixtures are derived empirically from curated sets of multiple alignments. They imply "background" amino acid frequencies characteristic of these sets, and should thus be non-optimal for comparing proteins with non-standard composition. Given a mixture {$\Theta$}, we seek an adjusted {$\Theta$}' that implies the desired composition, but that minimizes an appropriate relative-entropy-based distance function. To render the problem tractable, we fix the mixture parameter as well as the sum of the Dirichlet parameters for each component, allowing only its center of mass to vary. This linearizes the constraints on the remaining parameters. An approach to finding {$\Theta$}' may be based on small consecutive parameter adjustments. The relative entropy of two Dirichlet distributions separated by a small change in their parameter values implies a quadratic cost function for such changes. For a small change in implied background frequencies, this function can be minimized using the Lagrange-Newton method. We have implemented this method, and can compositionally adjust to good precision a 20-component Dirichlet mixture prior for proteins in under half a second on a standard workstation.},
  isbn = {1557-8666 (Electronic){\textbackslash}n1066-5277 (Linking)},
  pmid = {21128852},
  keywords = {Algorithms,Bayes Theorem,nosource,Proteins,Proteins: chemistry,Sequence Alignment,Sequence Alignment: methods}
}

@article{ye2011inference,
  title = {On the Inference of Dirichlet Mixture Priors for Protein Sequence Comparison.},
  author = {Ye, Xugang and Yu, Yi-Kuo and Altschul, Stephen F},
  year = {2011},
  journal = {Journal of computational biology : a journal of computational molecular cell biology},
  volume = {18},
  number = {8},
  pages = {941--54},
  publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
  issn = {1557-8666},
  doi = {10.1089/cmb.2011.0040},
  abstract = {Dirichlet mixtures provide an elegant formalism for constructing and evaluating protein multiple sequence alignments. Their use requires the inference of Dirichlet mixture priors from curated sets of accurately aligned sequences. This article addresses two questions relevant to such inference: of how many components should a Dirichlet mixture consist, and how may a maximum-likelihood mixture be derived from a given data set. To apply the Minimum Description Length principle to the first question, we extend an analytic formula for the complexity of a Dirichlet model to Dirichlet mixtures by informal argument. We apply a Gibbs-sampling based approach to the second question. Using artificial data generated by a Dirichlet mixture, we demonstrate that our methods are able to approximate well the true theory, when it exists. We apply our methods as well to real data, and infer Dirichlet mixtures that describe the data better than does a mixture derived using previous approaches.},
  isbn = {1557-8666 (Electronic){\textbackslash}n1066-5277 (Linking)},
  pmid = {21702690},
  keywords = {Algorithms,Amino Acid Motifs,Computational Biology,Computational Biology: methods,Computational Biology: statistics \& numerical data,Models,nosource,Probability,Proteins,Proteins: analysis,Sequence Alignment,Sequence Alignment: methods,Sequence Alignment: statistics \& numerical data,Statistical}
}

@article{year,
  title = {Derivation of Marine Water Quality Criteria for Metals Based on a Novel {{QICAR-SSD}} Model},
  author = {Chen, Cheng and Mu, Yunsong and Wu, Fengchang and Zhang, Ruiqing and Su, Hailei and Giesy, John P.},
  year = {2015},
  journal = {Environmental Science and Pollution Research},
  volume = {22},
  number = {6},
  pages = {4297--4304},
  issn = {16147499},
  doi = {10.1007/s11356-014-3655-4},
  abstract = {Establishment of water quality criteria (WQC) is one procedure for protection of marine organisms and their ecosystems. This study, which integrated two separate ap-proaches, quantitative ion character--activity relationships (QICARs) and species sensitivity distributions (SSDs), devel-oped a novel QICAR-SSD model. The QICARs predict rela-tive potencies of individual elements while SSDs integrate relative sensitivities among organisms. The QICAR-SSD ap-proach was applied to derive saltwater WQC for 34 metals or metalloids. Relationships between physicochemical proper-ties of metal ions and their corresponding potencies for acute toxicity to eight selected marine species were determined. The softness index ({$\sigma$}p) exhibited the strongest correlation with the acute toxicity of metals (r 2 {$>$}0.66, F{$>$}5.88, P{$<$}0.94{\texttimes}10 -2). Predictive criteria maximum concentrations for the eight metals, derived by applying the SSD approach to values predicted by use of QICARs, were within the same order of magnitude as values recommended by the US EPA (2009). In general, the results support that the QICAR-SSD approach is a rapid method to estimate WQC for metals for which little or no information is available for marine organisms.},
  keywords = {Marine water quality criteria,Metals or metalloids,nosource,Quantitative ion character???activity relationship,Species sensitivity distribution,The softness index}
}

@article{yi2018volatility,
  title = {Volatility Connectedness in the Cryptocurrency Market: {{Is Bitcoin}} a Dominant Cryptocurrency?},
  author = {Yi, Shuyue and Xu, Zishuang and Wang, Gang-Jin},
  year = {2018},
  journal = {International Review of Financial Analysis},
  volume = {60},
  pages = {98--114},
  publisher = {Elsevier},
  keywords = {nosource}
}

@article{Zafirakou-Koulouris1998,
  title = {L Moment Diagrams for Censored Observations},
  author = {{Zafirakou-Koulouris}, Antigoni and Vogel, Richard M. and Craig, Scott M. and Habermeier, Joerg},
  year = {1998},
  month = may,
  journal = {Water Resources Research},
  volume = {34},
  number = {5},
  pages = {1241},
  issn = {0043-1397},
  doi = {10.1029/97WR03712},
  abstract = {Observed data sets containing values above or below the analytical threshold of measuring equipment are referred to as censored. .....Mst of the previous literature on the statistical analysis of censored data relates to the problems of moment, parameter, and quantile estimation methods. Such estimation methods usually assume an underlying probability distribution. Few goodness-of-fit methods exist for censored data. We introduce L moment diagrams for the evalution of the goodness of fit of alternative distrubutional hypotheses for left-censored data. Experiments with artificial censored data sets document the conditions under which L moment idagrams should be useful. Our approach, like Hoskings (1995) approach for right censoring, derived L moment diagrams for left-censored observations from partial probability-weighted mements.},
  keywords = {doi:10.1029/9,http://dx.doi.org/10.1029/97WR03712,nosource}
}

@article{zagheni2015demographic,
  title = {Demographic Research with Non-Representative Internet Data},
  author = {Zagheni, Emilio and Weber, Ingmar},
  year = {2015},
  journal = {International Journal of Manpower},
  volume = {36},
  number = {1},
  pages = {13--25},
  publisher = {Emerald Group Publishing Limited},
  doi = {10.1108/IJM-12-2014-0261}
}

@article{Zajdlik2009,
  title = {Estimating Water Quality Guidelines for Environmental Contaminants Using Multimodal Species Sensitivity Distributions: {{A}} Case Study with Atrazine},
  author = {Zajdlik, BA A and Dixon, DG G and Stephenson, G},
  year = {2009},
  journal = {Human and Ecological Risk Assessment},
  volume = {15},
  number = {3},
  pages = {554--564},
  keywords = {nosource}
}

@article{zanellaScalableImportanceTempering2019,
  title = {Scalable {{Importance Tempering}} and {{Bayesian Variable Selection}}},
  author = {Zanella, Giacomo and Roberts, Gareth},
  year = {2019},
  month = jul,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {81},
  number = {3},
  pages = {489--517},
  issn = {1369-7412},
  doi = {10.1111/rssb.12316},
  urldate = {2024-06-27},
  abstract = {We propose a Monte Carlo algorithm to sample from high dimensional probability distributions that combines Markov chain Monte Carlo and importance sampling. We provide a careful theoretical analysis, including guarantees on robustness to high dimensionality, explicit comparison with standard Markov chain Monte Carlo methods and illustrations of the potential improvements in efficiency. Simple and concrete intuition is provided for when the novel scheme is expected to outperform standard schemes. When applied to Bayesian variable-selection problems, the novel algorithm is orders of magnitude more efficient than available alternative sampling schemes and enables fast and reliable fully Bayesian inferences with tens of thousand regressors.},
  file = {/home/gkonkamking/pCloudDrive/papers/Zanella_Roberts_2019_Scalable Importance Tempering and Bayesian Variable Selection.pdf}
}

@article{zavalHowWarmDays2014,
  title = {How Warm Days Increase Belief in Global Warming},
  author = {Zaval, Lisa and Keenan, Elizabeth A. and Johnson, Eric J. and Weber, Elke U.},
  year = {2014},
  month = feb,
  journal = {Nature Climate Change},
  volume = {4},
  number = {2},
  pages = {143--147},
  publisher = {Nature Publishing Group},
  issn = {1758-6798},
  doi = {10.1038/nclimate2093},
  urldate = {2020-07-08},
  abstract = {Climate change judgements can depend on whether today seems warmer or colder than usual, termed the local warming effect. Although previous research has demonstrated that this effect occurs, studies have yet to explain why or how temperature abnormalities influence global warming attitudes. A better understanding of the underlying psychology of this effect can help explain the public's reaction to climate change and inform approaches used to communicate the phenomenon. Across five studies, we find evidence of attribute substitution, whereby individuals use less relevant but available information (for example, today's temperature) in place of more diagnostic but less accessible information (for example, global climate change patterns) when making judgements. Moreover, we rule out alternative hypotheses involving climate change labelling and lay mental models. Ultimately, we show that present temperature abnormalities are given undue weight and lead to an overestimation of the frequency of similar past events, thereby increasing belief in and concern for global warming.},
  copyright = {2014 Nature Publishing Group},
  langid = {english},
  keywords = {nosource}
}

@incollection{zellnerarnoldAssessingPriorDistributions1986,
  title = {On Assessing Prior Distributions and {{Bayesian}} Regression Analysis with G-Prior Distributions.},
  booktitle = {{{BAYESIAN INFERENCE AND DECISION TECHNIQUES}}: {{Essays}} in {{Honor}} of {{Bruno}} de {{Finetti}}},
  author = {Zellner, Arnold},
  year = {1986},
  series = {{{STUDIES IN BAYESIAN ECONOMETRICS AND STATISTICS}}},
  volume = {6},
  pages = {233--246},
  publisher = {ELSEVIER SCIENCE PUBLISHERS B.V.},
  address = {North-Holland, Amsterdam},
  isbn = {0 444 87712 6},
  file = {/home/gkonkamking/pCloudDrive/papers/Zellner, Arnold_On assessing prior distributions and Bayesian regression analysis with g-prior.pdf}
}

@techreport{ZH15,
  title = {Matrix Depot: {{An}} Extensible Test Matrix Collection for Julia},
  booktitle = {{{MIMS EPrint}}},
  author = {Zhang, Weijian and Higham, Nicholas J},
  year = {2015},
  number = {2015.118},
  pages = {25},
  institution = {Manchester Institute for Mathematical Sciences, The University of Manchester},
  abstract = {Matrix Depot is a Julia software package that provides easy access to a large and diverse collection of test matrices. Its novelty is threefold. First, it is extensible by the user, and so can be adapted to include the user's own test problems. In doing so it facilitates experimentation and makes it easier to carry out reproducible research. Second, it amalgamates in a single framework three different types of matrix collections, comprising parametrized test matrices, regularization test problems, and real-life sparse matrix data. Third, it fully exploits the Julia language. It uses multiple dispatch to help provide a simple interface and, in particular, to allow matrices to be generated in any of the numeric data types supported by the language.},
  keywords = {nosource}
}

@article{Zhang2013,
  title = {Did Advances in Global Surveillance and Notification Systems Make a Difference in the 2009 {{H1N1 Pandemic}}?-{{A}} Retrospective Analysis},
  author = {Zhang, Ying and {Lopez-Gatell}, Hugo and {Alpuche-Aranda}, Celia M. and Stoto, Michael A.},
  editor = {Guan, Yi},
  year = {2013},
  month = apr,
  journal = {PLoS ONE},
  volume = {8},
  number = {4},
  pages = {e59893},
  publisher = {Public Library of Science},
  issn = {19326203},
  doi = {10.1371/journal.pone.0059893},
  abstract = {BACKGROUND: The 2009 H1N1 outbreak provides an opportunity to identify strengths and weaknesses of disease surveillance and notification systems that have been implemented in the past decade.{\textbackslash}n{\textbackslash}nMETHODS: Drawing on a systematic review of the scientific literature, official documents, websites, and news reports, we constructed a timeline differentiating three kinds of events: (1) the emergence and spread of the pH1N1 virus, (2) local health officials' awareness and understanding of the outbreak, and (3) notifications about the events and their implications. We then conducted a "critical event" analysis of the surveillance process to ascertain when health officials became aware of the epidemiologic facts of the unfolding pandemic and whether advances in surveillance notification systems hastened detection.{\textbackslash}n{\textbackslash}nRESULTS: This analysis revealed three critical events. First, medical personnel identified pH1N1in California children because of an experimental surveillance program, leading to a novel viral strain being identified by CDC. Second, Mexican officials recognized that unconnected outbreaks represented a single phenomenon. Finally, the identification of a pH1N1 outbreak in a New York City high school was hastened by awareness of the emerging pandemic. Analysis of the timeline suggests that at best the global response could have been about one week earlier (which would not have stopped spread to other countries), and could have been much later.{\textbackslash}n{\textbackslash}nCONCLUSIONS: This analysis shows that investments in global surveillance and notification systems made an important difference in the 2009 H1N1 pandemic. In particular, enhanced laboratory capacity in the U.S. and Canada led to earlier detection and characterization of the 2009 H1N1. This includes enhanced capacity at the federal, state, and local levels in the U.S., as well as a trilateral agreement enabling collaboration among U.S., Canada, and Mexico. In addition, improved global notification systems contributed by helping health officials understand the relevance and importance of their own information.},
  isbn = {1932-6203 (Electronic){\textbackslash}r1932-6203 (Linking)},
  pmid = {23573217},
  keywords = {Disease informatics,Disease mapping,Epidemiological methods,Epidemiology,Global health,Health care policy,Health risk analysis,Infectious disease control,Infectious disease epidemiology,Infectious Diseases,Influenza,Medicine,Non-Clinical Medicine,nosource,Public health,Public Health and Epidemiology,Research Article,Social epidemiology,Viral diseases}
}

@article{Zhang2017,
  title = {Regression Models for Multivariate Count Data},
  author = {Zhang, Yiwen and Zhou, Hua and Zhou, Jin and Sun, Wei},
  year = {2017},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {1},
  pages = {1--13},
  publisher = {Taylor \& Francis},
  issn = {15372715},
  doi = {10.1080/10618600.2016.1154063},
  abstract = {Data with multivariate count responses frequently occur in modern applications. The commonly used multinomial-logit model is limiting due to its restrictive mean-variance structure. For instance, analyzing count data from the recent RNA-seq technology by the multinomial-logit model leads to serious errors in hypothesis testing. The ubiquity of over-dispersion and complicated correlation structures among multivariate counts calls for more flexible regression models. In this article, we study some generalized linear models that incorporate various correlation structures among the counts. Current literature lacks a treatment of these models, partly due to the fact that they do not belong to the natural exponential family. We study the estimation, testing, and variable selection for these models in a unifying framework. The regression models are compared on both synthetic and real RNA-seq data.},
  pmid = {28348500},
  keywords = {Analysis of deviance,Categorical data analysis,Dirichlet-multinomial,Generalized Dirichlet-multinomial,Iteratively reweighted Poisson regression (IRPR),Negative multinomial,Reduced rank GLM,Regularization},
  file = {/home/gkonkamking/Zotero/storage/JXLASY5L/Zhang et al. - 2017 - Regression models for multivariate count data.pdf}
}

@article{zhangEnhancingHiCData2018,
  title = {Enhancing {{Hi-C}} Data Resolution with Deep Convolutional Neural Network {{HiCPlus}}},
  author = {Zhang, Yan and An, Lin and Xu, Jie and Zhang, Bo and Zheng, W. Jim and Hu, Ming and Tang, Jijun and Yue, Feng},
  year = {2018},
  month = feb,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {750},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-03113-2},
  urldate = {2024-10-22},
  abstract = {Although Hi-C technology is one of the most popular tools for studying 3D genome organization, due to sequencing cost, the resolution of most Hi-C datasets are coarse and cannot be used to link distal regulatory elements to their target genes. Here we develop HiCPlus, a computational approach based on deep convolutional neural network, to infer high-resolution Hi-C interaction matrices from low-resolution Hi-C data. We demonstrate that HiCPlus can impute interaction matrices highly similar to the original ones, while only using 1/16 of the original sequencing reads. We show that~the models learned from one cell type can be applied to make predictions in other cell or tissue types. Our work not only provides a computational framework to enhance Hi-C data resolution but also reveals features underlying the formation of 3D chromatin interactions.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Computational models},
  file = {/home/gkonkamking/pCloudDrive/papers/Zhang et al. - 2018 - Enhancing Hi-C data resolution with deep convolutional neural network HiCPlus.pdf}
}

@article{Zhao:2000p98,
  title = {Bayesian Aspects of Some Nonparametric Problems},
  author = {Zhao, Linda H.},
  year = {2000},
  journal = {Annals of Statistics},
  volume = {28},
  number = {2},
  eprint = {2674040},
  eprinttype = {jstor},
  pages = {532--552},
  issn = {00905364},
  doi = {10.1214/aos/1016218229},
  abstract = {Ann. Statist. 2000, Vol. 28, No. 2, 532-552 BAYESIAN ASPECTS OF SOME NONPARAMETRIC PROBLEMS1 By H. University of Pennsylvania We study the Bayesian approach to nonparametric function estima- tion problems such as},
  keywords = {Bayes,Conjugate priors,Minimax,Nonparametric regression,nosource,White noise}
}

@phdthesis{zhao1993frequentist,
  title = {Frequentist and {{Bayesian}} Aspects of Some Nonparametric Estimation Problems},
  author = {Zhao, L},
  year = {1993},
  school = {Ph. D. thesis. Cornell University},
  keywords = {nosource}
}

@article{Zhao2009,
  title = {Nonparametric and Parametric Survival Analysis of Censored Data with Possible Violation of Method Assumptions},
  author = {Zhao, Guolin},
  year = {2008},
  pages = {57},
  abstract = {Estimating survival functions has interested statisticians for numerous years. A survival function gives information on the probability of a time-to-event of interest. Research in the area of survival analysis has increased greatly over the last several decades because of its large usage in areas related to biostatistics and the pharmaceu- tical industry. Among the methods which estimate the survival function, several are widely used and available in popular statistical software programs. One purpose of this research is to compare the efficiency between competing estimators of the survival function. Results are given for simulations which use nonparametric and parametric estimation methods on censored data. The simulated data sets have right-, left-, or interval-censored time points. Comparisons are done on various types of data to see which survival function estimation methods are more suitable. We consider scenarios where distributional assumptions or censoring type assumptions are violated. Another goal of this research is to examine the effects of these incorrect assumptions.},
  keywords = {nosource}
}

@article{Zhao2016,
  title = {Species Sensitivity Distribution for Chlorpyrifos to Aquatic Organisms: {{Model}} Choice and Sample Size.},
  author = {Zhao, Jinsong and Chen, Boyu},
  year = {2016},
  month = mar,
  journal = {Ecotoxicology and Environmental Safety},
  volume = {125},
  pages = {161--9},
  issn = {1090-2414},
  doi = {10.1016/j.ecoenv.2015.11.039},
  abstract = {Species sensitivity distribution (SSD) is a widely used model that extrapolates the ecological risk to ecosystem levels from the ecotoxicity of a chemical to individual organisms. However, model choice and sample size significantly affect the development of the SSD model and the estimation of hazardous concentrations at the 5th centile (HC5). To interpret their effects, the SSD model for chlorpyrifos, a widely used organophosphate pesticide, to aquatic organisms is presented with emphases on model choice and sample size. Three subsets of median effective concentration (EC50) with different sample sizes were obtained from ECOTOX and used to build SSD models based on parametric distribution (normal, logistic, and triangle distribution) and nonparametric bootstrap. The SSD models based on the triangle distribution are superior to the normal and logistic distributions according to several goodness-of-fit techniques. Among all parametric SSD models, the one with the largest sample size based on the triangle distribution gives the most strict HC5 with 0.141{$\mu$}molL(-1). The HC5 derived from the nonparametric bootstrap is 0.159{$\mu$}mol L(-1). The minimum sample size required to build a stable SSD model is 11 based on parametric distribution and 23 based on nonparametric bootstrap. The study suggests that model choice and sample size are important sources of uncertainty for application of the SSD model.},
  pmid = {26701839},
  keywords = {Aquatic organism,Model choice,nosource,Organophosphate pesticide,Parametric and nonparametric bootstrap,Sample size,Species sensitivity distribution}
}

@article{Zhao2017,
  title = {Species Sensitivity Distribution for Pentachlorophenol to Aquatic Organisms Based on Interval Ecotoxicological Data},
  author = {Zhao, Jinsong and Zhang, Run},
  year = {2017},
  journal = {Ecotoxicology and Environmental Safety},
  volume = {145},
  number = {July},
  pages = {193--199},
  publisher = {Elsevier Inc.},
  issn = {01476513},
  doi = {10.1016/j.ecoenv.2017.07.029},
  keywords = {Aquatic organisms,Bayesian statistics,interval ecotoxicological data,Interval ecotoxicological data,nosource,Pentachlorophenol,species sensitivity distribution,Species sensitivity distribution}
}

@article{Zhou201411,
  title = {Can We Predict Temperature-Dependent Chemical Toxicity to Marine Organisms and Set Appropriate Water Quality Guidelines for Protecting Marine Ecosystems under Different Thermal Scenarios?},
  author = {Zhou, Guang Jie and Wang, Zhen and Lau, Edward Tak Chuen and Xu, Xiang Rong and Leung, Kenneth Mei Yee},
  year = {2014},
  journal = {Marine Pollution Bulletin},
  volume = {87},
  number = {1},
  pages = {11--21},
  issn = {18793363},
  doi = {10.1016/j.marpolbul.2014.08.003},
  abstract = {Temperature changes due to climate change and seasonal fluctuation can have profound implications on chemical toxicity to marine organisms. Through a comprehensive meta-analysis by comparing median lethal or effect concentration data of six chemicals for various saltwater species obtained at different temperatures, we reveal that the chemical toxicity generally follows two different models: (1) it increases with increasing temperature and (2) it is the lowest at an optimal temperature and increases with increasing or decreasing temperature from the optimal temperature. Such observations are further supported by temperature-dependent hazardous concentration 10\% (HC{$<$}inf{$>$}10{$<$}/inf{$>$}) values derived from species sensitivity distributions which are constructed using the acute toxicity data generated at different temperatures. Considering these two models and natural variations of seawater temperature, we can scientifically assess whether applying an assessment factor (e.g. 10) to modify water quality guidelines of the chemicals can adequately protect marine ecosystems in tropics, subtropics and temperate regions, respectively.},
  pmid = {25176278},
  keywords = {Assessment factor,Environmental risk assessment,nosource,Species sensitivity distribution,Temperature,Toxicity}
}

@article{zhouPairCloneBayesianSubclone2019,
  title = {{{PairClone}}: A {{Bayesian}} Subclone Caller Based on Mutation Pairs},
  shorttitle = {{{PairClone}}},
  author = {Zhou, Tianjian and M{\"u}ller, Peter and Sengupta, Subhajit and Ji, Yuan},
  year = {2019},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {68},
  number = {3},
  pages = {705--725},
  issn = {1467-9876},
  doi = {10.1111/rssc.12328},
  urldate = {2021-09-30},
  abstract = {Tumour cell populations can be thought of as a composition of heterogeneous cell subpopulations, with each subpopulation being characterized by overlapping sets of single-nucleotide variants. Such subpopulations are known as subclones and are an important target for precision medicine. Reconstructing subclones from next generation sequencing data is one of the major challenges in computational biology. We present PairClone as a new tool to implement this reconstruction. The main idea of PairClone is to model short reads mapped to pairs of proximal single-nucleotide variants, which we refer to as mutation pairs. In contrast, other existing methods use only marginal reads for unpaired single-nucleotide variants. Using Bayesian non-parametric models, we estimate posterior probabilities of the number, genotypes and population frequencies of subclones in one or more tumour sample. We use the categorical Indian buffet process as a prior probability model for subclones. Column vectors of categorical matrices record the corresponding sets of mutation pairs for subclones. The performance of PairClone is assessed by using simulated and real data sets with a comparison with existing methods. An open-source software package can be obtained from http://www.compgenome.org/pairclone.},
  langid = {english},
  keywords = {Categorical Indian buffet process,Latent feature model,Local haplotype,Next generation sequencing,Subclone,Tumour heterogeneity},
  file = {/home/gkonkamking/pCloudDrive/papers/Zhou et al_2019_PairClone.pdf}
}

@article{zhouTreeCloneReconstructionTumor2019,
  title = {{{TreeClone}}: {{Reconstruction}} of Tumor Subclone Phylogeny Based on Mutation Pairs Using next Generation Sequencing Data},
  shorttitle = {{{TreeClone}}},
  author = {Zhou, Tianjian and Sengupta, Subhajit and M{\"u}ller, Peter and Ji, Yuan},
  year = {2019},
  month = jun,
  journal = {The Annals of Applied Statistics},
  volume = {13},
  number = {2},
  pages = {874--899},
  publisher = {Institute of Mathematical Statistics},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/18-AOAS1224},
  urldate = {2021-05-04},
  abstract = {We present TreeClone, a latent feature allocation model to reconstruct tumor subclones subject to phylogenetic evolution that mimics tumor evolution. Similar to most current methods, we consider data from next-generation sequencing of tumor DNA. Unlike most methods that use information in short reads mapped to single nucleotide variants (SNVs), we consider subclone phylogeny reconstruction using pairs of two proximal SNVs that can be mapped by the same short reads. As part of the Bayesian inference model, we construct a phylogenetic tree prior. The use of the tree structure in the prior greatly strengthens inference. Only subclones that can be explained by a phylogenetic tree are assigned non-negligible probabilities. The proposed Bayesian framework implies posterior distributions on the number of subclones, their genotypes, cellular proportions and the phylogenetic tree spanned by the inferred subclones. The proposed method is validated against different sets of simulated and real-world data using single and multiple tumor samples. An open source software package is available at http://www.compgenome.org/treeclone.},
  keywords = {latent feature model,mutation pair,NGS data,phylogenetic tree,subclone,tumor heterogeneity},
  file = {/home/gkonkamking/Zotero/storage/KJWQULPL/Zhou et al. - 2019 - TreeClone Reconstruction of tumor subclone phylog.pdf}
}

@article{Zolezzi2005,
  title = {Probabilistic Ecological Risk Assessment of 1,2,4-Trichlorobenzene at a Former Industrial Contaminated Site.},
  author = {Zolezzi, Marcello and Cattaneo, Claudia and Tarazona, Jos{\'e} V},
  year = {2005},
  month = may,
  journal = {Environmental science \& technology},
  volume = {39},
  number = {9},
  eprint = {15926534},
  eprinttype = {pubmed},
  pages = {2920--6},
  issn = {0013-936X},
  abstract = {Measured concentrations of 1,2,4-trichlorobenzene (1,2,4-TCB) in soil and groundwater detected in an industrial contaminated site were used to test several probabilistic options for refining site-specific ecological risks assessment, ranging from comparison of single effects and exposure values through comparison of probabilistic distributions for exposure and effects to the use of distribution based quotients (DBQs) obtained through Monte Carlo simulations. The results of the deterministic approach, which suggest that risk exceeds a level of concern for soil organisms, were influenced mainly by the presence of hot spots reaching concentrations able to affect acutely a large proportion of species, while the large majority of the area presents 1,2,4-TCB concentrations below those reported as toxic. Ground-(pore)water concentrations were compared with aquatic ecotoxicity data in orderto obtain an estimation of the potential risk for aquifers and streams in the adjacent area as well as for soil-dwelling organisms exposed via pore water. In this case, the risk is distributed over a large proportion of the site, while the local risk of hot spots was low, showing that risk characterization based exclusively on soil concentrations might be insufficient.},
  isbn = {0013-936X (Print){\textbackslash}r0013-936X (Linking)},
  pmid = {15926534},
  keywords = {Animals,Chemical,Chemical: poisoning,Chlorobenzenes,Chlorobenzenes: analysis,Chlorobenzenes: poisoning,duplicate-citation-key,Hazardous Waste,Models,Monte Carlo Method,nosource,Risk Assessment,Soil Pollutants,Soil Pollutants: poisoning,Statistical,Water Pollutants}
}

@article{Zwart2005,
  title = {Impact of Toxicants on Species Composition of Aquatic Communities: Concordance of Predictions and Field Observations},
  author = {Zwart, Dick De},
  year = {2005},
  keywords = {duplicate-citation-key,nosource}
}
